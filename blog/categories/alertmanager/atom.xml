<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Alertmanager | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/alertmanager/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2021-07-31T04:13:02-04:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[How to Setup Alerting With Loki]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/11/06/how-to-setup-alerting-with-loki/"/>
    <updated>2020-11-06T15:13:53+00:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/11/06/how-to-setup-alerting-with-loki</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/98380823-bd948880-2051-11eb-8ab4-c8d5f5d3e612.png" alt="image" /></p>

<p>Recently Grafana Labs announced <strong><a href="https://grafana.com/blog/2020/10/28/loki-2.0-released-transform-logs-as-youre-querying-them-and-set-up-alerts-within-loki/">Loki v2</a></strong> and its awesome! Definitely check out their blog post on more details.</p>

<p>Loki has a index option called <strong>boltdb-shipper</strong>, which allows you to run Loki with only a object store and you <strong>no longer need a dedicated index store</strong> such as DynamoDB. You can extract labels from log lines at query time, which is CRAZY! And I really like how they&rsquo;ve implemented it, you can parse, filter and format like mad. I really like that.</p>

<p>And then generating alerts from any query, which we will go into today. Definitely check out <a href="https://grafana.com/blog/2020/10/28/loki-2.0-released-transform-logs-as-youre-querying-them-and-set-up-alerts-within-loki/">this blogpost</a> and <a href="https://grafana.com/blog/2020/11/04/video-top-three-features-of-the-new-loki-2.0/">this video</a> for more details on the features of Loki v2.</p>

<h2>What will we be doing today</h2>

<p>In this tutorial we will setup a alert using the Loki local ruler to alert us when we have <strong>high number of log events coming in</strong>. For example, let&rsquo;s say someone has debug logging enabled in their application and we want to send a alert to slack when it breaches the threshold.</p>

<p>I will simulate this with a <code>http-client</code> container which runs <code>curl</code> in a while loop to fire a bunch of http requests against the nginx container which logs to Loki, so we can see how the alerting works, and in this scenario we will alert to Slack.</p>

<p>And after that we will stop our http-client container to see how the alarm resolves when the log rate comes down again.</p>

<p>All the components are available in the <code>docker-compose.yml</code> on my <a href="https://github.com/ruanbekker/loki-alerts-docker">github repository</a></p>

<h2>Components</h2>

<p>Let&rsquo;s break it down and start with the loki config:</p>

<pre><code>...
ruler:
  storage:
    type: local
    local:
      directory: /etc/loki/rules
  rule_path: /tmp/loki/rules-temp
  alertmanager_url: http://alertmanager:9093
  ring:
    kvstore:
      store: inmemory
  enable_api: true
  enable_alertmanager_v2: true
</code></pre>

<p>In the section of the loki config, I will be making use of the local ruler and map my alert rules under <code>/etc/loki/rules/</code> and we are also defining our alertmanager instance where these alerts should be shipped to.</p>

<p>In my rule definition <code>/etc/loki/rules/demo/rules.yml</code>:</p>

<pre><code>groups:
  - name: rate-alerting
    rules:
      - alert: HighLogRate
        expr: |
          sum by (compose_service)
            (rate({job="dockerlogs"}[1m]))
            &gt; 60
        for: 1m
        labels:
            severity: warning
            team: devops
            category: logs
        annotations:
            title: "High LogRate Alert"
            description: "something is logging a lot"
            impact: "impact"
            action: "action"
            dashboard: "https://grafana.com/service-dashboard"
            runbook: "https://wiki.com"
            logurl: "https://grafana.com/log-explorer"
</code></pre>

<p>In my expression, I am using LogQL to return per second rate of all my docker logs within the last minute per compose service for my dockerlogs job and we are specifying that it should alert when the threshold is above 60.</p>

<p>As you can see I have a couple of <strong>labels and annotations</strong>, which becomes <strong>very useful</strong> when you have dashboard links, runbooks etc and you would like to map that to your alert. I am doing the mapping in my <code>alertmanager.yml</code> config:</p>

<pre><code>route:
...
  receiver: 'default-catchall-slack'
  routes:
  - match:
      severity: warning
    receiver: warning-devops-slack
    routes:
    - match_re:
        team: .*(devops).*
      receiver: warning-devops-slack

receivers:
...
- name: 'warning-devops-slack'
  slack_configs:
    - send_resolved: true
      channel: '__SLACK_CHANNEL__'
      title: ':fire::white_check_mark: []  '
      text: &gt;-
        
          *Description:* 
          *Severity:* ``
          *Graph:* &lt;|:chart_with_upwards_trend:&gt;&lt;|:chart_with_upwards_trend:&gt; *Dashboard:* &lt;|:bar_chart:&gt; *Runbook:* &lt;|:spiral_note_pad:&gt;
          *Details:*
           - *:* ``
          
           - *Impact*: 
           - *Receiver*: warning--slack
        
</code></pre>

<p>As you can see, when my alert matches nothing it will go to my catchall receiver, but when my label contains <code>devops</code> and the route the alert to my <code>warning-devops-slack</code> receiver, and then we will be parsing our labels and annotations to include the values in our alarm on slack.</p>

<h2>Demo</h2>

<p>Enough with the background details, and it&rsquo;s time to get into the action.</p>

<p>All the code for this demonstration will be available in my github repository: <strong><a href="https://github.com/ruanbekker/loki-alerts-docker">github.com/ruanbekker/loki-alerts-docker</a></strong></p>

<p>The docker-compose will have a container of <strong>grafana</strong>, <strong>alertmanager</strong>, <strong>loki</strong>, <strong>nginx</strong> and a <strong>http-client</strong>.</p>

<p>The http-client is curl in a while loop that will just make a bunch of http requests against the nginx container, which will be logging to loki.</p>

<h2>Get the source</h2>

<p>Get the code from my github repository:</p>

<pre><code>$ git clone https://github.com/ruanbekker/loki-alerts-docker
$ cd loki-alerts-docker
</code></pre>

<p>You will need to replace the slack webhook url and the slack channel where you want your alerts to be sent to. This will take the environment variables and replace the values in <code>config/alertmanager.yml</code> (always check out the script first, before executing it)</p>

<pre><code>$ SLACK_WEBHOOK_URL="https://hooks.slack.com/services/xx/xx/xx" SLACK_CHANNEL="#notifications" ./parse_configs.sh
</code></pre>

<p>You can double check by running <code>cat config/alertmanager.yml</code>, once you are done, boot the stack:</p>

<pre><code>$ docker-compose up -d
</code></pre>

<p>Open up grafana:</p>

<pre><code>$ open http://grafana.localdns.xyz:3000
</code></pre>

<p>Use the initial user and password combination <code>admin/admin</code> and then reset your password:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98379039-7efdce80-204f-11eb-9c8a-3ed12a63cb14.png" alt="image" /></p>

<p>Browse for your labels on the log explorer section, in my example it will be <code>{job="dockerlogs"}</code>:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98379172-ace31300-204f-11eb-8e6c-3cf073afe771.png" alt="image" /></p>

<p>When we select our job=&ldquo;dockerlogs&rdquo; label, we will see our logs:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98379288-c71cf100-204f-11eb-911c-043a983bae6d.png" alt="image" /></p>

<p>As I explained earlier the query that we will be running in our ruler, can be checked what the rate currently is:</p>

<pre><code>sum by (compose_project, compose_service) (rate({job="dockerlogs"}[1m]))
</code></pre>

<p>Which will look like this:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98379765-54604580-2050-11eb-9c90-5e0adf2bb586.png" alt="image" /></p>

<p>In the configured expression in our ruler config, we have set to alarm once the value goes above 60, we can validate this by running:</p>

<pre><code>sum by (compose_project, compose_service) (rate({job="dockerlogs"}[1m])) &gt; 60
</code></pre>

<p>And we can verify that this is the case, and by now it should be alarming:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98379900-84a7e400-2050-11eb-87d0-ae52617d195e.png" alt="image" /></p>

<p>Head over to alertmanager:</p>

<pre><code>$ open http://alertmanager.localdns.xyz:9093
</code></pre>

<p>We can see alertmanager is showing the alarm:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98380013-af923800-2050-11eb-8585-f7489bf722cb.png" alt="image" /></p>

<p>When we head over to slack, we can see our notification:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98380158-de101300-2050-11eb-8d73-20828124fab5.png" alt="image" /></p>

<p>So let&rsquo;s stop our http client:</p>

<pre><code>$ docker-compose stop http-client
Stopping http-client ... done
</code></pre>

<p>Then we can see the logging stopped:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98380907-e0bf3800-2051-11eb-99c3-b3b9ac22bba5.png" alt="image" /></p>

<p>And in slack, we should see that the alarm recovered and we should see the notification:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98381360-722eaa00-2052-11eb-8bb4-07cdc8ffa7ee.png" alt="image" /></p>

<p>Then you can terminate your stack:</p>

<pre><code>$ docker-compose down
</code></pre>

<p>Pretty epic stuff right? I really love how cost effective Loki is as logging use to be so expensive to run and especially maintain, Grafana Labs are really doing some epic work and my hat goes off to them.</p>

<h2>Thanks</h2>

<p>I hope you found this useful, feel free to reach out to me on Twitter <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong> or visit me on my website <strong><a href="https://ruan.dev">ruan.dev</a></strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Prometheus Series of Tutorials for Your Guide to Epic Metrics]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/05/17/prometheus-series-of-tutorials-for-your-guide-to-epic-metrics/"/>
    <updated>2019-05-17T14:24:40-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/05/17/prometheus-series-of-tutorials-for-your-guide-to-epic-metrics</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/57307750-696bb980-70e5-11e9-9b0b-73ad88bde6a3.png" alt="prometheus" /></p>

<p>This is a curated list of tutorials of prometheus, from installing prometheus, installing grafana, exporters, docker versions of the prometheus / grafana / node exporter stack, etc.</p>

<h2>The List</h2>

<ul>
<li>Install <a href="http://blog.ruanbekker.com/blog/2019/05/07/setup-prometheus-and-node-exporter-on-ubuntu-for-epic-monitoring/">Prometheus</a></li>
<li>Install <a href="http://blog.ruanbekker.com/blog/2019/05/07/setup-prometheus-and-node-exporter-on-ubuntu-for-epic-monitoring/">Node Exporter</a></li>
<li>Install <a href="http://blog.ruanbekker.com/blog/2019/05/17/install-pushgateway-to-expose-metrics-to-prometheus/">Pushgateway</a></li>
<li>Install <a href="http://blog.ruanbekker.com/blog/2019/05/17/install-grafana-to-visualize-your-metrics-from-datasources-such-as-prometheus-on-linux/">Grafana</a></li>
<li>Install <a href="http://blog.ruanbekker.com/blog/2019/05/17/install-alertmanager-to-alert-based-on-metrics-from-prometheus/">Alertmananger</a></li>
<li>Install <a href="http://blog.ruanbekker.com/blog/2019/05/17/install-blackbox-exporter-to-monitor-websites-with-prometheus/">Blackbox Exporter</a></li>
<li>Install <a href="">Docker Prometheus Grafana Stack</a></li>
</ul>


<p>This list will be updated as I publish more tutorials</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Install Alertmanager to Alert Based on Metrics From Prometheus]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/05/17/install-alertmanager-to-alert-based-on-metrics-from-prometheus/"/>
    <updated>2019-05-17T12:49:26-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/05/17/install-alertmanager-to-alert-based-on-metrics-from-prometheus</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/57307750-696bb980-70e5-11e9-9b0b-73ad88bde6a3.png" alt="prometheus" /></p>

<p>So we are pushing our time series metrics into prometheus, and now we would like to alarm based on certain metric dimensions. That&rsquo;s where alertmanager fits in. We can setup targets and rules, once rules for our targets does not match, we can alarm to destinations suchs as slack, email etc.</p>

<p><a href="https://bekkerclothing.com/collections/developer?utm_source=blog.ruanbekker.com&utm_medium=blog&utm_campaign=leaderboard_ad" target="_blank"><img alt="bekker-clothing-developer-tshirts" src="https://user-images.githubusercontent.com/567298/70170981-7c278a80-16d6-11ea-9759-6621d02c1423.png"></a></p>

<h2>What we will be doing:</h2>

<p>In our previous tutorial we installed blackbox exporter to probe a endpoint. Now we will install Alertmanager and configure an alert to notify us via email and slack when our endpoint goes down. See <a href="https://blog.ruanbekker.com/blog/2019/05/17/install-blackbox-exporter-to-monitor-websites-with-prometheus/">this post</a> if you have not seen the previous tutorial.</p>

<h2>Install Alertmanager</h2>

<p>Create the user for alertmanager:</p>

<pre><code>$ useradd --no-create-home --shell /bin/false alertmanager
</code></pre>

<p>Download alertmanager and extract:</p>

<pre><code>$ https://github.com/prometheus/alertmanager/releases/download/v0.17.0/alertmanager-0.17.0.linux-amd64.tar.gz
$ tar -xvf alertmanager-0.17.0.linux-amd64.tar.gz
</code></pre>

<p>Move alertmanager and amtool birnaries in place:</p>

<pre><code>$ cp alertmanager-0.17.0.linux-amd64/alertmanager /usr/local/bin/
$ cp alertmanager-0.17.0.linux-amd64/amtool /usr/local/bin/
</code></pre>

<p>Ensure that the correct permissions are in place:</p>

<pre><code>$ chown alertmanager:alertmanager /usr/local/bin/alertmanager
$ chown alertmanager:alertmanager /usr/local/bin/amtool
</code></pre>

<p>Cleanup:</p>

<pre><code>$ rm -rf alertmanager-0.17.0*
</code></pre>

<h2>Configure Alertmanager:</h2>

<p>Create the alertmanager directory and configure the global alertmanager configuration:</p>

<pre><code>$ mkdir /etc/alertmanager
$ vim /etc/alertmanager/alertmanager.yml
</code></pre>

<p>Provide the global config and ensure to populate your personal information. See <a href="https://blog.ruanbekker.com/blog/2019/04/18/setup-a-slack-webhook-for-sending-messages-from-applications/">this post</a> to create a slack webhook.</p>

<pre><code>global:
  smtp_smarthost: 'smtp.domain.net:587'
  smtp_from: 'AlertManager &lt;mailer@domain.com&gt;'
  smtp_require_tls: true
  smtp_hello: 'alertmanager'
  smtp_auth_username: 'username'
  smtp_auth_password: 'password'

  slack_api_url: 'https://hooks.slack.com/services/x/xx/xxx'

route:
  group_by: ['instance', 'alert']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 3h
  receiver: team-1

receivers:
  - name: 'team-1'
    email_configs:
      - to: 'user@domain.com'
    slack_configs:
      # https://prometheus.io/docs/alerting/configuration/#slack_config
      - channel: 'system_events'
      - username: 'AlertManager'
      - icon_emoji: ':joy:'
</code></pre>

<p>Ensure the permissions are in place:</p>

<pre><code>$ chown alertmanager:alertmanager -R /etc/alertmanager
</code></pre>

<p>Create the alertmanager systemd unit file:</p>

<pre><code>$ vim /etc/systemd/system/alertmanager.service
</code></pre>

<p>And supply the unit file configuration. Note that I am exposing port <code>9093</code> directly as Im not using a reverse proxy.</p>

<pre><code>[Unit]
Description=Alertmanager
Wants=network-online.target
After=network-online.target

[Service]
User=alertmanager
Group=alertmanager
Type=simple
WorkingDirectory=/etc/alertmanager/
ExecStart=/usr/local/bin/alertmanager --config.file=/etc/alertmanager/alertmanager.yml --web.external-url http://0.0.0.0:9093

[Install]
WantedBy=multi-user.target
</code></pre>

<p>Now we need to inform prometheus that we will send alerts to alertmanager to it&rsquo;s exposed port:</p>

<pre><code>$ vim /etc/prometheus/prometheus.yml
</code></pre>

<p>And supply the alertmanager configuration for prometheus:</p>

<pre><code>...
alerting:
  alertmanagers:
  - static_configs:
    - targets:
      - localhost:9093
...
</code></pre>

<p>So when we get alerted, our alert will include a link to our alert. We need to provide the base url of that alert. That get&rsquo;s done in our alertmanager systemd unit file: <code>/etc/systemd/system/alertmanager.service</code> under <code>--web.external-url</code> passing the alertmanager base ip address:</p>

<pre><code>[Unit]
Description=Alertmanager
Wants=network-online.target
After=network-online.target

[Service]
User=alertmanager
Group=alertmanager
Type=simple
WorkingDirectory=/etc/alertmanager/
ExecStart=/usr/local/bin/alertmanager --config.file=/etc/alertmanager/alertmanager.yml --web.external-url http://&lt;your.alertmanager.ip.address&gt;:9093

[Install]
WantedBy=multi-user.target
</code></pre>

<p>Then we need to do the same with the prometheus systemd unit file: <code>/etc/systemd/system/prometheus.service</code> under <code>--web.external-url</code> passing the prometheus base ip address:</p>

<pre><code>[Unit]
Description=Prometheus
Wants=network-online.target
After=network-online.target

[Service]
User=prometheus
Group=prometheus
Type=simple
ExecStart=/usr/local/bin/prometheus \
    --config.file /etc/prometheus/prometheus.yml \
    --storage.tsdb.path /var/lib/prometheus/ \
    --web.console.templates=/etc/prometheus/consoles \
    --web.console.libraries=/etc/prometheus/console_libraries \
    --web.external-url http://&lt;your.prometheus.ip.address&gt;

[Install]
WantedBy=multi-user.target
</code></pre>

<p>Since we have edited the systemd unit files, we need to reload the systemd daemon:</p>

<pre><code>$ systemctl daemon-reload
</code></pre>

<p>Then restart prometheus and alertmanager:</p>

<pre><code>$ systemctl restart prometheus
$ systemctl restart alertmanager
</code></pre>

<p>Inspect the status of alertmanager and prometheus:</p>

<pre><code>$ systemctl status alertmanager
$ systemctl status prometheus
</code></pre>

<p>If everything seems good, enable alertmanager on boot:</p>

<pre><code>$ systemctl enable alertmanager
</code></pre>

<h2>Access Alertmanager:</h2>

<p>Access alertmanager on your endpoint on port <code>9093</code>:</p>

<p><img src="https://user-images.githubusercontent.com/567298/57946361-69856980-78dc-11e9-8c48-ebcc3b0d201e.png" alt="alertmanager" /></p>

<p>From our previous tutorial we started a local web service on port <code>8080</code> that is being monitored by prometheus. Let&rsquo;s stop that service to test out the alerting. You should get a notification via email:</p>

<p><img src="https://user-images.githubusercontent.com/567298/57946586-f29ca080-78dc-11e9-983c-6b857ef21bae.png" alt="alertmanager" /></p>

<p>And the notification via slack:</p>

<p><img src="https://user-images.githubusercontent.com/567298/57946602-03e5ad00-78dd-11e9-9ecc-c3d58b2ad3ec.png" alt="alertmanager" /></p>

<p>When you start the service again and head over to the prometheus ui under alerts, you will see that the service recovered:</p>

<p><img src="https://user-images.githubusercontent.com/567298/57946647-2677c600-78dd-11e9-95a9-b9f4190172bf.png" alt="prometheus" /></p>

<h2>Install Prometheus Alertmanager Plugin</h2>

<p>Install the Prometheus Alertmanager Plugin in Grafana. Head to the instance where grafana is installed and install the plugin:</p>

<pre><code>$ grafana-cli plugins install camptocamp-prometheus-alertmanager-datasource
</code></pre>

<p>Once the plugin is installed, restart grafana:</p>

<pre><code>$ service grafana-server restart
</code></pre>

<p>Install the dasboard <a href="https://grafana.com/dashboards/8010">grafana.com/dashboards/8010</a>. Create a new datasource, select the prometheus-alertmanager datasource, configure and save.</p>

<p>Add a new dasboard, select import and provide the ID <code>8010</code>, select the prometheus-alertmanager datasource and save. You should see the following (more or less):</p>

<p><img src="https://user-images.githubusercontent.com/567298/57947092-3f34ab80-78de-11e9-904b-f42d5ecd7d0a.png" alt="prometheus-alertmanager" /></p>

<h2>Resources</h2>

<p>See all <a href="https://blog.ruanbekker.com/blog/categories/prometheus/">#prometheus</a> blogposts</p>
]]></content>
  </entry>
  
</feed>
