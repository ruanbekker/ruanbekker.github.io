<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Ansible | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/ansible/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2021-10-11T19:18:07-04:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A Tour With Vagrant and Virtualbox on Mac]]></title>
    <link href="https://blog.ruanbekker.com/blog/2021/08/14/a-tour-with-vagrant-and-virtualbox-on-mac/"/>
    <updated>2021-08-14T13:41:32-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2021/08/14/a-tour-with-vagrant-and-virtualbox-on-mac</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/58658188-37cec280-8320-11e9-90ca-1226b3ccb292.png" alt="vagrant" /></p>

<p><a href="https://www.vagrantup.com/">Vagrant</a>, yet another amazing product from <a href="https://www.hashicorp.com/">Hashicorp</a>.</p>

<p>Vagrant makes it really easy to provision virtual servers for local development (not limited to), which they refer as &ldquo;boxes&rdquo;, that enables developers to run their jobs/tasks/applications in a really easy and fast way. Vagrant utilizes a declarative configuration model, so you can describe which OS you want, bootstrap them with installation instructions as soon as it boots, etc.</p>

<h2>What are we doing today?</h2>

<p>When completing this tutorial, you will have Vagrant and Virtualbox installed on your Mac and should be able to launch a Ubuntu Virtual Server locally with Vagrant and using the Virtualbox provider which will be responsible for running our VM&rsquo;s.</p>

<p>We will also look at different configuration options to configure the VM, bootstrapping software, using the shell, docker and ansible provisioner.</p>

<p>For this demonstration, I am using a Mac OSX, but you can run this on Mac, Windows or Linux. First we will use Homebrew to install Virtualbox, then Vagrant, then we will provision a Ubuntu box and I will also show how to inject shell commands into your Vagrantfile so that you can provision software to your VM, and also forward traffic to a web server from the host to the guest.</p>

<p>If you are looking for a Linux version instead of mac, you can look at this post:
* <a href="https://blog.ruanbekker.com/blog/2019/05/30/use-vagrant-to-setup-a-local-development-environment-on-linux/">Use Vagrant to Setup a Local Development Environment on Linux</a></p>

<h2>Pre-Requisites</h2>

<p>I will be installing Vagrant and Virtualbox with Homebrew, if you do not have homebrew installed, you can install homebrew with:</p>

<pre><code class="bash">$ ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"
</code></pre>

<p>Once homebrew is installed, it&rsquo;s a good thing to update the indexes:</p>

<pre><code class="bash">$ brew update
</code></pre>

<h2>Virtualbox</h2>

<p>Install <a href="https://www.virtualbox.org/">VirtualBox</a> using homebrew:</p>

<pre><code class="bash">$ brew install --cask virtualbox
</code></pre>

<h2>Vagrant</h2>

<p>Install <a href="https://www.vagrantup.com/">Vagrant</a> using homebrew:</p>

<pre><code class="bash">$ brew install --cask vagrant
</code></pre>

<p>Install the virtualbox guest additions plugin for vagrant:</p>

<pre><code class="bash">$ vagrant plugin install vagrant-vbguest
</code></pre>

<p>If you would like a vagrant manager utility to help you manage your vagrant boxes, you can install <a href="http://www.vagrantmanager.com/">vagrant-manager</a> using homebrew:</p>

<pre><code class="bash">$ brew install --cask vagrant-manager
</code></pre>

<h2>Create your first Vagrant Box</h2>

<p>From <a href="https://app.vagrantup.com/boxes/search">app.vagrantup.com/boxes/search</a> you can search for any box, such as ubuntu, centos, alpine etc and for this demonstration I am going with <a href="https://app.vagrantup.com/ubuntu/boxes/focal64">ubuntu/focal64</a>.</p>

<p>I am creating a new directory for my devbox:</p>

<pre><code class="bash">$ mkdir devbox 
$ cd devbox
</code></pre>

<p>Then initialize the Vagrantfile by running:</p>

<pre><code class="bash">$ vagrant init ubuntu/focal64
</code></pre>

<p>A <code>Vagrantfile</code> has been created in the current working directory:</p>

<pre><code>$ cat Vagrantfile | grep -v "#"

Vagrant.configure("2") do |config|
  config.vm.box = "ubuntu/focal64"
end
</code></pre>

<p>Boot the VM:</p>

<pre><code class="bash">$ vagrant up
</code></pre>

<p>The box should now be in a started state, and we can verify that by running:</p>

<pre><code class="bash">$ vagrant status
Current machine states:

default                   running (virtualbox)
</code></pre>

<p>We can now SSH to our VM by running:</p>

<pre><code class="bash">$ vagrant ssh
vagrant@ubuntu-focal:~$
</code></pre>

<h2>Installing Software with Vagrant</h2>

<p>First let&rsquo;s destroy the VM that we created:</p>

<pre><code class="bash">$ vagrant destroy --force
</code></pre>

<p>Then edit the <code>Vagrantfile</code> and add the commands that we want to be executed when the VM boots, in our case, installing Nginx:</p>

<pre><code class="ruby">Vagrant.configure("2") do |config|
  config.vm.box = "ubuntu/focal64"
  config.vm.network "forwarded_port", guest: 80, host: 8080
  config.vm.provision "shell", inline: &lt;&lt;-SHELL
     apt update
     apt install nginx -y
  SHELL
end
</code></pre>

<p>You will also notice that we are forwarding port 8080 from our host, to port 80 on the VM so that we can access the webserver on port 8080 from our laptop. Then boot the VM:</p>

<pre><code>$ vagrant up
</code></pre>

<p>Once the VM has booted and installed our software, we should be able to access the index document served by Nginx on our VM:</p>

<pre><code class="bash">$ curl -I http://localhost:8080/

HTTP/1.1 200 OK
Server: nginx/1.18.0 (Ubuntu)
Date: Sat, 14 Aug 2021 18:11:59 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Sat, 14 Aug 2021 18:11:10 GMT
Connection: keep-alive
ETag: "6118073e-264"
Accept-Ranges: bytes
</code></pre>

<h2>Shared Folders</h2>

<p>Let&rsquo;s say you want to map your local directory to your VM, in a scenario where you want to store your <code>index.html</code> on your laptop and map it to the VM, we can use <code>config.vm.synced_folder</code>.</p>

<p>On our laptop, create a <code>html</code> directory where we will store our <code>index.hml</code>:</p>

<pre><code>$ mkdir html
</code></pre>

<p>Now create the content in the <code>index.html</code> under the <code>html</code> directory:</p>

<pre><code>$ echo "Hello, World" &gt; html/index.html
</code></pre>

<p>Now we need to make vagrant aware of the folder that we are mapping to the VM, so we need to edit the <code>Vagrantfile</code> and it will now look like this:</p>

<pre><code class="ruby"># -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure("2") do |config|
  config.vm.box = "ubuntu/focal64"
  config.vm.network "forwarded_port", guest: 80, host: 8080
  config.vm.provision "shell", inline: &lt;&lt;-SHELL
     apt update
     apt install nginx -y
  SHELL
  config.vm.synced_folder "html", "/var/www/html"
end
</code></pre>

<p>To reload the VM with our changes, we use <code>vagrant provision</code> to update our VM when changes to provisioners are made, and <code>vagrant reload</code> when we have config changes such as <code>config.vm.network</code>, but to restart the VM and forcing provisioners to run, we can use the following:</p>

<p>Thanks <a href="https://twitter.com/joshva_jebaraj">@joshva_jebaraj</a></p>

<pre><code class="bash">$ vagrant reload --provision
</code></pre>

<p>Once the VM is up, we can verify the changes:</p>

<pre><code>$ curl http://localhost:8080/
Hello, World
</code></pre>

<p>Now we can edit our content locally which is synced to our VM.</p>

<h2>Setting Hostname and Configure Memory</h2>

<p>We can also configure the hostname of our VM and configure the amount of memory that we want to allocate to our VM using:</p>

<ul>
<li><code>config.vm.hostname</code></li>
<li><code>vb.memory</code></li>
</ul>


<p>An example of that will look like the following:</p>

<pre><code class="ruby"># -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure("2") do |config|
  config.vm.box = "ubuntu/focal64"
  config.vm.hostname = "mydevbox"
  config.vm.network "forwarded_port", guest: 80, host: 8080
  config.vm.provision "shell", inline: &lt;&lt;-SHELL
     apt update
     apt install nginx -y
  SHELL
  config.vm.synced_folder "html", "/var/www/html"
  config.vm.provider "virtualbox" do |vb|
    vb.memory = "1024"
  end
end
</code></pre>

<p>In this example our VM&rsquo;s hostname is <code>mydevbox</code> and we assigned 1024MB of memory to our VM.</p>

<h2>Provisioners: Shell</h2>

<p>We can also run scripts from our local directory on our laptop on our VM using the <a href="https://www.vagrantup.com/docs/provisioning/shell">shell provisioner</a>.</p>

<p>First we need to create the script on our local directory:</p>

<pre><code class="bash">$ cat bootstrap.sh
#!/usr/bin/env bash
set -x
echo "my hostname is $(hostname)"
</code></pre>

<p>Then in our <code>Vagrantfile</code> we inform vagrant to execute the shell script:</p>

<pre><code class="ruby"># -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure("2") do |config|
  config.vm.box = "ubuntu/focal64"
  config.vm.hostname = "mydevbox"
  config.vm.provision :shell, :path =&gt; "bootstrap.sh"
end
</code></pre>

<p>Since my VM is already running, I will be doing a <code>reload</code>:</p>

<pre><code class="bash">$ vagrant reload --provision
...
==&gt; default: Running provisioner: shell...
    default: Running: /var/folders/04/r10yvb8d5dgfvd167jz5z23w0000gn/T/vagrant-shell20210814-70233-1p9dump.sh
    default: ++ hostname
    default: my hostname is mydevbox
    default: + echo 'my hostname is mydevbox'
</code></pre>

<p>As you can see the shell script from our local directory was executed on our VM, you can use this method to automate installations as well, etc.</p>

<h2>Provisioners: Docker</h2>

<p>Vagrant offers a <a href="https://www.vagrantup.com/docs/provisioning/docker">docker provisioner</a>, and for this example we will be hosting a mysql server using docker container in our VM.</p>

<p>Our <code>Vagrantfile</code>:</p>

<pre><code class="ruby"># -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure("2") do |config|
  config.vm.box = "ubuntu/focal64"
  config.vm.hostname = "mydevbox"
  config.vm.network "forwarded_port", guest: 3306, host: 3306
  config.vm.provision "docker" do |d|
    d.run "mysql", image: "mysql:8.0",
      args: "-p 3306:3306 -e MYSQL_ROOT_PASSWORD=password"
  end
end
</code></pre>

<p>Since I don&rsquo;t have port <code>3306</code> listening locally, I have mapped port <code>3306</code> from my laptop to port <code>3306</code> on my VM and I am using the <code>mysql:8.0</code> container image from docker hub and passing the arguments which is specific to the container.</p>

<p>The convenient thing about the docker provisioner, is that it will install docker onto the VM for you.</p>

<p>Once the config has been set in your <code>Vagrantfile</code> do a reload:</p>

<pre><code class="bash">$ vagrant reload --provision
...
    default: /vagrant =&gt; /Users/ruanbekker/workspace/vagrant/devbox
==&gt; default: Running provisioner: docker...
    default: Installing Docker onto machine...
==&gt; default: Starting Docker containers...
==&gt; default: -- Container: mysql
</code></pre>

<p>From our laptop we should be able to communicate with our mysql server:</p>

<pre><code class="bash">$ nc -vz localhost 3306
found 0 associations
found 1 connections:
     1: flags=82&lt;CONNECTED,PREFERRED&gt;
    outif lo0
    src 127.0.0.1 port 58745
    dst 127.0.0.1 port 3306
    rank info not available
    TCP aux info available

Connection to localhost port 3306 [tcp/mysql] succeeded!
</code></pre>

<p>We can also SSH to our VM and verify if the container is running:</p>

<pre><code class="bash">$ vagrant ssh
</code></pre>

<p>And then list the containers:</p>

<pre><code class="bash">$  docker ps
CONTAINER ID   IMAGE       COMMAND                  CREATED         STATUS         PORTS                                                  NAMES
30a843a486ae   mysql:8.0   "docker-entrypoint.sh    2 minutes ago   Up 2 minutes   0.0.0.0:3306-&gt;3306/tcp, :::3306-&gt;3306/tcp, 33060/tcp   mysql
</code></pre>

<h2>Provisioners: Ansible</h2>

<p>We can also execute <a href="https://www.ansible.com/">Ansible</a> playbooks on our VM using the <a href="https://www.vagrantup.com/docs/provisioning/ansible">Ansible Provisioner</a>.</p>

<p>Something to note is that we use <code>ansible</code> to execute the playbook on the host, and <code>ansible_local</code> to execute the playbook on the VM.</p>

<p>First we will create our <a href="https://docs.ansible.com/playbooks_best_practices.html#directory-layout">project structure</a> for ansible, so that we have the following in place:</p>

<pre><code class="bash">.
Vagrantfile
provisioning/playbook.yml
provisioning/group_vars/all
</code></pre>

<p>Create the <code>provisioning</code> directory:</p>

<pre><code class="bash">$ mkdir provisioning
</code></pre>

<p>Then the content for our <code>provisioning/playbook.yml</code> playbook:</p>

<pre><code class="yaml">---
- hosts: all
  become: yes
  tasks:
    - name: ensure ntpd is at the latest version
      apt:
        pkg: ntp
        state: ""
      notify:
      - restart ntpd
  handlers:
    - name: restart ntpd
      service:
        name: ntp
        state: restarted
</code></pre>

<p>Our <code>provisioning/group_vars/all</code> file that will contain the variables for the all group:</p>

<pre><code class="yaml">desired_state: "latest"
</code></pre>

<p>In our <code>Vagrantfile</code>:</p>

<pre><code class="ruby"># -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure("2") do |config|
  config.vm.box = "ubuntu/focal64"
  config.vm.hostname = "mydevbox"
  config.vm.provision :ansible do |ansible|
    ansible.playbook = "provisioning/playbook.yml"
  end
end
</code></pre>

<p>When using ansible with vagrant the inventory is <a href="https://www.vagrantup.com/docs/provisioning/ansible_intro#auto-generated-inventory">auto-generated</a> when then inventory is not specified. Vagrant will store the inventory on the host at <code>.vagrant/provisioners/ansible/inventory/vagrant_ansible_inventory</code>.</p>

<p>To execute playbooks with ansible, we need ansible installed on our host machine, for this demonstration I will be using virtualenv and then install ansible using pip:</p>

<pre><code class="bash">$ python3 -m pip install virtualenv
$ virtualenv -p $(which python3) .venv
$ source .venv/bin/activate
$ pip install ansible
</code></pre>

<p>Now that we have ansible installed, reload the VM to execute the playbook on our VM:</p>

<pre><code class="bash">$ vagrant reload --provision
...
==&gt; default: Running provisioner: ansible...
    default: Running ansible-playbook...

PLAY [all] *********************************************************************

TASK [Gathering Facts] *********************************************************
ok: [default]

TASK [ensure ntpd is at the latest version] ************************************
ok: [default]

PLAY RECAP *********************************************************************
default                    : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
</code></pre>

<p>Pretty neat right?</p>

<h2>Tear Down</h2>

<p>To destroy the VM:</p>

<pre><code class="bash">$ vagrant destroy --force
</code></pre>

<h2>Resources</h2>

<p>For more information on vagrant, check out their documentation:</p>

<ul>
<li><a href="https://www.vagrantup.com/docs">https://www.vagrantup.com/docs</a></li>
</ul>


<p>On provisioning documentation:</p>

<ul>
<li><a href="https://www.vagrantup.com/docs/provisioning/shell">https://www.vagrantup.com/docs/provisioning/shell</a></li>
<li><a href="https://www.vagrantup.com/docs/provisioning/docker">https://www.vagrantup.com/docs/provisioning/docker</a></li>
<li><a href="https://www.vagrantup.com/docs/provisioning/ansible_intro">https://www.vagrantup.com/docs/provisioning/ansible_intro</a></li>
</ul>


<p>I have a couple of example <code>Vagrantfile</code>s available on my github repository:</p>

<ul>
<li><a href="https://github.com/ruanbekker/vagrantfiles">https://github.com/ruanbekker/vagrantfiles</a></li>
</ul>


<h2>Thank You</h2>

<p>Thanks for reading, if you like my content, check out my <strong><a href="https://ruan.dev">website</a></strong> or follow me at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Upload Public SSH Keys Using Ansible]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/10/26/upload-public-ssh-keys-using-ansible/"/>
    <updated>2020-10-26T07:44:25+00:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/10/26/upload-public-ssh-keys-using-ansible</id>
    <content type="html"><![CDATA[<p>In this post I will demonstrate how you can use ansible to automate the task of adding one or more ssh public keys to multiple servers authorized_keys file.</p>

<p>This will be focused in a scenario where you have 5 new ssh keys that we would want to copy to our bastion hosts authorized_keys file</p>

<h2>The User Accounts</h2>

<p>We have our bastion server named <code>bastion.mydomain.com</code> where would like to create the following accounts: <code>john, bob, sarah, sam, adam</code> and also upload their personal ssh public keys to those accounts so that they can logon with their ssh private keys.</p>

<p>On my local directory, I have their ssh public keys as:</p>

<pre><code>~/workspace/sshkeys/john.pub
~/workspace/sshkeys/bob.pub
~/workspace/sshkeys/sarah.pub
~/workspace/sshkeys/sam.pub
~/workspace/sshkeys/adam.pub
</code></pre>

<p>They will be referenced in our playbook as <code>key: ".pub') }}"</code> but if they were on github we can reference them as <code>key: https://github.com/.keys</code>, more info on that can be found on the <a href="https://docs.ansible.com/ansible/2.4/authorized_key_module.html">authorized_key_module</a> documentation.</p>

<h2>The Target Server</h2>

<p>Our inventory for the target server only includes one host, but we can add as many as we want, but our inventory will look like this:</p>

<pre><code>$ cat inventory.ini
[bastion]
bastion-host ansible_host=34.x.x.x ansible_user=ubuntu ansible_ssh_private_key_file=~/.ssh/ansible.pem ansible_python_interpreter=/usr/bin/python3
[bastion:vars]
ansible_ssh_common_args='-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null'
</code></pre>

<p>Test if the target server is reachable using the user <code>ubuntu</code> using our admin accounts ssh key <code>ansible.pem</code>:</p>

<pre><code>$ ansible -i inventory.ini -m ping bastion
bastion | SUCCESS =&gt; {
    "changed": false,
    "ping": "pong"
}
</code></pre>

<h2>Our Playbook</h2>

<p>In this playbook, we will reference the users that we want to create and it will loop through those users, creating them on the target server and also use those names to match to the files on our laptop to match the ssh public keys:</p>

<pre><code>$ cat playbook.yml
---
- hosts: bastion
  become: yes
  become_user: root
  become_method: sudo
  tasks:
    - name: create local user account on the target server
      user:
        name: ''
        comment: ''
        shell: /bin/bash
        append: yes
        groups: sudo
        generate_ssh_key: yes
        ssh_key_type: rsa
      with_items:
        - john
        - bob
        - sarah
        - sam
        - adam

    - name: upload ssh public key to users authorized keys file
      authorized_key:
        user: ''
        state: present
        manage_dir: yes
        key: ".pub') }}"
      with_items:
        - john
        - bob
        - sarah
        - sam
        - adam
</code></pre>

<h2>Deploy</h2>

<p>Run the playbook:</p>

<pre><code>$ ansible-playbook -i inventory.ini ssh-setup.yml

PLAY [bastion]

TASK [Gathering Facts]
ok: [bastion-host]

TASK [create local user account on the target server]
changed: [bastion-host] =&gt; (item=john)
changed: [bastion-host] =&gt; (item=bob)
changed: [bastion-host] =&gt; (item=sarah)
changed: [bastion-host] =&gt; (item=sam)
changed: [bastion-host] =&gt; (item=adam)

TASK [upload ssh public key to users authorized keys file]
changed: [bastion-host] =&gt; (item=john)
changed: [bastion-host] =&gt; (item=bob)
changed: [bastion-host] =&gt; (item=sarah)
changed: [bastion-host] =&gt; (item=sam)
changed: [bastion-host] =&gt; (item=adam)

PLAY RECAP
bastion-host                   : ok=6    changed=5    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
</code></pre>

<p>Now when we ask one of the users, adam for example, to authenticate with:</p>

<pre><code>$ ssh -i ~/.ssh/path_to_his_private_key.pem adamin@bastion.mydomain.com
</code></pre>

<p>They should have access to the server.</p>

<h2>Thank You</h2>

<p>Thanks for reading, for more information on this module check out their documentation:</p>

<ul>
<li><a href="https://docs.ansible.com/ansible/2.4/authorized_key_module.html">https://docs.ansible.com/ansible/2.4/authorized_key_module.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Use a SSH Jump Host With Ansible]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/10/26/use-a-ssh-jump-host-with-ansible/"/>
    <updated>2020-10-26T05:25:18+00:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/10/26/use-a-ssh-jump-host-with-ansible</id>
    <content type="html"><![CDATA[<p>In this post we will demonstrate how to use a SSH Bastion or Jump Host with Ansible to reach the target server.</p>

<p>In some scenarios, the target server might be in a private range which is only accessible via a bastion host, and that counts the same for ansible as ansible is using SSH to reach to the target servers.</p>

<h2>SSH Config</h2>

<p>Our bastion host is configured as <code>bastion</code> and the config under <code>~/.ssh/config</code> looks like this:</p>

<pre><code>Host *
    Port 22
    StrictHostKeyChecking no
    UserKnownHostsFile /dev/null
    ServerAliveInterval 60
    ServerAliveCountMax 30

Host bastion
    HostName bastion.mydomain.com
    User bastion
    IdentityFile ~/.ssh/id_rsa
</code></pre>

<p>To verify that our config is working, you should be able to use:</p>

<pre><code>$ ssh bastion
</code></pre>

<h2>Using a Bastion with Ansible</h2>

<p>In order to reach our target server we need to use the bastion, so to test the SSH connection we can use this SSH one-liner. Our target server has a IP address of <code>172.31.81.94</code> and expects us to provide a <code>ansible.pem</code> private key and we need to authenticate with the <code>ubuntu</code> user:</p>

<pre><code>$ ssh -o ProxyCommand="ssh -W %h:%p -q bastion" -i ~/.ssh/ansible.pem ubuntu@172.31.81.94
</code></pre>

<p>If we can reach our server its time to include it in our playbook.</p>

<p>In our inventory:</p>

<pre><code>$ cat inventory.ini
[deployment]
server-a ansible_host=172.31.81.94 ansible_user=ubuntu ansible_ssh_private_key_file=~/.ssh/ansible.pem
[deployment:vars]
ansible_ssh_common_args='-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ProxyCommand="ssh -W %h:%p -q bastion"'
</code></pre>

<p>And our playbook which will use the ping module:</p>

<pre><code>$ cat playbook.yml
- name: Test Ping
  hosts: deployment
  tasks:
  - action: ping
</code></pre>

<p>Test it out:</p>

<pre><code>$ ansible-playbook -i inventory.ini ping.yml

PLAY [Test Ping] ***********************************************************************************************************************************************************

TASK [Gathering Facts] *****************************************************************************************************************************************************
ok: [server-a]

TASK [ping] ****************************************************************************************************************************************************************
ok: [server-a]

PLAY RECAP *****************************************************************************************************************************************************************
server-a                   : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Basic Ping Role With Ansible in a Playbook]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/10/23/basic-ping-role-with-ansible-in-a-playbook/"/>
    <updated>2020-10-23T13:13:16+00:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/10/23/basic-ping-role-with-ansible-in-a-playbook</id>
    <content type="html"><![CDATA[<p>This is a short post on how to create a basic role to reference the ping module in Ansible.</p>

<h2>Directory Structure</h2>

<p>This is our directory strucuture:</p>

<pre><code>$ tree .
.
├── inventory.ini
├── playbooks
│   └── myplaybook.yml
└── roles
    └── ping
        └── tasks
            └── main.yml

4 directories, 3 files
</code></pre>

<p>Create the directories:</p>

<pre><code>$ mkdir -p playbooks
$ mkdir -p roles/ping/tasks
</code></pre>

<p>Our <code>inventory.ini</code> includes the hosts that we will be using, and in this case I will be defining a group named <code>rpifleet</code> with all the host nested under that group and I&rsquo;m using the user <code>pi</code> and my private ssh key <code>~/.ssh/id_rsa</code>:</p>

<pre><code>$ cat inventory.ini
[rpifleet]
rpi-01 ansible_host=rpi-01.local ansible_user=pi ansible_ssh_private_key_file=~/.ssh/id_rsa ansible_python_interpreter=/usr/bin/python3
rpi-02 ansible_host=rpi-02.local ansible_user=pi ansible_ssh_private_key_file=~/.ssh/id_rsa ansible_python_interpreter=/usr/bin/python3
[rpifleet:vars]
ansible_ssh_common_args='-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null'
</code></pre>

<p>Next our role is a basic role that will reference our ping module, so from our main playbook we will reference the role that we are defining:</p>

<pre><code>$ cat roles/ping/tasks/main.yml
---
- name: Test Ping
  action: ping
</code></pre>

<p>Now that we have defined our <code>ping</code> role, we need to include it into our playbook:</p>

<pre><code>$ cat playbooks/myplaybook.yml
---
- name: ping raspberry pi fleet
  hosts: rpifleet
  roles:
    - { role: ../roles/ping }
</code></pre>

<p>You will see due to my playbooks directory being non-default, I defined the path to the role directory.</p>

<h2>Install Ansible</h2>

<p>Next we need to install ansible:</p>

<pre><code>$ pip install ansible
</code></pre>

<h2>Run the Ansible Playbook</h2>

<p>Now run the playbook which will ping the nodes using ssh. Using the ping module is useful when testing the connection to your nodes:</p>

<pre><code>$ ansible-playbook -i inventory.ini playbooks/myplaybook.yml

PLAY [ping raspberry pi fleet] *****************************************************

TASK [Gathering Facts] *************************************************************
ok: [rpi-02]
ok: [rpi-01]

TASK [../roles/ping : Test Ping] ***************************************************
ok: [rpi-02]
ok: [rpi-01]

PLAY RECAP *************************************************************************
rpi-01                     : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
rpi-02                     : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
</code></pre>

<h2>Thank You</h2>

<p>Thanks for reading</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using the Libvirt Provisioner With Terraform for KVM]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/10/08/using-the-libvirt-provisioner-with-terraform-for-kvm/"/>
    <updated>2020-10-08T00:06:21+00:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/10/08/using-the-libvirt-provisioner-with-terraform-for-kvm</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/95402415-e836d880-090f-11eb-8977-d1e2f842ef34.png" alt="terraform-ansible-kvm" /></p>

<p>In this post we will use the <a href="https://github.com/dmacvicar/terraform-provider-libvirt">libvirt provisioner</a> with Terraform to deploy a KVM Virtual Machine on a Remote KVM Host using SSH and use Ansible to deploy Nginx on our VM.</p>

<p>In my <a href="https://blog.ruanbekker.com/blog/2020/10/07/setup-a-kvm-host-for-virtualization-on-oneprovider/">previous post</a> I demonstrated how I provisioned my KVM Host and created a dedicated user for Terraform to authenticate to our KVM host to provision VMs.</p>

<p>Once you have KVM installed and your SSH access is sorted, we can start by installing our dependencies.</p>

<h2>Install our Dependencies</h2>

<p>First we will install Terraform:</p>

<pre><code>$ wget https://releases.hashicorp.com/terraform/0.13.3/terraform_0.13.3_linux_amd64.zip
$ unzip terraform_0.13.3_linux_amd64.zip
$ sudo mv terraform /usr/local/bin/terraform
</code></pre>

<p>Then we will install Ansible:</p>

<pre><code>$ virtualenv -p python3 .venv
$ source .venv/bin/activate
$ pip install ansible
</code></pre>

<p>Now in order to use the libvirt provisioner, we need to install it where we will run our Terraform deployment:</p>

<pre><code>$ cd /tmp/
$ mkdir -p ~/.local/share/terraform/plugins/registry.terraform.io/dmacvicar/libvirt/0.6.2/linux_amd64
$ wget https://github.com/dmacvicar/terraform-provider-libvirt/releases/download/v0.6.2/terraform-provider-libvirt-0.6.2+git.1585292411.8cbe9ad0.Ubuntu_18.04.amd64.tar.gz
$ tar -xvf terraform-provider-libvirt-0.6.2+git.1585292411.8cbe9ad0.Ubuntu_18.04.amd64.tar.gz
$ mv ./terraform-provider-libvirt  ~/.local/share/terraform/plugins/registry.terraform.io/dmacvicar/libvirt/0.6.2/linux_amd64/
</code></pre>

<p>Our ssh config for our KVM host in <code>~/.ssh/config</code>:</p>

<pre><code>Host *
    Port 22
    StrictHostKeyChecking no
    UserKnownHostsFile /dev/null

Host ams-kvm-remote-host
    HostName ams-kvm.mydomain.com
    User deploys
    IdentityFile ~/.ssh/deploys.pem
</code></pre>

<h2>Terraform all the things</h2>

<p>Create a workspace directory for our demonstration:</p>

<pre><code>$ mkdir -p ~/workspace/terraform-kvm-example/
$ cd ~/workspace/terraform-kvm-example/
</code></pre>

<p>First let&rsquo;s create our <code>providers.tf</code>:</p>

<pre><code>terraform {
  required_providers {
    libvirt = {
      source  = "dmacvicar/libvirt"
      version = "0.6.2"
    }
  }
}
</code></pre>

<p>Then our <code>variables.tf</code>, just double check where you need to change values to suite your environment:</p>

<pre><code>variable "libvirt_disk_path" {
  description = "path for libvirt pool"
  default     = "/opt/kvm/pool1"
}

variable "ubuntu_18_img_url" {
  description = "ubuntu 18.04 image"
  default     = "http://cloud-images.ubuntu.com/releases/bionic/release-20191008/ubuntu-18.04-server-cloudimg-amd64.img"
}

variable "vm_hostname" {
  description = "vm hostname"
  default     = "terraform-kvm-ansible"
}

variable "ssh_username" {
  description = "the ssh user to use"
  default     = "ubuntu"
}

variable "ssh_private_key" {
  description = "the private key to use"
  default     = "~/.ssh/id_rsa"
}
</code></pre>

<p>Create the <code>main.tf</code>, you will notice that we are using ssh to connect to KVM, and because the private range of our VM&rsquo;s are not routable via the internet, I&rsquo;m using a bastion host to reach them.</p>

<p>The bastion host (ssh config from the pre-requirements section) is the KVM host and you will see that ansible is also using that host as a jump box, to get to the VM. I am also using cloud-init to bootstrap the node with SSH, etc.</p>

<p>The reason why I&rsquo;m using remote-exec before the ansible deployment, is to ensure that we can establish a command via SSH before Ansible starts.</p>

<pre><code>provider "libvirt" {
  uri = "qemu+ssh://deploys@ams-kvm-remote-host/system"
}

resource "libvirt_pool" "ubuntu" {
  name = "ubuntu"
  type = "dir"
  path = var.libvirt_disk_path
}

resource "libvirt_volume" "ubuntu-qcow2" {
  name = "ubuntu-qcow2"
  pool = libvirt_pool.ubuntu.name
  source = var.ubuntu_18_img_url
  format = "qcow2"
}

data "template_file" "user_data" {
  template = file("${path.module}/config/cloud_init.yml")
}

data "template_file" "network_config" {
  template = file("${path.module}/config/network_config.yml")
}

resource "libvirt_cloudinit_disk" "commoninit" {
  name           = "commoninit.iso"
  user_data      = data.template_file.user_data.rendered
  network_config = data.template_file.network_config.rendered
  pool           = libvirt_pool.ubuntu.name
}

resource "libvirt_domain" "domain-ubuntu" {
  name   = var.vm_hostname
  memory = "512"
  vcpu   = 1

  cloudinit = libvirt_cloudinit_disk.commoninit.id

  network_interface {
    network_name   = "default"
    wait_for_lease = true
    hostname       = var.vm_hostname
  }

  console {
    type        = "pty"
    target_port = "0"
    target_type = "serial"
  }

  console {
    type        = "pty"
    target_type = "virtio"
    target_port = "1"
  }

  disk {
    volume_id = libvirt_volume.ubuntu-qcow2.id
  }

  graphics {
    type        = "spice"
    listen_type = "address"
    autoport    = true
  }

  provisioner "remote-exec" {
    inline = [
      "echo 'Hello World'"
    ]

    connection {
      type                = "ssh"
      user                = var.ssh_username
      host                = libvirt_domain.domain-ubuntu.network_interface[0].addresses[0]
      private_key         = file(var.ssh_private_key)
      bastion_host        = "ams-kvm-remote-host"
      bastion_user        = "deploys"
      bastion_private_key = file("~/.ssh/deploys.pem")
      timeout             = "2m"
    }
  }

  provisioner "local-exec" {
    command = &lt;&lt;EOT
      echo "[nginx]" &gt; nginx.ini
      echo "${libvirt_domain.domain-ubuntu.network_interface[0].addresses[0]}" &gt;&gt; nginx.ini
      echo "[nginx:vars]" &gt;&gt; nginx.ini
      echo "ansible_ssh_common_args='-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ProxyCommand=\"ssh -W %h:%p -q ams-kvm-remote-host\"'" &gt;&gt; nginx.ini
      ansible-playbook -u ${var.ssh_username} --private-key ${var.ssh_private_key} -i nginx.ini ansible/playbook.yml
      EOT
  }
}
</code></pre>

<p>As I&rsquo;ve mentioned, Im using cloud-init, so lets setup the network config and cloud init under the <code>config/</code> directory:</p>

<pre><code>$ mkdir config
</code></pre>

<p>And our <code>config/cloud_init.yml</code>, just make sure that you configure your public ssh key for ssh access in the config:</p>

<pre><code>#cloud-config
# vim: syntax=yaml
# examples:
# https://cloudinit.readthedocs.io/en/latest/topics/examples.html
bootcmd:
  - echo 192.168.0.1 gw.homedns.xyz &gt;&gt; /etc/hosts
runcmd:
 - [ ls, -l, / ]
 - [ sh, -xc, "echo $(date) ': hello world!'" ]
ssh_pwauth: true
disable_root: false
chpasswd:
  list: |
     root:password
  expire: false
users:
  - name: ubuntu
    sudo: ALL=(ALL) NOPASSWD:ALL
    groups: users, admin
    home: /home/ubuntu
    shell: /bin/bash
    lock_passwd: false
    ssh-authorized-keys:
      - ssh-rsa AAAA ...your-public-ssh-key-goes-here... user@host
final_message: "The system is finally up, after $UPTIME seconds"
</code></pre>

<p>And our network config, in <code>config/network_config.yml</code>:</p>

<pre><code>version: 2
ethernets:
  ens3:
    dhcp4: true
</code></pre>

<p>Now we will create our Ansible playbook, to deploy nginx to our VM, create the ansible directory:</p>

<pre><code>$ mkdir ansible
</code></pre>

<p>Then create the <code>ansible/playbook.yml</code>:</p>

<pre><code>---
# https://docs.ansible.com/ansible/latest/collections/ansible/builtin/apt_module.html
# https://docs.ansible.com/ansible/latest/collections/ansible/builtin/systemd_module.html#examples
- hosts: nginx
  become: yes
  become_user: root
  become_method: sudo
  tasks:
    - name: Install nginx
      apt:
        name: nginx
        state: latest
        update_cache: yes

    - name: Enable service nginx and ensure it is not masked
      systemd:
        name: nginx
        enabled: yes
        masked: no

    - name: ensure nginx is started
      systemd:
        state: started
        name: nginx
</code></pre>

<p>This is optional, but I&rsquo;m using a <code>ansible.cfg</code> file to define my defaults:</p>

<pre><code>[defaults]
host_key_checking = False
ansible_port = 22
ansible_user = ubuntu
ansible_ssh_private_key_file = ~/.ssh/id_rsa
ansible_python_interpreter = /usr/bin/python3
</code></pre>

<p>And lastly, our <code>outputs.tf</code> which will display our IP address of our VM:</p>

<pre><code>output "ip" {
  value = libvirt_domain.domain-ubuntu.network_interface[0].addresses[0]
}

output "url" {
  value = "http://${libvirt_domain.domain-ubuntu.network_interface[0].addresses[0]}"
}
</code></pre>

<h2>Deploy our Terraform Deployment</h2>

<p>It&rsquo;s time to deploy a KVM instance with Terraform and deploy Nginx to our VM with Ansible using the local-exec provisioner.</p>

<p>Initialize terraform to download all the plugins:</p>

<pre><code>$ terraform init

Initializing the backend...

Initializing provider plugins...
- Finding latest version of hashicorp/template...
- Finding dmacvicar/libvirt versions matching "0.6.2"...
- Installing hashicorp/template v2.1.2...
- Installed hashicorp/template v2.1.2 (signed by HashiCorp)
- Installing dmacvicar/libvirt v0.6.2...
- Installed dmacvicar/libvirt v0.6.2 (unauthenticated)

The following providers do not have any version constraints in configuration,
so the latest version was installed.

To prevent automatic upgrades to new major versions that may contain breaking
changes, we recommend adding version constraints in a required_providers block
in your configuration, with the constraint strings suggested below.

* hashicorp/template: version = "~&gt; 2.1.2"

Terraform has been successfully initialized!
</code></pre>

<p>Run a plan, to see what will be done:</p>

<pre><code>$ terraform plan

...
Plan: 4 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + ip  = (known after apply)
  + url = (known after apply)
...
</code></pre>

<p>And run a apply to run our deployment:</p>

<pre><code>$ terraform apply -auto-approve
...
libvirt_domain.domain-ubuntu (local-exec): PLAY RECAP *********************************************************************
libvirt_domain.domain-ubuntu (local-exec): 192.168.122.213            : ok=4    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
libvirt_domain.domain-ubuntu: Creation complete after 2m24s [id=c96def6e-0361-441c-9e1f-5ba5f3fa5aec]

Apply complete! Resources: 4 added, 0 changed, 0 destroyed.

Outputs:

ip = 192.168.122.213
url = http://192.168.122.213
</code></pre>

<p>You can always get the output afterwards using show or output:</p>

<pre><code>$ terraform show -json | jq -r '.values.outputs.ip.value'
192.168.122.213

$ terraform output -json ip | jq -r '.'
192.168.122.213
</code></pre>

<h2>Test our VM</h2>

<p>Hop onto the KVM host, and test out nginx:</p>

<pre><code>$ curl -I http://192.168.122.213
HTTP/1.1 200 OK
Server: nginx/1.14.0 (Ubuntu)
Date: Thu, 08 Oct 2020 00:37:43 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Thu, 08 Oct 2020 00:33:04 GMT
Connection: keep-alive
ETag: "5f7e5e40-264"
Accept-Ranges: bytes
</code></pre>

<iframe src="https://giphy.com/embed/3ohzdIuqJoo8QdKlnW" width="480" height="222" frameBorder="0" class="giphy-embed" allowFullScreen></iframe>


<p><a href="https://giphy.com/gifs/reactionseditor-yes-awesome-3ohzdIuqJoo8QdKlnW">via GIPHY</a></p>


<h2>Thank You</h2>

<p><a href="https://saythanks.io/to/ruan.ru.bekker@gmail.com"><img src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" alt="Say Thanks!" /></a></p>

<p>Thanks for reading, check out my <strong><a href="" rel="nofollow" target="_blank">website</a></strong> or follow me at <strong><a href="https://twitter.com/ruanbekker" rel="nofollow" target="_blank">@ruanbekker</a></strong> on Twitter.</p>
]]></content>
  </entry>
  
</feed>
