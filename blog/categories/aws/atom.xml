<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Aws | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/aws/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2019-06-25T08:48:39-04:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Play With Kinesis Data Streams for Free]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/06/22/play-with-kinesis-data-streams-for-free/"/>
    <updated>2019-06-22T17:35:19-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/06/22/play-with-kinesis-data-streams-for-free</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/59969559-3f187300-9550-11e9-9e6d-7fa4dbc30198.png" alt="image" /></p>

<p>Misleading title? Perhaps, depends on how you look at it. Amazon Kinesis is a fully managed, cloud-based service for real-time processing of distributed data streams. So if you&rsquo;re a curious mad person like me, you want to test out stuff and when you can test stuff out for free, why not.</p>

<p>So before paying for that, why not spin something up locally, such as <a href="https://github.com/mhart/kinesalite">Kinesisalite</a> which is an implementation of Amazon Kinesis built on top of LevelDB.</p>

<p>Kinesis overview:</p>

<p><img src="https://user-images.githubusercontent.com/567298/59969540-caddcf80-954f-11e9-8e3d-23c932d35ef1.png" alt="image" /></p>

<h2>What will we be doing?</h2>

<p>In this tutorial we will setup a local kinesis instance using docker then do the following:</p>

<ul>
<li>Create a Kinesis Stream, List, Describe, PutRecord, GetRecords using Python&rsquo;s Boto3 Interface</li>
<li>Write a Python Producer and Consumer</li>
<li>Write and Read Records from our Local Kinesis Stream</li>
</ul>


<h2>Building Kinesis Local on Docker</h2>

<p>If you would like to skip this step, you can use my docker image: <a href="https://hub.docker.com/r/ruanbekker/kinesis-local">ruanbekker/kinesis-local:latest</a></p>

<p>Our Dockerfile:</p>

<pre><code>FROM node:8.16.0-stretch-slim

RUN apt update &amp;&amp; apt install build-essential python-minimal -y
RUN npm install --unsafe-perm -g kinesalite
RUN apt-get clean

CMD ["kinesalite", "--port", "4567", "--createStreaMs", "5"]
</code></pre>

<p>Build:</p>

<pre><code>$ docker build -t kinesis-local .
</code></pre>

<p>Run and expose port 4567:</p>

<pre><code>$ docker run -it -p 4567:4567 kinesis-local:latest
</code></pre>

<h2>Interact with Kinesis Local:</h2>

<p>In this next steps we will setup our environment, which will only require <code>python</code> and <code>boto3</code>. To keep things isolated, I will do this with a docker container:</p>

<pre><code>$ docker run -it python:3.7-alpine sh
</code></pre>

<p>Now we need to install boto3 and enter the python repl:</p>

<pre><code>$ pip3 install boto3
$ python3
Python 3.7.3 (default, May 11 2019, 02:00:41)
[GCC 8.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt;
</code></pre>

<p>Import boto and create the connection to our kinesis local instance:</p>

<pre><code>&gt;&gt;&gt; import boto3
&gt;&gt;&gt; client = boto3.Session(
    region_name='eu-west-1').client('kinesis', aws_access_key_id='', aws_secret_access_key='', endpoint_url='http://localhost:4567'
)
</code></pre>

<p>Let&rsquo;s list our streams and as expected, we should have zero streams available:</p>

<pre><code>&gt;&gt;&gt; client.list_streams()
{u'StreamNames': [], u'HasMoreStreams': False, 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': '637xx', 'HTTPHeaders': {'x-amzn-requestid': '6xx', 'content-length': '41', 'x-amz-id-2': 'xx', 'connection': 'keep-alive', 'date': 'Sat, 22 Jun 2019 19:17:34 GMT', 'content-type': 'application/x-amz-json-1.1'}}}
</code></pre>

<p>Let&rsquo;s create a stream named <code>mystream</code> with 1 primary shard:</p>

<pre><code>&gt;&gt;&gt; client.create_stream(StreamName='mystream', ShardCount=1)
</code></pre>

<p>Let&rsquo;s list our streams again:</p>

<pre><code>&gt;&gt;&gt; client.list_streams()
{u'StreamNames': [u'mystream'], u'HasMoreStreams': False, 'ResponseMetadata': ...
</code></pre>

<p>Let&rsquo;s put some data in our kinesis stream, we will push a payload with the body: <code>{"name": "ruan"}</code> to our kinesis stream with partition key: <code>a01</code> which is used for sharding:</p>

<pre><code>&gt;&gt;&gt; response = client.put_record(StreamName='mystream', Data=json.dumps({"name": "ruan"}), PartitionKey='a01')
&gt;&gt;&gt; response
{u'ShardId': u'shardId-000000000000', 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': 'cb0xx', 'HTTPHeaders': {'x-amzn-requestid': 'xx', 'content-length': '110', 'x-amz-id-2': 'xx', 'connection': 'keep-alive', 'date': 'Sat, 22 Jun 2019 19:20:27 GMT', 'content-type': 'application/x-amz-json-1.1'}}, u'SequenceNumber': u'490xx'}
</code></pre>

<p>Now that we have data in our stream we need to read data from our kinesis stream. Before data can be read from the stream we need to obtain the shard iterator for the shard we are interested in. A shard iterator represents the position of the stream and shard from which the consumer will read, in this case we will call the get_shard_operator method and passing the stream name, shard id and shard iterator type.</p>

<p>There are 2 comman iterator types:</p>

<ul>
<li>TRIM_HORIZON: Points to the last untrimmed record in the shard</li>
<li>LATEST: Reads the most recent data in the shard</li>
</ul>


<p>We will use TRIM_HORIZON in this case, get the shard iterator id:</p>

<pre><code>&gt;&gt;&gt; shard_id = response['ShardId']
&gt;&gt;&gt; response = client.get_shard_iterator(StreamName='mystream', ShardId=shard_id, ShardIteratorType='TRIM_HORIZON')
&gt;&gt;&gt; response
{u'ShardIterator': u'AAAxx=', 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': '22dxx', 'HTTPHeaders': {'x-amzn-requestid': '22dxx', 'content-length': '224', 'x-amz-id-2': 'xx', 'connection': 'keep-alive', 'date': 'Sat, 22 Jun 2019 19:22:55 GMT', 'content-type': 'application/x-amz-json-1.1'}}}
</code></pre>

<p>Now that we have the shard iterator id, we can call the get_records method with the shard iterator id, to read the data from the stream:</p>

<pre><code>&gt;&gt;&gt; shard_iterator = response['ShardIterator']
&gt;&gt;&gt; response = client.get_records(ShardIterator=shard_iterator)
&gt;&gt;&gt; response
{u'Records': [{u'Data': '{"name": "ruan"}', u'PartitionKey': u'a01', u'ApproximateArrivalTimestamp': datetime.datetime(2019, 6, 22, 21, 20, 27, 937000, tzinfo=tzlocal()), u'SequenceNumber': u'495xx'}], 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': '2b6xx', 'HTTPHeaders': {'x-amzn-requestid': '2b6xx', 'content-length': '441', 'x-amz-id-2': 'xx', 'connection': 'keep-alive', 'date': 'Sat, 22 Jun 2019 19:30:19 GMT', 'content-type': 'application/x-amz-json-1.1'}}, u'NextShardIterator': u'AAAxx=', u'MillisBehindLatest': 0}
</code></pre>

<p>To loop and parse through the response to make it more readable:</p>

<pre><code>&gt;&gt;&gt; for record in response['Records']:
...     if 'Data' in record:
...         json.loads(record['Data'])
...
{u'name': u'ruan'}
</code></pre>

<p>Once we are done, we can delete our stream:</p>

<pre><code>&gt;&gt;&gt; client.delete_stream(StreamName='mystream')
</code></pre>

<p>Now that we have the basics, lets create our producer and consumer for a demonstration on pushing data to a kinesis stream from one process and consuming it from another process. As this demonstration we will be producing and consuming data from the same laptop, in real use-cases, you will do them from seperate servers and using Amazon Kinesis.</p>

<h2>Our Kinesis Producer</h2>

<p>The following will create a Kinesis Local Stream and Write 25 JSON Documents to our stream:</p>

<pre><code class="python">import boto3
import random
import json
import time

names = ['james', 'stefan', 'pete', 'tom', 'frank', 'peter', 'ruan']

session = boto3.Session(region_name='eu-west-1')
client = session.client(
    'kinesis', 
    aws_access_key_id='', 
    aws_secret_access_key='', 
    endpoint_url='http://localhost:4567'
)

list_streams = client.list_streams()

if 'mystream' not in list_streams['StreamNames']:
    client.create_stream(StreamName='mystream', ShardCount=1)
    time.sleep(1)

count = 0
print("Starting at {}".format(time.strftime("%H:%m:%S")))

while count != 25:
    count += 1
    response = client.put_record(
        StreamName='mystream', 
        Data=json.dumps({
            "number": count, 
            "name": random.choice(names), 
            "age": random.randint(20,50)}
        ), 
        PartitionKey='a01'
    )
    time.sleep(1)

print("Finished at {}".format(time.strftime("%H:%m:%S")))
</code></pre>

<h2>Our Kinesis Local Consumer:</h2>

<p>This will read 5 records at a time from our stream, you will notice if you run them on the same time it will only read one at a time as the producer only writes one per second.</p>

<pre><code class="python">import boto3
import json
import time
import os

session = boto3.Session(region_name='eu-west-1')
client = session.client(
    'kinesis', 
    aws_access_key_id='', 
    aws_secret_access_key='', 
    endpoint_url='http://localhost:4567'
)

stream_details = client.describe_stream(StreamName='mystream')
shard_id = stream_details['StreamDescription']['Shards'][0]['ShardId']

response = client.get_shard_iterator(
    StreamName='mystream', 
    ShardId=shard_id, 
    ShardIteratorType='TRIM_HORIZON'
)

shard_iterator = response['ShardIterator']

while True:
    response = client.get_records(ShardIterator=shard_iterator, Limit=5)
    shard_iterator = response['NextShardIterator']
    for record in response['Records']:
        if 'Data' in record and len(record['Data']) &gt; 0:
            print(json.loads(record['Data']))
    time.sleep(0.75)
</code></pre>

<h2>Demo Time!</h2>

<p>Now that we have our <code>producer.py</code> and <code>consumer.py</code>, lets test this out.</p>

<p>Start the server:</p>

<pre><code>$ docker run -it -p 4567:4567 ruanbekker/kinesis-local:latest
Listening at http://:::4567
</code></pre>

<p>Run the Producer from your Python Environment:</p>

<pre><code>$ python producer.py
Starting at 00:06:16
Finished at 00:06:42
</code></pre>

<p>Run the Consumer from your Python Environment:</p>

<pre><code>$ python consumer.py
Starting Consuming at 00:06:31
{u'age': 30, u'number': 1, u'name': u'pete'}
{u'age': 23, u'number': 2, u'name': u'ruan'}
{u'age': 22, u'number': 3, u'name': u'peter'}
{u'age': 45, u'number': 4, u'name': u'stefan'}
{u'age': 49, u'number': 5, u'name': u'tom'}
{u'age': 47, u'number': 6, u'name': u'pete'}
{u'age': 35, u'number': 7, u'name': u'stefan'}
{u'age': 45, u'number': 8, u'name': u'ruan'}
{u'age': 38, u'number': 9, u'name': u'frank'}
{u'age': 20, u'number': 10, u'name': u'tom'}
{u'age': 38, u'number': 11, u'name': u'james'}
{u'age': 20, u'number': 12, u'name': u'james'}
{u'age': 38, u'number': 13, u'name': u'tom'}
{u'age': 25, u'number': 14, u'name': u'tom'}
{u'age': 20, u'number': 15, u'name': u'peter'}
{u'age': 50, u'number': 16, u'name': u'james'}
{u'age': 29, u'number': 17, u'name': u'james'}
{u'age': 42, u'number': 18, u'name': u'pete'}
{u'age': 25, u'number': 19, u'name': u'pete'}
{u'age': 36, u'number': 20, u'name': u'tom'}
{u'age': 45, u'number': 21, u'name': u'peter'}
{u'age': 39, u'number': 22, u'name': u'ruan'}
{u'age': 43, u'number': 23, u'name': u'tom'}
{u'age': 38, u'number': 24, u'name': u'pete'}
{u'age': 40, u'number': 25, u'name': u'frank'}
Finshed Consuming at 00:06:35
</code></pre>

<h2>Thanks</h2>

<p><a href="https://saythanks.io/to/ruanbekker"><img src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" alt="Say Thanks!" /></a></p>

<p>Hope that was useful, feel free to check out <a href="https://aws.amazon.com/kinesis/">Amazon&rsquo;s Kinesis</a> out if you are planning to run this in any non-testing environment</p>

<p><a href="https://saythanks.io/to/ruanbekker"><img src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" alt="Say Thanks!" /></a></p>

<center>
        <script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Buy Me a Coffee', '#46b798', 'A6423ZIQ');kofiwidget2.draw();</script>
</center>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a Logstash Server for Amazon Elasticsearch Service and Auth With IAM]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/06/04/setup-a-logstash-server-for-amazon-elasticsearch-service-and-auth-with-iam/"/>
    <updated>2019-06-04T17:46:27-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/06/04/setup-a-logstash-server-for-amazon-elasticsearch-service-and-auth-with-iam</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/59209960-ca872100-8bac-11e9-8672-8c6af502afe0.png" alt="logstash" /></p>

<p>As many of you might know, when you deploy a ELK stack on Amazon Web Services, you only get E and K in the ELK stack, which is Elasticsearch and Kibana. Here we will be dealing with Logstash on EC2.</p>

<h2>What will we be doing</h2>

<p>In this tutorial we will setup a Logstash Server on EC2, setup a IAM Role and Autenticate Requests to Elasticsearch with an IAM Role, setup Nginx so that logstash can ship logs to Elasticsearch.</p>

<p>I am not fond of working with access key&rsquo;s and secret keys, and if I can stay away from handling secret information the better. So instead of creating a access key and secret key for logstash, we will instead create a IAM Policy that will allow the actions to Elasticsearch, associate that policy to an IAM Role, set EC2 as a trusted entity and strap that IAM Role to the EC2 Instance.</p>

<p>Then we will allow the IAM Role ARN to the Elasticsearch Policy, then when Logstash makes requests against Elasticsearch, it will use the IAM Role to assume temporary credentials to authenticate. That way we don&rsquo;t have to deal with keys. But I mean you can create access keys if that is your preferred method, I&rsquo;m just not a big fan of keeping secret keys.</p>

<p>The benefit of authenticating with IAM, allows you to remove a reverse proxy that is another hop to the path of your target.</p>

<h2>Create the IAM Policy:</h2>

<p>Create a IAM Policy that will allow actions to Elasticsearch:</p>

<pre><code>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "es:ESHttpHead",
                "es:ESHttpPost",
                "es:ESHttpGet",
                "es:ESHttpPut"
            ],
            "Resource": "arn:aws:es:eu-west-1:0123456789012:domain/my-es-domain"
        }
    ]
}
</code></pre>

<p>Create Role logstash-system-es with &ldquo;ec2.amazonaws.com&rdquo; as trusted entity in trust the relationship and associate the above policy to the role.</p>

<h2>Authorize your Role in Elasticsearch Policy</h2>

<p>Head over to your Elasticsearch Domain and configure your Elasticsearch Policy to include your IAM Role to grant requests to your Domain:</p>

<pre><code>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "arn:aws:iam::0123456789012:role/logstash-system-es"
        ]
      },
      "Action": "es:*",
      "Resource": "arn:aws:es:eu-west-1:0123456789012:domain/my-es-domain/*"
    }
  ]
}
</code></pre>

<h2>Install Logstash on EC2</h2>

<p>I will be using Ubuntu Server 18. Update the repositories and install dependencies:</p>

<pre><code>$ apt update &amp;&amp; apt upgrade -y
$ apt install build-essential apt-transport-https -y
$ wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
$ echo "deb https://artifacts.elastic.co/packages/6.x/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-6.x.list
$ apt update
</code></pre>

<p>As logstash requires Java, install the the Java OpenJDK Runtime Environment:</p>

<pre><code>$ apt install default-jre -y
</code></pre>

<p>Verify that Java is installed:</p>

<pre><code>$ java -version
openjdk version "11.0.3" 2019-04-16
OpenJDK Runtime Environment (build 11.0.3+7-Ubuntu-1ubuntu218.04.1)
OpenJDK 64-Bit Server VM (build 11.0.3+7-Ubuntu-1ubuntu218.04.1, mixed mode, sharing)
</code></pre>

<p>Now, install logstash and enable the service on boot:</p>

<pre><code>$ apt install logstash -y
$ systemctl enable logstash.service
$ service logstash stop
</code></pre>

<h2>Install the Amazon ES Logstash Output Plugin</h2>

<p>For us to be able to authenticate using IAM, we should use the Amazon-ES Logstash Output Plugin. Update and install the plugin:</p>

<pre><code>$ /usr/share/logstash/bin/logstash-plugin update
$ /usr/share/logstash/bin/logstash-plugin install logstash-output-amazon_es
</code></pre>

<h2>Configure Logstash</h2>

<p>I like to split up my configuration in 3 parts, (input, filter, output).</p>

<p>Let&rsquo;s create the input configuration: <code>/etc/logstash/conf.d/10-input.conf</code></p>

<pre><code>input {
  file {
    path =&gt; "/var/log/nginx/access.log"
    start_position =&gt; "beginning"
  }
}
</code></pre>

<p>Our filter configuration: <code>/etc/logstash/conf.d/20-filter.conf</code></p>

<pre><code>filter {
  grok {
    match =&gt; { "message" =&gt; "%{HTTPD_COMMONLOG}" }
  }
  mutate {
    add_field =&gt; {
      "custom_field1" =&gt; "hello from: %{host}"
    }
  }
}
</code></pre>

<p>And lastly, our output configuration: <code>/etc/logstash/conf.d/30-outputs.conf</code>:</p>

<pre><code>output {
  amazon_es {
      hosts =&gt; ["my-es-domain.abcdef.eu-west-1.es.amazonaws.com"]
      index =&gt; "new-logstash-%{+YYYY.MM.dd}"
      region =&gt; "eu-west-1"
      aws_access_key_id =&gt; ''
      aws_secret_access_key =&gt; ''
  }
}
</code></pre>

<p>Note that the <code>aws_</code> directives has been left empty as that seems to be the way it needs to be set when using roles. Authentication will be assumed via the Role which is associated to the EC2 Instance.</p>

<p>If you are using access keys, you can populate them there.</p>

<h2>Start Logstash</h2>

<p>Start logstash:</p>

<pre><code>$ service logstash start
</code></pre>

<p>Tail the logs to see if logstash starts up correctly, it should look more or less like this:</p>

<pre><code>$ tail -f /var/log/logstash/logstash-plain.log

[2019-06-04T16:38:12,087][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=&gt;"6.8.0"}
[2019-06-04T16:38:14,480][INFO ][logstash.pipeline        ] Starting pipeline {:pipeline_id=&gt;"main", "pipeline.workers"=&gt;2, "pipeline.batch.size"=&gt;125, "pipeline.batch.delay"=&gt;50}
[2019-06-04T16:38:15,226][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=&gt;{:removed=&gt;[], :added=&gt;[https://search-my-es-domain-xx.eu-west-1.es.amazonaws.com:443/]}}
[2019-06-04T16:38:15,234][INFO ][logstash.outputs.elasticsearch] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=&gt;https://search-my-es-domain-xx.eu-west-1.es.amazonaws.com:443/, :path=&gt;"/"}
</code></pre>

<h2>Install Nginx</h2>

<p>As you noticed, I have specified <code>/var/log/nginx/access.log</code> as my input file for logstash, as we will test logstash by shipping nginx access logs to Elasticsearch Service.</p>

<p>Install Nginx:</p>

<pre><code>$ apt install nginx -y
</code></pre>

<p>Start the service:</p>

<pre><code>$ systemctl restart nginx 
$ systemctl enable nginx
</code></pre>

<p>Make a GET request on your Nginx Web Server and inspect the log on Kibana, where it should look like this:</p>

<p><img src="https://user-images.githubusercontent.com/567298/58917559-4dc8f280-8727-11e9-9e9d-7950217abe34.png" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Snippet: Create Custom CloudWatch Metrics With Python]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/03/28/snippet-create-custom-cloudwatch-metrics-with-python/"/>
    <updated>2019-03-28T08:05:28-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/03/28/snippet-create-custom-cloudwatch-metrics-with-python</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/53865781-a984c200-3ff8-11e9-9ffa-ccad62ac08f6.png" alt="" /></p>

<p>A quick post on how create custom CloudWatch Metrics using Python on AWS.</p>

<p>After you produced the metrics into CloudWatch, you will be able to see them when navigating to:</p>

<ul>
<li>CloudWatch / Metrics / Custom Namespaces / statusdash/ec2client</li>
</ul>


<p>When selecting:</p>

<pre><code>Select Metric: SomeKey1, SomeKey2
Select MetricName HttpResponseTime
</code></pre>

<p>And should look like this:</p>

<p><img src="https://user-images.githubusercontent.com/567298/53865426-d4224b00-3ff7-11e9-8bd5-bd04dfdd9f43.png" alt="" /></p>

<h2>The Script:</h2>

<p>The python script that will be using boto3 to talk to AWS:</p>

<pre><code class="python">import boto3
import random
cloudwatch = boto3.Session(region_name='eu-west-1').client('cloudwatch')
response = cloudwatch.put_metric_data(
MetricData = [
    {
        'MetricName': 'HttpResponseTime',
        'Dimensions': [
            {
                'Name': 'Server',
                'Value': 'app.example.com'
            },
            {
                'Name': 'Client',
                'Value': 'Client-ABC'
            },
        ],
        'Unit': 'Milliseconds',
        'Value': random.randint(20, 50)
    },
],
Namespace = 'statusdash/ec2client'
)
print response
</code></pre>

<h2>Resources:</h2>

<p><a href="https://stackify.com/custom-metrics-aws-lambda/">https://stackify.com/custom-metrics-aws-lambda/</a>
<a href="https://www.syntouch.nl/custom-cloudwatch-metrics-in-python-yes-we-can/">https://www.syntouch.nl/custom-cloudwatch-metrics-in-python-yes-we-can/</a> &lt;- psutil
<a href="https://aws.amazon.com/blogs/devops/new-how-to-better-monitor-your-custom-application-metrics-using-amazon-cloudwatch-agent/">https://aws.amazon.com/blogs/devops/new-how-to-better-monitor-your-custom-application-metrics-using-amazon-cloudwatch-agent/</a>
<a href="https://medium.com/@mrdoro/aws-lambda-as-the-website-monitoring-tool-184b09202ae2">https://medium.com/@mrdoro/aws-lambda-as-the-website-monitoring-tool-184b09202ae2</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Tag All Your AWS IAM Users With Python]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/02/25/how-to-tag-all-your-aws-iam-users-with-python/"/>
    <updated>2019-02-25T06:44:55-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/02/25/how-to-tag-all-your-aws-iam-users-with-python</id>
    <content type="html"><![CDATA[<p>Let&rsquo;s say that all your IAM users are named in <code>name.surname</code> and your system accounts are named as <code>my-system-account</code> and you find yourself in a position that you need to tag all your IAM users based on Human/System account type.</p>

<script src="//ap.lijit.com/www/delivery/fpi.js?z=601358&width=300&height=250"></script>


<p>With AWS and Python&rsquo;s Boto library, it makes things easy. We would list all our users, loop through each one and tag them with the predefined tag values that we chose.</p>

<h2>Batch Tagging AWS IAM Users with Python</h2>

<p>This script wil tag all users with the tag: Name, Email, Environment and Account_Type.</p>

<pre><code class="python">import boto3

iam = boto3.Session(profile_name='test', region_name='eu-west-1').client('iam')
paginator = iam.get_paginator('list_users')

iam_environment = 'test'

unstructed_users = []
userlist = []
taggable_users = []
already_tagged_users = []
email_address_domain = '@example.com'

# generate tag list based on account type
def tag_template(username, environment):
    if '.' in username:
        account_type = 'human'
    email = username
    else:
        account_type = 'system'
    email = 'system-admin'

    template = [
        {'Key': 'Name','Value': username.lower()}, 
        {'Key': 'Email', 'Value': email.lower() + email_address_domain}, 
        {'Key': 'Environment','Value': environment}, 
        {'Key': 'Account_Type','Value': account_type}
    ]

    return template

# generate userlist
for response in paginator.paginate():
    unstructed_users.append(response['Users'])

for iteration in range(len(unstructed_users)):
    for userobj in range(len(unstructed_users[iteration])):
        userlist.append((unstructed_users[iteration][userobj]['UserName']))

# generate taggable userlist:
for user in userlist:
    tag_response = iam.list_user_tags(UserName=user)
    if len(tag_response['Tags']) == 0:
        taggable_users.append(user)
    else:
        already_tagged_users.append(user)

# tag users from taggable_list
for tag_user in taggable_users:
    user_template = tag_template(tag_user, iam_environment)
    print(tag_user, user_template)
    response = iam.tag_user(UserName=tag_user, Tags=user_template)

# print lists
print('Userlists: {}'.format(userlist))
print('Taggable Users: {}'.format(taggable_users))
print('Already Tagged Users: {}'.format(already_tagged_users))
</code></pre>

<p>After it completes, your IAM users should be tagged in the following format:</p>

<pre><code>Name: john.doe
Email: john.doe@example.com
Environment: test
Account_Type: human

or:

Name: system-account
Email: system-admin@example.com
Environment: test
Account-Type: system
</code></pre>

<h2>Thank You</h2>

<p>Please feel free to show support by, <strong>sharing</strong> this post, making a <strong>donation</strong>, <strong>subscribing</strong> or <strong>reach out to me</strong> if you want me to demo and write up on any specific tech topic.</p>

<center>
<form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_top">
<input type="hidden" name="cmd" value="_s-xclick" />
<input type="hidden" name="hosted_button_id" value="W7CBGYTCWGANQ" />
<input type="image" src="https://user-images.githubusercontent.com/567298/49853901-461c3700-fdf1-11e8-9d80-8a424a3173af.png" border="0" name="submit" title="PayPal - The safer, easier way to pay online!" alt="Donate with PayPal button" />
<img alt="" border="0" src="https://www.paypal.com/en_ZA/i/scr/pixel.gif" width="1" height="1" />
</form>
</center>


<p><br></p>

<script type="text/javascript">
  ( function() {
    if (window.CHITIKA === undefined) { window.CHITIKA = { 'units' : [] }; };
    var unit = {"calltype":"async[2]","publisher":"rbekker87","width":728,"height":90,"sid":"Chitika Default"};
    var placement_id = window.CHITIKA.units.length;
    window.CHITIKA.units.push(unit);
    document.write('<div id="chitikaAdBlock-' + placement_id + '"></div>');
}());
</script>


<script type="text/javascript" src="//cdn.chitika.net/getads.js" async></script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Parallel Processing on AWS Lambda With Python Using Multiprocessing]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/02/19/parallel-processing-on-aws-lambda-with-python-using-multiprocessing/"/>
    <updated>2019-02-19T09:39:47-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/02/19/parallel-processing-on-aws-lambda-with-python-using-multiprocessing</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/53020033-c51b9480-345e-11e9-9625-b73062e2464a.png" alt="" /></p>

<p>If you are trying to use <code>multiprocessing.Queue</code> or <code>multiprocessing.Pool</code> on AWS Lambda, you are probably getting the exception:</p>

<pre><code>[Errno 38] Function not implemented: OSError

    sl = self._semlock = _multiprocessing.SemLock(kind, value, maxvalue)
OSError: [Errno 38] Function not implemented
</code></pre>

<p>The reason for that is due to the Lambda execution environment not having support on shared memory for processes, therefore you canâ€™t use <code>multiprocessing.Queue</code> or <code>multiprocessing.Pool</code>.</p>

<p>As a workaround, Lambda does support the usage of <code>multiprocessing.Pipe</code> instead of Queue.</p>

<h2>Parallel Processing on Lambda Example</h2>

<p>Below is a very basic example on how you would achieve the task of executing parallel processing on AWS Lambda for Python:</p>

<pre><code class="python">import time
import multiprocessing

region_maps = {
        "eu-west-1": {
            "dynamodb":"dynamodb.eu-west-1.amazonaws.com"
        },
        "us-east-1": {
            "dynamodb":"dynamodb.us-east-1.amazonaws.com"
        },
        "us-east-2": {
            "dynamodb": "dynamodb.us-east-2.amazonaws.com"
        }
    }

def multiprocessing_func(region):
    time.sleep(1)
    endpoint = region_maps[region]['dynamodb']
    print('endpoint for {} is {}'.format(region, endpoint))

def lambda_handler(event, context):
    starttime = time.time()
    processes = []
    regions = ['us-east-1', 'us-east-2', 'eu-west-1']
    for region in regions:
        p = multiprocessing.Process(target=multiprocessing_func, args=(region,))
        processes.append(p)
        p.start()

    for process in processes:
        process.join()

    output = 'That took {} seconds'.format(time.time() - starttime)
    print(output)
    return output
</code></pre>

<p>The output when the function gets invoked:</p>

<pre><code>pid: 30913 - endpoint for us-east-1 is dynamodb.us-east-1.amazonaws.com
pid: 30914 - endpoint for us-east-2 is dynamodb.us-east-2.amazonaws.com
pid: 30915 - endpoint for eu-west-1 is dynamodb.eu-west-1.amazonaws.com
That took 1.014902114868164 seconds
</code></pre>

<h2>Thank You</h2>

<p>Please feel free to show support by, <strong>sharing</strong> this post, making a <strong>donation</strong>, <strong>subscribing</strong> or <strong>reach out to me</strong> if you want me to demo and write up on any specific tech topic.</p>

<center>
<form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_top">
<input type="hidden" name="cmd" value="_s-xclick" />
<input type="hidden" name="hosted_button_id" value="W7CBGYTCWGANQ" />
<input type="image" src="https://user-images.githubusercontent.com/567298/49853901-461c3700-fdf1-11e8-9d80-8a424a3173af.png" border="0" name="submit" title="PayPal - The safer, easier way to pay online!" alt="Donate with PayPal button" />
<img alt="" border="0" src="https://www.paypal.com/en_ZA/i/scr/pixel.gif" width="1" height="1" />
</form>
</center>


<p><br></p>

<script type="text/javascript">
  ( function() {
    if (window.CHITIKA === undefined) { window.CHITIKA = { 'units' : [] }; };
    var unit = {"calltype":"async[2]","publisher":"rbekker87","width":728,"height":90,"sid":"Chitika Default"};
    var placement_id = window.CHITIKA.units.length;
    window.CHITIKA.units.push(unit);
    document.write('<div id="chitikaAdBlock-' + placement_id + '"></div>');
}());
</script>


<script type="text/javascript" src="//cdn.chitika.net/getads.js" async></script>

]]></content>
  </entry>
  
</feed>
