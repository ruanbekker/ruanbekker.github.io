<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Aws | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/aws/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2019-09-12T22:52:02+02:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Expire Objects in AWS S3 Automatically After 30 Days]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/09/12/expire-objects-in-aws-s3-automatically-after-30-days/"/>
    <updated>2019-09-12T22:37:11+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/09/12/expire-objects-in-aws-s3-automatically-after-30-days</id>
    <content type="html"><![CDATA[<p><img src="https://objects.ruanbekker.com/assets/images/aws-logo.png" alt="" /></p>

<p>In AWS S3 you can make use of lifecycle policies to manage the lifetime of your objects stored in S3.</p>

<p>In this tutorial, I will show you how to delete objects automatically from S3 after 30 days.</p>

<h2>Navigate to your Bucket</h2>

<p>Head over to your AWS S3 bucket where you want to delete objects after they have been stored for 30 days:</p>

<p><img width="1039" alt="0400F9CB-9223-4FDF-8FA5-D0BC1FA8EB71" src="https://user-images.githubusercontent.com/567298/64819546-c3f2b600-d5ae-11e9-93ba-13777e9b02b0.png"></p>

<h2>Lifecycle Policies</h2>

<p>Select &ldquo;Management&rdquo; and click on &ldquo;Add lifecycle rule&rdquo;:</p>

<p><img width="701" alt="9BB26C7C-F251-45C4-AE44-A34459BD0F4B" src="https://user-images.githubusercontent.com/567298/64819628-f00e3700-d5ae-11e9-9740-8aa3608163a7.png"></p>

<p>Set a rule name of choice and you have the option to provide a prefix if you want to delete objects based on a specific prefix. I will leave this blank as I want to delete objects in the root level of the bucket. Head to next on the following section:</p>

<p><img width="700" alt="AEF8B151-3FA8-454F-AC71-778A531BD1EE" src="https://user-images.githubusercontent.com/567298/64819785-58f5af00-d5af-11e9-8485-fb0dca3a02ac.png"></p>

<p>From the &ldquo;Transitions&rdquo; section, configure the transition section, by selecting to expire the current version of the object after 30 days:</p>

<p><img width="701" alt="2B395671-A4C0-4E5A-82E7-00EE6579DB5A" src="https://user-images.githubusercontent.com/567298/64819851-7c205e80-d5af-11e9-98d7-7e1dd09bcfef.png"></p>

<p>Review the configuration:</p>

<p><img width="705" alt="F7F8E800-62FF-4156-B506-5FB9BCC148E0" src="https://user-images.githubusercontent.com/567298/64819869-893d4d80-d5af-11e9-8034-8a2e3a8939f8.png"></p>

<p>When you select &ldquo;Save&rdquo;, you should be returned to the following section:</p>

<p><img width="1041" alt="8421EBCE-9503-4259-92AA-DB66C6F532AF" src="https://user-images.githubusercontent.com/567298/64819895-99edc380-d5af-11e9-84b4-7f4cc69cfd2e.png"></p>

<h2>Housecleaning on your S3 Bucket</h2>

<p>Now 30 days after you created objects on AWS S3, they will be deleted.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AWS S3 KMS and Python for Secrets Management]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/09/04/aws-s3-kms-and-python-for-secrets-management/"/>
    <updated>2019-09-04T19:58:45+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/09/04/aws-s3-kms-and-python-for-secrets-management</id>
    <content type="html"><![CDATA[<p><img src="https://miro.medium.com/max/2400/1*9PSzVZDHjr321CpxJHcxPQ.png" alt="" /></p>

<p>So your application need to store secrets and you are looking for a home for them. In this tutorial we will see how we can use Python, S3 and KMS to build our own solution for managing secrets.</p>

<p>There is SSM and Secrets Manager that probably does a better job, but my mind got curious :D</p>

<h2>High Level Goal</h2>

<p>From a High-Level we want to store secrets encrypted on S3 with KMS, namespaced with <strong>team/application/environment/value</strong> in json format so that our application receives the json dictionary of configured key/value pairs.</p>

<p>We can leverage <strong>IAM</strong> to delegate permissions on the namespacing that we decide on, for my example the namespace will look like this on S3:</p>

<pre><code>s3://s3bucket/secrets/engineering/app1/production/appconfig.json
</code></pre>

<p>We will apply <strong>IAM</strong> permissions for our user to only <strong>Put</strong> and <strong>Get</strong> on <code>secrets/engineering*</code>. So with this idea we can apply IAM permissions on groups for different departments, or even let users manage their own secrets such as:</p>

<pre><code>s3://s3bucket/secrets/personal/user.name/app/appconfig.json
</code></pre>

<p>After the object has been downloaded from S3 and decrypted using KMS, the value of the object will look like this:</p>

<pre><code>{u'surname': u'bekker', u'name': u'ruan', u'job_title': u'systems-development-engineer'}
</code></pre>

<h2>Requirements</h2>

<p>We will create the following resources on AWS:</p>

<ul>
<li>KMS Key</li>
<li>S3 Bucket</li>
<li>IAM User</li>
<li>IAM Policy</li>
<li>Python Dependencies: Boto3</li>
</ul>


<h2>Provision AWS Resources</h2>

<p><img src="https://miro.medium.com/max/2728/1*Lq9xaUXuNo2Nb8kQakYdsg.png" alt="" /></p>

<p>First we will create our <strong>S3 Bucket</strong>,  head over to <a href="https://s3.console.aws.amazon.com/s3/home?region=eu-west-1">Amazon S3</a> create a new s3 bucket, make sure that the bucket is <strong>NOT</strong> public, by using the default configuration, you should be good.</p>

<p>Once your S3 Bucket is provisioned, head over to <a href="https://console.aws.amazon.com/iam/home#/users">Amazon IAM</a> and create a IAM User, enable programmatic access, and keep your access key and secret key safe. For now we will not apply any permissions as we will come back to this step.</p>

<p>Head over to <a href="https://eu-west-1.console.aws.amazon.com/kms/home?region=eu-west-1#/kms/home">Amazon KMS</a> and create a KMS Key, we will define the <strong>key administrator</strong>, which will be my user (ruan.bekker in this case) with more privileged permissions:</p>

<p><img src="https://miro.medium.com/max/5120/1*EUPWbCQ8nsfbBWHQI6srYw.png" alt="" /></p>

<p>and then we will define the <strong>key usage permissions</strong> (app.user in this case), which will be the user that we provisioned from the previous step, this will be the user that will encrypt and decrypt the data:</p>

<p><img src="https://miro.medium.com/max/5120/1*5xA5H0qpJ1FYTG1hUjy_Tw.png" alt="" /></p>

<p>Next, review the policy generated from the previous selected sections:</p>

<p><img src="https://miro.medium.com/max/5120/1*bLDVPFaZUDQ4EyWjACYRUw.png" alt="" /></p>

<p>Once you select finish, you will be returned to the section where your KMS Key information will be displayed, keep note of your <strong>KMS Key Alias</strong>, as we will need it later:</p>

<p><img src="https://miro.medium.com/max/5120/1*aooUMS0OyEd5hopnOUrcmA.png" alt="" /></p>

<h2>Create a IAM Policy for our App User</h2>

<p>Next we will create the IAM Policy for the user that will encrypt/decrypt and store data in S3</p>

<pre><code class="json">{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "S3PutAndGetAccess",
            "Effect": "Allow",
            "Action": [
                "s3:PutObject",
                "s3:GetObject"
            ],
            "Resource": "arn:aws:s3:::arn:aws:s3:::s3-bucket-name/secrets/engineering*"
        },
        {
            "Sid": "KMSDecryptAndEncryptAccess",
            "Effect": "Allow",
            "Action": [
                "kms:Decrypt",
                "kms:Encrypt"
            ],
            "Resource": "arn:aws:kms:eu-west-1:123456789012:key/xxxx-xxxx-xxxx-xxxx-xxxx"
        }
    ]
}
</code></pre>

<p>After the policy has been saved, associate the policy to the IAM User</p>

<h2>Encrypt and Put to S3</h2>

<p>Now we will use Python to define the data that we want to <strong>store in S3</strong>, we will then <strong>encrypt</strong> the data with <strong>KMS</strong>, use base64 to <strong>encode</strong> the ciphertext and push the encrypted value to <strong>S3</strong>, with Server Side Encryption enabled, which we will also use our KMS key.</p>

<p>Install boto3 in Python:</p>

<pre><code class="bash">$ pip install boto3
</code></pre>

<p>Enter the Python REPL and import the required packages, we will also save the access key and secret key as variables so that we can use it with boto3. You can also save it to the <a href="https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html">credential provider</a> and utilise the profile name:</p>

<pre><code class="python">&gt;&gt;&gt; import boto3
&gt;&gt;&gt; import json
&gt;&gt;&gt; import base64
&gt;&gt;&gt; aws_access_key_id='redacted'
&gt;&gt;&gt; aws_secret_access_key='redacted'
</code></pre>

<p>Next define the data that we want to <strong>encrypt and store</strong> in S3:</p>

<pre><code class="python">&gt;&gt;&gt; mydata = {
    "name": "ruan",
    "surname": "bekker",
    "job_title": "systems-development-engineer"
}
</code></pre>

<p>Next we will use KMS to encrypt the data and use base64 to encode the ciphertext:</p>

<pre><code class="python">&gt;&gt;&gt; kms = boto3.Session(
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key
).client('kms')
&gt;&gt;&gt; ciphertext = kms.encrypt(
    KeyId='alias/secrets-key',
    Plaintext=json.dumps(mydata)
)
&gt;&gt;&gt; encoded_ciphertext = base64.b64encode(ciphertext["CiphertextBlob"])
# preview the data
&gt;&gt;&gt; encoded_ciphertext
'AQICAHiKOz...42720nCleoI26UW7P89lPdwvV8Q=='
</code></pre>

<p>Next we will use S3 to push the encrypted data onto S3 in our name spaced key: <strong>secrets/engineering/app1/production/appconfig.json</strong></p>

<pre><code class="python">&gt;&gt;&gt; s3 = boto3.Session(
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key,
    region_name='eu-west-1'
).client('s3')
&gt;&gt;&gt; response = s3.put_object(
    Body=encoded_ciphertext,
    Bucket='ruan-secret-store',
    Key='secrets/engineering/app1/production/appconfig.json',
    ServerSideEncryption='aws:kms',
    SSEKMSKeyId='alias/secrets-key'
)
</code></pre>

<p>Now our object is stored in S3, encrypted with KMS and ServerSideEncryption Enabled.</p>

<p>You can try to download the object and decode the base64 encoded file and you will find that its complete garbage as its encrypted.</p>

<p>Next we will use S3 to Get the object and use KMS to decrypt and use base64 to decode after the object has been decrypted:</p>

<pre><code class="python">&gt;&gt;&gt; response = s3.get_object(
    Bucket='ruan-secret-store',
    Key='secrets/engineering/app1/production/appconfig.json'
)
&gt;&gt;&gt; encoded_ciphertext = response['Body'].read()
&gt;&gt;&gt; encoded_ciphertext
'AQICAHiKOz...42720nCleoI26UW7P89lPdwvV8Q=='
</code></pre>

<p>Now let’s decode the result with base64:</p>

<pre><code class="python">&gt;&gt;&gt; decoded_ciphertext = base64.b64decode(encoded_ciphertext)
&gt;&gt;&gt; plaintext = kms.decrypt(CiphertextBlob=bytes(decoded_ciphertext))
</code></pre>

<p>Now we need to deserialize the JSON as it’s in string format:</p>

<pre><code>&gt;&gt;&gt; json.loads(plaintext["Plaintext"])
{u'surname': u'bekker', u'name': u'ruan', u'job_title': u'systems-development-engineer'}
</code></pre>

<h2>Using it in a Application</h2>

<p>Let’s say you are using <strong>Docker</strong> and you want to bootstrap your application configs to your environment that you are retrieving from S3.</p>

<p>We will use a <code>get_secrets.py</code> python script that will read the data into memory, decrypt and write the values in plaintext to disk, then we will use the <code>boot.sh</code> script to read the values into the environment and remove the temp file that was written to disk, then start the application since we have the values stored in our environment.</p>

<p>Our <strong>&ldquo;application&rdquo;</strong> in this example will just be a line of echo to return the values for demonstration.</p>

<p>The <code>get_secrets.py</code> file:</p>

<pre><code class="python">import boto3
import json
import base64

aws_access_key_id='redacted'
aws_secret_access_key='redacted'

kms = boto3.Session(aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key).client('kms')
s3 = boto3.Session(aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, region_name='eu-west-1').client('s3')

response = s3.get_object(Bucket='ruan-secret-store', Key='secrets/engineering/app1/production/appconfig.json')
encoded_ciphertext = response['Body'].read()

decoded_ciphertext = base64.b64decode(encoded_ciphertext)
plaintext = kms.decrypt(CiphertextBlob=bytes(decoded_ciphertext))
values = json.loads(plaintext["Plaintext"])

with open('envs.tmp', 'w') as f:
    for key in values.keys():
        f.write("{}={}".format(key.upper(), values[key]) + "\n")
</code></pre>

<p>And our <code>boot.sh</code> script:</p>

<pre><code class="bash">#!/usr/bin/env bash
source ./envs.tmp
rm -rf ./envs.tmp
echo "Hello, my name is ${NAME} ${SURNAME}, and I am a ${JOB_TITLE}"
</code></pre>

<p>Running that will produce:</p>

<pre><code>$ bash boot.sh
Hello, my name is ruan bekker, and I am a systems-development-engineer
</code></pre>

<h2>Thank You</h2>

<p>And there we have a simple and effective way of encrypting/decrypting data using S3, KMS and Python at a ridiculously cheap cost, its almost free.</p>

<p>If you liked my content, feel free to checkout my content on <strong><a href="https://ruan.dev/">ruan.dev</a></strong> or follow me on twitter at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using OpenFaas With Amazon DynamoDB]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/07/07/using-openfaas-with-amazon-dynamodb/"/>
    <updated>2019-07-07T01:11:23+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/07/07/using-openfaas-with-amazon-dynamodb</id>
    <content type="html"><![CDATA[<p><img width="1105" alt="image" src="https://user-images.githubusercontent.com/567298/60761941-f4205480-a053-11e9-9ad5-9e45948c9833.png"></p>

<p><a href="https://saythanks.io/to/ruanbekker"><img src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" alt="Say Thanks!" /></a> <a href="https://linux-hackers-slack.herokuapp.com/"><img src="https://linux-hackers-slack.herokuapp.com/badge.svg" alt="Slack Status" /></a> <a href="https://linux-hackers.slack.com/"><img src="https://img.shields.io/badge/chat-on_slack-orange.svg" alt="Chat on Slack" /></a> <img src="https://img.shields.io/github/followers/ruanbekker.svg?label=Follow&amp;style=social" alt="GitHub followers" /> <img src="https://img.shields.io/twitter/follow/ruanbekker.svg?style=social" alt="Twitter Follow" /></p>

<h1>Using OpenFaaS with Amazon DynamoDB</h1>

<p>You can use your OpenFaaS functions to store and retrieve data to and from a persistent layer that sits outside the OpenFaaS framework. The database that we will use in this tutorial is Amazon&rsquo;s DynamoDB.</p>

<p>If you are not familiar with the service, Amazon&rsquo;s DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability.</p>

<p>At the end of this tutorial you will be able to invoke your functions to read and write items to DynamoDB with a dedicated IAM User that is only allowed to access DynamoDB, and secrets managed by your OpenFaaS framework.</p>

<h2>What we will be doing in this Tutorial</h2>

<p>In this tutorial we will cover a couple of things, and a summary on the to do list is:</p>

<ul>
<li>Create a OpenFaaS IAM User, DynamoDB IAM Policy, associate the Policy to the User using the AWS CLI</li>
<li>Create a AWS Access Key, and save the Access Key and Secret key to file</li>
<li>Create OpenFaaS Secrets of the Access Key and Secret Key, remove the files from disk</li>
<li>Create 3 OpenFaaS Functions: write, lookup and get</li>
<li>Invoke the functions, read and write from DynamoDB</li>
</ul>


<p>Our 3 functions will do very basic operations for this demonstration, but I believe this is a good starting point.</p>

<p>All the examples of this blog post is available in <a href="https://github.com/ruanbekker/blog-assets/tree/master/openfaas-dynamodb">this github repository</a></p>

<h2>The Use-Case Scenario</h2>

<p>In this scenario we want to store user information into DynamoDB, we will use a hash that we will calculate using the users ID Number + Lastname. So when we have thousands or millions of items, we dont need to search through the entire table, but since we can re-calculate the sha hash, we can do a single GetItem operation to find the entry about the user in question.</p>

<ul>
<li>Lookup Function:</li>
</ul>


<p>The lookup function will calculate the hash by passing the users ID Number and Lastname, this will return a hash which will be teh primary key attribute of our table design. This hash value is required to do a GetItem on the user in question.</p>

<ul>
<li>Get Function:</li>
</ul>


<p>The Get function will interface with DynamoDB, it reads the AWS access key and secret key from the secrets path to authenticate with AWS and utilizes environment variables for the region and table name. This will do a GetItem on the DynamoDB Table and retrieve the Item. If the item is not found, it will return it in the response.</p>

<ul>
<li>Write Function:</li>
</ul>


<p>The write function will also interface with DynamoDB, the ID, Name and Payload will be included in the request body on our POST Request.</p>

<h2>Note on Secrets and Environment Variables</h2>

<p>I am treating my environment variables and secrets different from each other. The secrets such as my AWS access keys are stored on the cluster and the application reads them and stores the values in memory.</p>

<p>The environment variables such as non-secret information, such as my dynamodb table name and aws region, is defined in my environment variables.</p>

<p>This <a href="http://movingfast.io/articles/environment-variables-considered-harmful/">post</a> and this <a href="https://diogomonica.com/2017/03/27/why-you-shouldnt-use-env-variables-for-secret-data/">post</a> goes a bit more into detail on why you should not use environment variables for secret data, which I found from <a href="https://github.com/openfaas/faas-netes/issues/153#issuecomment-370924478">this link</a></p>

<p>Enough info, let&rsquo;s get to the fun stuff</p>

<h2>Pre-Requirements:</h2>

<p>You need a AWS Account (or you can use dynamodb-local), OpenFaaS and faas-cli. Documentation available below:
- <a href="https://docs.openfaas.com/contributing/get-started/">https://docs.openfaas.com/contributing/get-started/</a></p>

<h2>Provision a DynamoDB Table</h2>

<p>I have a admin IAM account configured on my default profile, using the aws-cli tools generate the cli-skeleton that is required to provision a dynamodb table:</p>

<pre><code class="bash">$ aws dynamodb create-table --generate-cli-skeleton &gt; ddb.json
</code></pre>

<p>My table name will be <code>lookup-table</code> with the primary key <code>hash_value</code> and provisoned my throughput to 1 Read and Write Capacity Unit. Which will enable us 4KB/s for reads and 1KB/s for writes.</p>

<p>For demonstration purposes, I am sharing my altered <code>ddb.json</code> file:</p>

<pre><code class="json">{
    "AttributeDefinitions": [
        {
            "AttributeName": "hash_value",
            "AttributeType": "S"
        }
    ],
    "TableName": "lookup_table",
    "KeySchema": [
        {
            "AttributeName": "hash_value",
            "KeyType": "HASH"
        }
    ],
    "ProvisionedThroughput": {
        "ReadCapacityUnits": 1,
        "WriteCapacityUnits": 1
    },
    "Tags": [
        {
            "Key": "Name",
            "Value": "lookup-table"
        }
    ]
}
</code></pre>

<p>Now that we have the file saved, create the dynamodb table:</p>

<pre><code class="bash">$ aws dynamodb create-table --cli-input-json file://ddb.json
</code></pre>

<p>List the tables:</p>

<pre><code class="bash">$ aws dynamodb list-tables
{
    "TableNames": [
        "lookup_table"
    ]
}
</code></pre>

<p>Check if the table is provisioned:</p>

<pre><code class="bash">$ aws dynamodb describe-table --table-name lookup_table | jq -r '.Table.TableStatus'
ACTIVE
</code></pre>

<p>Getting the ARN string, as we will need it when we create our IAM Policy:</p>

<pre><code class="bash">$ aws dynamodb describe-table --table-name lookup_table | jq -r '.Table.TableArn'
arn:aws:dynamodb:eu-west-1:x-x:table/lookup_table
</code></pre>

<h2>Create the OpenFaaS IAM User</h2>

<p>Create the IAM Policy document which defines the access that we want to grant. You can see that we are only allowing Put and GetItem on the provisioned DynamoDB resource:</p>

<pre><code class="bash">$ cat dynamodb-iam-policy.json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "OpenFaasFunctionAceessForDynamoDB",
            "Effect": "Allow",
            "Action": [
                "dynamodb:PutItem",
                "dynamodb:GetItem"
            ],
            "Resource": "arn:aws:dynamodb:eu-west-1:x-accountid-x:table/lookup_table"
        }
    ]
}
</code></pre>

<p>Create the IAM Policy and provide the policy document for the given policy name:</p>

<pre><code class="bash">$ aws iam create-policy --policy-name openfaas-dynamodb-access --policy-document file://dynamodb-iam-policy.json
{
    "Policy": {
        "PolicyName": "openfaas-dynamodb-access",
        "PolicyId": "ANPATPRT2G4SL4K63SUWQ",
        "Arn": "arn:aws:iam::x-accountid-x:policy/openfaas-dynamodb-access",
        "Path": "/",
        "DefaultVersionId": "v1",
        "AttachmentCount": 0,
        "PermissionsBoundaryUsageCount": 0,
        "IsAttachable": true,
        "CreateDate": "2019-07-06T11:54:26Z",
        "UpdateDate": "2019-07-06T11:54:26Z"
    }
}
</code></pre>

<p>Create the IAM User that will be used to authenticate requests against DynamoDB:</p>

<pre><code class="bash">$ aws iam create-user --user-name openfaas-user
{
    "User": {
        "Path": "/",
        "UserName": "openfaas-user",
        "UserId": "AIDATPRT2G4SIRYTNHLZK",
        "Arn": "arn:aws:iam::x-accountid-x:user/openfaas-user",
        "CreateDate": "2019-07-06T11:56:53Z"
    }
}
</code></pre>

<p>Create the Access Key, which will be our API keys for our application to authenticate requests. Save the AccessKeyId and SecretAccessKey temporarily to 2 seperate files, which we will delete after we create our secrets to our cluster:</p>

<pre><code class="bash">$ aws iam create-access-key --user-name openfaas-user
{
    "AccessKey": {
        "UserName": "openfaas-user",
        "AccessKeyId": "AKIAT..redacted.x",
        "Status": "Active",
        "SecretAccessKey": "b..redacted.x",
        "CreateDate": "2019-07-06T11:57:37Z"
    }
}
</code></pre>

<p>Associate the IAM Policy to the IAM User:</p>

<pre><code class="bash">$ aws iam attach-user-policy --user-name openfaas-user --policy-arn arn:aws:iam::x-x:policy/openfaas-dynamodb-access
</code></pre>

<p>To test if the access keys work, save them to a new profile using the aws-cli tools:</p>

<pre><code class="bash">$ aws configure --profile openfaas
AWS Access Key ID [None]: AKIAT..
AWS Secret Access Key [None]: b..x
Default region name [None]: eu-west-1
Default output format [None]: json
</code></pre>

<p>Write an Item to DynamoDB:</p>

<pre><code class="bash">$ aws --profile openfaas dynamodb put-item \
--table-name lookup_table \
--item '{"hash_value": {"S": "aGVsbG8td29ybGQK"}, "message": {"S": "hello-world"}}'
</code></pre>

<p>Read the Item from DynamoDB:</p>

<pre><code class="bash">$ aws --profile openfaas dynamodb get-item \
--table-name lookup_table \
--key '{"hash_value": {"S": "aGVsbG8td29ybGQK"}}'
{
    "Item": {
        "hash_value": {
            "S": "aGVsbG8td29ybGQK"
        },
        "message": {
            "S": "hello-world"
        }
    }
}
</code></pre>

<p>We can now confirm our permissions are in place to continue.</p>

<h3>Create OpenFaaS Secrets</h3>

<p>The AccessKeyId and SecretKey has been saved to disk, and we will use those files to create secrets from:</p>

<pre><code class="bash">$ faas-cli secret create openfaas-aws-access-key --from-file=openfaas_aws_access_key.txt
Creating secret: openfaas-aws-access-key
Created: 201 Created
</code></pre>

<pre><code class="bash">$ faas-cli secret create openfaas-aws-secret-key --from-file=openfaas_aws_secret_key.txt
Creating secret: openfaas-aws-secret-key
Created: 201 Created
</code></pre>

<p>Now that the secrets are securely stored in our cluster, we can delete the temporary files:</p>

<pre><code>$ rm -f ./openfaas_aws_*_key.txt
</code></pre>

<h2>Login to OpenFaaS</h2>

<p>Login to OpenFaasS using faas-cli:</p>

<pre><code class="bash">$ faas-cli login \
--gateway https://openfaas.domain.com \
--username ${OPENFAAS_USER} \
--password ${OPENFAAS_PASSWORD}
</code></pre>

<p>Export the OPENFAAS_URL:</p>

<pre><code class="bash">$ export OPENFAAS_URL=https://openfaas.domain.com
</code></pre>

<h2>One Stack File for All 3 Functions:</h2>

<p>We will create our first function to generate the yaml definition, then we will rename our generated filename to <code>stack.yml</code> then the next 2 functions, we will use the append flag to append the functions yaml to our <code>stack.yml</code> file, so that we can simply use <code>faas-cli up</code></p>

<h2>Create the Lookup Function:</h2>

<p>Create a Python3 Function, and prefix it with your dockerhub user:</p>

<pre><code class="bash">$ faas-cli new \
--lang python3 fn-dynamodb-lookup \
--prefix=ruanbekker \
--gateway https://openfaas.domain.com

Function created in folder: fn-foo
Stack file written: fn-dynamodb-lookup.yml
</code></pre>

<p>As we will be using one stack file, rename the generated stack file:</p>

<pre><code class="bash">$ mv fn-dynamodb-lookup.yml stack.yml
</code></pre>

<p>Open the stack file and set the environment variables:</p>

<pre><code class="bash">$ cat stack.yml
provider:
  name: openfaas
  gateway: https://openfaas.domain.com
functions:
  fn-dynamodb-lookup:
    lang: python3
    handler: ./fn-dynamodb-lookup
    image: ruanbekker/fn-dynamodb-lookup:latest
    environment:
      dynamodb_region: eu-west-1
      dynamodb_table: lookup_table
</code></pre>

<p>The python code for our function:</p>

<pre><code class="bash">$ cat fn-dynamodb-lookup/handler.py
</code></pre>

<pre><code class="python">import json
import hashlib

def calc_sha(id_number, lastname):
    string = json.dumps({"id": id_number, "lastname": lastname}, sort_keys=True)
    hash_value = hashlib.sha1(string.encode("utf-8")).hexdigest()
    return hash_value

def handle(req):
    event = json.loads(req)
    hash_value = calc_sha(event['id'], event['lastname'])
    return hash_value
</code></pre>

<h2>Create the Write Function:</h2>

<p>Create a Python3 Function, and prefix it with your dockerhub user, and use the append flag to update our stack file:</p>

<pre><code class="bash">$ faas-cli new \
--lang python3 fn-dynamodb-write \
--prefix=ruanbekker \
--gateway https://openfaas.domain.com
--append stack.yml

Function created in folder: fn-dynamodb-write
Stack file updated: stack.yml
</code></pre>

<p>Open the stack file and set the environment variables and include the secrets that was created:</p>

<pre><code class="bash">$ cat stack.yml
provider:
  name: openfaas
  gateway: https://openfaas.domain.com
functions:
  fn-dynamodb-lookup:
  # ...
  fn-dynamodb-write:
    lang: python3
    handler: ./fn-dynamodb-write
    image: ruanbekker/fn-dynamodb-write:latest
    environment:
      dynamodb_region: eu-west-1
      dynamodb_table: lookup_table
    secrets:
      - openfaas-aws-access-key
      - openfaas-aws-secret-key
</code></pre>

<p>Our function relies on a external dependency which we need to install to interact with aws:</p>

<pre><code class="bash">$ cat fn-dynamodb-write/requirements.txt
boto3
</code></pre>

<p>Our python code for our function:</p>

<pre><code class="bash">$ cat fn-dynamodb-write/handler.py
</code></pre>

<pre><code class="python">import boto3
import os
import json
import hashlib
import datetime

aws_key = open('/var/openfaas/secrets/openfaas-aws-access-key', 'r').read()
aws_secret = open('/var/openfaas/secrets/openfaas-aws-secret-key', 'r').read()
dynamodb_region = os.environ['dynamodb_region']
dynamodb_table  = os.environ['dynamodb_table']

client = boto3.Session(region_name=dynamodb_region).resource('dynamodb', aws_access_key_id=aws_key, aws_secret_access_key=aws_secret)
table = client.Table(dynamodb_table)

def calc_sha(id_number, lastname):
    string = json.dumps({"id": id_number, "lastname": lastname}, sort_keys=True)
    hash_value = hashlib.sha1(string.encode("utf-8")).hexdigest()
    return hash_value

def create_timestamp():
    response = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M")
    return response

def handle(req):
    event = json.loads(req)
    unique_id = calc_sha(event['id'], event['lastname'])
    response = table.put_item(
        Item={
            'hash_value': unique_id,
            'timestamp': create_timestamp(),
            'payload': event['payload']
        }
    )
    return response
</code></pre>

<h2>Create the Get Function:</h2>

<p>Create a Python3 Function, and prefix it with your dockerhub user, and use the append flag to specify the stack file:</p>

<pre><code class="bash">$ faas-cli new \
--lang python3 fn-dynamodb-get \
--prefix=ruanbekker \
--gateway https://openfaas.domain.com
--append stack.yml

Function created in folder: fn-dynamodb-get
Stack file updated: stack.yml
</code></pre>

<p>Open the stack file and set the environment variables and include the secrets that was created:</p>

<pre><code class="bash">$ cat stack.yml
provider:
  name: openfaas
  gateway: https://openfaas.domain.com
functions:
  fn-dynamodb-lookup:
  # .. 
  fn-dynamodb-write:
  # ..
  fn-dynamodb-get:
    lang: python3
    handler: ./fn-dynamodb-get
    image: ruanbekker/fn-dynamodb-get:latest
    environment:
      dynamodb_region: eu-west-1
      dynamodb_table: lookup_table
    secrets:
      - openfaas-aws-access-key
      - openfaas-aws-secret-key
</code></pre>

<p>Include the external dependency for aws:</p>

<pre><code class="bash">$ cat fn-dynamodb-get/requirements.txt
boto3
</code></pre>

<p>Our python code for our function:</p>

<pre><code class="bash">$ cat fn-dynamodb-get/handler.py
</code></pre>

<pre><code class="python">import boto3
import os
import json

aws_key = open('/var/openfaas/secrets/openfaas-aws-access-key', 'r').read()
aws_secret = open('/var/openfaas/secrets/openfaas-aws-secret-key', 'r').read()
dynamodb_region = os.environ['dynamodb_region']
dynamodb_table  = os.environ['dynamodb_table']

client = boto3.Session(region_name=dynamodb_region).resource('dynamodb', aws_access_key_id=aws_key, aws_secret_access_key=aws_secret)
table = client.Table(dynamodb_table)

def handle(req):
    event = json.loads(req)
    response = table.get_item(
        Key={
            'hash_value': event['hash_value']
        }
    )

    if 'Item' not in response:
        item_data = 'Item not found'
    else:
        item_data = response['Item']

    return item_data
</code></pre>

<h2>Build, Push and Deploy:</h2>

<p>It&rsquo;s time to deploy our functions and since we have all our stack info in one file, we can use <code>faas-cli up</code> which will build, push and deploy our functions.</p>

<p>By default it expects the filename to be <code>stack.yml</code> therefore we don&rsquo;t need to specify the filename, but if you had a different filename, you can overwrite the default behaviour with <code>-f</code>:</p>

<pre><code class="bash">$ faas-cli up

Deploying: fn-dynamodb-lookup.
Deployed. 202 Accepted.
URL: https://openfaas.domain.com/function/fn-dynamodb-lookup

Deploying: fn-dynamodb-write.
Deployed. 202 Accepted.
URL: https://openfaas.domain.com/function/fn-dynamodb-write

Deploying: fn-dynamodb-get.
Deployed. 202 Accepted.
URL: https://openfaas.domain.com/function/fn-dynamodb-get
</code></pre>

<h2>Time for our Functions to interact with DynamoDB:</h2>

<p>Write an Item to DynamoDB:</p>

<pre><code class="bash">$ curl -XPOST https://openfaas.domain.com/function/fn-dynamodb-write -d '{"id": 8700000000001, "lastname": "smith", "payload": {"name": "james", "role": "reader"}}'
{'ResponseMetadata': {'RequestId': 'CNHEFHMSL4KGRDE0HRVQ69D5H7VV4KQNSO5AEMVJF66Q9ASUAAJG', 'HTTPStatusCode': 200, 'HTTPHeaders': {'server': 'Server', 'date': 'Sat, 06 Jul 2019 20:47:00 GMT', 'content-type': 'application/x-amz-json-1.0', 'content-length': '2', 'connection': 'keep-alive', 'x-amzn-requestid': 'CNHEFHMSL4KGRDE0HRVQ69D5H7VV4KQNSO5AEMVJF66Q9ASUAAJG', 'x-amz-crc32': '2745614147'}, 'RetryAttempts': 0}}
</code></pre>

<p>Write another Item to DynamoDB:</p>

<pre><code class="bash">$ curl -XPOST https://openfaas.doamin.com/function/fn-dynamodb-write -d '{"id": 8700000000002, "lastname": "adams", "payload": {"name": "samantha", "role": "admin"}}'
{'ResponseMetadata': {'RequestId': 'KRQL838BVGC9LIUSCOUB7MOEQ7VV4KQNSO5AEMVJF66Q9ASUAAJG', 'HTTPStatusCode': 200, 'HTTPHeaders': {'server': 'Server', 'date': 'Sat, 06 Jul 2019 20:48:09 GMT', 'content-type': 'application/x-amz-json-1.0', 'content-length': '2', 'connection': 'keep-alive', 'x-amzn-requestid': 'KRQL838BVGC9LIUSCOUB7MOEQ7VV4KQNSO5AEMVJF66Q9ASUAAJG', 'x-amz-crc32': '2745614147'}, 'RetryAttempts': 0}}
</code></pre>

<p>Now recalculate the hash by passing the ID Number and Lastname to get the hash value for the primary key:</p>

<pre><code class="bash">$ curl -XPOST https://openfaas.domain.com/function/fn-dynamodb-lookup -d '{"id": 8700000000002, "lastname": "adams"}'
bd0a248aff2b50b288ba504bd7142ef11b164901
</code></pre>

<p>Now that we have the hash value, do a GetItem by using the hash value in the request body:</p>

<pre><code class="bash">$ curl -XPOST https://openfaas.domain.com/function/fn-dynamodb-get -d '{"hash_value": "bd0a248aff2b50b288ba504bd7142ef11b164901"}'
{'payload': {'name': 'samantha', 'role': 'admin'}, 'hash_value': 'bd0a248aff2b50b288ba504bd7142ef11b164901', 'timestamp': '2019-07-06T20:48'}
</code></pre>

<p>Note that the lookup function calculates a hash based on the input that you provide it, for example calculating a hash with userdata that does not exist in our table:</p>

<pre><code class="bash">$ curl -XPOST https://openfaas.domain.com/function/fn-dynamodb-lookup -d '{"id": 8700000000003, "lastname": "williams"}'
c68dc272873140f4ae93bb3a3317772a6bdd9aa1
</code></pre>

<p>Using that hash value in our request body to read from dynamodb, will show us that the item has not been found:</p>

<pre><code class="bash">$ curl -XPOST https://openfaas.domain.com/function/fn-dynamodb-get -d '{"hash_value": "c68dc272873140f4ae93bb3a3317772a6bdd9aa1"}'
Item not found
</code></pre>

<p>You might want to change this behavior but this is just for the demonstration of this post.</p>

<p>When you head over to DynamoDB&rsquo;s console you will see this in your table:</p>

<p><img width="873" alt="image" src="https://user-images.githubusercontent.com/567298/60761025-9e8e7c80-a040-11e9-83a3-ad5b474a28ff.png"></p>

<h2>Thanks</h2>

<p>This was a basic example using OpenFaaS with Amazon DynamoDB with Python and secrets managed with OpenFaas. I really like the way OpenFaaS let&rsquo;s you work with secrets, it works great and don&rsquo;t need an additional resource to manage your sensitive data.</p>

<p>Although this was basic usage with OpenFaaS and DynamoDB, the sky is the limit what you can do with it.</p>

<h2>Resources:</h2>

<ul>
<li><a href="https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/">DynamoDB: Choosing the right Partition Key</a></li>
<li><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-uniform-load.html">Designing Partition Keys to Distribute Your Workload Evenly</a></li>
<li><a href="https://docs.openfaas.com/contributing/get-started/">OpenFaaS: Getting Started</a></li>
<li><a href="https://docs.openfaas.com/reference/secrets/">OpenFaaS: Secrets</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Play With Kinesis Data Streams for Free]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/06/22/play-with-kinesis-data-streams-for-free/"/>
    <updated>2019-06-22T23:35:19+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/06/22/play-with-kinesis-data-streams-for-free</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/59969559-3f187300-9550-11e9-9e6d-7fa4dbc30198.png" alt="image" /></p>

<p>Misleading title?? Perhaps, depends on how you look at it. Amazon Kinesis is a fully managed, cloud-based service for real-time processing of distributed data streams. So if you&rsquo;re a curious mad person like me, you want to test out stuff and when you can test stuff out for free, why not.</p>

<p>So before paying for that, why not spin something up locally, such as <a href="https://github.com/mhart/kinesalite">Kinesisalite</a> which is an implementation of Amazon Kinesis built on top of LevelDB.</p>

<p>Kinesis overview:</p>

<p><img src="https://user-images.githubusercontent.com/567298/59969540-caddcf80-954f-11e9-8e3d-23c932d35ef1.png" alt="image" /></p>

<h2>What will we be doing?</h2>

<p>In this tutorial we will setup a local kinesis instance using docker then do the following:</p>

<ul>
<li>Create a Kinesis Stream, List, Describe, PutRecord, GetRecords using Python&rsquo;s Boto3 Interface</li>
<li>Write a Python Producer and Consumer</li>
<li>Write and Read Records from our Local Kinesis Stream</li>
</ul>


<h2>Building Kinesis Local on Docker</h2>

<p>If you would like to skip this step, you can use my docker image: <a href="https://hub.docker.com/r/ruanbekker/kinesis-local">ruanbekker/kinesis-local:latest</a></p>

<p>Our Dockerfile:</p>

<pre><code>FROM node:8.16.0-stretch-slim

RUN apt update &amp;&amp; apt install build-essential python-minimal -y
RUN npm install --unsafe-perm -g kinesalite
RUN apt-get clean

CMD ["kinesalite", "--port", "4567", "--createStreaMs", "5"]
</code></pre>

<p>Build:</p>

<pre><code>$ docker build -t kinesis-local .
</code></pre>

<p>Run and expose port 4567:</p>

<pre><code>$ docker run -it -p 4567:4567 kinesis-local:latest
</code></pre>

<h2>Interact with Kinesis Local:</h2>

<p>In this next steps we will setup our environment, which will only require <code>python</code> and <code>boto3</code>. To keep things isolated, I will do this with a docker container:</p>

<pre><code>$ docker run -it python:3.7-alpine sh
</code></pre>

<p>Now we need to install boto3 and enter the python repl:</p>

<pre><code>$ pip3 install boto3
$ python3
Python 3.7.3 (default, May 11 2019, 02:00:41)
[GCC 8.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt;
</code></pre>

<p>Import boto and create the connection to our kinesis local instance:</p>

<pre><code>&gt;&gt;&gt; import boto3
&gt;&gt;&gt; client = boto3.Session(
    region_name='eu-west-1').client('kinesis', aws_access_key_id='', aws_secret_access_key='', endpoint_url='http://localhost:4567'
)
</code></pre>

<p>Let&rsquo;s list our streams and as expected, we should have zero streams available:</p>

<pre><code>&gt;&gt;&gt; client.list_streams()
{u'StreamNames': [], u'HasMoreStreams': False, 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': '637xx', 'HTTPHeaders': {'x-amzn-requestid': '6xx', 'content-length': '41', 'x-amz-id-2': 'xx', 'connection': 'keep-alive', 'date': 'Sat, 22 Jun 2019 19:17:34 GMT', 'content-type': 'application/x-amz-json-1.1'}}}
</code></pre>

<p>Let&rsquo;s create a stream named <code>mystream</code> with 1 primary shard:</p>

<pre><code>&gt;&gt;&gt; client.create_stream(StreamName='mystream', ShardCount=1)
</code></pre>

<p>Let&rsquo;s list our streams again:</p>

<pre><code>&gt;&gt;&gt; client.list_streams()
{u'StreamNames': [u'mystream'], u'HasMoreStreams': False, 'ResponseMetadata': ...
</code></pre>

<p>Let&rsquo;s put some data in our kinesis stream, we will push a payload with the body: <code>{"name": "ruan"}</code> to our kinesis stream with partition key: <code>a01</code> which is used for sharding:</p>

<pre><code>&gt;&gt;&gt; response = client.put_record(StreamName='mystream', Data=json.dumps({"name": "ruan"}), PartitionKey='a01')
&gt;&gt;&gt; response
{u'ShardId': u'shardId-000000000000', 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': 'cb0xx', 'HTTPHeaders': {'x-amzn-requestid': 'xx', 'content-length': '110', 'x-amz-id-2': 'xx', 'connection': 'keep-alive', 'date': 'Sat, 22 Jun 2019 19:20:27 GMT', 'content-type': 'application/x-amz-json-1.1'}}, u'SequenceNumber': u'490xx'}
</code></pre>

<p>Now that we have data in our stream we need to read data from our kinesis stream. Before data can be read from the stream we need to obtain the shard iterator for the shard we are interested in. A shard iterator represents the position of the stream and shard from which the consumer will read, in this case we will call the get_shard_operator method and passing the stream name, shard id and shard iterator type.</p>

<p>There are 2 comman iterator types:</p>

<ul>
<li>TRIM_HORIZON: Points to the last untrimmed record in the shard</li>
<li>LATEST: Reads the most recent data in the shard</li>
</ul>


<p>We will use TRIM_HORIZON in this case, get the shard iterator id:</p>

<pre><code>&gt;&gt;&gt; shard_id = response['ShardId']
&gt;&gt;&gt; response = client.get_shard_iterator(StreamName='mystream', ShardId=shard_id, ShardIteratorType='TRIM_HORIZON')
&gt;&gt;&gt; response
{u'ShardIterator': u'AAAxx=', 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': '22dxx', 'HTTPHeaders': {'x-amzn-requestid': '22dxx', 'content-length': '224', 'x-amz-id-2': 'xx', 'connection': 'keep-alive', 'date': 'Sat, 22 Jun 2019 19:22:55 GMT', 'content-type': 'application/x-amz-json-1.1'}}}
</code></pre>

<p>Now that we have the shard iterator id, we can call the get_records method with the shard iterator id, to read the data from the stream:</p>

<pre><code>&gt;&gt;&gt; shard_iterator = response['ShardIterator']
&gt;&gt;&gt; response = client.get_records(ShardIterator=shard_iterator)
&gt;&gt;&gt; response
{u'Records': [{u'Data': '{"name": "ruan"}', u'PartitionKey': u'a01', u'ApproximateArrivalTimestamp': datetime.datetime(2019, 6, 22, 21, 20, 27, 937000, tzinfo=tzlocal()), u'SequenceNumber': u'495xx'}], 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': '2b6xx', 'HTTPHeaders': {'x-amzn-requestid': '2b6xx', 'content-length': '441', 'x-amz-id-2': 'xx', 'connection': 'keep-alive', 'date': 'Sat, 22 Jun 2019 19:30:19 GMT', 'content-type': 'application/x-amz-json-1.1'}}, u'NextShardIterator': u'AAAxx=', u'MillisBehindLatest': 0}
</code></pre>

<p>To loop and parse through the response to make it more readable:</p>

<pre><code>&gt;&gt;&gt; for record in response['Records']:
...     if 'Data' in record:
...         json.loads(record['Data'])
...
{u'name': u'ruan'}
</code></pre>

<p>Once we are done, we can delete our stream:</p>

<pre><code>&gt;&gt;&gt; client.delete_stream(StreamName='mystream')
</code></pre>

<p>Now that we have the basics, lets create our producer and consumer for a demonstration on pushing data to a kinesis stream from one process and consuming it from another process. As this demonstration we will be producing and consuming data from the same laptop, in real use-cases, you will do them from seperate servers and using Amazon Kinesis.</p>

<h2>Our Kinesis Producer</h2>

<p>The following will create a Kinesis Local Stream and Write 25 JSON Documents to our stream:</p>

<pre><code class="python">import boto3
import random
import json
import time

names = ['james', 'stefan', 'pete', 'tom', 'frank', 'peter', 'ruan']

session = boto3.Session(region_name='eu-west-1')
client = session.client(
    'kinesis', 
    aws_access_key_id='', 
    aws_secret_access_key='', 
    endpoint_url='http://localhost:4567'
)

list_streams = client.list_streams()

if 'mystream' not in list_streams['StreamNames']:
    client.create_stream(StreamName='mystream', ShardCount=1)
    time.sleep(1)

count = 0
print("Starting at {}".format(time.strftime("%H:%m:%S")))

while count != 25:
    count += 1
    response = client.put_record(
        StreamName='mystream', 
        Data=json.dumps({
            "number": count, 
            "name": random.choice(names), 
            "age": random.randint(20,50)}
        ), 
        PartitionKey='a01'
    )
    time.sleep(1)

print("Finished at {}".format(time.strftime("%H:%m:%S")))
</code></pre>

<h2>Our Kinesis Local Consumer:</h2>

<p>This will read 5 records at a time from our stream, you will notice if you run them on the same time it will only read one at a time as the producer only writes one per second.</p>

<pre><code class="python">import boto3
import json
import time
import os

session = boto3.Session(region_name='eu-west-1')
client = session.client(
    'kinesis', 
    aws_access_key_id='', 
    aws_secret_access_key='', 
    endpoint_url='http://localhost:4567'
)

stream_details = client.describe_stream(StreamName='mystream')
shard_id = stream_details['StreamDescription']['Shards'][0]['ShardId']

response = client.get_shard_iterator(
    StreamName='mystream', 
    ShardId=shard_id, 
    ShardIteratorType='TRIM_HORIZON'
)

shard_iterator = response['ShardIterator']

while True:
    response = client.get_records(ShardIterator=shard_iterator, Limit=5)
    shard_iterator = response['NextShardIterator']
    for record in response['Records']:
        if 'Data' in record and len(record['Data']) &gt; 0:
            print(json.loads(record['Data']))
    time.sleep(0.75)
</code></pre>

<h2>Demo Time!</h2>

<p>Now that we have our <code>producer.py</code> and <code>consumer.py</code>, lets test this out.</p>

<p>Start the server:</p>

<pre><code>$ docker run -it -p 4567:4567 ruanbekker/kinesis-local:latest
Listening at http://:::4567
</code></pre>

<p>Run the Producer from your Python Environment:</p>

<pre><code>$ python producer.py
Starting at 00:06:16
Finished at 00:06:42
</code></pre>

<p>Run the Consumer from your Python Environment:</p>

<pre><code>$ python consumer.py
Starting Consuming at 00:06:31
{u'age': 30, u'number': 1, u'name': u'pete'}
{u'age': 23, u'number': 2, u'name': u'ruan'}
{u'age': 22, u'number': 3, u'name': u'peter'}
{u'age': 45, u'number': 4, u'name': u'stefan'}
{u'age': 49, u'number': 5, u'name': u'tom'}
{u'age': 47, u'number': 6, u'name': u'pete'}
{u'age': 35, u'number': 7, u'name': u'stefan'}
{u'age': 45, u'number': 8, u'name': u'ruan'}
{u'age': 38, u'number': 9, u'name': u'frank'}
{u'age': 20, u'number': 10, u'name': u'tom'}
{u'age': 38, u'number': 11, u'name': u'james'}
{u'age': 20, u'number': 12, u'name': u'james'}
{u'age': 38, u'number': 13, u'name': u'tom'}
{u'age': 25, u'number': 14, u'name': u'tom'}
{u'age': 20, u'number': 15, u'name': u'peter'}
{u'age': 50, u'number': 16, u'name': u'james'}
{u'age': 29, u'number': 17, u'name': u'james'}
{u'age': 42, u'number': 18, u'name': u'pete'}
{u'age': 25, u'number': 19, u'name': u'pete'}
{u'age': 36, u'number': 20, u'name': u'tom'}
{u'age': 45, u'number': 21, u'name': u'peter'}
{u'age': 39, u'number': 22, u'name': u'ruan'}
{u'age': 43, u'number': 23, u'name': u'tom'}
{u'age': 38, u'number': 24, u'name': u'pete'}
{u'age': 40, u'number': 25, u'name': u'frank'}
Finshed Consuming at 00:06:35
</code></pre>

<h2>Thanks</h2>

<p><a href="https://saythanks.io/to/ruanbekker"><img src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" alt="Say Thanks!" /></a></p>

<p>Hope that was useful, feel free to check out <a href="https://aws.amazon.com/kinesis/">Amazon&rsquo;s Kinesis</a> out if you are planning to run this in any non-testing environment</p>

<center>
        <script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Buy Me a Coffee', '#46b798', 'A6423ZIQ');kofiwidget2.draw();</script>
</center>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a Logstash Server for Amazon Elasticsearch Service and Auth With IAM]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/06/04/setup-a-logstash-server-for-amazon-elasticsearch-service-and-auth-with-iam/"/>
    <updated>2019-06-04T23:46:27+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/06/04/setup-a-logstash-server-for-amazon-elasticsearch-service-and-auth-with-iam</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/59209960-ca872100-8bac-11e9-8672-8c6af502afe0.png" alt="logstash" /></p>

<p>As many of you might know, when you deploy a ELK stack on Amazon Web Services, you only get E and K in the ELK stack, which is Elasticsearch and Kibana. Here we will be dealing with Logstash on EC2.</p>

<h2>What will we be doing</h2>

<p>In this tutorial we will setup a Logstash Server on EC2, setup a IAM Role and Autenticate Requests to Elasticsearch with an IAM Role, setup Nginx so that logstash can ship logs to Elasticsearch.</p>

<p>I am not fond of working with access key&rsquo;s and secret keys, and if I can stay away from handling secret information the better. So instead of creating a access key and secret key for logstash, we will instead create a IAM Policy that will allow the actions to Elasticsearch, associate that policy to an IAM Role, set EC2 as a trusted entity and strap that IAM Role to the EC2 Instance.</p>

<p>Then we will allow the IAM Role ARN to the Elasticsearch Policy, then when Logstash makes requests against Elasticsearch, it will use the IAM Role to assume temporary credentials to authenticate. That way we don&rsquo;t have to deal with keys. But I mean you can create access keys if that is your preferred method, I&rsquo;m just not a big fan of keeping secret keys.</p>

<p>The benefit of authenticating with IAM, allows you to remove a reverse proxy that is another hop to the path of your target.</p>

<h2>Create the IAM Policy:</h2>

<p>Create a IAM Policy that will allow actions to Elasticsearch:</p>

<pre><code>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "es:ESHttpHead",
                "es:ESHttpPost",
                "es:ESHttpGet",
                "es:ESHttpPut"
            ],
            "Resource": "arn:aws:es:eu-west-1:0123456789012:domain/my-es-domain"
        }
    ]
}
</code></pre>

<p>Create Role logstash-system-es with &ldquo;ec2.amazonaws.com&rdquo; as trusted entity in trust the relationship and associate the above policy to the role.</p>

<h2>Authorize your Role in Elasticsearch Policy</h2>

<p>Head over to your Elasticsearch Domain and configure your Elasticsearch Policy to include your IAM Role to grant requests to your Domain:</p>

<pre><code>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "arn:aws:iam::0123456789012:role/logstash-system-es"
        ]
      },
      "Action": "es:*",
      "Resource": "arn:aws:es:eu-west-1:0123456789012:domain/my-es-domain/*"
    }
  ]
}
</code></pre>

<h2>Install Logstash on EC2</h2>

<p>I will be using Ubuntu Server 18. Update the repositories and install dependencies:</p>

<pre><code>$ apt update &amp;&amp; apt upgrade -y
$ apt install build-essential apt-transport-https -y
$ wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
$ echo "deb https://artifacts.elastic.co/packages/6.x/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-6.x.list
$ apt update
</code></pre>

<p>As logstash requires Java, install the the Java OpenJDK Runtime Environment:</p>

<pre><code>$ apt install default-jre -y
</code></pre>

<p>Verify that Java is installed:</p>

<pre><code>$ java -version
openjdk version "11.0.3" 2019-04-16
OpenJDK Runtime Environment (build 11.0.3+7-Ubuntu-1ubuntu218.04.1)
OpenJDK 64-Bit Server VM (build 11.0.3+7-Ubuntu-1ubuntu218.04.1, mixed mode, sharing)
</code></pre>

<p>Now, install logstash and enable the service on boot:</p>

<pre><code>$ apt install logstash -y
$ systemctl enable logstash.service
$ service logstash stop
</code></pre>

<h2>Install the Amazon ES Logstash Output Plugin</h2>

<p>For us to be able to authenticate using IAM, we should use the Amazon-ES Logstash Output Plugin. Update and install the plugin:</p>

<pre><code>$ /usr/share/logstash/bin/logstash-plugin update
$ /usr/share/logstash/bin/logstash-plugin install logstash-output-amazon_es
</code></pre>

<h2>Configure Logstash</h2>

<p>I like to split up my configuration in 3 parts, (input, filter, output).</p>

<p>Let&rsquo;s create the input configuration: <code>/etc/logstash/conf.d/10-input.conf</code></p>

<pre><code>input {
  file {
    path =&gt; "/var/log/nginx/access.log"
    start_position =&gt; "beginning"
  }
}
</code></pre>

<p>Our filter configuration: <code>/etc/logstash/conf.d/20-filter.conf</code></p>

<pre><code>filter {
  grok {
    match =&gt; { "message" =&gt; "%{HTTPD_COMMONLOG}" }
  }
  mutate {
    add_field =&gt; {
      "custom_field1" =&gt; "hello from: %{host}"
    }
  }
}
</code></pre>

<p>And lastly, our output configuration: <code>/etc/logstash/conf.d/30-outputs.conf</code>:</p>

<pre><code>output {
  amazon_es {
      hosts =&gt; ["my-es-domain.abcdef.eu-west-1.es.amazonaws.com"]
      index =&gt; "new-logstash-%{+YYYY.MM.dd}"
      region =&gt; "eu-west-1"
      aws_access_key_id =&gt; ''
      aws_secret_access_key =&gt; ''
  }
}
</code></pre>

<p>Note that the <code>aws_</code> directives has been left empty as that seems to be the way it needs to be set when using roles. Authentication will be assumed via the Role which is associated to the EC2 Instance.</p>

<p>If you are using access keys, you can populate them there.</p>

<h2>Start Logstash</h2>

<p>Start logstash:</p>

<pre><code>$ service logstash start
</code></pre>

<p>Tail the logs to see if logstash starts up correctly, it should look more or less like this:</p>

<pre><code>$ tail -f /var/log/logstash/logstash-plain.log

[2019-06-04T16:38:12,087][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=&gt;"6.8.0"}
[2019-06-04T16:38:14,480][INFO ][logstash.pipeline        ] Starting pipeline {:pipeline_id=&gt;"main", "pipeline.workers"=&gt;2, "pipeline.batch.size"=&gt;125, "pipeline.batch.delay"=&gt;50}
[2019-06-04T16:38:15,226][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=&gt;{:removed=&gt;[], :added=&gt;[https://search-my-es-domain-xx.eu-west-1.es.amazonaws.com:443/]}}
[2019-06-04T16:38:15,234][INFO ][logstash.outputs.elasticsearch] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=&gt;https://search-my-es-domain-xx.eu-west-1.es.amazonaws.com:443/, :path=&gt;"/"}
</code></pre>

<h2>Install Nginx</h2>

<p>As you noticed, I have specified <code>/var/log/nginx/access.log</code> as my input file for logstash, as we will test logstash by shipping nginx access logs to Elasticsearch Service.</p>

<p>Install Nginx:</p>

<pre><code>$ apt install nginx -y
</code></pre>

<p>Start the service:</p>

<pre><code>$ systemctl restart nginx 
$ systemctl enable nginx
</code></pre>

<p>Make a GET request on your Nginx Web Server and inspect the log on Kibana, where it should look like this:</p>

<p><img src="https://user-images.githubusercontent.com/567298/58917559-4dc8f280-8727-11e9-9e9d-7950217abe34.png" alt="" /></p>
]]></content>
  </entry>
  
</feed>
