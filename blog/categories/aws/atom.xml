<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Aws | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/aws/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2021-07-31T03:59:55-04:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Difference With ECS Task and Execution IAM Roles on AWS]]></title>
    <link href="https://blog.ruanbekker.com/blog/2021/07/31/difference-with-ecs-task-and-execution-iam-roles-on-aws/"/>
    <updated>2021-07-31T03:37:34-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2021/07/31/difference-with-ecs-task-and-execution-iam-roles-on-aws</id>
    <content type="html"><![CDATA[<p><img src="../images/ruanbekker-header-photo.png" alt="" /></p>

<p>In this post we will look at what the difference is between the <a href="https://docs.aws.amazon.com/AmazonECS/latest/userguide/task-iam-roles.html">AWS ECS Task Execution IAM Role</a> and the <a href="https://docs.aws.amazon.com/AmazonECS/latest/userguide/task-iam-roles.html">IAM Role for Tasks</a> and give a example policy to demonstrate.</p>

<h2>ECS Task Execution Role</h2>

<p>The ECS Execution Role is used by the ecs-agent which runs on ECS and is responsible for:
- Pulling down docker images from ECR
- Fetching the SSM Parameters from SSM for your Task (Secrets and LogConfigurations)
- Writing Logs to CloudWatch</p>

<p>The IAM Role has been configured that the Trusted Identity is ecs so only ECS is allowed to assume credentials from the IAM Policy that is associated to the Role.</p>

<p>The trusted identity in the IAM Role to be ecs:</p>

<pre><code class="json">{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "",
      "Effect": "Allow",
      "Principal": {
        "Service": "ecs-tasks.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
</code></pre>

<p>and the policy will look like this more or less for a example service, I am demonstrating my-dev-service:</p>

<pre><code class="json">{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ecr:GetAuthorizationToken",
                "ecr:BatchCheckLayerAvailability",
                "ecr:GetDownloadUrlForLayer",
                "ecr:BatchGetImage",
                "logs:CreateLogStream",
                "logs:PutLogEvents"
            ],
            "Resource": "*"
        },
        {
            "Sid": "SSMGetParameters",
            "Effect": "Allow",
            "Action": [
                "ssm:GetParameter"
            ],
            "Resource": "arn:aws:ssm:eu-west-1:*:parameter/my-service/dev/*"
        },
        {
            "Sid": "KMSDecryptParametersWithKey",
            "Effect": "Allow",
            "Action": [
                "kms:GetPublicKey",
                "kms:Decrypt",
                "kms:GenerateDataKey",
                "kms:DescribeKey"
            ],
            "Resource": "*"
        }
    ]
}
</code></pre>

<p>In the ECS Task Definition the role arn is specified as <code>"executionRoleArn"</code> in:</p>

<pre><code class="json">{
  "family": "my-dev-service",
  "executionRoleArn":"arn:aws:iam::000000000000:role/ecs-exec-role",
  "taskRoleArn":"arn:aws:iam::000000000000:role/ecs-task-role",
  "containerDefinitions": []
}
</code></pre>

<h2>ECS Task Role</h2>

<p>The ECS Task Role is used by the service that is deployed to ECS, so this will be your application requiring access to SQS as an example</p>

<p>Same as before, we set the trusted identity in the IAM Role to be ecs:</p>

<pre><code class="json">{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "",
      "Effect": "Allow",
      "Principal": {
        "Service": "ecs-tasks.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
</code></pre>

<p>So only the ECS tasks using the role is allowed to assume credentials from the IAM Role, and the policy associated to the role, can look something like this:</p>

<pre><code class="json">{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "AllowDevSQS",
            "Effect": "Allow",
            "Action": [
                "sqs:GetQueueUrl",
                "sqs:ReceiveMessage",
                "sqs:SendMessage",
                "sqs:ChangeMessageVisibility"
            ],
            "Resource": [
                "arn:aws:sqs:eu-west-1:000000000000:dev-pending-queue",
                "arn:aws:sqs:eu-west-1:000000000000:dev-confirmed-queue"
            ]
        }
    ]
}
</code></pre>

<p>The role arn will be specified in <code>"taskRoleArn"</code> from the following in the ECS Task Definition:</p>

<pre><code class="json">{
  "family": "my-dev-service",
  "executionRoleArn":"arn:aws:iam::000000000000:role/ecs-exec-role",
  "taskRoleArn":"arn:aws:iam::000000000000:role/ecs-task-role",
  "containerDefinitions": []
}
</code></pre>

<h2>Application Code</h2>

<p>In your application you don’t need to reference any aws access keys as the role will assume credentials for you by the SDK, with python a short example will be:</p>

<pre><code class="python">import boto3
sqs = boto3.Session(region_name='eu-west-1').client('sqs')
</code></pre>

<h2>Thanks</h2>

<p>Thanks for reading, if you like my content, check out my <strong><a href="https://ruan.dev">website</a></strong> or follow me at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SSH Using AWS SSM Session Manager]]></title>
    <link href="https://blog.ruanbekker.com/blog/2021/03/10/ssh-using-aws-ssm-session-manager/"/>
    <updated>2021-03-10T00:52:54-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2021/03/10/ssh-using-aws-ssm-session-manager</id>
    <content type="html"><![CDATA[<p>You can use SSM Session Manager to connect to your EC2 instances, as long as your EC2 instance has the associated IAM Role which includes the AmazonSSMManagedInstanceCore managed policy.</p>

<h2>AWS EC2 Console</h2>

<p>Head over to &ldquo;Connect&rdquo; and select &ldquo;Session Manager&rdquo;:</p>

<p><img src="https://user-images.githubusercontent.com/567298/103775580-e8da2a80-5036-11eb-9e00-0fd9b4d9d467.png" alt="image" /></p>

<p>You should get a shell:</p>

<p><img src="https://user-images.githubusercontent.com/567298/103775597-f2639280-5036-11eb-8101-768f1c81108a.png" alt="image" /></p>

<h2>AWS CLI</h2>

<p>You can also use the CLI:</p>

<pre><code>aws --profile prod ssm start-session --target i-0ebba722b102179b6
</code></pre>

<p>If you get this error:</p>

<p><img src="https://user-images.githubusercontent.com/567298/103775625-ff808180-5036-11eb-88dc-be8fde3586ad.png" alt="image" /></p>

<p>Head over to:</p>

<p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-install-plugin.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-install-plugin.html</a></p>

<p>Install the session manager plugin, for Mac:</p>

<pre><code>$ curl "https://s3.amazonaws.com/session-manager-downloads/plugin/latest/mac/sessionmanager-bundle.zip" -o "sessionmanager-bundle.zip"
$ unzip sessionmanager-bundle.zip
$ sudo ./sessionmanager-bundle/install -i /usr/local/sessionmanagerplugin -b /usr/local/bin/session-manager-plugin
$ rm -rf sessionmanager-bundle
</code></pre>

<p>After installation:</p>

<pre><code>$ aws --profile prod ssm start-session --target i-0ebba722b102179b6
Starting session with SessionId: ruan.bekker-0b07cbbe261885ad3

sh-4.2$ sudo su - ec2-user
Last login: Wed Jan  6 12:55:03 UTC 2021 on pts/0
[ec2-user@ip-172-31-23-246 ~]$
</code></pre>

<p>Note: when you are using ssm session manager you don’t require security groups or a direct routable network to your instance.</p>

<h2>Bash Functions FTW</h2>

<p>You can implement this into a bash function:</p>

<pre><code>$ cat ~/.functions.aws
aws-ssh(){
  instance_name=${1}
  instance_id=$(aws --profile prod ec2 describe-instances --filter "Name=tag:Name,Values=${instance_name}" --query "Reservations[].Instances[?State.Name == 'running'].InstanceId[]" --output text)
  aws --profile prod ssm start-session --target ${instance_id}
}

$ aws-ssh ssm-session-manager-ssh-test2
Starting session with SessionId: ruan.bekker-04daf56c5f3668790
sh-4.2$
</code></pre>

<p>If you have your own SSH key, you can use this ~/.ssh/config:</p>

<pre><code># AWS SSM Session Manager
Host i-*
    ProxyCommand sh -c "aws --profile prod ssm start-session --target %h --document-name AWS-StartSSHSession --parameters 'portNumber=%p'"
</code></pre>

<pre><code>$ ssh -i ~/.ssh/infra.pem ec2-user@i-0ebba722b102179b6
Warning: Permanently added 'i-0ebba722b102179b6' (ECDSA) to the list of known hosts.
Last login: Wed Jan  6 13:04:03 2021

       __|  __|_  )
       _|  (     /   Amazon Linux 2 AMI
      ___|\___|___|

https://aws.amazon.com/amazon-linux-2/
[ec2-user@ip-172-31-23-246 ~]$
</code></pre>

<h2>Related:</h2>

<ul>
<li><a href="https://aws.amazon.com/blogs/mt/amazon-ec2-instance-port-forwarding-with-aws-systems-manager/">https://aws.amazon.com/blogs/mt/amazon-ec2-instance-port-forwarding-with-aws-systems-manager/</a></li>
<li><a href="https://aws.amazon.com/blogs/aws/new-port-forwarding-using-aws-system-manager-sessions-manager/">https://aws.amazon.com/blogs/aws/new-port-forwarding-using-aws-system-manager-sessions-manager/</a></li>
</ul>


<h2>Thanks</h2>

<p>Thanks for reading, if you like my content, check out my <strong><a href="https://ruan.dev">website</a></strong> or follow me at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running SSH Commands on AWS EC2 Instances With Python]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/11/02/running-ssh-commands-on-aws-ec2-instances-with-python/"/>
    <updated>2020-11-02T09:55:43+00:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/11/02/running-ssh-commands-on-aws-ec2-instances-with-python</id>
    <content type="html"><![CDATA[<p>In this quick post I will demonstrate how to discover a EC2 Instance&rsquo;s Private IP Address using the AWS API by using Tags then use Paramiko in Python to SSH to the EC2 instance and run SSH commands on the target instance.</p>

<p>Install the required dependencies:</p>

<pre><code>$ virtualenv -p python3 .venv
$ source .venve/bin/activate
$ pip install boto3 paramiko
</code></pre>

<p>I have my development profile for aws configured under <code>dev</code> as can seen below:</p>

<pre><code>$ aws --profile dev configure list
      Name                    Value             Type    Location
      ----                    -----             ----    --------
   profile                      dev           manual    --profile
access_key     ****************xxxx      assume-role
secret_key     ****************xxxx      assume-role
    region                eu-west-1      config-file    ~/.aws/config
</code></pre>

<p>First we need to discover the private ip address from the api by referencing tags, and in this example we will use the <code>Name</code> tag:</p>

<pre><code>import boto3
ec2 = boto3.Session(profile_name='dev', region_name='eu-west-1').client('ec2')

target_instances = ec2.describe_instances(
    Filters=[{'Name':'tag:Name','Values':['my-demo-ec2-instance']}]
)

ec2_instances = []
for each_instance in target_instances['Reservations']:
    for found_instance in each_instance['Instances']:
        ec2_instances.append(found_instance['PrivateIpAddress'])

# ec2_instances
# ['172.31.2.89']
</code></pre>

<p>So we are instantiating a ec2 instance with our configured dev profile, then we describe all our instances using the tag key <code>Name</code> and value <code>my-demo-ec2-instance</code> and then access the private ip address and append it to our <code>ec2_instances</code> list.</p>

<p>Next we want to define the commands that we want to run on the target ec2 instance:</p>

<pre><code>commands = [
    "echo hi",
    "whoami",
    "hostname"
]
</code></pre>

<p>In my case I only have 1 ec2 instance with the name <code>my-demo-ec2-instance</code>, but if you have more you can just loop through the list and perform the actions.</p>

<p>Next we want to establish the SSH connection:</p>

<pre><code>k = paramiko.RSAKey.from_private_key_file("/Users/ruan/.ssh/id_rsa")
c = paramiko.SSHClient()
c.set_missing_host_key_policy(paramiko.AutoAddPolicy())
c.connect(hostname=ec2_instances[0], username="ruan", pkey=k, allow_agent=False, look_for_keys=False)
</code></pre>

<p>Once our SSH connection has established, we can loop through our commands and execute them:</p>

<pre><code>for command in commands:
    print("running command: {}".format(command))
    stdin , stdout, stderr = c.exec_command(command)
    print(stdout.read())
    print(stderr.read())
</code></pre>

<p>Which will output the folling:</p>

<pre><code>running command: echo hi
b'hi\n'
b''
running command: whoami
b'ruan\n'
b''
running command: hostname
b'ip-172-31-2-89\n'
b''
</code></pre>

<p>And then close the SSH connection:</p>

<pre><code>c.close()
</code></pre>

<p>And the full script will look like this:</p>

<pre><code class="python">import boto3
ssh_username = "ruan"
ssh_key_file = "/Users/ruan/.ssh/id_rsa"

ec2 = boto3.Session(profile_name='dev', region_name='eu-west-1').client('ec2')

target_instances = ec2.describe_instances(
    Filters=[{'Name':'tag:Name','Values':['my-demo-ec2-instance']}]
)

ec2_instances = []
for each_instance in target_instances['Reservations']:
    for found_instance in each_instance['Instances']:
        ec2_instances.append(found_instance['PrivateIpAddress'])

commands = [
    "echo hi",
    "whoami",
    "hostname"
]

k = paramiko.RSAKey.from_private_key_file(ssh_key_file)
c = paramiko.SSHClient()
c.set_missing_host_key_policy(paramiko.AutoAddPolicy())
c.connect(hostname=ec2_instances[0], username=ssh_username, pkey=k, allow_agent=False, look_for_keys=False)

for command in commands:
    print("running command: {}".format(command))
    stdin , stdout, stderr = c.exec_command(command)
    print(stdout.read())
    print(stderr.read())

c.close()
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Run Localstack as a Service Container for AWS Mock Services on Drone CI]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/02/04/run-localstack-as-a-service-container-for-aws-mock-services-on-drone-ci/"/>
    <updated>2020-02-04T23:43:30+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/02/04/run-localstack-as-a-service-container-for-aws-mock-services-on-drone-ci</id>
    <content type="html"><![CDATA[<p>In this tutorial we will setup a basic pipeline in drone to make use of service containers, we will provision localstack so that we can provision AWS mock services.</p>

<p>We will create a kinesis stream on localstack, when the service is up, we will create a stream, put 100 records in the stream, read them from the stream and delete the kinesis stream.</p>

<h2>Gitea and Drone Stack</h2>

<p>If you don’t have the stack setup, have a look at <a href="https://blog.ruanbekker.com/blog/2020/02/04/setup-gitea-and-drone-on-docker-2020-edition/">this post</a> where I go into detail on how to get that setup.</p>

<h2>Create the Drone Config</h2>

<p>In gitea, I have created a new git repository and created my drone config as <code>.drone.yml</code> with this pipeline config:</p>

<pre><code>---
kind: pipeline
type: docker
name: localstack

platform:
  os: linux
  arch: amd64

steps:
  - name: wait-for-localstack
    image: busybox
    commands:
      - sleep 10

  - name: list-kinesis-streams
    image: ruanbekker/awscli
    environment:
      AWS_ACCESS_KEY_ID: 123
      AWS_SECRET_ACCESS_KEY: xyz
      AWS_DEFAULT_REGION: eu-west-1
    commands:
      - aws --endpoint-url=http://localstack:4568 kinesis list-streams

  - name: create-kinesis-streams
    image: ruanbekker/awscli
    environment:
      AWS_ACCESS_KEY_ID: 123
      AWS_SECRET_ACCESS_KEY: xyz
      AWS_DEFAULT_REGION: eu-west-1
    commands:
      - aws --endpoint-url=http://localstack:4568 kinesis create-stream --stream-name mystream --shard-count 1

  - name: describe-kinesis-streams
    image: ruanbekker/awscli
    environment:
      AWS_ACCESS_KEY_ID: 123
      AWS_SECRET_ACCESS_KEY: xyz
      AWS_DEFAULT_REGION: eu-west-1
    commands:
      - aws --endpoint-url=http://localstack:4568 kinesis describe-stream --stream-name mystream

  - name: put-record-into-kinesis
    image: ruanbekker/awscli
    environment:
      AWS_ACCESS_KEY_ID: 123
      AWS_SECRET_ACCESS_KEY: xyz
      AWS_DEFAULT_REGION: eu-west-1
    commands:
      - for record in $$(seq 1 100); do aws --endpoint-url=http://localstack:4568 kinesis put-record --stream-name mystream --partition-key 123 --data testdata_$$record ; done

  - name: get-record-from-kinesis
    image: ruanbekker/awscli
    environment:
      AWS_ACCESS_KEY_ID: 123
      AWS_SECRET_ACCESS_KEY: xyz
      AWS_DEFAULT_REGION: eu-west-1
    commands:
      - SHARD_ITERATOR=$$(aws --endpoint-url=http://localstack:4568 kinesis get-shard-iterator --shard-id shardId-000000000000 --shard-iterator-type TRIM_HORIZON --stream-name mystream --query 'ShardIterator' --output text)
      - for each in $$(aws --endpoint-url=http://localstack:4568 kinesis get-records --shard-iterator $$SHARD_ITERATOR | jq -cr '.Records[].Data'); do echo $each | base64 -d ; echo "" ; done

  - name: delete-kinesis-stream
    image: ruanbekker/awscli
    environment:
      AWS_ACCESS_KEY_ID: 123
      AWS_SECRET_ACCESS_KEY: xyz
      AWS_DEFAULT_REGION: eu-west-1
    commands:
      - aws --endpoint-url=http://localstack:4568 kinesis delete-stream --stream-name mystream

services:
  - name: localstack
    image: localstack/localstack
    privileged: true
    environment:
      DOCKER_HOST: unix:///var/run/docker.sock
    volumes:
      - name: docker-socket
        path: /var/run/docker.sock
      - name: localstack-vol
        path: /tmp/localstack
    ports:
      - 8080

volumes:
- name: localstack-vol
  temp: {}
- name: docker-socket
  host:
    path: /var/run/docker.sock
</code></pre>

<p>To explain what we are doing, we are bringing up localstack as a service container, then using the aws cli tools we point to the localstack kinesis endpoint, creating a kinesis stream, put 100 records to the stream, then we read from the stream and delete thereafter.</p>

<h2>Trigger the Pipeline</h2>

<p>Then I head to drone activate my new git repository and select the repository as &ldquo;Trusted&rdquo;. I commited a dummy file to trigger the pipeline and it should look like this:</p>

<p><img width="893" alt="image" src="https://user-images.githubusercontent.com/567298/73788817-63a32180-47a6-11ea-96c7-6abba7af2b27.png"></p>

<p>List Streams:</p>

<p><img width="974" alt="image" src="https://user-images.githubusercontent.com/567298/73788860-73bb0100-47a6-11ea-9c80-f2b8bfc18d53.png"></p>

<p>Put Records:</p>

<p><img width="896" alt="image" src="https://user-images.githubusercontent.com/567298/73788895-87666780-47a6-11ea-8d90-2c454ec9174a.png"></p>

<p>Delete Stream:</p>

<p><img width="924" alt="image" src="https://user-images.githubusercontent.com/567298/73788988-aebd3480-47a6-11ea-85d9-9ed7424c648b.png"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup AWS S3 Cross Account Access]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/11/26/setup-aws-s3-cross-account-access/"/>
    <updated>2019-11-26T22:40:12+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/11/26/setup-aws-s3-cross-account-access</id>
    <content type="html"><![CDATA[<p><a href="https://saythanks.io/to/ruanbekker"><img src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" alt="Say Thanks!" /></a> <a href="https://linux-hackers-slack.herokuapp.com/"><img src="https://linux-hackers-slack.herokuapp.com/badge.svg" alt="Slack Status" /></a> <a href="https://linux-hackers.slack.com/"><img src="https://img.shields.io/badge/chat-on_slack-orange.svg" alt="Chat on Slack" /></a> <a href="https://github.com/ruanbekker"><img src="https://img.shields.io/github/followers/ruanbekker.svg?label=Follow&amp;style=social" alt="GitHub followers" /></a></p>

<p><a href="https://twitter.com/ruanbekker?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @ruanbekker</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>

<p>In this tutorial I will demonstrate how to setup cross account access from S3.</p>

<h2>Scenario</h2>

<p>We will have 2 AWS Accounts:</p>

<ol>
<li><p>a Green AWS Account which will host the IAM Users, this account will only be used for our IAM Accounts.</p></li>
<li><p>a Blue AWS Account which will be the account that hosts our AWS Resources, S3 in this scenario.</p></li>
</ol>


<p>We will the allow the Green Account to access the Blue Account&rsquo;s S3 Bucket.</p>

<h2>Setup the Blue Account</h2>

<p>In the Blue Account, we will setup the S3 Bucket, as well as the Trust Relationship with the Policy, which is where we will define what we want to allow for the Green Account.</p>

<p><img width="1280" alt="9488F107-A5B0-4A9E-A7A4-5A91B9805DE3" src="https://user-images.githubusercontent.com/567298/69668149-fe40ff00-1097-11ea-896a-5f3106fe5dfa.png"></p>

<p>Now we need to setup the IAM Role which will allow the Green Account and also define what needs to be allowed.</p>

<p>Go ahead to your IAM Console and create a IAM Policy (just remember to replace the bucket name if you are following along)</p>

<pre><code>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "PutGetListAccessOnS3",
            "Effect": "Allow",
            "Action": [
                "s3:PutObject",
                "s3:GetObject",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::ruanbekker-prod-s3-bucket",
                "arn:aws:s3:::ruanbekker-prod-s3-bucket/*"
            ]
        }
    ]
}
</code></pre>

<p>In my case I have named my IAM Policy <code>CrossAccountS3Access</code>. After you have created your IAM Policy, go ahead and create a IAM Role. Here we need the source account that we want to allow as a trusted entity, which will be the AWS AccountId of the Green Account:</p>

<p><img width="1277" alt="E73FC957-EBFA-4E41-AFDB-D994D6D3110E" src="https://user-images.githubusercontent.com/567298/69668615-ee75ea80-1098-11ea-8536-b32c6c034f7a.png"></p>

<p>Associate the IAM Policy that you created earlier:</p>

<p><img width="1280" alt="610814A8-E8CB-45F7-A038-FE4274FD425C" src="https://user-images.githubusercontent.com/567298/69668712-19603e80-1099-11ea-8ba0-2d0bc84e21cf.png"></p>

<p>After you have done that, you should see a summary screen:</p>

<p><img width="1278" alt="ABAADD0E-9140-4EB1-855A-0B0E46F429FF" src="https://user-images.githubusercontent.com/567298/69668817-50ceeb00-1099-11ea-8bb2-98537a742857.png"></p>

<p>Make note of your IAM Role ARN, it will look something like this: <code>arn:aws:iam::xxxxxxxxxxxx:role/CrossAccountS3Access-Role</code></p>

<h2>Setup the Green Account</h2>

<p>In the Green Account is where we will create the IAM User and the credentials will be provided to the user which requires to access the S3 Bucket.</p>

<p>Let&rsquo;s create a IAM Group, I will name mine <code>prod-s3-users</code>. I will just create the group, as I will attach the policy later:</p>

<p><img width="1280" alt="459D98BF-7A5D-49B4-BBD9-11717655188D" src="https://user-images.githubusercontent.com/567298/69669190-07cb6680-109a-11ea-8193-db476f5fa1db.png"></p>

<p>From the IAM Group, select the Permissions tab and create a New Inline Policy:</p>

<p><img width="1280" alt="E55E521D-A3C1-4669-B0AB-C23A5BA51E21" src="https://user-images.githubusercontent.com/567298/69669427-81635480-109a-11ea-8b4b-7bd79f2a12cd.png"></p>

<p>Select the &ldquo;STS&rdquo; service, select the &ldquo;AssumeRole&rdquo; action, and provide the Role ARN of the Blue Account that we created earlier:</p>

<p><img width="1280" alt="FDECEF7C-14F1-41DC-94F5-B6E63FE46A7D" src="https://user-images.githubusercontent.com/567298/69669597-d8692980-109a-11ea-804c-914c9a8cb608.png"></p>

<p>This will allow the Blue account to assume the credentials from the Green account. And the Blue account will only obtain permissions to access the resources that we have defined in the policy document of the Blue Account. In summary, it should look like this:</p>

<p><img width="1280" alt="0133A1AF-D2B0-4A61-B179-B4B40B81953C" src="https://user-images.githubusercontent.com/567298/69669773-30079500-109b-11ea-83bd-69c8301c4f21.png"></p>

<p>Select the Users tab on the left hand side, create a New IAM User (I will name mine s3-prod-user) and select the &ldquo;Programmatic Access&rdquo; check box as we need API keys as we will be using the CLI to access S3:</p>

<p><img width="1278" alt="ACE1F066-4400-4000-A9D8-0FD438DB7028" src="https://user-images.githubusercontent.com/567298/69669927-82e14c80-109b-11ea-9adf-de5c01cec41c.png"></p>

<p>Then from the next window, add the user to the group that we have created earlier:</p>

<p><img width="1279" alt="0AEC8E84-091F-44CB-966D-BDA93970C881" src="https://user-images.githubusercontent.com/567298/69669976-9987a380-109b-11ea-9c16-ea63cebe2e82.png"></p>

<h2>Test Cross Account Access</h2>

<p>Let&rsquo;s configure our AWS CLI with the API Keys that we received. Our credential provider will consist with 2 profiles, the Green Profile which holds the API Keys of the Green Account:</p>

<pre><code>$ aws configure --profile green
AWS Access Key ID [None]: AKIATPRT2G4SAHA7ZQU2
AWS Secret Access Key [None]: x
Default region name [None]: eu-west-1
Default output format [None]: json
</code></pre>

<p>And configure the Blue profile that will reference the Green account as a source profile and also specify the IAM Role ARN of the Blue Account:</p>

<pre><code>$ vim ~/.aws/credentials
</code></pre>

<pre><code>[blue]
role_arn=arn:aws:iam::xxxxxxxxxxxx:role/CrossAccountS3Access-Role
source_profile=green
region=eu-west-1
</code></pre>

<p>Now we can test if we can authenticate with our Green AWS Account:</p>

<pre><code>$ aws --profile green sts get-caller-identity
{
    "UserId": "AKIATPRT2G4SAHA7ZQU2",
    "Account": "xxxxxxxxxxxx",
    "Arn": "arn:aws:iam:: xxxxxxxxxxxx:user/s3-prod-user"
}
</code></pre>

<p>Now let&rsquo;s upload an object to S3 using our blue profile:</p>

<pre><code>$ aws --profile blue s3 cp foo s3://ruanbekker-prod-s3-bucket/
upload: ./foo to s3://ruanbekker-prod-s3-bucket/foo
</code></pre>

<p>Let&rsquo;s verify if we can see the object:</p>

<pre><code>$ aws --profile blue s3 ls s3://ruanbekker-prod-s3-bucket/
2019-10-03 22:13:30      14582 foo
</code></pre>

<h2>Thank You</h2>

<p>Let me know what you think. If you liked my content, feel free to checkout my content on <strong><a href="https://ruan.dev/">ruan.dev</a></strong> or follow me on twitter at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong></p>

<center><script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Buy Me a Coffee', '#46b798', 'A6423ZIQ');kofiwidget2.draw();</script></center>


<p><br></p>

<p>I&rsquo;ve recently started a <a href="https://bekkerclothing.com/collections/developer">Developer Range</a> t-shirts, let me know what you think:</p>

<div id='product-component-1574800622582'></div>


<script type="text/javascript">
/*&lt;![CDATA[*/
(function () {
  var scriptURL = 'https://sdks.shopifycdn.com/buy-button/latest/buy-button-storefront.min.js';
  if (window.ShopifyBuy) {
    if (window.ShopifyBuy.UI) {
      ShopifyBuyInit();
    } else {
      loadScript();
    }
  } else {
    loadScript();
  }
  function loadScript() {
    var script = document.createElement('script');
    script.async = true;
    script.src = scriptURL;
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(script);
    script.onload = ShopifyBuyInit;
  }
  function ShopifyBuyInit() {
    var client = ShopifyBuy.buildClient({
      domain: 'bekkerclothing.myshopify.com',
      storefrontAccessToken: '68eb29a6d90539cb0321ea90bb043fae',
    });
    ShopifyBuy.UI.onReady(client).then(function (ui) {
      ui.createComponent('product', {
        id: '4392613544020',
        node: document.getElementById('product-component-1574800622582'),
        moneyFormat: '%24%7B%7Bamount%7D%7D',
        options: {
  "product": {
    "styles": {
      "product": {
        "@media (min-width: 601px)": {
          "max-width": "calc(25% - 20px)",
          "margin-left": "20px",
          "margin-bottom": "50px"
        }
      }
    },
    "text": {
      "button": "Add to cart"
    }
  },
  "productSet": {
    "styles": {
      "products": {
        "@media (min-width: 601px)": {
          "margin-left": "-20px"
        }
      }
    }
  },
  "modalProduct": {
    "contents": {
      "img": false,
      "imgWithCarousel": true,
      "button": false,
      "buttonWithQuantity": true
    },
    "styles": {
      "product": {
        "@media (min-width: 601px)": {
          "max-width": "100%",
          "margin-left": "0px",
          "margin-bottom": "0px"
        }
      }
    },
    "text": {
      "button": "Add to cart"
    }
  },
  "cart": {
    "text": {
      "total": "Subtotal",
      "button": "Checkout"
    }
  }
},
      });
    });
  }
})();
/*]]&gt;*/
</script>

]]></content>
  </entry>
  
</feed>
