<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Clustering | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/clustering/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2022-02-08T04:53:53-05:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Setup a 3 Node Replicated Storage Volume With GlusterFS]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/03/05/setup-a-3-node-replicated-storage-volume-with-glusterfs/"/>
    <updated>2019-03-05T14:01:37-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/03/05/setup-a-3-node-replicated-storage-volume-with-glusterfs</id>
    <content type="html"><![CDATA[<p><img src="https://access.redhat.com/documentation/en-US/Red_Hat_Storage/2.1/html/Administration_Guide/images/Replicated_Volume.png" alt="" /></p>

<p>In one of my earlier posts on <a href="https://sysadmins.co.za/tag/glusterfs">GlusterFS</a>, we went through the steps on how to setup a <a href="https://sysadmins.co.za/setup-a-distributed-storage-volume-with-glusterfs/">Distributed Storage Volume</a>, where the end result was to have scalable storage, where size was the requirement.</p>

<p><a href="https://bekkerclothing.com/collections/developer?utm_source=blog.ruanbekker.com&utm_medium=blog&utm_campaign=leaderboard_ad" target="_blank"><img alt="bekker-clothing-developer-tshirts" src="https://user-images.githubusercontent.com/567298/70170981-7c278a80-16d6-11ea-9759-6621d02c1423.png"></a></p>

<h2>What will we be doing today with GlusterFS?</h2>

<p>Today, we will be going through the steps on how to setup a Replicated Storage Volume with GlusterFS, where we will have 3 GlusterFS Nodes, and using the replication factor of 3.</p>

<p><strong>Replication Factor of 3:</strong></p>

<p>In other words, having 3 copies of our data and in our case, since we will have 3 nodes in our cluster, a copy of our data will reside on each node.</p>

<p><strong>What about Split-Brain:</strong></p>

<p>In Clustering, we get the term Split-Brain, where a node dies or leaves the cluster, the cluster reforms itself with the available nodes and then during this reformation, instead of the remaining nodes staying with the same cluster, 2 subset of cluster are created, and they are not aware of each other, which causes data corruption, here&rsquo;s a great resource on <a href="http://techthoughts.typepad.com/managing_computers/2007/10/split-brain-quo.html">Split-Brain</a></p>

<p>To prevent Split-Brain in GlusterFS, we can setup a <a href="https://gluster.readthedocs.io/en/latest/Administrator%20Guide/arbiter-volumes-and-quorum/">Arbiter Volume</a>. In a Replica Count of 3 and Arbiter count of 1: 2 Nodes will hold the replicated data, and the 1 Node which will be the Arbiter node, will only host the file/directory names and metadata but not any data. I will write up an <a href="">article</a> on this in the future.</p>

<h2>Getting Started:</h2>

<p>Let&rsquo;s get started on setting up a 3 Node Replicated GlusterFS. Each node will have an additional drive that is 50GB in size, which will be part of our GlusterFS Replicated Volume. I will also be using Ubuntu 16.04 as my linux distro.</p>

<p><strong>Preparing DNS Resolution:</strong></p>

<p>I will install GlusterFS on each node, and in my setup I have the following DNS entries:</p>

<ul>
<li>gfs01 (10.0.0.2)</li>
<li>gfs02 (10.0.0.3)</li>
<li>gfs03 (10.0.0.4)</li>
</ul>


<p><strong>Preparing our Secondary Drives:</strong></p>

<p>I will be formatting my drives with <code>XFS</code>. Listing our block volumes:</p>

<pre><code class="bash">$ lsblk
NAME MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vdb  253:16   0 46.6G  0 disk
vda  253:0    0 18.6G  0 disk /
</code></pre>

<p>Creating the FileSystem with XFS, which we will be running on each node:</p>

<pre><code class="bash">$ mkfs.xfs /dev/vdb
</code></pre>

<p>Then creating the directories where our bricks will reside, and also add an entry to our <code>/etc/fstab</code> so that our disk gets mounted when the operating system boots:</p>

<pre><code class="bash"># node: gfs01
$ mkdir /gluster/bricks/1 -p
$ echo '/dev/vdb /gluster/bricks/1 xfs defaults 0 0' &gt;&gt; /etc/fstab
$ mount -a
$ mkdir /gluster/bricks/1/brick

# node: gfs02
$ mkdir /gluster/bricks/2 -p
$ echo '/dev/vdb /gluster/bricks/2 xfs defaults 0 0' &gt;&gt; /etc/fstab
$ mount -a
$ mkdir /gluster/bricks/2/brick

# node: gfs03
$ mkdir /gluster/bricks/3 -p
$ echo '/dev/vdb /gluster/bricks/3 xfs defaults 0 0' &gt;&gt; /etc/fstab
$ mount -a
$ mkdir /gluster/bricks/3/brick
</code></pre>

<p>After this has been done, we should see that the disks are mounted, for example on node: <code>gfs01</code>:</p>

<pre><code>$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/vda         18G  909M   17G   3% /
/dev/vdb         47G   80M   47G   1% /gluster/bricks/1
</code></pre>

<h2>Installing GlusterFS on Each Node:</h2>

<p>Installing GlusterFS, repeat this on all 3 Nodes:</p>

<pre><code class="bash">$ apt update &amp;&amp; sudo apt upgrade -y
$ apt install xfsprogs attr glusterfs-server glusterfs-common glusterfs-client -y
$ systemctl enable glusterfs-server
</code></pre>

<p>In order to add the nodes to the trusted storage pool, we will have to add them by using <code>gluster peer probe</code>. Make sure that you can resolve the hostnames to the designated IP Addresses, and that traffic is allowed.</p>

<pre><code class="bash">$ gluster peer probe gfs01
$ gluster peer probe gfs02
$ gluster peer probe gfs03
</code></pre>

<p>Now that we have added our nodes to our trusted storage pool, lets verify that by listing our pool:</p>

<pre><code class="bash">$ gluster pool list
UUID                                    Hostname                State
f63d0e77-9602-4024-8945-5a7f7332bf89    gfs02                   Connected
2d4ac6c1-0611-4e2e-b4af-9e4aa8c1556d    gfs03                   Connected
6a604cd9-9a9c-406d-b1b7-69caf166a20e    localhost               Connected
</code></pre>

<p>Great! All looks good.</p>

<h2>Create the Replicated GlusterFS Volume:</h2>

<p>Let&rsquo;s create our Replicated GlusterFS Volume, named <code>gfs</code>:</p>

<pre><code class="bash">$ gluster volume create gfs \
  replica 3 \
  gfs01:/gluster/bricks/1/brick \
  gfs02:/gluster/bricks/2/brick \
  gfs03:/gluster/bricks/2/brick 

volume create: gfs: success: please start the volume to access data
</code></pre>

<p>Now that our volume is created, lets list it to verify that it is created:</p>

<pre><code class="bash">$ gluster volume list
gfs
</code></pre>

<p>Now, start the volume:</p>

<pre><code class="bash">$ gluster volume start gfs
volume start: gfs: success
</code></pre>

<p>View the status of our volume:</p>

<pre><code class="bash">$ gluster volume status gfs
Status of volume: gfs
Gluster process                             TCP Port  RDMA Port  Online  Pid
------------------------------------------------------------------------------
Brick gfs01:/gluster/bricks/1/brick         49152     0          Y       6450
Brick gfs02:/gluster/bricks/2/brick         49152     0          Y       3460
Brick gfs03:/gluster/bricks/3/brick         49152     0          Y       3309
</code></pre>

<p>Next, view the volume inforation:</p>

<pre><code class="bash">$ gluster volume info gfs

Volume Name: gfs
Type: Replicate
Volume ID: 6f827df4-6df5-4c25-99ee-8d1a055d30f0
Status: Started
Number of Bricks: 1 x 3 = 3
Transport-type: tcp
Bricks:
Brick1: gfs01:/gluster/bricks/1/brick
Brick2: gfs02:/gluster/bricks/2/brick
Brick3: gfs03:/gluster/bricks/3/brick
</code></pre>

<h2>Security:</h2>

<p>From a GlusterFS level, it will allow clients to connect by default. To authorize these 3 nodes to connect to the GlusterFS Volume:</p>

<pre><code class="bash">$ gluster volume set gfs auth.allow 10.0.0.2,10.0.0.3,10.0.0.4
</code></pre>

<p>Then if you would like to remove this rule:</p>

<pre><code class="bash">$ gluster volume set gfs auth.allow *
</code></pre>

<h2>Mount the GlusterFS Volume to the Host:</h2>

<p>Mount the GlusterFS Volume to each node, so we will have to mount it to each node, and also append it to our <code>/etc/fstab</code> file so that it mounts on boot:</p>

<pre><code class="bash">$ echo 'localhost:/gfs /mnt glusterfs defaults,_netdev,backupvolfile-server=localhost 0 0' &gt;&gt; /etc/fstab
$ mount.glusterfs localhost:/gfs /mnt
</code></pre>

<p><strong>Verify the Mounted Volume:</strong></p>

<p>Check the mounted disks, and you will find that the Replicated GlusterFS Volume is mounted on our <code>/mnt</code> partition.</p>

<pre><code class="bash">$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/vda         18G  909M   17G   3% /
/dev/vdb         47G   80M   47G   1% /gluster/bricks/1
localhost:/gfs   47G   80M   47G   1% /mnt
</code></pre>

<p>You will note that GlusterFS Volume has a total size of 47GB usable space, which is the same size as one of our disks, but that is because we have a replicated volume with a replication factor of 3:  <code>(47 * 3 / 3)</code></p>

<p>Now we have a Storage Volume which has 3 Replicas, one copy on each node, which allows us Data Durability on our Storage.</p>

<p><p></p>

<p><center><script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init(&lsquo;Buy Me a Coffee&rsquo;, &lsquo;#46b798&rsquo;, &lsquo;A6423ZIQ&rsquo;);kofiwidget2.draw();</script></center></p>

<p><p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a Distributed Storage Volume With GlusterFS]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/03/04/setup-a-distributed-storage-volume-with-glusterfs/"/>
    <updated>2019-03-04T15:32:53-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/03/04/setup-a-distributed-storage-volume-with-glusterfs</id>
    <content type="html"><![CDATA[<p><img src="https://glusterdocs-beta.readthedocs.io/en/latest/_images/dist-volume.png" alt="" /></p>

<p>GlusterFS is a Awesome Scalable Networked Filesystem, which makes it Easy to Create Large and Scalable Storage Solutions on Commodity Hardware.</p>

<p><a href="https://bekkerclothing.com/collections/developer?utm_source=blog.ruanbekker.com&utm_medium=blog&utm_campaign=leaderboard_ad" target="_blank"><img alt="bekker-clothing-developer-tshirts" src="https://user-images.githubusercontent.com/567298/70170981-7c278a80-16d6-11ea-9759-6621d02c1423.png"></a></p>

<p><strong>Basic Concepts of GlusterFS:</strong></p>

<p>Brick:
* In GlusterFS, a brick is the basic unit of storage, represented by a directory on the server in the trusted storage pool.</p>

<p>Gluster Volume:
* A Gluster volume is a Logical Collection of Bricks.</p>

<p>Distributed Filesystem:
* The concept is to enable multiple clients to concurrently access data which is spread across multple servers in a trusted storage pool. This is also a great solution to prevent data corruption, enable highly available storage systems, etc.</p>

<p><a href="http://gluster.readthedocs.io/en/latest/Administrator%20Guide/glossary/">More concepts</a> can be retrieved from their documentation.</p>

<h2>Different GlusterFS Volume Types:</h2>

<p>With GlusterFS you can create the following types of Gluster Volumes:</p>

<ul>
<li>Distributed Volumes: (Ideal for Scalable Storage, No Data Redundancy)</li>
<li>Replicated Volumes: (Better reliability and data redundancy)</li>
<li>Distributed-Replicated Volumes: (HA of Data due to Redundancy and Scaling Storage)</li>
<li><a href="http://gluster.readthedocs.io/en/latest/Quick-Start-Guide/Architecture/">More detail</a> on GlusterFS Architecture</li>
</ul>


<h2>Setup a Distributed Gluster Volume:</h2>

<p>In this guide we will setup a 3 Node Distributed GlusterFS Volume on Ubuntu 16.04.</p>

<p>For this use case we would like to achieve a storage solution to scale the size of our storage, and not really worried about redundancy as, with a Distributed Setup we can increase the size of our volume, the more bricks we add to our GlusterFS Volume.</p>

<h2>Setup: Our Environment</h2>

<p>Each node has 2 disks, <code>/dev/xvda</code> for the Operating System wich is 20GB and <code>/dev/xvdb</code> which has 100GB. After we have created our GlusterFS Volume, we will have a Gluster Volume of 300GB.</p>

<p>Having a look at our disks:</p>

<pre><code class="bash">$ lsblk
NAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
xvda    202:0    0   20G  0 disk
└─xvda1 202:1    0   20G  0 part /
xvdb    202:16   0  100G  0 disk 
</code></pre>

<p>If you don&rsquo;t have DNS setup for your nodes, you can use your /etc/hosts file for all 3 nodes, which I will be using in this demonstration:</p>

<pre><code class="bash">$ cat /etc/hosts
172.31.13.226   gluster-node-1
172.31.9.7      gluster-node-2
172.31.15.34    gluster-node-3
127.0.0.1       localhost
</code></pre>

<h2>Install GlusterFS from the Package Manager:</h2>

<p>Note that all the steps below needs to be performed on all 3 nodes, unless specified otherwise:</p>

<pre><code class="bash">$ apt update &amp;&amp; apt upgrade -y
$ apt install xfsprogs attr glusterfs-server glusterfs-client glusterfs-common -y
</code></pre>

<h2>Format and Prepare the Gluster Disks:</h2>

<p>We will create a XFS Filesystem for our 100GB disk, create the directory path where we will mount our disk onto, and also load it into <code>/etc/fstab</code>:</p>

<pre><code class="bash">$ mkfs.xfs /dev/xvdb
$ mkdir /gluster
$ echo '/dev/xvdb /gluster xfs defaults 0 0' &gt;&gt; /etc/fstab
$ mount -a
</code></pre>

<p>After we mounted the disk, we should see that our disk is mounted to <code>/gluster</code>:</p>

<pre><code class="bash">$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/xvda1       20G  1.2G   19G   7% /
/dev/xvdb       100G   33M  100G   1% /gluster
</code></pre>

<p>After our disk is mounted, we can proceed by creating the brick directory on our disk that we mounted, from the step above:</p>

<pre><code class="bash">$ mkdir /gluster/brick
</code></pre>

<h2>Start GlusterFS Service:</h2>

<p>Enable GlusterFS on startup, start the service and make sure that the service is running:</p>

<pre><code class="bash">$ systemctl enable glusterfs-server
$ systemctl restart glusterfs-server
$ systemctl is-active glusterfs-server
active
</code></pre>

<h2>Discover All the Nodes for our Cluster:</h2>

<p>The following will only be done on one of the nodes. First we need to discover our other nodes.</p>

<p>The node that you are currently on, will be discovered by default and only needs the other 2 nodes to be discovered:</p>

<pre><code class="bash">$ gluster peer probe gluster-node-2
$ gluster peer probe gluster-node-3
</code></pre>

<p>Let&rsquo;s verify this by listing all the nodes in our cluster:</p>

<pre><code class="bash">$ gluster pool list
UUID                                    Hostname        State
6e02731c-6472-4ea4-bd48-d5dd87150e8b    gluster-node-2  Connected
9d4c2605-57ba-49e2-b5da-a970448dc886    gluster-node-3  Connected
608f027e-e953-413b-b370-ce84050a83c9    localhost       Connected
</code></pre>

<h2>Create the Distributed GlusterFS Volume:</h2>

<p>We will create a Distributed GlusterFS Volume across 3 nodes, and we will name the volume <code>gfs</code>:</p>

<pre><code class="bash">$ gluster volume create gfs \
  gluster-node-1:/gluster/brick \
  gluster-node-2:/gluster/brick \
  gluster-node-3:/gluster/brick

volume create: gfs: success: please start the volume to access data
</code></pre>

<h2>Start the GlusterFS Volume:</h2>

<p>Now start the <code>gfs</code> GlusterFS Volume:</p>

<pre><code class="bash">$ gluster volume start gfs
volume start: gfs: success
</code></pre>

<p>To get information about the volume:</p>

<pre><code class="bash">$ gluster volume info gfs

Volume Name: gfs
Type: Distribute
Volume ID: c08bc2e8-59b3-49e7-bc17-d4bc8d99a92f
Status: Started
Number of Bricks: 3
Transport-type: tcp
Bricks:
Brick1: gluster-node-1:/gluster/brick
Brick2: gluster-node-2:/gluster/brick
Brick3: gluster-node-3:/gluster/brick
Options Reconfigured:
performance.readdir-ahead: on
</code></pre>

<p>Status information about our Volume:</p>

<pre><code class="bash">$ gluster volume status

Status of volume: gfs
Gluster process                             TCP Port  RDMA Port  Online  Pid
------------------------------------------------------------------------------
Brick gluster-node-1:/gluster/brick         49152     0          Y       7139
Brick gluster-node-2:/gluster/brick         49152     0          Y       7027
Brick gluster-node-3:/gluster/brick         49152     0          Y       7099
NFS Server on localhost                     2049      0          Y       7158
NFS Server on gluster-node-2                2049      0          Y       7046
NFS Server on gluster-node-3                2049      0          Y       7118

Task Status of Volume gfs
------------------------------------------------------------------------------
There are no active volume tasks
</code></pre>

<h2>Mounting our GlusterFS Volume:</h2>

<p>On all the clients, in this case our 3 nodes, load the mount information into <code>/etc/fstab</code> and then mount the GlusterFS Volume:</p>

<pre><code class="bash">$ echo 'localhost:/gfs /mnt glusterfs defaults,_netdev,backupvolfile-server=gluster-node-1 0 0' &gt;&gt; /etc/fstab
$ mount -a
</code></pre>

<p>Now that the volume is mounted, have a look at your disk info, and you will find that you have a <code>300GB</code> GlusterFS Volume mounted:</p>

<pre><code class="bash">$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/xvda1       20G  1.3G   19G   7% /
/dev/xvdb       100G   33M  100G   1% /gluster
localhost:/gfs  300G   98M  300G   1% /mnt
</code></pre>

<p>As mentioned before, this is most probably for a scenario where you would like to achieve a high storage size and not really concerned about data availability.</p>

<p>In the next couple of weeks I will also go through the Replicated, Distributed-Replicated and <a href="https://gluster.readthedocs.io/en/latest/Administrator%20Guide/Gluster%20On%20ZFS/">GlusterFS with ZFS</a> setups.</p>

<h2>Resources:</h2>

<ul>
<li><a href="http://gluster.readthedocs.io/en/latest/Quick-Start-Guide/Terminologies/">GlusterFS Terminologies</a></li>
<li><a href="http://gluster.readthedocs.io/en/latest/Quick-Start-Guide/Architecture/">GlusterFS Architecture</a></li>
<li><a href="http://gluster.readthedocs.io/en/latest/Administrator%20Guide/Gluster%20On%20ZFS/">GlusterFS with ZFS</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a 3 Node Docker Swarm Cluster on Ubuntu 16.04]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/01/10/setup-a-3-node-docker-swarm-cluster-on-ubuntu-16-dot-04/"/>
    <updated>2019-01-10T09:52:07-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/01/10/setup-a-3-node-docker-swarm-cluster-on-ubuntu-16-dot-04</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/53351889-85572000-392a-11e9-9720-464e9318206e.jpg" alt="" /></p>

<p>Docker Swarm is a Clustering and Orchestration Framework for the Docker ecosystem. Have a look at their <a href="https://docs.docker.com/engine/swarm/">official documentation</a> for detailed information.</p>

<p>In this Tutorial we will Setup a 3 Node Docker Swarm Cluster and to Demonstrate How Easy it is to Deploy a Web Application with 2 Replicas from a Docker Image.</p>

<p><br></p>

<script type="text/javascript">
  ( function() {
    if (window.CHITIKA === undefined) { window.CHITIKA = { 'units' : [] }; };
    var unit = {"calltype":"async[2]","publisher":"rbekker87","width":728,"height":90,"sid":"Chitika Default"};
    var placement_id = window.CHITIKA.units.length;
    window.CHITIKA.units.push(unit);
    document.write('<div id="chitikaAdBlock-' + placement_id + '"></div>');
}());
</script>


<script type="text/javascript" src="//cdn.chitika.net/getads.js" async></script>


<p><br></p>

<h2>Overview of What we will be Doing</h2>

<ul>
<li>Install Docker on 3 Servers with Ubuntu 16.04</li>
<li>Initialize the Swarm and Join the Worker Nodes</li>
<li>Create a Nginx Service with 2 Replicas</li>
<li>Do some Inspection: View some info on the Service</li>
</ul>


<h2>Prerequisites</h2>

<p>3 Fresh Deployed Ubuntu 16.04 Servers. ( 1GB Memory Servers will be good for development )</p>

<h2>What is Docker</h2>

<p>Docker is a Open Source Technology that allows you to create lightweight, isolated, reproducible application instances which is called Containers. Docker is built on top of the LXC technology, so it uses Linux Containers and as mentioned, it&rsquo;s lightweight compared to a traditional VM.</p>

<p>A Container is isolated and uses the Kernel of the Docker host, it also utilizes Kernel features such as cgroups and namespaces in order to make them isolated.</p>

<h2>Installing Docker Community Edition</h2>

<p>Remove any older versions of Docker that might be present and install the dependencies:</p>

<pre><code class="bash">$ sudo apt remove docker docker-engine -y
$ sudo apt install linux-image-extra-$(uname -r) linux-image-extra-virtual python-setuptools -y
$ sudo apt install apt-transport-https ca-certificates curl software-properties-common -y
</code></pre>

<p>Get the needed repository to setup Docker Community Edition:</p>

<pre><code class="bash">$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
$ sudo apt-key fingerprint 0EBFCD88
$ sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
</code></pre>

<p>Update the repository index and Install Docker Community Edition:</p>

<pre><code class="bash">$ sudo apt update
$ sudo apt install docker-ce -y
$ sudo easy_install pip
$ sudo pip install docker-compose
</code></pre>

<p>Enable Docker on Startup and Start the Docker Engine:</p>

<pre><code class="bash">$ sudo systemctl enable docker
$ sudo systemctl restart docker
</code></pre>

<p>If you would like to execute your docker commands without sudo, add your user to the docker group:</p>

<pre><code class="bash">$ sudo usermod -aG docker $(whoami)
</code></pre>

<p>Test your Setup by Running a Hello World Container. You will see that if the image is not in the local docker image cache, it will pull the image from docker hub (or the respective docker registry), then once the image is saved locally, docker will then instantiate the container from that image:</p>

<pre><code class="bash">$ docker run hello-world
Unable to find image 'hello-world:latest' locally
latest: Pulling from library/hello-world
78445dd45222: Pull complete
Digest: sha256:c5515758d4c5e1e838e9cd307f6c6a0d620b5e07e6f927b07d05f6d12a1ac8d7
Status: Downloaded newer image for hello-world:latest

Hello from Docker!
This message shows that your installation appears to be working correctly.
</code></pre>

<h2>DNS Configuration</h2>

<p>If you have a DNS Server you can configure the A Records for these hosts on DNS, but for simplicity, I will add the noted IP Addresses from the previous step into my <code>/etc/hosts</code> file so we can resolve names to IP&rsquo;s</p>

<p>Open up the the hosts file:</p>

<pre><code class="bash">$ sudo vim /etc/hosts
</code></pre>

<p>In my example, my IP Addresses:</p>

<pre><code>192.0.2.41  manager
192.0.2.42  worker-1
192.0.2.43  worker-2
</code></pre>

<p>Repeat the above steps on the other 2 Servers and make note of the IP Addresses of each node. You should be able to ping and reach the nodes that was configured. Make sure to allow all traffic between these nodes.</p>

<h2>Initialize the Swarm:</h2>

<p>Now we will initialize the swarm on the manager node and as we have more than one network interface, we will specify the &ndash;advertise-addr option:</p>

<pre><code class="bash">$ docker swarm init --advertise-addr 192.0.2.41
Swarm initialized: current node (siqyf3yricsvjkzvej00a9b8h) is now a manager.

    To add a worker to this swarm, run the following command:

    docker swarm join \
    --token SWMTKN-1-0eith07xkcg93lzftuhjmxaxwfa6mbkjsmjzb3d3sx9cobc2zp-97s6xzdt27y2gk3kpm0cgo6y2 \
    192.0.2.41:2377

    To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
</code></pre>

<p>From the response above, we received the join token that allows the workers to register with the manager node. If its a scenario where you want to have more than one manager node, you can run <code>docker swarm join-token manager</code> to receive the join token for additional manager.</p>

<p>Let&rsquo;s add the two worker nodes to the manager:</p>

<pre><code class="bash">$ [worker-1] docker swarm join --token SWMTKN-1-0eith07xkcg93lzftuhjmxaxwfa6mbkjsmjzb3d3sx9cobc2zp-97s6xzdt27y2gk3kpm0cgo6y2 192.0.2.41:2377
This node joined a swarm as a worker.
</code></pre>

<pre><code class="bash">$ [worker-2] docker swarm join --token SWMTKN-1-0eith07xkcg93lzftuhjmxaxwfa6mbkjsmjzb3d3sx9cobc2zp-97s6xzdt27y2gk3kpm0cgo6y2 192.0.2.41:2377
This node joined a swarm as a worker.
</code></pre>

<p>To see the node status, so that we can determine if the nodes are active/available etc, from the manager node, list all the nodes in the swarm:</p>

<pre><code class="bash">[manager] $ docker node ls
ID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS
j14mte3v1jhtbm3pb2qrpgwp6    worker-1  Ready   Active 
siqyf3yricsvjkzvej00a9b8h *  master    Ready   Active        Leader
srl5yzme5hxnzxal2t1efmwje    worker-2  Ready   Active
</code></pre>

<h2>Reobtaining the Join Tokens</h2>

<p>If at any time, you lost your join token, it can be retrieved by running the following for the manager token:</p>

<pre><code class="bash">$ docker swarm join-token manager -q SWMTKN-1-67chzvi4epx28ii18gizcia8idfar5hokojz660igeavnrltf0-09ijujbnnh4v960b8xel58pmj
</code></pre>

<p>And the following to retrieve the worker token:</p>

<pre><code class="bash">$ docker swarm join-token worker -q SWMTKN-1-67chzvi4epx28ii18gizcia8idfar5hokojz660igeavnrltf0-acs21nn28v17uwhw0oqg5ibwx
</code></pre>

<p>Swarm Services in Docker uses a declarative model which means that you define the desired state of the service, and rely on Docker to maintain this state. More information on this can be found on their <a href="https://docs.docker.com/engine/swarm/how-swarm-mode-works/services/">Documentation</a></p>

<p>At this moment, we will see that we have no services running in our swarm:</p>

<pre><code class="bash">[manager] $ docker service ls
ID  NAME  MODE  REPLICAS  IMAGE
</code></pre>

<h2>Deploying our First Service</h2>

<p>Now onto the creation of a standard nginx service with 2 replicas, which means that there will be 2 containers of nginx running in our swarm.</p>

<p>But first, we need to create a overlay network, which is a network driver that creates a distributed network among multiple Docker daemon hosts. Swarm takes care of the routing automatically, which is routed via the port mappings. So you can have that your container sits on worker-2, when you hit your manager node on the published port, it will route the request to the desired application that resides on the respective container.</p>

<p>To create a overlay network called mynet:</p>

<pre><code class="bash">[manager] $ docker network create --driver overlay mynet
</code></pre>

<p>Now onto creating the Service. If any of these containers fail, they will handled by the manager node and will be spawned again to have the desired number that we set on the replica option:</p>

<pre><code class="bash">[manager] $ docker service create --name my-web --publish 8080:80 --replicas 2 --network mynet nginx
</code></pre>

<p>Let&rsquo;s have a look at our nginx service:</p>

<pre><code class="bash">[manager] $ docker service ls
ID            NAME    MODE        REPLICAS  IMAGE
1okycpshfusq  my-web  replicated  2/2       nginx:latest
</code></pre>

<p>After we see that the replica count is 2/2 our service is ready.</p>

<p>To see on which nodes our containers are running that makes up our service:</p>

<pre><code class="bash">[manager] $ docker service ps my-web
ID            NAME      IMAGE         NODE      DESIRED STATE  CURRENT STATE           ERROR  PORTS
k0qqrh8s0c2d  my-web.1  nginx:latest  worker-1  Running        Running 30 seconds ago
nku9wer6tmll  my-web.2  nginx:latest  worker-2  Running        Running 30 seconds ago
</code></pre>

<p>From the above output, we can see that worker-1 and worker-2 are serving our containers for our service. We can also retrieve more information of our service by using the inspect option, which will give you a detailed response in json format of the service:</p>

<pre><code class="bash">[manager] $ docker service inspect my-web
</code></pre>

<p>We can get the Endpoint Port info by using inspect and using the &ndash;format parameter to filter the output:</p>

<pre><code class="bash">[manager] $ docker service inspect --format="" my-web  | python -m json.tool
</code></pre>

<p>From the output we will find the PublishedPort is the Port that we Expose, which will be the listener. Our TargetPort will be the port that is listening on the container:</p>

<pre><code class="json">[
    {
        "Protocol": "tcp",
        "PublishMode": "ingress",
        "PublishedPort": 8080,
        "TargetPort": 80
    }
]
</code></pre>

<p>Now that we went through the inspection of our service, its time to test our base nginx service.</p>

<h2>Testing Nginx in our Swarm</h2>

<p>Make a request against your docker node manager address on the port that was exposed, in this case 8080:</p>

<pre><code class="bash">$ curl -I http://docker-node-manager-ip:8080

HTTP/1.1 200 OK
Server: nginx/1.15.5
Date: Thu, 10 Jan 2019 14:48:40 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Tue, 02 Oct 2018 14:49:27 GMT
Connection: keep-alive
ETag: "5bb38577-264"
Accept-Ranges: bytes
</code></pre>

<p>Now we have successfull setup a 3 node docker swarm cluster and deployed a basic nginx service to our swarm. Please have a look at my other <a href="https://blog.ruanbekker.com/blog/categories/docker/">Docker Swarm Tutorials</a> for other content.</p>

<h2>Thank You</h2>

<p>Please feel free to show support by, <strong>sharing</strong> this post, making a <strong>donation</strong>, <strong>subscribing</strong> or <strong>reach out to me</strong> if you want me to demo and write up on any specific tech topic.</p>

<center>
<form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_top">
<input type="hidden" name="cmd" value="_s-xclick" />
<input type="hidden" name="hosted_button_id" value="W7CBGYTCWGANQ" />
<input type="image" src="https://user-images.githubusercontent.com/567298/49853901-461c3700-fdf1-11e8-9d80-8a424a3173af.png" border="0" name="submit" title="PayPal - The safer, easier way to pay online!" alt="Donate with PayPal button" />
</form>
</center>




<p><p></p>

<p>Thanks for reading!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rejoining or Bootstrapping MySQL Galera Cluster Nodes After Shutdown]]></title>
    <link href="https://blog.ruanbekker.com/blog/2017/12/10/rejoining-or-bootstrapping-mysql-galera-cluster-nodes-after-shutdown/"/>
    <updated>2017-12-10T18:03:44-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2017/12/10/rejoining-or-bootstrapping-mysql-galera-cluster-nodes-after-shutdown</id>
    <content type="html"><![CDATA[<p>I have a 3 Node MySQL Galera Cluster that faced a shutdown on all 3 nodes at the same time, luckily this is only a testing environment, but at that time it was down and did not want to start up.</p>

<h2>Issues Faced</h2>

<p>When trying to start MySQL the only error visible was:</p>

<pre><code class="bash">$ /etc/init.d/mysql restart
 * MySQL server PID file could not be found!
Starting MySQL
........ * The server quit without updating PID file (/var/run/mysqld/mysqld.pid).
 * Failed to restart server.
</code></pre>

<p>At that time I can see that the galera port is started, but not mysql:</p>

<pre><code class="bash">$ ps aux | grep mysql
root     23580  0.0  0.0   4508  1800 pts/0    S    00:37   0:00 /bin/sh /usr/bin/mysqld_safe --datadir=/var/lib/mysql --pid-file=/var/run/mysqld/mysqld.pid
mysql    24144  0.7 22.2 1185116 455660 pts/0  Sl   00:38   0:00 /usr/sbin/mysqld --basedir=/usr --datadir=/var/lib/mysql --plugin-dir=/usr/lib/mysql/plugin --user=mysql --log-error=/var/log/mysql/error.log --pid-file=/var/run/mysqld/mysqld.pid --socket=/var/run/mysqld/mysqld.sock --port=3306 --wsrep_start_position=long:string

$ netstat -tulpn
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:4567            0.0.0.0:*               LISTEN      25507/mysqld
</code></pre>

<h2>Why?</h2>

<p>More in detail is explained on a <a href="https://severalnines.com/blog/how-bootstrap-mysqlmariadb-galera-cluster">SeveralNines Blog Post</a>, but due to the fact that all the nodes left the cluster, one of the nodes needs to be started as a referencing point, before the other nodes can rejoin or bootstrapped to the cluster.</p>

<h2>Rejoining the Cluster</h2>

<p>Consult the blog for more information, but from my end, I had a look at the node with the highest seqno and then updated <code>safe_to_bootstrap</code> to <code>1</code>:</p>

<pre><code class="bash">$ cat /var/lib/mysql/grastate.dat
# GALERA saved state
version: 2.1
uuid:    e9f9cf6a-87a1-11e7-9fb4-52612b906897
seqno:   123512
safe_to_bootstrap: 1
</code></pre>

<p>Then made sure that no mysql processes are running, then did a bootstrap:</p>

<pre><code class="bash">$ /etc/init.d/mysql bootstrap
Bootstrapping the cluster
Starting MySQL
</code></pre>

<p>Then restarted mysql on the other nodes.</p>

<h2>Verifying</h2>

<p>To verify that all your nodes has checked in, I have 3 nodes:</p>

<pre><code class="sql">mysql&gt; SHOW STATUS LIKE 'wsrep_%';
+------------------------------+---------------------------------------------------+
| Variable_name                | Value                                             |
+------------------------------+---------------------------------------------------+
| wsrep_local_recv_queue_avg   | 0.000000                                          |
| wsrep_local_state_comment    | Synced                                            |
| wsrep_incoming_addresses     | 10.3.132.91:3306,10.4.1.201:3306,10.4.113.21:3306 |
| wsrep_evs_state              | OPERATIONAL                                       |
| wsrep_cluster_size           | 3                                                 |
| wsrep_cluster_status         | Primary                                           |
| wsrep_connected              | ON                                                |
+------------------------------+---------------------------------------------------+
</code></pre>

<p>or a shorter version:</p>

<pre><code class="sql">mysql&gt; SHOW GLOBAL STATUS LIKE 'wsrep_cluster_size';
+------------------------------+---------------------------------------------------+
| Variable_name                | Value                                             |
+------------------------------+---------------------------------------------------+
| wsrep_cluster_size           | 3                                                 |
+------------------------------+---------------------------------------------------+
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a 3 Node Galera MariaDB Cluster on Ubuntu 16]]></title>
    <link href="https://blog.ruanbekker.com/blog/2017/11/22/setup-a-3-node-galera-mariadb-cluster-on-ubuntu-16/"/>
    <updated>2017-11-22T18:17:14-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2017/11/22/setup-a-3-node-galera-mariadb-cluster-on-ubuntu-16</id>
    <content type="html"><![CDATA[<p><img src="https://i.snag.gy/lpT6Du.jpg" alt="" /></p>

<p>Today we will setup a 3-Node Galera MariaDB Cluster which is a Multi Master MySQL/MariaDB Cluster on Ubuntu 16.04</p>

<h2>Our Server Details:</h2>

<pre><code class="bash">172.31.11.174     mysql-1
172.31.13.206     mysql-2
172.31.6.93       mysql-3
</code></pre>

<h2>Update Repo Index and Upgrade:</h2>

<p>Update the repository indexes and install the needed packages:</p>

<pre><code class="bash">$ sudo apt update &amp;&amp; sudo apt upgrade -y
</code></pre>

<p>Install the needed repository and packages:</p>

<pre><code class="bash">$ apt install software-properties-common -y
$ apt-key adv --recv-keys --keyserver hkp://keyserver.ubuntu.com:80 0xF1656F24C74CD1D8
$ add-apt-repository 'deb [arch=amd64,i386,ppc64el] http://mirror.lstn.net/mariadb/repo/10.1/ubuntu xenial main'
$ apt update
$ apt install mariadb-server rsync -y
</code></pre>

<h2>Configuration:</h2>

<pre><code class="bash">cat &gt; /etc/mysql/conf.d/galera.cnf &lt;&lt; EOF
[mysqld]
binlog_format=ROW
default-storage-engine=innodb
innodb_autoinc_lock_mode=2
bind-address=0.0.0.0

# Galera Provider Configuration
wsrep_on=ON
wsrep_provider=/usr/lib/galera/libgalera_smm.so

# Galera Cluster Configuration
wsrep_cluster_name="my-galera-cluster"
wsrep_cluster_address="gcomm://172.31.11.174,172.31.13.206,172.31.6.93"
# Galera Synchronization Configuration
wsrep_sst_method=rsync

# Galera Node Configuration
wsrep_node_address="172.31.11.174"
wsrep_node_name="mysql-1"
EOF
</code></pre>

<p>Comment out bind-address, so that MariaDB process is reachable from other nodes, by default it wont be in the config, but just to make sure, if it is uncommented, comment the config:</p>

<pre><code class="bash /etc/mysql/my.cnf"># bind-address = 127.0.0.1
</code></pre>

<p>Stop the MariaDB Process:</p>

<pre><code class="bash">$ systemctl stop mariadb
</code></pre>

<p>Note: Repeat the above steps on all 3 nodes.</p>

<h2>Initialize the Cluster:</h2>

<p>On the First Node, Initialize the Galera Cluster:</p>

<pre><code class="bash">$ /usr/bin/galera_new_cluster
$ systemctl enable mariadb
</code></pre>

<p>Check how many nodes are active in the Cluster:</p>

<pre><code class="mysql">$ mysql -u root -p -e "SHOW STATUS LIKE 'wsrep_cluster_size';"
Enter password:
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| wsrep_cluster_size | 1     |
+--------------------+-------+
</code></pre>

<h2>Node-2: Start and Enable MariaDB</h2>

<pre><code class="bash">$ systemctl start mariadb
$ systemctl enable mariadb
</code></pre>

<p>Verify that the Node has checked in with the Cluster:</p>

<pre><code class="mysql">$ mysql -u root -p -e "SHOW STATUS LIKE 'wsrep_cluster_size';"
Enter password:
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| wsrep_cluster_size | 2     |
+--------------------+-------+
</code></pre>

<h2>Node-3: Start and Enable MariaDB</h2>

<pre><code class="bash">$ systemctl start mariadb
$ systemctl enable mariadb
</code></pre>

<p>Verify that the Node has checked in with the Cluster:</p>

<pre><code class="mysql">$ mysql -u root -p -e "SHOW STATUS LIKE 'wsrep_cluster_size';"
Enter password:
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| wsrep_cluster_size | 3     |
+--------------------+-------+
</code></pre>

<h2>Create a Database, Table and Record:</h2>

<p>Write some data to the table, then reboot the node, in this example on node-1, then logon to node-2 check the number of nodes that&rsquo;s active in the cluster, which should be 2, then at the same time, look if the data is replicated:</p>

<h2>Node-1: Writing the Data to Our Galera Cluster</h2>

<pre><code class="mysql">MariaDB [(none)]&gt; create database test;
MariaDB [(none)]&gt; use test;
MariaDB [test]&gt;   create database test;
MariaDB [test]&gt;   create table foo (name VARCHAR(20));
MariaDB [test]&gt;   insert into foo values('ruan');
MariaDB [test]&gt;   select * from foo;
+------+
| name |
+------+
| ruan |
+------+
</code></pre>

<p>Now that our data is in our database, reboot the node, logon to node-2 and check if the data is replicated:</p>

<pre><code class="mysql">$ mysql -u root -p
MariaDB [(none)]&gt; use test;
MariaDB [test]&gt;   select * from foo;
+------+
| name |
+------+
| ruan |
+------+
</code></pre>

<p>While the one node is rebooting, check how many nodes are checked into our cluster:</p>

<pre><code class="mysql">$ mysql -u root -p -e "SHOW STATUS LIKE 'wsrep_cluster_size';"
Enter password:
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| wsrep_cluster_size | 2     |
+--------------------+-------+
</code></pre>

<p>Our data is replicated, and after waiting for a couple of seconds, we retry our command to see if the rebooted node checked into the cluster:</p>

<pre><code class="mysql">$ mysql -u root -p -e "SHOW STATUS LIKE 'wsrep_cluster_size';"
Enter password:
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| wsrep_cluster_size | 3     |
+--------------------+-------+
</code></pre>

<p>We can confirm that the node that was rebooted, has checked in with the cluster again.</p>

<h2>Firewall Rules opened while testing:</h2>

<p>TCP: <code>3306, 4567, 4568, 4444</code>
UDP: <code>4567</code></p>
]]></content>
  </entry>
  
</feed>
