<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Containers | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/containers/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2020-11-02T10:16:58+00:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Build Small Golang Docker Containers]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/04/03/build-small-golang-docker-containers/"/>
    <updated>2019-04-03T08:24:08-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/04/03/build-small-golang-docker-containers</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/55478306-aabb0600-561b-11e9-9cc6-730fadb4beeb.png" alt="" /></p>

<p>In this tutorial I will show you how to build really small docker containers for golang applications. And I mean the difference between 310MB down to 2MB</p>

<h2>But Alpine..</h2>

<p>So we thinking lets go with alpine right? Yeah sure lets build a small, app running on go with alpine.</p>

<p>Our application code:</p>

<pre><code class="go app.go">package main

import (
  "fmt"
  "math/rand"
  "time"
)

func main() {
  lekkewords := []string{
    "dog", "cat", "fish", "giraffe",
    "moo", "spider", "lion", "apple",
    "tree", "moon", "snake", "mountain lion",
    "trooper", "burger", "nasa", "yes",
  }

  rand.Seed(time.Now().UnixNano())
  var zelength int = len(lekkewords)
  var indexnum int = rand.Intn(zelength-1)
  word := lekkewords[indexnum]

  fmt.Println("Number of words:", zelength)
  fmt.Println("Selected index number:", indexnum)
  fmt.Println("Selected word is:", word)
}
</code></pre>

<p>Our Dockerfile:</p>

<pre><code class="docker Dockerfile">FROM golang:alpine

WORKDIR $GOPATH/src/mylekkepackage/mylekkeapp/
COPY app.go .
RUN go build -o /go/app

CMD ["/go/app"]
</code></pre>

<p>Let&rsquo;s package our app to an image:</p>

<pre><code>❯ docker build -t mygolangapp:using-alpine .
</code></pre>

<p>Inspect the size of our image, as you can see it being <strong>310MB</strong></p>

<pre><code>❯ docker images "mygolangapp:*"
REPOSITORY          TAG                 IMAGE ID            CREATED              SIZE
mygolangapp         using-alpine        eea1d7bde218        About a minute ago   310MB
</code></pre>

<p>Just make sure it actually works:</p>

<pre><code>❯ docker run mygolangapp:using-alpine
Number of words: 16
Selected index number: 11
Selected word is: mountain lion
</code></pre>

<p>But for something just returning random selected text, 310MB is a bit crazy.</p>

<h2>Multi Stage Builds</h2>

<p>As Go binaries are self-contained, we can make use of docker&rsquo;s multi stage builds, where we can build our application on alpine and use the binary on a scratch image:</p>

<p>Our multi stage Dockerfile:</p>

<pre><code class="docker Dockerfile.mult">FROM golang:alpine AS builder

WORKDIR $GOPATH/src/mylekkepackage/mylekkeapp/
COPY app.go .
RUN go build -o /go/app

FROM scratch

COPY --from=builder /go/app /go/app

CMD ["/go/app"]
</code></pre>

<p>Build it:</p>

<pre><code>❯ docker build -t mygolangapp:using-multistage -f Dockerfile.multi .
</code></pre>

<p>Notice that the image is only <strong>2.01MB</strong>, say w000t!</p>

<pre><code>❯ docker images "mygolangapp:*"
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
mygolangapp         using-multistage    31474c61ba5b        15 seconds ago      2.01MB
mygolangapp         using-alpine        eea1d7bde218        2 minutes ago       310MB
</code></pre>

<p>Run the app:</p>

<pre><code>❯ docker run mygolangapp:using-multistage
Number of words: 16
Selected index number: 5
Selected word is: spider
</code></pre>

<h2>Resources</h2>

<p>Source code for this demonstration can be found at <a href="https://github.com/ruanbekker/golang-build-small-images">github.com/ruanbekker/golang-build-small-images</a></p>

<p><img src="https://user-images.githubusercontent.com/567298/55478904-236e9200-561d-11e9-9382-f31b25a9ae03.png" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Container Persistent Storage for Docker Swarm Using a GlusterFS Volume Plugin]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/03/05/container-persistent-storage-for-docker-swarm-using-a-glusterfs-volume-plugin/"/>
    <updated>2019-03-05T13:18:30-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/03/05/container-persistent-storage-for-docker-swarm-using-a-glusterfs-volume-plugin</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/53351889-85572000-392a-11e9-9720-464e9318206e.jpg" alt="" /></p>

<p>From one of my previous posts I demonstrated how to provide persistent storage for your containers by using a <a href="https://blog.ruanbekker.com/blog/2018/02/16/guide-to-setup-docker-convoy-volume-driver-for-docker-swarm-with-nfs/">Convoy NFS Plugin</a>.</p>

<p>I&rsquo;ve stumbled upon one AWESOME GlusterFS Volume Plugin for Docker by <a href="https://github.com/trajano/docker-volume-plugins/tree/master/glusterfs-volume-plugin">@trajano</a>, please have a look at his repository. I&rsquo;ve been waiting for some time for one solid glusterfs volume plugin, and it works great.</p>

<p><a href="https://bekkerclothing.com/collections/developer?utm_source=blog.ruanbekker.com&utm_medium=blog&utm_campaign=leaderboard_ad" target="_blank"><img alt="bekker-clothing-developer-tshirts" src="https://user-images.githubusercontent.com/567298/70170981-7c278a80-16d6-11ea-9759-6621d02c1423.png"></a></p>

<h2>What we will be doing today</h2>

<p>We will setup a 3 node replicated glusterfs volume and show how easy it is to install the volume plugin and then demonstrate how storage from our swarms containers are persisted.</p>

<p>Our servers that we will be using will have the private ip&rsquo;s as shown below:</p>

<pre><code>10.22.125.101
10.22.125.102
10.22.125.103
</code></pre>

<h2>Setup GlusterFS</h2>

<p>Have a look at <a href="https://blog.ruanbekker.com/blog/2019/03/05/setup-a-3-node-replicated-storage-volume-with-glusterfs/?referral=github.com">this</a> post to setup the glusterfs volume.</p>

<h2>Install the GlusterFS Volume Plugin</h2>

<p>Below I&rsquo;m installing the plugin and setting the alias name as <code>glusterfs</code>, granting all permissions and keeping the plugin in a disabled state.</p>

<pre><code class="bash">$ docker plugin install --alias glusterfs trajano/glusterfs-volume-plugin --grant-all-permissions --disable
</code></pre>

<p>Set the glusterfs servers:</p>

<pre><code>$ docker plugin set glusterfs SERVERS=10.22.125.101,10.22.125.102,10.22.125.103
</code></pre>

<p>Enable the glusterfs plugin:</p>

<pre><code>$ docker plugin enable glusterfs
</code></pre>

<h2>Create a Service in Docker Swarm</h2>

<p>Deploy a sample service on docker swarm with a volume backed by glusterfs. Note that my glusterfs volume is called <code>gfs</code></p>

<pre><code class="yaml">version: "3.4"

services:
  foo:
    image: alpine
    command: ping localhost
    networks:
      - net
    volumes:
      - vol1:/tmp

networks:
  net:
    driver: overlay

volumes:
  vol1:
    driver: glusterfs
    name: "gfs/vol1"
</code></pre>

<p>Deploy the stack:</p>

<pre><code class="bash">$ docker stack deploy -c docker-compose.yml test
Creating service test_foo
</code></pre>

<p>Have a look on which node is your container running:</p>

<pre><code class="bash">$ docker service ps test_foo
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
jfwzb7yxnrxx        test_foo.1          alpine:latest       swarm-worker-1      Running             Running 37 seconds ago
</code></pre>

<p>Now jump to the <code>swarm-worker-1</code> node and verify that the container is running on that node:</p>

<pre><code class="bash">$ docker ps
CONTAINER ID        IMAGE                                          COMMAND                  CREATED             STATUS                  PORTS               NAMES
d469f341d836        alpine:latest                                  "ping localhost"           59 seconds ago      Up 57 seconds                               test_foo.1.jfwzb7yxnrxxnd0qxtcjex8lu
</code></pre>

<p>Now since the container is running on this node, we will also see that the volume defined in our task configuration will also be present:</p>

<pre><code class="bash">$ docker volume ls
DRIVER                       VOLUME NAME
glusterfs:latest             gfs/vol1
</code></pre>

<p>Exec into the container and look at the disk layout:</p>

<pre><code class="bash">$ docker exec -it d469f341d836 sh
/ # df -h
Filesystem                Size      Used Available Use% Mounted on
overlay                  45.6G      3.2G     40.0G   7% /
10.22.125.101:gfs/vol1   45.6G      3.3G     40.0G   8% /tmp
</code></pre>

<p>While you are in the container, write the hostname&rsquo;s value into a file which is mapped to the glusterfs volume:</p>

<pre><code class="bash">$ echo $HOSTNAME &gt; /tmp/data.txt
$ cat /tmp/data.txt
d469f341d836
</code></pre>

<h2>Testing Data Persistence</h2>

<p>Time to test the data persistence. Scale the service to 3 replicas, then hop onto a new node where a replica resides and check if the data was persisted.</p>

<pre><code class="bash">$ docker service scale test_foo=3
test_foo scaled to 3
overall progress: 3 out of 3 tasks
1/3: running   [==================================================&gt;]
2/3: running   [==================================================&gt;]
3/3: running   [==================================================&gt;]
verify: Service converged
</code></pre>

<p>Check where the containers are running:</p>

<pre><code class="bash">$ docker service ps test_foo
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
jfwzb7yxnrxx        test_foo.1          alpine:latest       swarm-worker-1      Running             Running 2 minutes ago
mdsg6c5b2nqb        test_foo.2          alpine:latest       swarm-worker-3      Running             Running 15 seconds ago
iybat57t4lha        test_foo.3          alpine:latest       swarm-worker-2      Running             Running 15 seconds ago
</code></pre>

<p>Hop onto the <code>swarm-worker-2</code> node and check if the data is persisted from our previous write:</p>

<pre><code class="bash">$ docker exec -it 4228529aba29 sh
$ cat /tmp/data.txt
d469f341d836
</code></pre>

<p>Now let&rsquo;s append data to that file, then delete the stack and recreate to test if the data is still persisted:</p>

<pre><code class="bash">$ echo $HOSTNAME &gt;&gt; /tmp/data.txt
$ cat /tmp/data.txt
d469f341d836
4228529aba29
</code></pre>

<p>On the manager delete the stack:</p>

<pre><code class="bash">$ docker stack rm test
Removing service test_foo
</code></pre>

<p>The deploy the stack again:</p>

<pre><code class="bash">$ docker stack deploy -c docker-compose.yml test
Creating service test_foo
</code></pre>

<p>Check where the container is running:</p>

<pre><code class="bash">$ docker service ps test_foo
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS
9d6z02m123jk        test_foo.1          alpine:latest       swarm-worker-1      Running             Running 2 seconds ago
</code></pre>

<p>Exec into the container and read the data:</p>

<pre><code class="bash">$ docker exec -it 3008b1e1bba1 cat /tmp/data.txt
d469f341d836
4228529aba29
</code></pre>

<p>And as you can see the data is persisted.</p>

<h2>Resources</h2>

<p>Please have a look and star <a href="https://github.com/trajano/docker-volume-plugins">@trajano&rsquo;s</a> repository:</p>

<ul>
<li><a href="https://github.com/trajano/docker-volume-plugins">https://github.com/trajano/docker-volume-plugins</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Create a Local Docker Swarm Cluster With Docker in Docker on Your Workstation]]></title>
    <link href="https://blog.ruanbekker.com/blog/2017/09/07/how-to-create-a-local-docker-swarm-cluster-with-docker-in-docker-on-your-workstation/"/>
    <updated>2017-09-07T04:03:37-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2017/09/07/how-to-create-a-local-docker-swarm-cluster-with-docker-in-docker-on-your-workstation</id>
    <content type="html"><![CDATA[<p>Creating a Docker Swarm Cluster, locally on your Workstation using Docker in Docker (DID) for testing Purposes:</p>

<h2>Project Resources:</h2>

<ul>
<li><a href="https://hub.docker.com/_/docker/">Docker in Docker</a></li>
<li><a href="https://docs.traefik.io/">Traefik</a></li>
</ul>


<h2>Create The Nodes:</h2>

<p>Create the Docker containers that will act as our Docker nodes:</p>

<pre><code class="bash">$ docker run --privileged --name docker-node1 -v /Users/ruan/docker/did/vols/node1:/var/lib/docker -d docker:dind --storage-driver=vfs
$ docker run --privileged --name docker-node2 -v /Users/ruan/docker/did/vols/node2:/var/lib/docker -d docker:dind --storage-driver=vfs
$ docker run --privileged --name docker-node3 -v /Users/ruan/docker/did/vols/node3:/var/lib/docker -d docker:dind --storage-driver=vfs
</code></pre>

<h2>Initialize the Swarm:</h2>

<p>Log onto the manager node:</p>

<pre><code class="bash">$ docker exec -it docker-node1 sh
</code></pre>

<p>Initialize the Swarm:</p>

<pre><code class="bash">$ docker swarm init --advertise-addr eth0
Swarm initialized: current node (17ydtkqdwxzwea2riadxj4zbw) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-4goolm8dvwictc7d39aonpcv6ca1pfj31q7irjga17o2srzf6f-b4k3hln6ogvjgmnbs1qxnjvj9 172.17.0.2:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
</code></pre>

<p>Join the Worker Nodes to the Swarm:</p>

<pre><code class="bash">$ docker exec -it docker-node2 sh
/ # docker swarm join --token SWMTKN-1-4mvb68vefr3dogxr6omu3uq04r4gddftdbmfomxo9pefks9siu-3t7ua7k2xigl9rwgp4dwzcxm0 172.17.0.2:2377
This node joined a swarm as a worker.
</code></pre>

<pre><code class="bash">$ docker exec -it docker-node3 sh
/ # docker swarm join --token SWMTKN-1-4mvb68vefr3dogxr6omu3uq04r4gddftdbmfomxo9pefks9siu-3t7ua7k2xigl9rwgp4dwzcxm0 172.17.0.2:2377
This node joined a swarm as a worker.
</code></pre>

<h2>List the Nodes:</h2>

<p>Log onto the Manager node and list the nodes:</p>

<pre><code class="bash">$ docker exec -it docker-node1 sh
/ # docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS
1hnq4b4w87w6trobwye5ap4sh *   5657c28bf618        Ready               Active              Leader
wglbb029g1kczttiaf5r6iavi     b2924bb8e555        Ready               Active
xxr9kdqy49u2tx61w31ife90j     6622a06a1b3c        Ready               Active
</code></pre>

<h2>Traefik:</h2>

<p>Creating a HTTP Reverse Proxy, using Traefik:</p>

<pre><code class="bash">$ docker network create --driver overlay traefik-net
$ docker service create \
--name traefik \
--constraint 'node.role==manager' \
--publish 80:80 \
--publish 8080:8080 \
--mount type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock \
--network traefik-net \
traefik:camembert --docker --docker.swarmmode --docker.domain=ruanbekker.internal --docker.watch --logLevel=DEBUG --web
</code></pre>
]]></content>
  </entry>
  
</feed>
