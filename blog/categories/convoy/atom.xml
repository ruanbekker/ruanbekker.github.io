<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Convoy | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/convoy/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2022-05-05T06:43:10-04:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Guide to Setup Docker Convoy Volume Driver for Docker Swarm With NFS]]></title>
    <link href="https://blog.ruanbekker.com/blog/2018/02/16/guide-to-setup-docker-convoy-volume-driver-for-docker-swarm-with-nfs/"/>
    <updated>2018-02-16T08:51:59-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2018/02/16/guide-to-setup-docker-convoy-volume-driver-for-docker-swarm-with-nfs</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/53351889-85572000-392a-11e9-9720-464e9318206e.jpg" alt="" /></p>

<p>In this post we will setup <a href="https://github.com/rancher/convoy">Rancher&rsquo;s Convoy Storage Plugin</a> with NFS, to provide data persistence in Docker Swarm.</p>

<h2>The Overview:</h2>

<p>This essentially means that we will have a NFS Volume, when the service gets created on Docker Swarm, the cluster creates these volumes with path mapping, so when a container gets spawned, restarted, scaled etc, the container that gets started on the new node will be aware of the volume, and will get the data that its expecting.</p>

<p>Its also good to note that our NFS Server will be a single point of failure, therefore its also good to look at a Distributed Volume like <a href="https://sysadmins.co.za/tag/glusterfs">GlusterFS</a>, <a href="https://sysadmins.co.za/tag/xtreemfs/">XtreemFS</a>, <a href="">Ceph</a>, etc.</p>

<ul>
<li>NFS Server (10.8.133.83)</li>
<li>Rancher Convoy Plugin on Each Docker Node in the Swarm (10.8.133.83, 10.8.166.19, 10.8.142.195)</li>
</ul>


<h2>Setup NFS:</h2>

<p>Setup the NFS Server</p>

<p><em>Update:</em></p>

<p>In order for the containers to be able to change permissions, you need to set <code>(rw,async,no_subtree_check,no_wdelay,crossmnt,insecure,all_squash,insecure_locks,sec=sys,anonuid=0,anongid=0)</code></p>

<ul>
<li><a href="https://github.com/rancher/rancher/issues/6452">https://github.com/rancher/rancher/issues/6452</a></li>
</ul>


<pre><code class="bash">$ sudo apt-get install nfs-kernel-server nfs-common -y
$ mkdir /vol
$ chown -R nobody:nogroup /vol
$ echo '/vol 10.8.133.83(rw,sync,no_subtree_check) 10.8.166.19(rw,sync,no_subtree_check) 10.8.142.195(rw,sync,no_subtree_check)' &gt;&gt; /etc/exports
$ sudo systemctl restart nfs-kernel-server
$ sudo systemctl enable nfs-kernel-server
</code></pre>

<p>Setup the NFS Clients on each Docker Node:</p>

<pre><code class="bash">$ sudo apt-get install nfs-common -y
$ mount 10.8.133.83:/vol /mnt
$ umount /mnt
$ df -h
</code></pre>

<p>If you can see tht the volume is mounted, unmount it and add it to the <code>fstab</code> so the volume can be mounted on boot:</p>

<pre><code class="bash">$ sudo bash -c "echo '10.8.133.83:/vol /mnt nfs auto,nofail,noatime,nolock,intr,tcp,actimeo=1800 0 0' &gt;&gt; /etc/fstab"
$ sudo mount -a
</code></pre>

<h2>Install Rancher Convoy Plugin:</h2>

<p>The Plugin needs to be installed on each docker node that will be part of the swarm:</p>

<pre><code class="bash">$ cd /tmp
$ wget https://github.com/rancher/convoy/releases/download/v0.5.0/convoy.tar.gz
$ tar xzf convoy.tar.gz
$ sudo cp convoy/convoy convoy/convoy-pdata_tools /usr/local/bin/
$ sudo mkdir -p /etc/docker/plugins/
$ sudo bash -c 'echo "unix:///var/run/convoy/convoy.sock" &gt; /etc/docker/plugins/convoy.spec'
</code></pre>

<h2>Create the init script:</h2>

<p>Thanks to <a href="https://gist.github.com/deviantony/557984d62e867e6f505577b207db6ffc">deviantony</a></p>

<pre><code class="bash">#!/bin/sh
### BEGIN INIT INFO
# Provides:
# Required-Start:    $remote_fs $syslog
# Required-Stop:     $remote_fs $syslog
# Default-Start:     2 3 4 5
# Default-Stop:      0 1 6
# Short-Description: Start daemon at boot time
# Description:       Enable service provided by daemon.
### END INIT INFO

dir="/usr/local/bin"
cmd="convoy daemon --drivers vfs --driver-opts vfs.path=/mnt/docker/volumes"
user="root"
name="convoy"

pid_file="/var/run/$name.pid"
stdout_log="/var/log/$name.log"
stderr_log="/var/log/$name.err"

get_pid() {
    cat "$pid_file"
}

is_running() {
    [ -f "$pid_file" ] &amp;&amp; ps `get_pid` &gt; /dev/null 2&gt;&amp;1
}

case "$1" in
    start)
    if is_running; then
        echo "Already started"
    else
        echo "Starting $name"
        cd "$dir"
        if [ -z "$user" ]; then
            sudo $cmd &gt;&gt; "$stdout_log" 2&gt;&gt; "$stderr_log" &amp;
        else
            sudo -u "$user" $cmd &gt;&gt; "$stdout_log" 2&gt;&gt; "$stderr_log" &amp;
        fi
        echo $! &gt; "$pid_file"
        if ! is_running; then
            echo "Unable to start, see $stdout_log and $stderr_log"
            exit 1
        fi
    fi
    ;;
    stop)
    if is_running; then
        echo -n "Stopping $name.."
        kill `get_pid`
        for i in {1..10}
        do
            if ! is_running; then
                break
            fi

            echo -n "."
            sleep 1
        done
        echo

        if is_running; then
            echo "Not stopped; may still be shutting down or shutdown may have failed"
            exit 1
        else
            echo "Stopped"
            if [ -f "$pid_file" ]; then
                rm "$pid_file"
            fi
        fi
    else
        echo "Not running"
    fi
    ;;
    restart)
    $0 stop
    if is_running; then
        echo "Unable to stop, will not attempt to start"
        exit 1
    fi
    $0 start
    ;;
    status)
    if is_running; then
        echo "Running"
    else
        echo "Stopped"
        exit 1
    fi
    ;;
    *)
    echo "Usage: $0 {start|stop|restart|status}"
    exit 1
    ;;
esac

exit 0
</code></pre>

<p>Make the script executable:</p>

<pre><code class="bash">$ chmod +x /etc/init.d/convoy
</code></pre>

<p>Enable the service on boot:</p>

<pre><code class="bash">$ sudo systemctl enable convoy
</code></pre>

<p>Start the service:</p>

<pre><code class="bash">$ sudo /etc/init.d/convoy start
</code></pre>

<p>This should be done on all the nodes.</p>

<h2>Externally Managed Convoy Volumes</h2>

<p>One thing to note is that, after your delete a volume, you will still need to delete the directory from the path where its hosted, as the application does not do that by itself.</p>

<p>Creating the Volume Before hand:</p>

<pre><code class="bash">$ convoy create test1
test1

$ docker volume ls
DRIVER              VOLUME NAME
convoy              test1

$ cat /mnt/docker/volumes/config/vfs_volume_test1.json
{"Name":"test1","Size":0,"Path":"/mnt/docker/volumes/test1","MountPoint":"","PrepareForVM":false,"CreatedTime":"Mon Feb 05 13:07:05 +0000 2018","Snapshots":{}}
</code></pre>

<p>Viewing the volume from another node:</p>

<pre><code class="bash">$ docker volume ls
DRIVER              VOLUME NAME
convoy              test1
</code></pre>

<h2>Creating a Test Service:</h2>

<p>Create a test service to test the data persistence, our docker-compose.yml:</p>

<pre><code class="bash">version: '3.4'

volumes:
  test1:
    external: true

networks:
  appnet:
    external: true

services:
  test:
    image: alpine:edge
    command: sh -c "ping 127.0.0.1"
    volumes:
      - test1:/data
    networks:
      - appnet
</code></pre>

<p>Creating the Overlay Network and Deploying the Stack:</p>

<pre><code class="bash">$ docker network create -d overlay appnet
$ docker stack deploy -c docker-compose.yml apps
Creating service apps_test
</code></pre>

<p>Write data to the volume in the container:</p>

<pre><code>$ docker exec -it apps_test.1.iojo7fpw8jirqjs3iu8qr7qpe sh
/ # echo "ok" &gt; /data/file.txt
/ # cat /data/file.txt
ok
</code></pre>

<p>Scale the service:</p>

<pre><code class="bash">$ docker service scale apps_test=2
apps_test scaled to 2
</code></pre>

<p>Inspect to see if the new replica is on another node:</p>

<pre><code class="bash">$ docker service ps apps_test
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE               ERROR                         PORTS
myrq2pc3z26z        apps_test.1         alpine:edge         scw-docker-1        Running             Running 45 seconds ago
ny8t97l2q00c         \_ apps_test.1     alpine:edge         scw-docker-1        Shutdown            Failed 51 seconds ago       "task: non-zero exit (137)"
iojo7fpw8jir         \_ apps_test.1     alpine:edge         scw-docker-1        Shutdown            Failed about a minute ago   "task: non-zero exit (137)"
tt0nuusvgeki        apps_test.2         alpine:edge         scw-docker-2        Running             Running 15 seconds ago
</code></pre>

<p>Logon to the new container and test if the data is persisted:</p>

<pre><code class="bash">$ docker exec -it apps_test.2.tt0nuusvgekirw1c5myu720ga sh
/ # cat /data/file.txt
ok
</code></pre>

<p>Delete the Stack and Redeploy and have a look at the data we created earlier, and you will notice the data is persisted:</p>

<pre><code class="bash">$ docker stack rm apps
$ docker stack deploy -c docker-compose.yml apps
$ docker exec -it apps_test.1.la4w2sbuu8cmv6xamwxl7n0ip cat /data/file.txt
ok
$ docker stack rm apps
</code></pre>

<h2>Create Volume via Compose:</h2>

<p>You can also create the volume on service/stack creation level, so you dont need to create the volume before hand, the compose file:</p>

<pre><code class="yml">version: '3.4'

volumes:
  test2:
    driver: convoy
    driver_opts:
      size: 10

networks:
  appnet:
    external: true

services:
  test:
    image: alpine:edge
    command: sh -c "ping 127.0.0.1"
    volumes:
      - test2:/data
    networks:
      - appnet
</code></pre>

<p>Deploy the Stack:</p>

<pre><code class="bash">$ docker stack deploy -c docker-compose-new.yml apps
Creating service apps_test
</code></pre>

<p>List the volumes and you will notice that the volume was created:</p>

<pre><code class="bash">$ docker volume ls
DRIVER              VOLUME NAME
convoy              apps_test2
convoy              test1
</code></pre>

<p>Lets inspect the volume, to see more details about it:</p>

<pre><code class="bash">docker volume inspect apps_test2
[
    {
        "CreatedAt": "0001-01-01T00:00:00Z",
        "Driver": "convoy",
        "Labels": {
            "com.docker.stack.namespace": "apps"
        },
        "Mountpoint": "/mnt/docker/volumes/apps_test2",
        "Name": "apps_test2",
        "Options": {
            "size": "10"
        },
        "Scope": "local"
    }
]
</code></pre>

<p>As mentioned earlier, if you delete the volume, you need to delete the data directories as well</p>

<pre><code class="bash">$ docker volume rm test1
test1

$ ls /mnt/docker/volumes/
apps_test2  config  test1

$ rm -rf /mnt/docker/volumes/test1
</code></pre>

<p>More info about the project:
- <a href="https://github.com/rancher/convoy">https://github.com/rancher/convoy</a></p>
]]></content>
  </entry>
  
</feed>
