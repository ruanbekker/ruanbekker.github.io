<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Databases | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/databases/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2022-05-05T04:27:30-04:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Improve MySQL Write Performance Using Batch Writes]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/06/13/improve-mysql-write-performance-using-batch-writes/"/>
    <updated>2020-06-13T19:31:32+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/06/13/improve-mysql-write-performance-using-batch-writes</id>
    <content type="html"><![CDATA[<p><img src="https://img.sysadmins.co.za/wngib2.png" alt="mysql-python-performance" /></p>

<p>I am no DBA, but I got curious when I noticed sluggish write performance on a mysql database, and I remembered somewhere that you should always use batch writes over sequential writes. So I decided to test it out, using a python script and a mysql server.</p>

<h2>What will we be doing</h2>

<p>I wrote a python script that writes 100,000 records to a database and keeps time of how long the writes took, 2 examples which I will compare:</p>

<ul>
<li>One script writing each record to the database</li>
<li>One script writing all the records as batch</li>
</ul>


<h2>Sequential Writes</h2>

<p>It took 48 seconds to write 100,000 records into a database using sequential writes:</p>

<pre><code class="python">...
for user in userids:
    userid = user["uid"]
    name = user["uid"].split('_')[0]
    job = random.choice(job)
    age = random.randint(24,49)
    credit_card_num = user["ccnum"]
    status = random.choice(["active", "inactive", "disabled"])

    cur.execute(
        """INSERT INTO customers(userid, name, job, age, credit_card_num, status) VALUES(%s, %s, %s, %s, %s, %s)""",
        (userid, name, job, age, credit_card_num, status)
    )
...
</code></pre>

<p>Running that shows us this:</p>

<pre><code>$ python3 mysql_seq_writes.py
start
writing customers to database
finish
inserted 100000 records in 48s
</code></pre>

<h2>Batch Writes</h2>

<p>It took 3 seconds to write to write 100,000 records using batch writes:</p>

<pre><code class="python">...
for user in userids:
    userid = user["uid"]
    name = user["uid"].split('_')[0]
    job = random.choice(job)
    age = random.randint(24,49)
    credit_card_num = user["ccnum"]
    status = random.choice(["active", "inactive", "disabled"])

    bunch_users.append((userid, name, job, age, credit_card_num, status))

cur.executemany(
    """INSERT INTO customers(userid, name, job, age, credit_card_num, status) VALUES(%s, %s, %s, %s, %s, %s)""",
    bunch_users
)
...
</code></pre>

<p>Running that shows us this:</p>

<pre><code>$ python3 mysql_batch_writes.py
start
writing customers to database
finish
inserted 100000 records in 3s
</code></pre>

<h2>Looking at the Scripts</h2>

<p>The script used for sequential writes:</p>

<pre><code class="python">import datetime
import random
import MySQLdb
from datetime import datetime as dt

host="172.18.0.1"
user="root"
password="password"
dbname="shopdb"
records=100000

db = MySQLdb.connect(host, user, password, dbname)

names = ['ruan', 'donovan', 'james', 'warren', 'angie', 'nicole', 'jenny', 'penny', 'amber']
job = ['doctor', 'scientist', 'teacher', 'police officer', 'waiter', 'banker', 'it']

cur = db.cursor()
cur.execute("DROP TABLE IF EXISTS customers")
cur.execute("CREATE TABLE customers(userid VARCHAR(50), name VARCHAR(50), surname VARCHAR(50), job VARCHAR(50), age INT(2), credit_card_num VARCHAR(50), status VARCHAR(10))")

bunch_users = []
userids = []

print("start")

def gen_id():
    return str(random.randint(0,9999)).zfill(4)

def gen_user(username):
    ccnum = '{0}-{1}-{2}-{3}'.format(gen_id(), gen_id(), gen_id(), gen_id())
    userid = username + '_' + ccnum.split('-')[0] + ccnum.split('-')[2]
    return {"uid": userid, "ccnum": ccnum}

for name in range(records):
    userids.append(gen_user(random.choice(names)))

print("writing customers to database")

timestart = int(dt.now().strftime("%s"))

for user in userids:
    userid = user["uid"]
    name = user["uid"].split('_')[0]
    job = random.choice(job)
    age = random.randint(24,49)
    credit_card_num = user["ccnum"]
    status = random.choice(["active", "inactive", "disabled"])

    #bunch_users.append((userid, name, job, age, credit_card_num, status))

    cur.execute(
        """INSERT INTO customers(userid, name, job, age, credit_card_num, status) VALUES(%s, %s, %s, %s, %s, %s)""",
        (userid, name, job, age, credit_card_num, status)
    )

db.commit()
db.close()
timefinish = int(dt.now().strftime("%s"))
print("finish")
print("inserted {} records in {}s".format(records, timefinish-timestart))
</code></pre>

<p>The script used for the batch writes:</p>

<pre><code class="python">import datetime
import random
import MySQLdb
from datetime import datetime as dt

host="172.18.0.1"
user="root"
password="password"
dbname="shopdb"
records=100000

db = MySQLdb.connect(host, user, password, dbname)

names = ['ruan', 'donovan', 'james', 'warren', 'angie', 'nicole', 'jenny', 'penny', 'amber']
job = ['doctor', 'scientist', 'teacher', 'police officer', 'waiter', 'banker', 'it']

cur = db.cursor()
cur.execute("DROP TABLE IF EXISTS customers")
cur.execute("CREATE TABLE customers(userid VARCHAR(50), name VARCHAR(50), surname VARCHAR(50), job VARCHAR(50), age INT(2), credit_card_num VARCHAR(50), status VARCHAR(10))")

bunch_users = []
userids = []

print("start")

def gen_id():
    return str(random.randint(0,9999)).zfill(4)

def gen_user(username):
    ccnum = '{0}-{1}-{2}-{3}'.format(gen_id(), gen_id(), gen_id(), gen_id())
    userid = username + '_' + ccnum.split('-')[0] + ccnum.split('-')[2]
    return {"uid": userid, "ccnum": ccnum}

for name in range(records):
    userids.append(gen_user(random.choice(names)))

for user in userids:
    userid = user["uid"]
    name = user["uid"].split('_')[0]
    job = random.choice(job)
    age = random.randint(24,49)
    credit_card_num = user["ccnum"]
    status = random.choice(["active", "inactive", "disabled"])

    bunch_users.append((userid, name, job, age, credit_card_num, status))

timestart = int(dt.now().strftime("%s"))

print("writing customers to database")
cur.executemany(
    """INSERT INTO customers(userid, name, job, age, credit_card_num, status) VALUES(%s, %s, %s, %s, %s, %s)""",
    bunch_users
)

db.commit()
db.close()
timefinish = int(dt.now().strftime("%s"))
print("finish")
print("inserted {} records in {}s".format(records, timefinish-timestart))
</code></pre>

<h2>Thanks</h2>

<p>Thanks for reading, so this was kind of interesting to see to never do sequential writes but write them in bulk when you have large amount of writes.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running a HA MySQL Galera Cluster on Docker Swarm]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/05/10/running-a-ha-mysql-galera-cluster-on-docker-swarm/"/>
    <updated>2019-05-10T07:02:39-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/05/10/running-a-ha-mysql-galera-cluster-on-docker-swarm</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/57523982-c904d780-7326-11e9-981a-7a9cb9552c2f.png" alt="image" /></p>

<p>In this post we will setup a highly available mysql galera cluster on docker swarm.</p>

<h2>About</h2>

<p>The service is based of <a href="https://github.com/toughIQ/docker-mariadb-cluster">docker-mariadb-cluster</a> repository and it&rsquo;s designed not to have any persistent data attached to the service, but rely on the &ldquo;nodes&rdquo; to replicate the data.</p>

<p>Note, that however this proof of concept works, I always recommend to use a remote mysql database outside your cluster, such as RDS etc.</p>

<p>Since we don&rsquo;t persist any data on the mysql cluster, I have associated a dbclient service that will run continious backups, which we will persist the path where the backups reside to disk.</p>

<h2>Deploy the MySQL Cluster</h2>

<p>The <a href="https://raw.githubusercontent.com/ruanbekker/dockerfiles/master/mysql-cluster/docker-compose.yml">docker-compose.yml</a> that we will use looks like this:</p>

<pre><code class="yaml">version: '3.5'
services:
  dbclient:
    image: alpine
    environment:
      - BACKUP_ENABLED=1
      - BACKUP_INTERVAL=3600
      - BACKUP_PATH=/data
      - BACKUP_FILENAME=db_backup
    networks:
      - dbnet
    entrypoint: |
      sh -c 'sh -s &lt;&lt; EOF
      apk add --no-cache mysql-client
      while true
        do
          if [ $$BACKUP_ENABLED == 1 ]
            then
              sleep $$BACKUP_INTERVAL
              mkdir -p $$BACKUP_PATH/$$(date +%F)
              echo "$$(date +%FT%H.%m) - Making Backup to : $$BACKUP_PATH/$$(date +%F)/$$BACKUP_FILENAME-$$(date +%FT%H.%m).sql.gz"
              mysqldump -u root -ppassword -h dblb --all-databases | gzip &gt; $$BACKUP_PATH/$$(date +%F)/$$BACKUP_FILENAME-$$(date +%FT%H.%m).sql.gz
              find $$BACKUP_PATH -mtime 7 -delete
          fi
        done
      EOF'
    volumes:
      - vol_dbclient:/data
    deploy:
      mode: replicated
      replicas: 1

  dbcluster:
    image: toughiq/mariadb-cluster
    networks:
      - dbnet
    environment:
      - DB_SERVICE_NAME=dbcluster
      - MYSQL_ROOT_PASSWORD=password
      - MYSQL_DATABASE=mydb
      - MYSQL_USER=mydbuser
      - MYSQL_PASSWORD=mydbpass
    deploy:
      mode: replicated
      replicas: 1

  dblb:
    image: toughiq/maxscale
    networks:
      - dbnet
    ports:
      - 3306:3306
    environment:
      - DB_SERVICE_NAME=dbcluster
      - ENABLE_ROOT_USER=1
    deploy:
      mode: replicated
      replicas: 1

volumes:
  vol_dbclient:
    driver: local

networks:
  dbnet:
    name: dbnet
    driver: overlay
</code></pre>

<p>The dbclient is configured to be in the same network as the cluster so it can reach the mysql service. The default behavior is that it will make a backup every hour (3600 seconds) to the <code>/data/{date}/</code> path.</p>

<p>Deploy the stack:</p>

<pre><code>$ docker stack deploy -c docker-compose.yml galera
Creating network dbnet
Creating service galera_dbcluster
Creating service galera_dblb
Creating service galera_dbclient
</code></pre>

<p>Have a look to see if all the services is running:</p>

<pre><code>$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE                            PORTS
jm7p70qre72u        galera_dbclient     replicated          1/1                 alpine:latest
p8kcr5y7szte        galera_dbcluster    replicated          1/1                 toughiq/mariadb-cluster:latest
1hu3oxhujgfm        galera_dblb         replicated          1/1                 toughiq/maxscale:latest          :3306-&gt;3306/tcp
</code></pre>

<h2>The Backup Client</h2>

<p>As mentioned the backup client backs up to the <code>/data/</code> path:</p>

<pre><code>$ docker exec -it $(docker ps -f name=galera_dbclient -q) find /data/
/data/
/data/2019-05-10
/data/2019-05-10/db_backup-2019-05-10T10.05.sql.gz
</code></pre>

<p>Let&rsquo;s go ahead and populate some data into our mysql database:</p>

<pre><code>$ docker exec -it $(docker ps -f name=galera_dbclient -q) mysql -uroot -ppassword -h dblb
MySQL [(none)]&gt; create table mydb.foo (name varchar(10));
MySQL [(none)]&gt; insert into mydb.foo values('ruan');
MySQL [(none)]&gt; exit
</code></pre>

<h2>Scale the Cluster</h2>

<p>At the moment we only have 1 replica for our mysql cluster, let&rsquo;s go ahead and scale the cluster to 3 replicas:</p>

<pre><code>$ docker service scale galera_dbcluster=3
galera_dbcluster scaled to 3
overall progress: 3 out of 3 tasks
1/3: running   [==================================================&gt;]
2/3: running   [==================================================&gt;]
3/3: running   [==================================================&gt;]
verify: Service converged
</code></pre>

<p>Verify that the service has been scaled:</p>

<pre><code>$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE                            PORTS
jm7p70qre72u        galera_dbclient     replicated          1/1                 alpine:latest
p8kcr5y7szte        galera_dbcluster    replicated          3/3                 toughiq/mariadb-cluster:latest
1hu3oxhujgfm        galera_dblb         replicated          1/1                 toughiq/maxscale:latest          :3306-&gt;3306/tcp
</code></pre>

<p>Test, by reading from the database:</p>

<pre><code>$ docker exec -it $(docker ps -f name=galera_dbclient -q) mysql -uroot -ppassword -h dblb -e'select * from mydb.foo;'
+------+
| name |
+------+
| ruan |
+------+
</code></pre>

<h2>Simulate a Node Failure:</h2>

<p>Simulate a node failure by killing one of the mysql containers:</p>

<pre><code>$ docker kill 9e336032ab52
</code></pre>

<p>Verify that one container is missing from our service:</p>

<pre><code>$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE                            PORTS
p8kcr5y7szte        galera_dbcluster    replicated          2/3                 toughiq/mariadb-cluster:latest
</code></pre>

<p>While the container is provisioning, as we have 2 out of 3 running containers, read the data 3 times so test that the round robin queries dont hit the affected container (the dblb wont route traffic to the affected container):</p>

<pre><code>$ docker exec -it $(docker ps -f name=galera_dbclient -q) mysql -uroot -ppassword -h dblb -e'select * from mydb.foo;'
+------+
| name |
+------+
| ruan |
+------+

$ docker exec -it $(docker ps -f name=galera_dbclient -q) mysql -uroot -ppassword -h dblb -e'select * from mydb.foo;'
+------+
| name |
+------+
| ruan |
+------+

$ docker exec -it $(docker ps -f name=galera_dbclient -q) mysql -uroot -ppassword -h dblb -e'select * from mydb.foo;'
+------+
| name |
+------+
| ruan |
+------+
</code></pre>

<p>Verify that the 3rd container has checked in:</p>

<pre><code>$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE                            PORTS
p8kcr5y7szte        galera_dbcluster    replicated          3/3                 toughiq/mariadb-cluster:latest
</code></pre>

<h2>How to Restore?</h2>

<p>I&rsquo;m deleting the database to simulate the scenario where we need to restore:</p>

<pre><code>$ docker exec -it $(docker ps -f name=galera_dbclient -q) sh
&gt; mysql -uroot -ppassword -h dblb -e'drop database mydb;'
</code></pre>

<p>Ensure the db is not present:</p>

<pre><code>&gt; mysql -uroot -ppassword -h dblb -e'select * from mydb.foo;'
ERROR 1146 (42S02) at line 1: Table 'mydb.foo' doesn't exist
</code></pre>

<p>Find the archive and extract:</p>

<pre><code>&gt; find /data/
/data/
/data/2019-05-10
/data/2019-05-10/db_backup-2019-05-10T10.05.sql.gz

&gt; gunzip /data/2019-05-10/db_backup-2019-05-10T10.05.sql.gz
</code></pre>

<p>Restore the backed up database to MySQL:</p>

<pre><code>&gt; mysql -uroot -ppassword -h dblb &lt; /data/2019-05-10/db_backup-2019-05-10T10.05.sql
</code></pre>

<p>Test that we can read our data:</p>

<pre><code>&gt; mysql -uroot -ppassword -h dblb -e'select * from mydb.foo;'
+------+
| name |
+------+
| ruan |
+------+
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MongoDB Examples With Golang]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/04/17/mongodb-examples-with-golang/"/>
    <updated>2019-04-17T08:51:35-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/04/17/mongodb-examples-with-golang</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/55478904-236e9200-561d-11e9-9382-f31b25a9ae03.png" alt="" /></p>

<p>While looking into working with mongodb using golang, I found it quite frustrating getting it up and running and decided to make a quick post about it.</p>

<p><a href="https://bekkerclothing.com/collections/developer?utm_source=blog.ruanbekker.com&utm_medium=blog&utm_campaign=leaderboard_ad" target="_blank"><img alt="bekker-clothing-developer-tshirts" src="https://user-images.githubusercontent.com/567298/70170981-7c278a80-16d6-11ea-9759-6621d02c1423.png"></a></p>

<h2>What are we doing?</h2>

<p>Examples using the golang driver for mongodb to connect, read, update and delete documents from mongodb.</p>

<h2>Environment:</h2>

<p>Provision a mongodb server in docker:</p>

<pre><code>$ docker network create container-net
$ docker run -itd --name mongodb --network container-net -p 27017:27017 ruanbekker/mongodb
</code></pre>

<p>Drop into a golang environment using docker:</p>

<pre><code>$ docker run -it golang:alpine sh
</code></pre>

<p>Get the dependencies:</p>

<pre><code>$ apk add --no-cache git
</code></pre>

<p>Change to your project path:</p>

<pre><code>$ mkdir $GOPATH/src/myapp
$ cd $GOPATH/src/myapp
</code></pre>

<p>Download the golang mongodb driver:</p>

<pre><code>$ go get go.mongodb.org/mongo-driver
</code></pre>

<h2>Connecting to MongoDB in Golang</h2>

<p>First example will be to connect to your mongodb instance:</p>

<pre><code class="go">package main

import (
    "context"
    "fmt"
    "log"
    "go.mongodb.org/mongo-driver/mongo"
    "go.mongodb.org/mongo-driver/bson"
    "go.mongodb.org/mongo-driver/mongo/options"
)

type Person struct {
    Name string
    Age  int
    City string
}

func main() {
    clientOptions := options.Client().ApplyURI("mongodb://mongodb:27017")
    client, err := mongo.Connect(context.TODO(), clientOptions)

    if err != nil {
        log.Fatal(err)
    }

    err = client.Ping(context.TODO(), nil)

    if err != nil {
        log.Fatal(err)
    }

    fmt.Println("Connected to MongoDB!")

}
</code></pre>

<p>Running our app:</p>

<pre><code class="bash">$ go run main.go
Connected to MongoDB!
</code></pre>

<h2>Writing to MongoDB with Golang</h2>

<p>Let&rsquo;s insert a single document to MongoDB:</p>

<pre><code class="go">func main() {
    ..
    collection := client.Database("mydb").Collection("persons")

    ruan := Person{"Ruan", 34, "Cape Town"}

    insertResult, err := collection.InsertOne(context.TODO(), ruan)
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println("Inserted a Single Document: ", insertResult.InsertedID)
}
</code></pre>

<p>Running it will produce:</p>

<pre><code class="bash">$ go run main.go
Connected to MongoDB!
Inserted a single document:  ObjectID("5cb717dcf597b4411252341f")
</code></pre>

<p>Writing more than one document:</p>

<pre><code>func main() {
    ..
    collection := client.Database("mydb").Collection("persons")

    ruan := Person{"Ruan", 34, "Cape Town"}
    james := Person{"James", 32, "Nairobi"}
    frankie := Person{"Frankie", 31, "Nairobi"}

    trainers := []interface{}{james, frankie}

    insertManyResult, err := collection.InsertMany(context.TODO(), trainers)
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println("Inserted multiple documents: ", insertManyResult.InsertedIDs)
}
</code></pre>

<p>This will output in:</p>

<pre><code class="bash">$ go run main.go
Inserted Multiple Documents:  [ObjectID("5cb717dcf597b44112523420") ObjectID("5cb717dcf597b44112523421")]
</code></pre>

<h2>Updating Documents in MongoDB using Golang</h2>

<p>Updating Frankie&rsquo;s age:</p>

<pre><code>func main() {
    ..
    filter := bson.Dname
    update := bson.D{
        {"$inc", bson.D{
            {"age", 1},
        }},
    }

    updateResult, err := collection.UpdateOne(context.TODO(), filter, update)
    if err != nil {
        log.Fatal(err)
    }
    fmt.Printf("Matched %v documents and updated %v documents.\n", updateResult.MatchedCount, updateResult.ModifiedCount)
}
</code></pre>

<p>Running that will update Frankie&rsquo;s age:</p>

<pre><code class="bash">$ go run main.go
Matched 1 documents and updated 1 documents.
</code></pre>

<h2>Reading Data from MongoDB</h2>

<p>Reading the data:</p>

<pre><code class="go">funct main() {
    ..
    filter := bson.Dname
    var result Trainer

    err = collection.FindOne(context.TODO(), filter).Decode(&amp;result)
    if err != nil {
        log.Fatal(err)
    }

    fmt.Printf("Found a single document: %+v\n", result)

    findOptions := options.Find()
    findOptions.SetLimit(2)

}
</code></pre>

<pre><code class="bash">$ go run main.go
Found a single document: {Name:Frankie Age:32 City:Nairobi}
</code></pre>

<p>Finding multiple documents and returning the cursor</p>

<pre><code class="go">func main() {
    ..
    var results []*Trainer
    cur, err := collection.Find(context.TODO(), bson.D, findOptions)
    if err != nil {
        log.Fatal(err)
    }

    for cur.Next(context.TODO()) {
        var elem Trainer
        err := cur.Decode(&amp;elem)
        if err != nil {
            log.Fatal(err)
        }

        results = append(results, &amp;elem)
    }

    if err := cur.Err(); err != nil {
        log.Fatal(err)
    }

    cur.Close(context.TODO())
    fmt.Printf("Found multiple documents (array of pointers): %+v\n", results)
}
</code></pre>

<p>Running the example:</p>

<pre><code>$ go run main.go
Found multiple documents (array of pointers): [0xc0001215c0 0xc0001215f0]
</code></pre>

<h2>Deleting Data from MongoDB:</h2>

<p>Deleting our data and closing the connection:</p>

<pre><code class="go">func main(){
    ..
    deleteResult, err := collection.DeleteMany(context.TODO(), bson.D)
    if err != nil {
        log.Fatal(err)
    }

    fmt.Printf("Deleted %v documents in the trainers collection\n", deleteResult.DeletedCount)

    err = client.Disconnect(context.TODO())

    if err != nil {
        log.Fatal(err)
    } else {
        fmt.Println("Connection to MongoDB closed.")
    }
}
</code></pre>

<p>Running the example:</p>

<pre><code class="bash">$ go run main.go
Deleted 3 documents in the trainers collection
Connection to MongoDB closed.
</code></pre>

<p>The code for this example can be found at <a href="https://github.com/ruanbekker/code-examples/blob/master/mongodb/golang/examples.go">github.com/ruanbekker/code-examples/mongodb/golang/examples.go</a></p>

<h2>Resources:</h2>

<ul>
<li><a href="https://www.mongodb.com/blog/post/mongodb-go-driver-tutorial">https://www.mongodb.com/blog/post/mongodb-go-driver-tutorial</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Inner Joins Examples With SQLite]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/04/06/sql-inner-joins-examples-with-sqlite/"/>
    <updated>2019-04-06T15:47:38-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/04/06/sql-inner-joins-examples-with-sqlite</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/55704774-53cb7d00-59dd-11e9-9f43-65ec3aa857b5.png" alt="sqlite" /></p>

<h2>Overview</h2>

<p>In this tutorial we will get started with sqlite and use inner joins to query data from multiple tables to answer specific use case needs.</p>

<h2>Connecting to your Sqlite Database</h2>

<p>Connecting to your database uses the argument to the target database. You can use additional flags to set the properties that you want to enable:</p>

<pre><code class="sql">$ sqlite3 -header -column mydatabase.db
</code></pre>

<p>or you can specify the additional options to your config:</p>

<pre><code class="bash">cat &gt; ~/.sqliterc &lt;&lt; EOF
.mode column
.headers on
EOF
</code></pre>

<p>Then connecting to your database:</p>

<pre><code class="bash">$ sqlite3 mydatabase.db
-- Loading resources from /Users/ruan/.sqliterc
SQLite version 3.16.0 2016-11-04 19:09:39
Enter ".help" for usage hints.
sqlite&gt;
</code></pre>

<h2>Create the Tables</h2>

<p>Create the <code>users</code> table:</p>

<pre><code class="sql">sqlite&gt; create table users (
   ...&gt; id INT(20), name VARCHAR(20), surname VARCHAR(20), city VARCHAR(20),
   ...&gt; age INT(2), credit_card_num VARCHAR(20), job_position VARCHAR(20)
   ...&gt; );
</code></pre>

<p>Create the <code>transactions</code> table:</p>

<pre><code class="sql">sqlite&gt; create table transactions (
   ...&gt; credit_card_num VARCHAR(20), transaction_id INT(20), shop_name VARCHAR(20),
   ...&gt; product_name VARCHAR(20), spent_total DECIMAL(6,2), purchase_option VARCHAR(20)
   ...&gt; );
</code></pre>

<p>You can view the tables using <code>.tables</code>:</p>

<pre><code class="sql">sqlite&gt; .tables
transactions  users 
</code></pre>

<p>And view the schema of the tables using <code>.schema &lt;table-name&gt;</code></p>

<pre><code class="sql">sqlite&gt; .schema users
CREATE TABLE users (
id INT(20), name VARCHAR(20), surname VARCHAR(20), city VARCHAR(20),
age INT(2), credit_card_num VARCHAR(20), job_position VARCHAR(20)
);
</code></pre>

<h2>Write to Sqlite Database</h2>

<p>Now we will populate data to our tables. Insert a record to our users table:</p>

<pre><code class="sql">sqlite&gt; insert into users values(1, 'ruan', 'bekker', 'cape town', 31, '2345-8970-6712-4352', 'devops');
</code></pre>

<p>Insert a record to our transactions table:</p>

<pre><code class="sql">sqlite&gt; insert into transactions values('2345-8970-6712-4352', 981623, 'spaza01', 'burger', 101.02, 'credit_card');
</code></pre>

<h2>Read from the Sqlite Database</h2>

<p>Read the data from the users table:</p>

<pre><code class="sql">sqlite&gt; select * from users;
id          name        surname     city        age         credit_card_num      job_position
----------  ----------  ----------  ----------  ----------  -------------------  ------------
1           ruan        bekker      cape town   31          2345-8970-6712-4352  devops      
</code></pre>

<p>Read the data from the transactions table:</p>

<pre><code class="sql">sqlite&gt; select * from transactions;
credit_card_num      transaction_id  shop_name   product_name  spent_total  purchase_option
-------------------  --------------  ----------  ------------  -----------  ---------------
2345-8970-6712-4352  981623          spaza01     burger        101.02       credit_card    
</code></pre>

<h2>Inner Joins with Sqlite</h2>

<p>This is where stuff gets interesting.</p>

<p>Let&rsquo;s say you want to join data from 2 tables, in this example we have a table with our userdata and a table with transaction data.</p>

<p>Say we want to see the user&rsquo;s name, transaction id, the product they purchased and the total amount spent, we will make use of inner joins.</p>

<p>Example looks like this:</p>

<pre><code class="sql">sqlite&gt; select a.name, b.transaction_id, b.product_name, b.spent_total
   ...&gt; from users
   ...&gt; as a inner join transactions
   ...&gt; as b on a.credit_card_num = b.credit_card_num
   ...&gt; where a.credit_card_num = '2345-8970-6712-4352';
name        transaction_id  product_name  spent_total
----------  --------------  ------------  -----------
ruan        981623          burger         101.02  
</code></pre>

<p>Let&rsquo;s say you dont know the credit_card number but you would like to do a lookup the credit card number via the user&rsquo;s id, then pass the value to the where statement:</p>

<pre><code class="sql">sqlite&gt; select a.name, b.transaction_id, b.product_name, b.spent_total
   ...&gt; from users
   ...&gt; as a inner join transactions
   ...&gt; as b on a.credit_card_num = b.credit_card_num
   ...&gt; where a.credit_card_num = (select credit_card_num from users where id = 1);
name        transaction_id  product_name  spent_total
----------  --------------  ------------  -----------
ruan        981623          burger         101.02   
</code></pre>

<p>Let&rsquo;s create another table called <code>products</code>:</p>

<pre><code class="sql">sqlite&gt; create table products (
   ...&gt; product_id INTEGER(18), product_name VARCHAR(20), 
   ...&gt; product_category VARCHAR(20), product_price DECIMAL(6,2)
   ...&gt; );
</code></pre>

<p>Write a record with product data to the table:</p>

<pre><code class="sql">sqlite&gt; insert into products values(0231, 'burger', 'fast foods', 101.02);
</code></pre>

<p>Now, lets say the question will be that we need to display the users name, credit card number, product name as well as the product category and products price, by only giving you the credit card number</p>

<pre><code class="sql">sqlite&gt; select a.name, b.credit_card_num, c.product_name, c.product_category, c.product_price
   ...&gt; from users
   ...&gt; as a inner join transactions
   ...&gt; as b on a.credit_card_num = b.credit_card_num inner join products
   ...&gt; as c on b.product_name = c.product_name
   ...&gt; where a.credit_card_num = '2345-8970-6712-4352' and c.product_name = 'burger';
name        credit_card_num      product_name  product_category   product_price
----------  -------------------  ------------  -----------------  -------------
ruan        2345-8970-6712-4352  burger        fast foods         101.02   
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Shrink Your Elasticsearch Index by Reducing the Shard Count With the Shards API]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/04/06/shrink-your-elasticsearch-index-by-reducing-the-shard-count-with-the-shards-api/"/>
    <updated>2019-04-06T15:33:48-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/04/06/shrink-your-elasticsearch-index-by-reducing-the-shard-count-with-the-shards-api</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/53352581-b3892f80-392b-11e9-9532-5db5cbfc8f1c.jpg" alt="elasticsearch" /></p>

<p>Resize your Elasticsearch Index with fewer Primary Shards by using the Shrink API.</p>

<p>In Elasticsearch, every index consists of multiple shards and every shard in your elasticsearch cluster contributes to the usage of your cpu, memory, file descriptors etc. This definitely helps for performance in parallel processing. As for an example with time series data, you would write and read a lot to an index with ie the current date.</p>

<p>If that index drops in requests and only read from the index every now and then, we dont need that many shards anymore and if we have multiple indexes, they may build up and take up unessacary compute power.</p>

<p>For a scenario where we want to reduce the size of our indexes, we can use the Shrink API to reduce the number of primary shards.</p>

<h2>The Shrink API</h2>

<p>The shrink index API allows you to shrink an existing index into a new index with fewer primary shards. The requested number of primary shards in the target index must be a factor of the number of shards in the source index. For example an index with 8 primary shards can be shrunk into 4, 2 or 1 primary shards or an index with 15 primary shards can be shrunk into 5, 3 or 1. If the number of shards in the index is a prime number it can only be shrunk into a single primary shard. Before shrinking, a (primary or replica) copy of every shard in the index must be present on the same node.</p>

<p>Steps on Shrinking:</p>

<p>Create the target index with the same definition as the source index, but with a smaller number of primary shards.
Then it hard-links segments from the source index into the target index.
Finally, it recovers the target index as though it were a closed index which had just been re-opened.</p>

<h2>Reduce the Primary Shards of an Index.</h2>

<p>As you may know, you can only set the Primary Shards on Index Creation time and Replica Shards you can set on the fly.</p>

<p>In this example we have a source index: <code>my-index-2019.01.10</code> with 5 primary shards and 1 replica shard, which gives us 10 shards for that index, that we would like to shrink to an index named: <code>archive_my-index-2019.01.10</code> with 1 primary shard and 1 replica shard, which will give us 2 shards for that index.</p>

<p>Have a look at your index:</p>

<pre><code>$ curl -XGET "http://127.0.0.1:9200/_cat/indices/my-index-2019.01.*?v"
health status index                                     uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   my-index-2019.01.10                       xAijUTSevXirdyTZTN3cuA   5   1   80795533            0      5.9gb          2.9gb
green  open   my-index-2019.01.11                       yb8Cjy9eQwqde8mJhR_vlw   5   5   80590481            0      5.7gb          2.8gb
...
</code></pre>

<p>And have a look at the nodes, as we will relocate the shards to a specific node:</p>

<pre><code>$ curl http://127.0.0.1:9200/_cat/nodes?v
ip            heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
x.x.x.x             8          98   0    0.04    0.03     0.01 m         -      3E9yp60
x.x.x.x            65          99   4    0.43    0.23     0.36 di        -      znFrs18
</code></pre>

<p>In this demonstration we only have 2 nodes with a replication factor of 1, which means a index&rsquo;s shards will always reside on both nodes. In a case with more nodes, we need to ensure that we choose a node where a primary index reside on.</p>

<p>Look at the shards api, by passing the index name to get the index to shard allocation:</p>

<pre><code>$ curl http://127.0.0.1:9200/_cat/shards/my-index-2019.01.10?v'
index               shard prirep state   docs  store ip       node
my-index-2019.01.10 2     p      STARTED  193  101mb x.x.x.x  Lq9P7eP
my-index-2019.01.10 2     r      STARTED  193  101mb x.x.x.x  F5edOwK
my-index-2019.01.10 4     p      STARTED  197  101mb x.x.x.x  Lq9P7eP
my-index-2019.01.10 4     r      STARTED  197  101mb x.x.x.x  F5edOwK
my-index-2019.01.10 3     r      STARTED  184  101mb x.x.x.x  Lq9P7eP
my-index-2019.01.10 3     p      STARTED  184  101mb x.x.x.x  F5edOwK
my-index-2019.01.10 1     r      STARTED  180  101mb x.x.x.x  Lq9P7eP
my-index-2019.01.10 1     p      STARTED  180  101mb x.x.x.x  F5edOwK
my-index-2019.01.10 0     p      STARTED  187  101mb x.x.x.x  Lq9P7eP
my-index-2019.01.10 0     r      STARTED  187  101mb x.x.x.x  F5edOwK
</code></pre>

<p>Create the target index:</p>

<pre><code>$ curl -XPUT -H 'Content-Type: application/json' http://127.0.0.1:9200/archive_my-index-2019.01.10 -d '
{
    "settings": {
        "number_of_shards": "1",
        "number_of_replicas": "1"
    }
}
'
</code></pre>

<p>Set the index as read only and relocate every copy of shard to node we indentified in a previous step:</p>

<pre><code>$ curl -XPUT -H 'Content-Type: application/json' http://127.0.0.1:9200/my-index-2019.01.10/_settings -d '
{
    "settings": {
        "index.routing.allocation.require._name": "Lq9P7eP",
        "index.blocks.write": true
    }
}
'
</code></pre>

<p>Now shrink the source index (my-index-2019.01.10) to the target index (archive_my-index-2019.01.10):</p>

<pre><code>$ curl -XPOST -H 'Content-Type: application/json' http://127.0.0.1:9200/my-index-2019.01.10/_shrink/archive_my-index-2019.01.10
</code></pre>

<p>You can monitor the progress by using the Recovery API:</p>

<pre><code>$ curl -s -XGET "http://127.0.0.1:9200/_cat/recovery/my-index-2019.01.10?human&amp;detailed=true"
my-index-2019.01.10 0 23.3s peer done x.x.x.x  F5edOwK x.x.x.x Lq9P7eP n/a n/a 15 15 100.0% 15 635836677 635836677 100.0% 635836677 0 0 100.0%
my-index-2019.01.10 1 22s   peer done x.x.x.x  Lq9P7eP x.x.x.x Lq9P7eP n/a n/a 15 15 100.0% 15 636392649 636392649 100.0% 636392649 0 0 100.0%
my-index-2019.01.10 2 19.6s peer done x.x.x.x  F5edOwK x.x.x.x Lq9P7eP n/a n/a 15 15 100.0% 15 636809671 636809671 100.0% 636809671 0 0 100.0%
my-index-2019.01.10 3 21.5s peer done x.x.x.x  Lq9P7eP x.x.x.x Lq9P7eP n/a n/a 15 15 100.0% 15 636378870 636378870 100.0% 636378870 0 0 100.0%
my-index-2019.01.10 4 23.3s peer done x.x.x.x F5edOwK- x.x.x.x Lq9P7eP n/a n/a 15 15 100.0% 15 636545756 636545756 100.0% 636545756 0 0 100.0%
</code></pre>

<p>You can also pass aliases as your table columns for output:</p>

<pre><code>$ curl -s -XGET "http://127.0.0.1:9200/_cat/recovery/my-index-2019.01.10?v&amp;detailed=true&amp;h=index,shard,time,ty,st,shost,thost,f,fp,b,bp"
index                            shard time  ty   st   shost         thost        f  fp     b         bp
my-index-2019.01.10              0     23.3s peer done x.x.x.x x.x.x.x 15 100.0% 635836677 100.0%
...
</code></pre>

<p>When the job is done, have a look at your indexes:</p>

<pre><code>$ curl -XGET "http://127.0.0.1:9200/_cat/indices/*my-index-2019.01.10?v"
health status index                                     uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   archive_my-index-2019.01.10               PAijUTSeRvirdyTZTN3cuA   1   1   80795533            0      5.9gb          2.9gb
green  open   my-index-2019.01.10                       Cb8Cjy9CQwqde8mJhR_vlw   5   1   80795533            0      2.9gb          2.9gb
</code></pre>

<p>Remove the block on your old index in order to make it writable:</p>

<pre><code>$ curl -XPUT -H 'Content-Type: application/json' http://127.0.0.1:9200/my-index-2019.01.10/_settings" -d '
{
    "settings": {
        "index.routing.allocation.require._name": null,
        "index.blocks.write": null
    }
}
'
</code></pre>

<p>Delete the old index:</p>

<pre><code>$ curl -XDELETE -H 'Content-Type: application/json' http://127.0.0.1:9200/my-index-2019.01.10
</code></pre>

<p>Note:, On AWS Elasticsearch Service, if you dont remove the block and you trigger a redeployment, you will end up with something like this. Shard may still be constraint to a node.</p>

<pre><code>$ curl -s -XGET ${ES_HOST/_cat/allocation?v
shards disk.indices disk.used disk.avail disk.total disk.percent host          ip  node
     0           0b    51.2gb    956.5gb   1007.8gb            5 x.x.x.x  x.x.x.x  ap9Mx1R
     1        3.6gb    54.9gb    952.8gb   1007.8gb            5 x.x.x.x  x.x.x.x  PqmoQpN   &lt;-----------
     0           0b    51.2gb    956.5gb   1007.8gb            5 x.x.x.x  x.x.x.x  5p7x4Lc
     0           0b    51.2gb    956.5gb   1007.8gb            5 x.x.x.x  x.x.x.x  c8kniP3
     0           0b    51.2gb    956.5gb   1007.8gb            5 x.x.x.x  x.x.x.x  jPwlwsD
     0           0b    51.2gb    956.5gb   1007.8gb            5 x.x.x.x  x.x.x.x  ljos4mu
   481      904.1gb   990.3gb    521.3gb      1.4tb           65 x.x.x.x  x.x.x.x  qAF-gIU
   481      820.2gb   903.6gb    608.1gb      1.4tb           59 x.x.x.x  x.x.x.x  dR3sNwA
   481      824.6gb   909.1gb    602.6gb      1.4tb           60 x.x.x.x  x.x.x.x  fvL4A9X
   481      792.7gb   876.5gb    635.2gb      1.4tb           57 x.x.x.x  x.x.x.x  lk4svht
   481      779.2gb   864.4gb    647.3gb      1.4tb           57 x.x.x.x  x.x.x.x  uLsej9m
     0           0b    51.2gb    956.5gb   1007.8gb            5 x.x.x.x  x.x.x.x  yM4Ka9l
</code></pre>

<h2>Resources:</h2>

<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-shrink-index.html">https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-shrink-index.html</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
