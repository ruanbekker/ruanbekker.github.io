<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Databases | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/databases/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2020-04-29T00:15:42+02:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Running a HA MySQL Galera Cluster on Docker Swarm]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/05/10/running-a-ha-mysql-galera-cluster-on-docker-swarm/"/>
    <updated>2019-05-10T13:02:39+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/05/10/running-a-ha-mysql-galera-cluster-on-docker-swarm</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/57523982-c904d780-7326-11e9-981a-7a9cb9552c2f.png" alt="image" /></p>

<p>In this post we will setup a highly available mysql galera cluster on docker swarm.</p>

<h2>About</h2>

<p>The service is based of <a href="https://github.com/toughIQ/docker-mariadb-cluster">docker-mariadb-cluster</a> repository and it&rsquo;s designed not to have any persistent data attached to the service, but rely on the &ldquo;nodes&rdquo; to replicate the data.</p>

<p>Note, that however this proof of concept works, I always recommend to use a remote mysql database outside your cluster, such as RDS etc.</p>

<p>Since we don&rsquo;t persist any data on the mysql cluster, I have associated a dbclient service that will run continious backups, which we will persist the path where the backups reside to disk.</p>

<h2>Deploy the MySQL Cluster</h2>

<p>The <a href="https://raw.githubusercontent.com/ruanbekker/dockerfiles/master/mysql-cluster/docker-compose.yml">docker-compose.yml</a> that we will use looks like this:</p>

<pre><code class="yaml">version: '3.5'
services:
  dbclient:
    image: alpine
    environment:
      - BACKUP_ENABLED=1
      - BACKUP_INTERVAL=3600
      - BACKUP_PATH=/data
      - BACKUP_FILENAME=db_backup
    networks:
      - dbnet
    entrypoint: |
      sh -c 'sh -s &lt;&lt; EOF
      apk add --no-cache mysql-client
      while true
        do
          if [ $$BACKUP_ENABLED == 1 ]
            then
              sleep $$BACKUP_INTERVAL
              mkdir -p $$BACKUP_PATH/$$(date +%F)
              echo "$$(date +%FT%H.%m) - Making Backup to : $$BACKUP_PATH/$$(date +%F)/$$BACKUP_FILENAME-$$(date +%FT%H.%m).sql.gz"
              mysqldump -u root -ppassword -h dblb --all-databases | gzip &gt; $$BACKUP_PATH/$$(date +%F)/$$BACKUP_FILENAME-$$(date +%FT%H.%m).sql.gz
              find $$BACKUP_PATH -mtime 7 -delete
          fi
        done
      EOF'
    volumes:
      - vol_dbclient:/data
    deploy:
      mode: replicated
      replicas: 1

  dbcluster:
    image: toughiq/mariadb-cluster
    networks:
      - dbnet
    environment:
      - DB_SERVICE_NAME=dbcluster
      - MYSQL_ROOT_PASSWORD=password
      - MYSQL_DATABASE=mydb
      - MYSQL_USER=mydbuser
      - MYSQL_PASSWORD=mydbpass
    deploy:
      mode: replicated
      replicas: 1

  dblb:
    image: toughiq/maxscale
    networks:
      - dbnet
    ports:
      - 3306:3306
    environment:
      - DB_SERVICE_NAME=dbcluster
      - ENABLE_ROOT_USER=1
    deploy:
      mode: replicated
      replicas: 1

volumes:
  vol_dbclient:
    driver: local

networks:
  dbnet:
    name: dbnet
    driver: overlay
</code></pre>

<p>The dbclient is configured to be in the same network as the cluster so it can reach the mysql service. The default behavior is that it will make a backup every hour (3600 seconds) to the <code>/data/{date}/</code> path.</p>

<p>Deploy the stack:</p>

<pre><code>$ docker stack deploy -c docker-compose.yml galera
Creating network dbnet
Creating service galera_dbcluster
Creating service galera_dblb
Creating service galera_dbclient
</code></pre>

<p>Have a look to see if all the services is running:</p>

<pre><code>$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE                            PORTS
jm7p70qre72u        galera_dbclient     replicated          1/1                 alpine:latest
p8kcr5y7szte        galera_dbcluster    replicated          1/1                 toughiq/mariadb-cluster:latest
1hu3oxhujgfm        galera_dblb         replicated          1/1                 toughiq/maxscale:latest          :3306-&gt;3306/tcp
</code></pre>

<h2>The Backup Client</h2>

<p>As mentioned the backup client backs up to the <code>/data/</code> path:</p>

<pre><code>$ docker exec -it $(docker ps -f name=galera_dbclient -q) find /data/
/data/
/data/2019-05-10
/data/2019-05-10/db_backup-2019-05-10T10.05.sql.gz
</code></pre>

<p>Let&rsquo;s go ahead and populate some data into our mysql database:</p>

<pre><code>$ docker exec -it $(docker ps -f name=galera_dbclient -q) mysql -uroot -ppassword -h dblb
MySQL [(none)]&gt; create table mydb.foo (name varchar(10));
MySQL [(none)]&gt; insert into mydb.foo values('ruan');
MySQL [(none)]&gt; exit
</code></pre>

<h2>Scale the Cluster</h2>

<p>At the moment we only have 1 replica for our mysql cluster, let&rsquo;s go ahead and scale the cluster to 3 replicas:</p>

<pre><code>$ docker service scale galera_dbcluster=3
galera_dbcluster scaled to 3
overall progress: 3 out of 3 tasks
1/3: running   [==================================================&gt;]
2/3: running   [==================================================&gt;]
3/3: running   [==================================================&gt;]
verify: Service converged
</code></pre>

<p>Verify that the service has been scaled:</p>

<pre><code>$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE                            PORTS
jm7p70qre72u        galera_dbclient     replicated          1/1                 alpine:latest
p8kcr5y7szte        galera_dbcluster    replicated          3/3                 toughiq/mariadb-cluster:latest
1hu3oxhujgfm        galera_dblb         replicated          1/1                 toughiq/maxscale:latest          :3306-&gt;3306/tcp
</code></pre>

<p>Test, by reading from the database:</p>

<pre><code>$ docker exec -it $(docker ps -f name=galera_dbclient -q) mysql -uroot -ppassword -h dblb -e'select * from mydb.foo;'
+------+
| name |
+------+
| ruan |
+------+
</code></pre>

<h2>Simulate a Node Failure:</h2>

<p>Simulate a node failure by killing one of the mysql containers:</p>

<pre><code>$ docker kill 9e336032ab52
</code></pre>

<p>Verify that one container is missing from our service:</p>

<pre><code>$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE                            PORTS
p8kcr5y7szte        galera_dbcluster    replicated          2/3                 toughiq/mariadb-cluster:latest
</code></pre>

<p>While the container is provisioning, as we have 2 out of 3 running containers, read the data 3 times so test that the round robin queries dont hit the affected container (the dblb wont route traffic to the affected container):</p>

<pre><code>$ docker exec -it $(docker ps -f name=galera_dbclient -q) mysql -uroot -ppassword -h dblb -e'select * from mydb.foo;'
+------+
| name |
+------+
| ruan |
+------+

$ docker exec -it $(docker ps -f name=galera_dbclient -q) mysql -uroot -ppassword -h dblb -e'select * from mydb.foo;'
+------+
| name |
+------+
| ruan |
+------+

$ docker exec -it $(docker ps -f name=galera_dbclient -q) mysql -uroot -ppassword -h dblb -e'select * from mydb.foo;'
+------+
| name |
+------+
| ruan |
+------+
</code></pre>

<p>Verify that the 3rd container has checked in:</p>

<pre><code>$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE                            PORTS
p8kcr5y7szte        galera_dbcluster    replicated          3/3                 toughiq/mariadb-cluster:latest
</code></pre>

<h2>How to Restore?</h2>

<p>I&rsquo;m deleting the database to simulate the scenario where we need to restore:</p>

<pre><code>$ docker exec -it $(docker ps -f name=galera_dbclient -q) sh
&gt; mysql -uroot -ppassword -h dblb -e'drop database mydb;'
</code></pre>

<p>Ensure the db is not present:</p>

<pre><code>&gt; mysql -uroot -ppassword -h dblb -e'select * from mydb.foo;'
ERROR 1146 (42S02) at line 1: Table 'mydb.foo' doesn't exist
</code></pre>

<p>Find the archive and extract:</p>

<pre><code>&gt; find /data/
/data/
/data/2019-05-10
/data/2019-05-10/db_backup-2019-05-10T10.05.sql.gz

&gt; gunzip /data/2019-05-10/db_backup-2019-05-10T10.05.sql.gz
</code></pre>

<p>Restore the backed up database to MySQL:</p>

<pre><code>&gt; mysql -uroot -ppassword -h dblb &lt; /data/2019-05-10/db_backup-2019-05-10T10.05.sql
</code></pre>

<p>Test that we can read our data:</p>

<pre><code>&gt; mysql -uroot -ppassword -h dblb -e'select * from mydb.foo;'
+------+
| name |
+------+
| ruan |
+------+
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MongoDB Examples With Golang]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/04/17/mongodb-examples-with-golang/"/>
    <updated>2019-04-17T14:51:35+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/04/17/mongodb-examples-with-golang</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/55478904-236e9200-561d-11e9-9382-f31b25a9ae03.png" alt="" /></p>

<p>While looking into working with mongodb using golang, I found it quite frustrating getting it up and running and decided to make a quick post about it.</p>

<p><a href="https://bekkerclothing.com/collections/developer?utm_source=blog.ruanbekker.com&utm_medium=blog&utm_campaign=leaderboard_ad" target="_blank"><img alt="bekker-clothing-developer-tshirts" src="https://user-images.githubusercontent.com/567298/70170981-7c278a80-16d6-11ea-9759-6621d02c1423.png"></a></p>

<h2>What are we doing?</h2>

<p>Examples using the golang driver for mongodb to connect, read, update and delete documents from mongodb.</p>

<h2>Environment:</h2>

<p>Provision a mongodb server in docker:</p>

<pre><code>$ docker network create container-net
$ docker run -itd --name mongodb --network container-net -p 27017:27017 ruanbekker/mongodb
</code></pre>

<p>Drop into a golang environment using docker:</p>

<pre><code>$ docker run -it golang:alpine sh
</code></pre>

<p>Get the dependencies:</p>

<pre><code>$ apk add --no-cache git
</code></pre>

<p>Change to your project path:</p>

<pre><code>$ mkdir $GOPATH/src/myapp
$ cd $GOPATH/src/myapp
</code></pre>

<p>Download the golang mongodb driver:</p>

<pre><code>$ go get go.mongodb.org/mongo-driver
</code></pre>

<h2>Connecting to MongoDB in Golang</h2>

<p>First example will be to connect to your mongodb instance:</p>

<pre><code class="go">package main

import (
    "context"
    "fmt"
    "log"
    "go.mongodb.org/mongo-driver/mongo"
    "go.mongodb.org/mongo-driver/bson"
    "go.mongodb.org/mongo-driver/mongo/options"
)

type Person struct {
    Name string
    Age  int
    City string
}

func main() {
    clientOptions := options.Client().ApplyURI("mongodb://mongodb:27017")
    client, err := mongo.Connect(context.TODO(), clientOptions)

    if err != nil {
        log.Fatal(err)
    }

    err = client.Ping(context.TODO(), nil)

    if err != nil {
        log.Fatal(err)
    }

    fmt.Println("Connected to MongoDB!")

}
</code></pre>

<p>Running our app:</p>

<pre><code class="bash">$ go run main.go
Connected to MongoDB!
</code></pre>

<h2>Writing to MongoDB with Golang</h2>

<p>Let&rsquo;s insert a single document to MongoDB:</p>

<pre><code class="go">func main() {
    ..
    collection := client.Database("mydb").Collection("persons")

    ruan := Person{"Ruan", 34, "Cape Town"}

    insertResult, err := collection.InsertOne(context.TODO(), ruan)
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println("Inserted a Single Document: ", insertResult.InsertedID)
}
</code></pre>

<p>Running it will produce:</p>

<pre><code class="bash">$ go run main.go
Connected to MongoDB!
Inserted a single document:  ObjectID("5cb717dcf597b4411252341f")
</code></pre>

<p>Writing more than one document:</p>

<pre><code>func main() {
    ..
    collection := client.Database("mydb").Collection("persons")

    ruan := Person{"Ruan", 34, "Cape Town"}
    james := Person{"James", 32, "Nairobi"}
    frankie := Person{"Frankie", 31, "Nairobi"}

    trainers := []interface{}{james, frankie}

    insertManyResult, err := collection.InsertMany(context.TODO(), trainers)
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println("Inserted multiple documents: ", insertManyResult.InsertedIDs)
}
</code></pre>

<p>This will output in:</p>

<pre><code class="bash">$ go run main.go
Inserted Multiple Documents:  [ObjectID("5cb717dcf597b44112523420") ObjectID("5cb717dcf597b44112523421")]
</code></pre>

<h2>Updating Documents in MongoDB using Golang</h2>

<p>Updating Frankie&rsquo;s age:</p>

<pre><code>func main() {
    ..
    filter := bson.Dname
    update := bson.D{
        {"$inc", bson.D{
            {"age", 1},
        }},
    }

    updateResult, err := collection.UpdateOne(context.TODO(), filter, update)
    if err != nil {
        log.Fatal(err)
    }
    fmt.Printf("Matched %v documents and updated %v documents.\n", updateResult.MatchedCount, updateResult.ModifiedCount)
}
</code></pre>

<p>Running that will update Frankie&rsquo;s age:</p>

<pre><code class="bash">$ go run main.go
Matched 1 documents and updated 1 documents.
</code></pre>

<h2>Reading Data from MongoDB</h2>

<p>Reading the data:</p>

<pre><code class="go">funct main() {
    ..
    filter := bson.Dname
    var result Trainer

    err = collection.FindOne(context.TODO(), filter).Decode(&amp;result)
    if err != nil {
        log.Fatal(err)
    }

    fmt.Printf("Found a single document: %+v\n", result)

    findOptions := options.Find()
    findOptions.SetLimit(2)

}
</code></pre>

<pre><code class="bash">$ go run main.go
Found a single document: {Name:Frankie Age:32 City:Nairobi}
</code></pre>

<p>Finding multiple documents and returning the cursor</p>

<pre><code class="go">func main() {
    ..
    var results []*Trainer
    cur, err := collection.Find(context.TODO(), bson.D, findOptions)
    if err != nil {
        log.Fatal(err)
    }

    for cur.Next(context.TODO()) {
        var elem Trainer
        err := cur.Decode(&amp;elem)
        if err != nil {
            log.Fatal(err)
        }

        results = append(results, &amp;elem)
    }

    if err := cur.Err(); err != nil {
        log.Fatal(err)
    }

    cur.Close(context.TODO())
    fmt.Printf("Found multiple documents (array of pointers): %+v\n", results)
}
</code></pre>

<p>Running the example:</p>

<pre><code>$ go run main.go
Found multiple documents (array of pointers): [0xc0001215c0 0xc0001215f0]
</code></pre>

<h2>Deleting Data from MongoDB:</h2>

<p>Deleting our data and closing the connection:</p>

<pre><code class="go">func main(){
    ..
    deleteResult, err := collection.DeleteMany(context.TODO(), bson.D)
    if err != nil {
        log.Fatal(err)
    }

    fmt.Printf("Deleted %v documents in the trainers collection\n", deleteResult.DeletedCount)

    err = client.Disconnect(context.TODO())

    if err != nil {
        log.Fatal(err)
    } else {
        fmt.Println("Connection to MongoDB closed.")
    }
}
</code></pre>

<p>Running the example:</p>

<pre><code class="bash">$ go run main.go
Deleted 3 documents in the trainers collection
Connection to MongoDB closed.
</code></pre>

<p>The code for this example can be found at <a href="https://github.com/ruanbekker/code-examples/blob/master/mongodb/golang/examples.go">github.com/ruanbekker/code-examples/mongodb/golang/examples.go</a></p>

<h2>Resources:</h2>

<ul>
<li><a href="https://www.mongodb.com/blog/post/mongodb-go-driver-tutorial">https://www.mongodb.com/blog/post/mongodb-go-driver-tutorial</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Inner Joins Examples With SQLite]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/04/06/sql-inner-joins-examples-with-sqlite/"/>
    <updated>2019-04-06T21:47:38+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/04/06/sql-inner-joins-examples-with-sqlite</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/55704774-53cb7d00-59dd-11e9-9f43-65ec3aa857b5.png" alt="sqlite" /></p>

<h2>Overview</h2>

<p>In this tutorial we will get started with sqlite and use inner joins to query data from multiple tables to answer specific use case needs.</p>

<h2>Connecting to your Sqlite Database</h2>

<p>Connecting to your database uses the argument to the target database. You can use additional flags to set the properties that you want to enable:</p>

<pre><code class="sql">$ sqlite3 -header -column mydatabase.db
</code></pre>

<p>or you can specify the additional options to your config:</p>

<pre><code class="bash">cat &gt; ~/.sqliterc &lt;&lt; EOF
.mode column
.headers on
EOF
</code></pre>

<p>Then connecting to your database:</p>

<pre><code class="bash">$ sqlite3 mydatabase.db
-- Loading resources from /Users/ruan/.sqliterc
SQLite version 3.16.0 2016-11-04 19:09:39
Enter ".help" for usage hints.
sqlite&gt;
</code></pre>

<h2>Create the Tables</h2>

<p>Create the <code>users</code> table:</p>

<pre><code class="sql">sqlite&gt; create table users (
   ...&gt; id INT(20), name VARCHAR(20), surname VARCHAR(20), city VARCHAR(20),
   ...&gt; age INT(2), credit_card_num VARCHAR(20), job_position VARCHAR(20)
   ...&gt; );
</code></pre>

<p>Create the <code>transactions</code> table:</p>

<pre><code class="sql">sqlite&gt; create table transactions (
   ...&gt; credit_card_num VARCHAR(20), transaction_id INT(20), shop_name VARCHAR(20),
   ...&gt; product_name VARCHAR(20), spent_total DECIMAL(6,2), purchase_option VARCHAR(20)
   ...&gt; );
</code></pre>

<p>You can view the tables using <code>.tables</code>:</p>

<pre><code class="sql">sqlite&gt; .tables
transactions  users 
</code></pre>

<p>And view the schema of the tables using <code>.schema &lt;table-name&gt;</code></p>

<pre><code class="sql">sqlite&gt; .schema users
CREATE TABLE users (
id INT(20), name VARCHAR(20), surname VARCHAR(20), city VARCHAR(20),
age INT(2), credit_card_num VARCHAR(20), job_position VARCHAR(20)
);
</code></pre>

<h2>Write to Sqlite Database</h2>

<p>Now we will populate data to our tables. Insert a record to our users table:</p>

<pre><code class="sql">sqlite&gt; insert into users values(1, 'ruan', 'bekker', 'cape town', 31, '2345-8970-6712-4352', 'devops');
</code></pre>

<p>Insert a record to our transactions table:</p>

<pre><code class="sql">sqlite&gt; insert into transactions values('2345-8970-6712-4352', 981623, 'spaza01', 'burger', 101.02, 'credit_card');
</code></pre>

<h2>Read from the Sqlite Database</h2>

<p>Read the data from the users table:</p>

<pre><code class="sql">sqlite&gt; select * from users;
id          name        surname     city        age         credit_card_num      job_position
----------  ----------  ----------  ----------  ----------  -------------------  ------------
1           ruan        bekker      cape town   31          2345-8970-6712-4352  devops      
</code></pre>

<p>Read the data from the transactions table:</p>

<pre><code class="sql">sqlite&gt; select * from transactions;
credit_card_num      transaction_id  shop_name   product_name  spent_total  purchase_option
-------------------  --------------  ----------  ------------  -----------  ---------------
2345-8970-6712-4352  981623          spaza01     burger        101.02       credit_card    
</code></pre>

<h2>Inner Joins with Sqlite</h2>

<p>This is where stuff gets interesting.</p>

<p>Let&rsquo;s say you want to join data from 2 tables, in this example we have a table with our userdata and a table with transaction data.</p>

<p>Say we want to see the user&rsquo;s name, transaction id, the product they purchased and the total amount spent, we will make use of inner joins.</p>

<p>Example looks like this:</p>

<pre><code class="sql">sqlite&gt; select a.name, b.transaction_id, b.product_name, b.spent_total
   ...&gt; from users
   ...&gt; as a inner join transactions
   ...&gt; as b on a.credit_card_num = b.credit_card_num
   ...&gt; where a.credit_card_num = '2345-8970-6712-4352';
name        transaction_id  product_name  spent_total
----------  --------------  ------------  -----------
ruan        981623          burger         101.02  
</code></pre>

<p>Let&rsquo;s say you dont know the credit_card number but you would like to do a lookup the credit card number via the user&rsquo;s id, then pass the value to the where statement:</p>

<pre><code class="sql">sqlite&gt; select a.name, b.transaction_id, b.product_name, b.spent_total
   ...&gt; from users
   ...&gt; as a inner join transactions
   ...&gt; as b on a.credit_card_num = b.credit_card_num
   ...&gt; where a.credit_card_num = (select credit_card_num from users where id = 1);
name        transaction_id  product_name  spent_total
----------  --------------  ------------  -----------
ruan        981623          burger         101.02   
</code></pre>

<p>Let&rsquo;s create another table called <code>products</code>:</p>

<pre><code class="sql">sqlite&gt; create table products (
   ...&gt; product_id INTEGER(18), product_name VARCHAR(20), 
   ...&gt; product_category VARCHAR(20), product_price DECIMAL(6,2)
   ...&gt; );
</code></pre>

<p>Write a record with product data to the table:</p>

<pre><code class="sql">sqlite&gt; insert into products values(0231, 'burger', 'fast foods', 101.02);
</code></pre>

<p>Now, lets say the question will be that we need to display the users name, credit card number, product name as well as the product category and products price, by only giving you the credit card number</p>

<pre><code class="sql">sqlite&gt; select a.name, b.credit_card_num, c.product_name, c.product_category, c.product_price
   ...&gt; from users
   ...&gt; as a inner join transactions
   ...&gt; as b on a.credit_card_num = b.credit_card_num inner join products
   ...&gt; as c on b.product_name = c.product_name
   ...&gt; where a.credit_card_num = '2345-8970-6712-4352' and c.product_name = 'burger';
name        credit_card_num      product_name  product_category   product_price
----------  -------------------  ------------  -----------------  -------------
ruan        2345-8970-6712-4352  burger        fast foods         101.02   
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Shrink Your Elasticsearch Index by Reducing the Shard Count With the Shards API]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/04/06/shrink-your-elasticsearch-index-by-reducing-the-shard-count-with-the-shards-api/"/>
    <updated>2019-04-06T21:33:48+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/04/06/shrink-your-elasticsearch-index-by-reducing-the-shard-count-with-the-shards-api</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/53352581-b3892f80-392b-11e9-9532-5db5cbfc8f1c.jpg" alt="elasticsearch" /></p>

<p>Resize your Elasticsearch Index with fewer Primary Shards by using the Shrink API.</p>

<p>In Elasticsearch, every index consists of multiple shards and every shard in your elasticsearch cluster contributes to the usage of your cpu, memory, file descriptors etc. This definitely helps for performance in parallel processing. As for an example with time series data, you would write and read a lot to an index with ie the current date.</p>

<p>If that index drops in requests and only read from the index every now and then, we dont need that many shards anymore and if we have multiple indexes, they may build up and take up unessacary compute power.</p>

<p>For a scenario where we want to reduce the size of our indexes, we can use the Shrink API to reduce the number of primary shards.</p>

<h2>The Shrink API</h2>

<p>The shrink index API allows you to shrink an existing index into a new index with fewer primary shards. The requested number of primary shards in the target index must be a factor of the number of shards in the source index. For example an index with 8 primary shards can be shrunk into 4, 2 or 1 primary shards or an index with 15 primary shards can be shrunk into 5, 3 or 1. If the number of shards in the index is a prime number it can only be shrunk into a single primary shard. Before shrinking, a (primary or replica) copy of every shard in the index must be present on the same node.</p>

<p>Steps on Shrinking:</p>

<p>Create the target index with the same definition as the source index, but with a smaller number of primary shards.
Then it hard-links segments from the source index into the target index.
Finally, it recovers the target index as though it were a closed index which had just been re-opened.</p>

<h2>Reduce the Primary Shards of an Index.</h2>

<p>As you may know, you can only set the Primary Shards on Index Creation time and Replica Shards you can set on the fly.</p>

<p>In this example we have a source index: <code>my-index-2019.01.10</code> with 5 primary shards and 1 replica shard, which gives us 10 shards for that index, that we would like to shrink to an index named: <code>archive_my-index-2019.01.10</code> with 1 primary shard and 1 replica shard, which will give us 2 shards for that index.</p>

<p>Have a look at your index:</p>

<pre><code>$ curl -XGET "http://127.0.0.1:9200/_cat/indices/my-index-2019.01.*?v"
health status index                                     uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   my-index-2019.01.10                       xAijUTSevXirdyTZTN3cuA   5   1   80795533            0      5.9gb          2.9gb
green  open   my-index-2019.01.11                       yb8Cjy9eQwqde8mJhR_vlw   5   5   80590481            0      5.7gb          2.8gb
...
</code></pre>

<p>And have a look at the nodes, as we will relocate the shards to a specific node:</p>

<pre><code>$ curl http://127.0.0.1:9200/_cat/nodes?v
ip            heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
x.x.x.x             8          98   0    0.04    0.03     0.01 m         -      3E9yp60
x.x.x.x            65          99   4    0.43    0.23     0.36 di        -      znFrs18
</code></pre>

<p>In this demonstration we only have 2 nodes with a replication factor of 1, which means a index&rsquo;s shards will always reside on both nodes. In a case with more nodes, we need to ensure that we choose a node where a primary index reside on.</p>

<p>Look at the shards api, by passing the index name to get the index to shard allocation:</p>

<pre><code>$ curl http://127.0.0.1:9200/_cat/shards/my-index-2019.01.10?v'
index               shard prirep state   docs  store ip       node
my-index-2019.01.10 2     p      STARTED  193  101mb x.x.x.x  Lq9P7eP
my-index-2019.01.10 2     r      STARTED  193  101mb x.x.x.x  F5edOwK
my-index-2019.01.10 4     p      STARTED  197  101mb x.x.x.x  Lq9P7eP
my-index-2019.01.10 4     r      STARTED  197  101mb x.x.x.x  F5edOwK
my-index-2019.01.10 3     r      STARTED  184  101mb x.x.x.x  Lq9P7eP
my-index-2019.01.10 3     p      STARTED  184  101mb x.x.x.x  F5edOwK
my-index-2019.01.10 1     r      STARTED  180  101mb x.x.x.x  Lq9P7eP
my-index-2019.01.10 1     p      STARTED  180  101mb x.x.x.x  F5edOwK
my-index-2019.01.10 0     p      STARTED  187  101mb x.x.x.x  Lq9P7eP
my-index-2019.01.10 0     r      STARTED  187  101mb x.x.x.x  F5edOwK
</code></pre>

<p>Create the target index:</p>

<pre><code>$ curl -XPUT -H 'Content-Type: application/json' http://127.0.0.1:9200/archive_my-index-2019.01.10 -d '
{
    "settings": {
        "number_of_shards": "1",
        "number_of_replicas": "1"
    }
}
'
</code></pre>

<p>Set the index as read only and relocate every copy of shard to node we indentified in a previous step:</p>

<pre><code>$ curl -XPUT -H 'Content-Type: application/json' http://127.0.0.1:9200/my-index-2019.01.10/_settings -d '
{
    "settings": {
        "index.routing.allocation.require._name": "Lq9P7eP",
        "index.blocks.write": true
    }
}
'
</code></pre>

<p>Now shrink the source index (my-index-2019.01.10) to the target index (archive_my-index-2019.01.10):</p>

<pre><code>$ curl -XPOST -H 'Content-Type: application/json' http://127.0.0.1:9200/my-index-2019.01.10/_shrink/archive_my-index-2019.01.10
</code></pre>

<p>You can monitor the progress by using the Recovery API:</p>

<pre><code>$ curl -s -XGET "http://127.0.0.1:9200/_cat/recovery/my-index-2019.01.10?human&amp;detailed=true"
my-index-2019.01.10 0 23.3s peer done x.x.x.x  F5edOwK x.x.x.x Lq9P7eP n/a n/a 15 15 100.0% 15 635836677 635836677 100.0% 635836677 0 0 100.0%
my-index-2019.01.10 1 22s   peer done x.x.x.x  Lq9P7eP x.x.x.x Lq9P7eP n/a n/a 15 15 100.0% 15 636392649 636392649 100.0% 636392649 0 0 100.0%
my-index-2019.01.10 2 19.6s peer done x.x.x.x  F5edOwK x.x.x.x Lq9P7eP n/a n/a 15 15 100.0% 15 636809671 636809671 100.0% 636809671 0 0 100.0%
my-index-2019.01.10 3 21.5s peer done x.x.x.x  Lq9P7eP x.x.x.x Lq9P7eP n/a n/a 15 15 100.0% 15 636378870 636378870 100.0% 636378870 0 0 100.0%
my-index-2019.01.10 4 23.3s peer done x.x.x.x F5edOwK- x.x.x.x Lq9P7eP n/a n/a 15 15 100.0% 15 636545756 636545756 100.0% 636545756 0 0 100.0%
</code></pre>

<p>You can also pass aliases as your table columns for output:</p>

<pre><code>$ curl -s -XGET "http://127.0.0.1:9200/_cat/recovery/my-index-2019.01.10?v&amp;detailed=true&amp;h=index,shard,time,ty,st,shost,thost,f,fp,b,bp"
index                            shard time  ty   st   shost         thost        f  fp     b         bp
my-index-2019.01.10              0     23.3s peer done x.x.x.x x.x.x.x 15 100.0% 635836677 100.0%
...
</code></pre>

<p>When the job is done, have a look at your indexes:</p>

<pre><code>$ curl -XGET "http://127.0.0.1:9200/_cat/indices/*my-index-2019.01.10?v"
health status index                                     uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   archive_my-index-2019.01.10               PAijUTSeRvirdyTZTN3cuA   1   1   80795533            0      5.9gb          2.9gb
green  open   my-index-2019.01.10                       Cb8Cjy9CQwqde8mJhR_vlw   5   1   80795533            0      2.9gb          2.9gb
</code></pre>

<p>Remove the block on your old index in order to make it writable:</p>

<pre><code>$ curl -XPUT -H 'Content-Type: application/json' http://127.0.0.1:9200/my-index-2019.01.10/_settings" -d '
{
    "settings": {
        "index.routing.allocation.require._name": null,
        "index.blocks.write": null
    }
}
'
</code></pre>

<p>Delete the old index:</p>

<pre><code>$ curl -XDELETE -H 'Content-Type: application/json' http://127.0.0.1:9200/my-index-2019.01.10
</code></pre>

<p>Note:, On AWS Elasticsearch Service, if you dont remove the block and you trigger a redeployment, you will end up with something like this. Shard may still be constraint to a node.</p>

<pre><code>$ curl -s -XGET ${ES_HOST/_cat/allocation?v
shards disk.indices disk.used disk.avail disk.total disk.percent host          ip  node
     0           0b    51.2gb    956.5gb   1007.8gb            5 x.x.x.x  x.x.x.x  ap9Mx1R
     1        3.6gb    54.9gb    952.8gb   1007.8gb            5 x.x.x.x  x.x.x.x  PqmoQpN   &lt;-----------
     0           0b    51.2gb    956.5gb   1007.8gb            5 x.x.x.x  x.x.x.x  5p7x4Lc
     0           0b    51.2gb    956.5gb   1007.8gb            5 x.x.x.x  x.x.x.x  c8kniP3
     0           0b    51.2gb    956.5gb   1007.8gb            5 x.x.x.x  x.x.x.x  jPwlwsD
     0           0b    51.2gb    956.5gb   1007.8gb            5 x.x.x.x  x.x.x.x  ljos4mu
   481      904.1gb   990.3gb    521.3gb      1.4tb           65 x.x.x.x  x.x.x.x  qAF-gIU
   481      820.2gb   903.6gb    608.1gb      1.4tb           59 x.x.x.x  x.x.x.x  dR3sNwA
   481      824.6gb   909.1gb    602.6gb      1.4tb           60 x.x.x.x  x.x.x.x  fvL4A9X
   481      792.7gb   876.5gb    635.2gb      1.4tb           57 x.x.x.x  x.x.x.x  lk4svht
   481      779.2gb   864.4gb    647.3gb      1.4tb           57 x.x.x.x  x.x.x.x  uLsej9m
     0           0b    51.2gb    956.5gb   1007.8gb            5 x.x.x.x  x.x.x.x  yM4Ka9l
</code></pre>

<h2>Resources:</h2>

<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-shrink-index.html">https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-shrink-index.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a 5 Node Highly Available Elasticsearch Cluster]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/04/02/setup-a-5-node-highly-available-elasticsearch-cluster/"/>
    <updated>2019-04-02T11:05:19+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/04/02/setup-a-5-node-highly-available-elasticsearch-cluster</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/53352581-b3892f80-392b-11e9-9532-5db5cbfc8f1c.jpg" alt="elasticsearch" /></p>

<p>This is post 1 of my big collection of <strong><a href="https://blog.ruanbekker.com/blog/categories/elasticsearch-tutorials">elasticsearch-tutorials</a></strong> which includes, setup, index, management, searching, etc. More details at the bottom.</p>

<p>In this tutorial we will setup a <strong>5 node highly available elasticsearch cluster</strong> that will consist of 3 Elasticsearch Master Nodes and 2 Elasticsearch Data Nodes.</p>

<blockquote><p>&ldquo;Three master nodes is the way to start, but only if you&rsquo;re building a full cluster, which at minimum is 3 master nodes plus at least 2 data nodes.&rdquo;
 - <a href="https://discuss.elastic.co/t/should-dedicated-master-nodes-and-data-nodes-be-considered-separately/75093/14">https://discuss.elastic.co/t/should-dedicated-master-nodes-and-data-nodes-be-considered-separately/75093/14</a></p></blockquote>

<p><a href="https://bekkerclothing.com/collections/developer?utm_source=blog.ruanbekker.com&utm_medium=blog&utm_campaign=leaderboard_ad" target="_blank"><img alt="bekker-clothing-developer-tshirts" src="https://user-images.githubusercontent.com/567298/70170981-7c278a80-16d6-11ea-9759-6621d02c1423.png"></a></p>

<h2>The Overview:</h2>

<p>In short the responsibilites of the node types:</p>

<p><strong>Master Nodes</strong>: Master nodes are responsible for Cluster related tasks, creating / deleting indexes, tracking of nodes, allocate shards to nodes, etc.</p>

<p><strong>Data Nodes</strong>: Data nodes are responsible for hosting the actual shards that has the indexed data also handles data related operations like CRUD, search, and aggregations.</p>

<p>For more concepts of Elasticsearch, have a look at their <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started-concepts.html">basic-concepts</a> documentation.</p>

<p>Our Inventory will consist of:</p>

<p><strong>Master Nodes:</strong></p>

<pre><code>Hostname: es-master-1, Private IP: 172.31.0.77
Hostname: es-master-2, Private IP: 172.31.0.45
Hostname: es-master-3, Private IP: 172.31.1.31
</code></pre>

<p><strong>Data Nodes:</strong></p>

<pre><code>Hostname: es-data-1, Private IP:172.31.2.30
Hostname: es-data-2, Private IP:172.31.0.83
</code></pre>

<p><strong>Reserved Volumes</strong> for Data Nodes:</p>

<pre><code>es-data-1: 10GB assigned to /dev/vdb
es-data-2: 10GB assigned to /dev/vdb
</code></pre>

<p><strong>Authentication:</strong></p>

<p>Note that I have configured the bind address for elasticsearch to <code>0.0.0.0</code> using <code>network.host: 0.0.0.0</code> for this demonstration, but this means that if your server has a public ip address with no firewall rules or no auth, that anyone will be able to interact with your cluster.</p>

<p>This address will also be reachable for all nodes to see each other.</p>

<p>It&rsquo;s advisable do protect your endpoint, either with <a href="https://blog.ruanbekker.com/blog/2017/08/31/secure-your-access-to-kibana-5-and-elasticsearch-5-with-nginx-for-aws/">basic auth using nginx</a> which can be found in the embedded link, or using firewall rules to protect communication from the outside (depending on your setup)</p>

<h2>Setup the Elasticsearch Master Nodes</h2>

<p>The setup below how to provision a elasticsearch master node. Repeat this on node: <code>es-master-1</code>, <code>es-master-2</code>, <code>es-master-3</code></p>

<p>Set your hosts file for name resolution (if you don&rsquo;t have private dns in place):</p>

<pre><code>$ cat &gt; /etc/hosts &lt;&lt; EOF
127.0.0.1 localhost
172.31.0.77 es-master-1
172.31.0.45 es-master-2
172.31.1.31 es-master-3
172.31.2.30 es-data-1
172.31.0.83 es-data-2
EOF
</code></pre>

<p>Get the elasticsearch repositories, install the java development kit dependency and install elasticsearch:</p>

<pre><code>$ apt update &amp;&amp; apt upgrade -y
$ apt install software-properties-common python-software-properties apt-transport-https -y
$ wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
$ echo "deb https://artifacts.elastic.co/packages/6.x/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-6.x.list
$ apt update
$ apt install default-jdk -y
$ apt install elasticsearch -y
</code></pre>

<p>The elasticsearch config, before we get to the full example config, I just want to show a snippet of how you could split up logs and data.</p>

<p>Note that you can seperate your logging between data/logs like this:</p>

<pre><code># example of log splitting:
...
path:
  logs: /var/log/elasticsearch
  data: /var/data/elasticsearch
...
</code></pre>

<p>Also, your data can be divided between paths:</p>

<pre><code># example of data paths:
...
path:
  data:
    - /mnt/elasticsearch_1
    - /mnt/elasticsearch_2
    - /mnt/elasticsearch_3
...
</code></pre>

<p>Bootstrap the elasticsearch config with a cluster name (all the nodes should have the same cluster name), set the nodes as master <code>node.master: true</code> disable the <code>node.data</code> and specify that the cluster should at least have a minimum of 2 master nodes before it stops. This is used to prevent split brain.</p>

<p>To avoid a split brain, this setting should be set to a quorum of master-eligible nodes:
<code>(master_eligible_nodes / 2) + 1</code></p>

<p>The full example config:</p>

<pre><code>$ cat &gt; /etc/elasticsearch/elasticsearch.yml &lt;&lt; EOF
cluster.name: es-cluster
node.name: \${HOSTNAME}
node.master: true
node.data: false
path.logs: /var/log/elasticsearch
bootstrap.memory_lock: true
network.host: 0.0.0.0
discovery.zen.minimum_master_nodes: 2
discovery.zen.ping.unicast.hosts: ["es-master-1", "es-master-2", "es-master-3"]
EOF
</code></pre>

<p>Important settings for your elasticsearch cluster is described on their <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/system-config.html">docs</a>:</p>

<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration-memory.html">Disable swapping</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/file-descriptors.html">Increase file descriptors</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html">Ensure sufficient virtual memory</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/max-number-of-threads.html">Ensure sufficient threads</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/networkaddress-cache-ttl.html">JVM DNS cache settings</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/executable-jna-tmpdir.html">Temporary directory not mounted with noexec</a></li>
</ul>


<pre><code>$ cat &gt; /etc/default/elasticsearch &lt;&lt; EOF
ES_STARTUP_SLEEP_TIME=5
MAX_OPEN_FILES=65536
MAX_LOCKED_MEMORY=unlimited
EOF
</code></pre>

<p>Ensure that pages are not swapped out to disk by requesting the JVM to lock the heap in memory by setting <code>LimitMEMLOCK=infinity</code>. Set the maxiumim file descriptor number for this process: <code>LimitNOFILE</code> and increase the number of threads using <code>LimitNPROC</code>:</p>

<pre><code>$ vim /usr/lib/systemd/system/elasticsearch.service

[Service]
LimitMEMLOCK=infinity
LimitNOFILE=65535
LimitNPROC=4096
...
</code></pre>

<p>Increase the limit on the number of open files descriptors to user elasticsearch of 65536 or higher</p>

<pre><code>$ cat &gt; /etc/security/limits.conf &lt;&lt; EOF
elasticsearch soft memlock unlimited
elasticsearch hard memlock unlimited
EOF
</code></pre>

<p>Increase the value of the mmap counts as elasticsearch uses mmapfs directory to store its indices:</p>

<pre><code>$ sysctl -w vm.max_map_count=262144
</code></pre>

<p>For a permanent setting, update <code>vm.max_map_count</code> in <code>/etc/sysctl.conf</code> and run</p>

<pre><code>$ sysctl -p /etc/sysctl.conf 
</code></pre>

<p>Prepare the directories and set the ownership to elasticsearch:</p>

<pre><code>$ mkdir /usr/share/elasticsearch/data
$ chown -R elasticsearch:elasticsearch /usr/share/elasticsearch/data
</code></pre>

<p>Reload the systemd daemon, enable and start elasticsearch</p>

<pre><code>$ systemctl daemon-reload
$ systemctl enable elasticsearch
$ systemctl restart elasticsearch
</code></pre>

<p>Once all 3 elasticsearch masters has been started, verify that they are listening: <code>netstat -tulpn | grep 9200</code> then look at the cluster health:</p>

<pre><code>$ curl http://127.0.0.1:9200/_cluster/health?pretty
{
  "cluster_name" : "es-cluster",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 3,
  "number_of_data_nodes" : 0,
  "active_primary_shards" : 0,
  "active_shards" : 0,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 100.0
}
</code></pre>

<p>Have a look at the nodes, you will see that the node.role for now shows <code>mi</code>:</p>

<pre><code>$ curl http://127.0.0.1:9200/_cat/nodes?v
ip          heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
10.163.68.8           11          80  18    0.28    0.14     0.09 mi        -      es-master-2
10.163.68.5           14          80  14    0.27    0.18     0.11 mi        *      es-master-1
10.163.68.4           15          79   6    0.62    0.47     0.18 mi        -      es-master-3
</code></pre>

<h2>Setup the Elasticsearch Data Nodes</h2>

<p>Now that we have our 3 elasticsearch master nodes running, its time to provision the 2 elasticsearch data nodes. This setup needs to be repeated on both <code>es-data-1</code> and <code>es-data-2</code>.</p>

<p>Configure the hosts file for name resolution:</p>

<pre><code>$ cat &gt; /etc/hosts &lt;&lt; EOF
127.0.0.1 localhost
172.31.0.77 es-master-1
172.31.0.45 es-master-2
172.31.1.31 es-master-3
172.31.2.30 es-data-1
172.31.0.83 es-data-2
EOF
</code></pre>

<p>Get the elasticsearch repositories, install the java development kit dependency and install elasticsearch:</p>

<pre><code>$ apt update &amp;&amp; apt upgrade -y
$ apt install software-properties-common python-software-properties apt-transport-https -y
$ wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
$ echo "deb https://artifacts.elastic.co/packages/6.x/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-6.x.list
$ apt update
$ apt install default-jdk -y
$ apt install elasticsearch -y
</code></pre>

<p>Since we attached an extra disk to our data nodes, verify that you can see the disk:</p>

<pre><code>$ lsblk
NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
vda    253:0    0  25G  0 disk
└─vda1 253:1    0  25G  0 part /
vdb    253:16   0  10G  0 disk             &lt;----
</code></pre>

<p>Provision the block device with xfs or anything else that you prefer, create the directory where elasticsearch data will reside, change the ownership that elasticsearch has permission to write/read, set the device on startup and mount the disk:</p>

<pre><code>$ mkfs.xfs /dev/vdb
$ mkdir /data
$ mkdir /data/nodes
$ chown -R elasticsearch:elasticsearch /data
$ chown -R elasticsearch:elasticsearch /data/nodes
$ echo '/dev/vdb /data xfs defaults 0 0' &gt;&gt; /etc/fstab
$ mount -a
</code></pre>

<p>Verify that the disk is mounted:</p>

<pre><code>$ df -h
Filesystem      Size  Used Avail Use% Mounted on
udev            994M     0  994M   0% /dev
tmpfs           201M  3.1M  197M   2% /run
/dev/vda1        25G  1.8G   23G   8% /
/dev/vdb         10G   33M   10G   1% /data
</code></pre>

<p>Bootstrap the elasticsearch config with a cluster name, set the <code>node.name</code> to an identifier, in this case I will use the servers hostname, set the <code>node.master</code> to false as this will be data nodes, also enable these nodes as data nodes: <code>node.data: true</code>, configure the <code>path.data: /data</code> to the path that we configured, etc:</p>

<pre><code>$ cat &gt; /etc/elasticsearch/elasticsearch.yml &lt;&lt; EOF
cluster.name: es-cluster
node.name: \${HOSTNAME}
node.master: false
node.data: true
path.data: /data
path.logs: /var/log/elasticsearch
bootstrap.memory_lock: true
network.host: 0.0.0.0
discovery.zen.minimum_master_nodes: 2
discovery.zen.ping.unicast.hosts: ["es-master-1", "es-master-2", "es-master-3"]
EOF
</code></pre>

<p>Set a couple of important settings for your elasticsearch cluster is described on their <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/system-config.html">docs</a>:</p>

<pre><code>$ cat &gt; /etc/default/elasticsearch &lt;&lt; EOF
ES_STARTUP_SLEEP_TIME=5
MAX_OPEN_FILES=65536
MAX_LOCKED_MEMORY=unlimited
EOF
</code></pre>

<p>Disable swapping, increase the file descriptors and increase the maximum number of threads:</p>

<pre><code>$ vim /usr/lib/systemd/system/elasticsearch.service
[Service]
LimitMEMLOCK=infinity
LimitNOFILE=65535
LimitNPROC=4096
</code></pre>

<p>Also update them via limits.conf:</p>

<pre><code>$ cat &gt; /etc/security/limits.conf &lt;&lt; EOF
elasticsearch soft memlock unlimited
elasticsearch hard memlock unlimited
EOF
</code></pre>

<p>Reload the systemd daemon, enable and start elasticsearch. Allow it to start and check if the ports are listening with <code>netstat -tulpn | grep 9200</code>, then:</p>

<pre><code>$ systemctl daemon-reload
$ systemctl enable elasticsearch
$ systemctl restart elasticsearch
</code></pre>

<p>Verify that everything works as expected, look at the cluster health and look at the status and number of nodes:</p>

<pre><code>$ curl http://127.0.0.1:9200/_cluster/health?pretty
{
  "cluster_name" : "es-cluster",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 5,
  "number_of_data_nodes" : 2,
  "active_primary_shards" : 0,
  "active_shards" : 0,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 100.0
}
</code></pre>

<p>Look at the nodes api and you will see that we now have the extra 2 nodes showing up on <code>node.role</code> as <code>di</code>:</p>

<pre><code>$ curl http://127.0.0.1:9200/_cat/nodes?v
ip           heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
10.163.68.7             9          96   6    0.12    0.11     0.03 di        -      es-data-2
10.163.68.5            10          80   2    0.20    0.09     0.08 mi        *      es-master-1
10.163.68.11           12          96   9    0.12    0.09     0.03 di        -      es-data-1
10.163.68.4            10          79   0    0.00    0.12     0.11 mi        -      es-master-3
10.163.68.8            12          79   1    0.05    0.06     0.07 mi        -      es-master-2
</code></pre>

<h2>Interact with Elasticsearch</h2>

<p>Let&rsquo;s interact with elasticsearch, the overview:</p>

<pre><code>$ curl http://127.0.0.1:9200
{
  "name" : "es-data-1",
  "cluster_name" : "es-cluster",
  "cluster_uuid" : "5BLs4sxsSEK-4OxlGnmlmw",
  "version" : {
    "number" : "6.7.0",
    "build_flavor" : "default",
    "build_type" : "deb",
    "build_hash" : "8453f77",
    "build_date" : "2019-03-21T15:32:29.844721Z",
    "build_snapshot" : false,
    "lucene_version" : "7.7.0",
    "minimum_wire_compatibility_version" : "5.6.0",
    "minimum_index_compatibility_version" : "5.0.0"
  },
  "tagline" : "You Know, for Search"
}
</code></pre>

<p>Let&rsquo;s look at the Health API:</p>

<pre><code>$ curl http://127.0.0.1:9200/_cat/health?v
epoch      timestamp cluster    status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent
1554154652 21:37:32  es-cluster green           5         2     10   5    0    0        0             0                  -                100.0%
</code></pre>

<p>Let&rsquo;s ingest some data into elasticsearch, we will create an index named <code>first-index</code> with some dummy data about people, username, name, surname, location and hobbies:</p>

<pre><code>$ curl -H 'Content-Type: application/json' -XPOST http://127.0.0.1:9200/first-index/docs/ -d '{"username": "mikes", "name": "mike", "surname": "steyn", "location": {"country": "south africa", "city": "cape town"}, "hobbies": ["sport", "coffee"]}'

$ curl -H 'Content-Type: application/json' -XPOST http://127.0.0.1:9200/first-index/docs/ -d '{"username": "clarissas", "name": "clarissa", "surname": "smith", "location": {"country": "ireland", "city": "dublin"}, "hobbies": ["shopping", "reading", "chess"]}'

$ curl -H 'Content-Type: application/json' -XPOST http://127.0.0.1:9200/first-index/docs/ -d '{"username": "franka", "name": "frank", "surname": "adams", "location": {"country": "new zealand", "city": "auckland"}, "hobbies": ["programming", "swimming", "rugby"]}'
</code></pre>

<p>Now that we ingested our data into elasticsearch, lets have a look at the Indices API, where the number of documents, size etc should reflect:</p>

<pre><code>$ curl http://127.0.0.1:9200/_cat/indices?v
health status index       uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   first-index 1o6yM7tCSqagqoeihKM7_g   5   1          3            0     40.6kb         20.3kb
</code></pre>

<p>Now lets request a search, which will give you by default 10 returned documents:</p>

<pre><code>$ curl http://127.0.0.1:9200/first-index/_search?pretty
{
  "took" : 116,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 3,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "first-index",
        "_type" : "docs",
        "_id" : "-NTO2mkB8pugP4aC2jtZ",
        "_score" : 1.0,
        "_source" : {
          "username" : "mikes",
          "name" : "mike",
          "surname" : "steyn",
          "location" : {
            "country" : "south africa",
            "city" : "cape town"
          },
          "hobbies" : [
            "sport",
            "coffee"
          ]
        }
      },
      {
        "_index" : "first-index",
        "_type" : "docs",
        "_id" : "-tTR2mkB8pugP4aCAzvG",
        "_score" : 1.0,
        "_source" : {
          "username" : "franka",
          "name" : "frank",
          "surname" : "adams",
          "location" : {
            "country" : "new zealand",
            "city" : "auckland"
          },
          "hobbies" : [
            "programming",
            "swimming",
            "rugby"
          ]
        }
      },
      {
        "_index" : "first-index",
        "_type" : "docs",
        "_id" : "-dTP2mkB8pugP4aC1ztI",
        "_score" : 1.0,
        "_source" : {
          "username" : "clarissas",
          "name" : "clarissa",
          "surname" : "smith",
          "location" : {
            "country" : "ireland",
            "city" : "dublin"
          },
          "hobbies" : [
            "shopping",
            "reading",
            "chess"
          ]
        }
      }
    ]
  }
}
</code></pre>

<p>Let&rsquo;s have a look at our shards using the Shards API, you will also see where each document is assigned to a specific shard, and also if its a primary or replica shard:</p>

<pre><code>$ curl http://127.0.0.1:9200/_cat/shards?v
index       shard prirep state   docs store ip           node
first-index 4     p      STARTED    0  230b 10.163.68.7  es-data-2
first-index 4     r      STARTED    0  230b 10.163.68.11 es-data-1
first-index 2     p      STARTED    0  230b 10.163.68.7  es-data-2
first-index 2     r      STARTED    0  230b 10.163.68.11 es-data-1
first-index 3     r      STARTED    1 6.6kb 10.163.68.7  es-data-2
first-index 3     p      STARTED    1 6.6kb 10.163.68.11 es-data-1
first-index 1     r      STARTED    2  13kb 10.163.68.7  es-data-2
first-index 1     p      STARTED    2  13kb 10.163.68.11 es-data-1
first-index 0     p      STARTED    0  230b 10.163.68.7  es-data-2
first-index 0     r      STARTED    0  230b 10.163.68.11 es-data-1
</code></pre>

<p>Then we can also use the Allocation API to see the size of our indices, disk space per node:</p>

<pre><code>$ curl http://127.0.0.1:9200/_cat/allocation?v
shards disk.indices disk.used disk.avail disk.total disk.percent host         ip           node
     5       20.3kb    32.4mb      9.9gb      9.9gb            0 10.163.68.11 10.163.68.11 es-data-1
     5       20.3kb    32.4mb      9.9gb      9.9gb            0 10.163.68.7  10.163.68.7  es-data-2
</code></pre>

<p>Let&rsquo;s search for anyone with the surname <code>smith</code>:</p>

<pre><code>$ curl -s http://127.0.0.1:9200/first-index/_search?q=surname=smith | jq .
{
  "took": 22,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 0.2876821,
    "hits": [
      {
        "_index": "first-index",
        "_type": "docs",
        "_id": "-dTP2mkB8pugP4aC1ztI",
        "_score": 0.2876821,
        "_source": {
          "username": "clarissas",
          "name": "clarissa",
          "surname": "smith",
          "location": {
            "country": "ireland",
            "city": "dublin"
          },
          "hobbies": [
            "shopping",
            "reading",
            "chess"
          ]
        }
      }
    ]
  }
}
</code></pre>

<p>Let&rsquo;s search for anyone with <code>rugby</code> as one of their hobbies:</p>

<pre><code>$ curl -s http://127.0.0.1:9200/first-index/_search?q=hobbies=rugby | jq .
{
  "took": 23,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 0.64072424,
    "hits": [
      {
        "_index": "first-index",
        "_type": "docs",
        "_id": "-tTR2mkB8pugP4aCAzvG",
        "_score": 0.64072424,
        "_source": {
          "username": "franka",
          "name": "frank",
          "surname": "adams",
          "location": {
            "country": "new zealand",
            "city": "auckland"
          },
          "hobbies": [
            "programming",
            "swimming",
            "rugby"
          ]
        }
      }
    ]
  }
}
</code></pre>

<h2>More on Elasticsearch</h2>

<p>I am planning to write up <strong>elasticsearch</strong> articles on the following topics:</p>

<ul>
<li><a href="">Setting up a 5 Node HA Elasticsearch Cluster</a></li>
<li>Indexes / Replicas</li>
<li>Search Queries</li>
<li>Delete Queries</li>
<li>Elasticsearch Snapshots and Restores on S3</li>
<li>Mapping Templates</li>
<li>Resizing Index Shards</li>
<li>Dealing with Old Timeseries Data</li>
<li>Elasticsearch Percentiles</li>
<li>Managing Yellow and Red Status Clusters</li>
<li>Managing High JVM Memory Pressure</li>
<li>and more</li>
</ul>


<p>As I finish up the writing of these posts they will be published under the <a href="https://blog.ruanbekker.com/blog/categories/elasticsearch-tutorials">#elasticsearch-tutorials</a> category on my blog and for any other elasticsearch tutorials, you can find them under the <a href="https://blog.ruanbekker.com/blog/categories/elasticsearch">#elasticsearch</a> category.</p>

<p>Oke byyyyyye :D</p>

<h2>Resources</h2>

<ul>
<li><a href="https://www.elastic.co/products/elasticsearch">Elasticsearch</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html">Elasticsearch Docs</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
