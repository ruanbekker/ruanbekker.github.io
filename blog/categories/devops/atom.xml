<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Devops | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/devops/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2018-10-11T16:52:13-04:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Setup a LAMP Stack With Ansible Using Ubuntu]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/07/08/setup-a-lamp-stack-with-ansible-using-ubuntu/"/>
    <updated>2018-07-08T17:15:15-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/07/08/setup-a-lamp-stack-with-ansible-using-ubuntu</id>
    <content type="html"><![CDATA[<p><img src="https://res.cloudinary.com/rbekker/image/upload/v1531083331/ansible_tojf8l.png" alt="" /></p>

<p>This is Part-2 of our <a href="http://blog.ruanbekker.com/blog/categories/ansible-tutorial">Ansible-Tutorial</a> and in this post we will cover how to setup a LAMP Stack on Ubuntu using Ansible. We will only have one host in our inventory, but this can be scaled easily by increasing the number of nodes in your invetory configuration file.</p>

<h2>Our Playbook:</h2>

<p>Our <code>lamp.yml</code> playbook:</p>

<pre><code class="yml lamp.yml">---
# Setup LAMP Stack
- hosts: newhost
  tasks:
    - name: install lamp stack
      sudo: yes
      apt: name= state=present update_cache=yes
      with_items:
        - apache2
        - mysql-server
        - php7.0
        - php7.0-mysql

    - name: start services
      service: name= state=started enabled=yes
      sudo: yes
      with_items:
        - apache2
        - mysql

    - name: deploy index.html
      sudo: yes
      copy:
        src: /tmp/index.html
        dest: /var/www/html/index.html
</code></pre>

<p>Our <code>index.html</code> that will be deployed on our servers:</p>

<pre><code class="html /tmp/index.html">&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;body&gt;
    &lt;h1&gt;Deployed with Ansible&lt;/h1&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>

<h2>Deploy your LAMP Stack:</h2>

<pre><code class="bash">$ ansible-playbook -i inventory.ini -u root lamp.yml

PLAY [newhost] ***************************************************************************************************************************

TASK [Gathering Facts] *******************************************************************************************************************
ok: [web-1]

TASK [install lamp stack] ****************************************************************************************************************
ok: [web-1] =&gt; (item=[u'apache2', u'mysql-server', u'php7.0', u'php7.0-mysql'])

TASK [start services] ********************************************************************************************************************
ok: [web-1] =&gt; (item=apache2)
ok: [web-1] =&gt; (item=mysql)

TASK [deploy index.html] *****************************************************************************************************************
changed: [web-1]

PLAY RECAP *******************************************************************************************************************************
web-1                      : ok=4    changed=1    unreachable=0    failed=0
</code></pre>

<p>Test our web server:</p>

<pre><code class="bash">$ curl http://10.0.0.4

Deployed with Ansible
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started With Ansible on Ubuntu]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/07/08/getting-started-with-ansible-on-ubuntu/"/>
    <updated>2018-07-08T15:56:06-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/07/08/getting-started-with-ansible-on-ubuntu</id>
    <content type="html"><![CDATA[<p><img src="https://res.cloudinary.com/rbekker/image/upload/v1531083331/ansible_tojf8l.png" alt="" /></p>

<p><a href="">Part 1</a> - This is a getting started series on Ansible.</p>

<p>The first post will be on how to setup ansible and how to reach your nodes in order to deploy software to your nodes.</p>

<h2>Install Ansible:</h2>

<p>Ansible relies on python, so we will first install the dependencies:</p>

<pre><code class="bash">$ apt update &amp;&amp; apt install python python-setuptools -y
$ easy_install pip
$ pip install ansible
</code></pre>

<h2>Populate the invetory configuration:</h2>

<p>Your invetory file will hold your host and variable information. Lets say we have 3 nodes that we want to deploy software to; <code>node-1</code>, <code>node-2</code> and <code>node-3</code>. We will group them under <code>nodes</code>. This will be saved under the a new file <code>inventory.init</code>:</p>

<pre><code class="bash inventory.ini">[nodes]
node-1
node-2
node-3
</code></pre>

<p>Next we will populate information about our node names, this will be done under our <code>~/.ssh/config</code> configuration:</p>

<p>```bash ~/.ssh/config
Host node-1
  Hostname 10.0.0.2
  User root
  IdentityFile ~/.ssh/id_rsa
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null</p>

<p>Host node-2
  Hostname 10.0.0.3
  User root
  IdentityFile ~/.ssh/id_rsa
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null</p>

<p>Host node-3
  Hostname 10.0.0.4
  User root
  IdentityFile ~/.ssh/id_rsa
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null
```</p>

<p>Now we need to generate a ssh key for our node where we will run our ansible commands from:</p>

<pre><code class="bash">$ ssh-keygen -b 2048 -f ~/.ssh/id_rsa -t rsa -q -N ""
</code></pre>

<p>Now we will copy the contents of <code>~/.ssh/id_rsa.pub</code> into our destination nodes <code>~/.ssh/authorized_keys</code> or if you have password authentication enabled, we can do <code>$ ssh-copy-id root@10.0.0.x</code> etc. Now we should be able to ssh to our nodes to <code>node-1, node-2</code> and <code>node-3</code>.</p>

<h2>Deploy Python:</h2>

<p>As Ansible requires Python, we need to bootstrap our nodes with Python. Since we are able to ssh to our nodes, we will use ansible to deploy Python to our nodes:</p>

<pre><code class="bash">$ ansible -m raw -s -a "apt update &amp;&amp; apt install python -y" -i inventory.ini nodes
</code></pre>

<p>This should succeed, then we can test our connection by running the ping module:</p>

<pre><code class="bash">$ ansible -i inventory.ini nodes -m ping
node-2 | SUCCESS =&gt; {
    "changed": false,
    "ping": "pong"
}
node-3 | SUCCESS =&gt; {
    "changed": false,
    "ping": "pong"
}
node-1 | SUCCESS =&gt; {
    "changed": false,
    "ping": "pong"
}
</code></pre>

<h2>Run a command on your nodes:</h2>

<p>Let&rsquo;s run a cat command on all the nodes:</p>

<pre><code class="bash">$ ansible -i inventory.ini nodes -a "/bin/cat /etc/hostname"
node-3 | SUCCESS | rc=0 &gt;&gt;
node-3

node-1 | SUCCESS | rc=0 &gt;&gt;
node-1

node-2 | SUCCESS | rc=0 &gt;&gt;
node-2
</code></pre>

<h2>Ansible Playbooks:</h2>

<p>Let&rsquo;s run shell commands, the traditional hello world, using the ansible-playbook command. First we need a task definition, which I will name <code>shell_command-1.yml</code>:</p>

<pre><code class="bash shell_command.yml">---
# Echo Static String
- hosts: nodes
  tasks:
  - name: echo static value
    shell: /bin/echo "hello world"
    register: echo_static
  - debug: msg=""
</code></pre>

<p>Now we have defined that our commands will be executed against the host group defined in our inventory.ini. Let&rsquo;s run our ansible playbook command:</p>

<pre><code class="bash">$ ansible-playbook -i inventory.ini shell_command.yml

PLAY [nodes] *************************************************************************************

TASK [Gathering Facts] **********************************************************************************
ok: [node-1]
ok: [node-2]
ok: [node-3]

TASK [echo static value] ********************************************************************************
changed: [node-1]
changed: [node-2]
changed: [node-3]

TASK [debug] ********************************************************************************************
ok: [node-1] =&gt; {
    "msg": "hello world"
}
ok: [node-2] =&gt; {
    "msg": "hello world"
}
ok: [node-3] =&gt; {
    "msg": "hello world"
}

PLAY RECAP **********************************************************************************************
node-1              : ok=3    changed=1    unreachable=0    failed=0
node-2              : ok=3    changed=1    unreachable=0    failed=0
node-3              : ok=3    changed=1    unreachable=0    failed=0
</code></pre>

<p>Let&rsquo;s define a variable <code>location_city = Cape Town</code> in our <code>inventory.ini</code> configuration, then we will call the variable key in our task definition:</p>

<pre><code class="bash inventory.ini">[nodes]
node-1
node-2
node-3

[nodes:vars]
location_city="Cape Town"
</code></pre>

<p>Now our task definition with our variable:</p>

<pre><code class="yml shell_command-2.yml">---
# Echo Variable
- hosts: nodes
  tasks:
  - name: echo variable value
    shell: /bin/echo ""
    register: echo
  - debug: msg=""
</code></pre>

<p>Running our ansible-playbook:</p>

<pre><code class="bash">$ ansible-playbook -i inventory.ini shell_command.yml

PLAY [nodes] **************************************************************************************************************************************************************************************

TASK [Gathering Facts] ***********************************************************************************************************************************************************************************
ok: [node-1]
ok: [node-2]
ok: [node-3]

TASK [echo variable value] *******************************************************************************************************************************************************************************
changed: [node-1]
changed: [node-2]
changed: [node-3]

TASK [debug] *********************************************************************************************************************************************************************************************
ok: [node-1] =&gt; {
    "msg": "Cape Town"
}
ok: [node-2] =&gt; {
    "msg": "Cape Town"
}
ok: [node-3] =&gt; {
    "msg": "Cape Town"
}

PLAY RECAP ***********************************************************************************************************************************************************************************************
node-1              : ok=3    changed=1    unreachable=0    failed=0
node-2              : ok=3    changed=1    unreachable=0    failed=0
node-3              : ok=3    changed=1    unreachable=0    failed=0
</code></pre>

<p>This is it for this post, all posts for this tutorial will be posted under <a href="http://blog.ruanbekker.com/blog/categories/ansible-tutorial">#ansible-tutorials</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deploy Docker Swarm Using Ansible]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/06/14/deploy-docker-swarm-using-ansible/"/>
    <updated>2018-06-14T06:05:46-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/06/14/deploy-docker-swarm-using-ansible</id>
    <content type="html"><![CDATA[<p><img src="http://obj-cache.cloud.ruanbekker.com/docker-logo.png" alt="" /></p>

<p>In this setup we will use Ansible to Deploy Docker Swarm.</p>

<p>With this setup, I have a client node, which will be my jump box, as it will be used to ssh with the docker user to my swarm nodes with passwordless ssh access.</p>

<p>The repository for the source code can be found on my <a href="https://github.com/ruanbekker/ansible-docker-swarm">Github Repository</a></p>

<h2>Pre-Check</h2>

<p>Hosts file:</p>

<pre><code>$ cat /etc/hosts
10.0.8.2 client
192.168.1.10 swarm-manager
192.168.1.11 swarm-worker-1
192.168.1.12 swarm-worker-2
</code></pre>

<p>SSH Config:</p>

<pre><code>$ cat ~/.ssh/config 
Host client
  Hostname client
  User root
  IdentityFile /tmp/key.pem
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null

Host swarm-manager
  Hostname swarm-manager
  User root
  IdentityFile /tmp/key.pem
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null

Host swarm-worker-1
  Hostname swarm-worker-1
  User root
  IdentityFile /tmp/key.pem
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null

Host swarm-worker-2
  Hostname swarm-worker-2
  User root
  IdentityFile /tmp/key.pem
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null
</code></pre>

<p>Install Ansible:</p>

<pre><code>$ apt install python-setuptools -y
$ easy_install pip
$ pip install ansible
</code></pre>

<p>Ensure passwordless ssh is working:</p>

<pre><code>$ ansible -i inventory.ini -u root -m ping all
client | SUCCESS =&gt; {
    "changed": false, 
    "ping": "pong"
}
swarm-manager | SUCCESS =&gt; {
    "changed": false, 
    "ping": "pong"
}
swarm-worker-2 | SUCCESS =&gt; {
    "changed": false, 
    "ping": "pong"
}
swarm-worker-1 | SUCCESS =&gt; {
    "changed": false, 
    "ping": "pong"
}
</code></pre>

<h2>Deploy Docker Swarm</h2>

<pre><code>$ ansible-playbook -i inventory.ini -u root deploy-swarm.yml 
PLAY RECAP 

client                     : ok=11   changed=3    unreachable=0    failed=0   
swarm-manager              : ok=18   changed=4    unreachable=0    failed=0   
swarm-worker-1             : ok=15   changed=1    unreachable=0    failed=0   
swarm-worker-2             : ok=15   changed=1    unreachable=0    failed=0   
</code></pre>

<p>SSH to the Swarm Manager and List the Nodes:</p>

<pre><code>$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
0ead0jshzkpyrw7livudrzq9o *   swarm-manager       Ready               Active              Leader              18.03.1-ce
iwyp6t3wcjdww0r797kwwkvvy     swarm-worker-1      Ready               Active                                  18.03.1-ce
ytcc86ixi0kuuw5mq5xxqamt1     swarm-worker-2      Ready               Active                                  18.03.1-ce
</code></pre>

<h2>Test Application on Swarm</h2>

<p>Create a Nginx Demo Service:</p>

<pre><code>$ docker network create --driver overlay appnet
$ docker service create --name nginx --publish 80:80 --network appnet --replicas 6 nginx
$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
k3vwvhmiqbfk        nginx               replicated          6/6                 nginx:latest        *:80-&gt;80/tcp

$ docker service ps nginx
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
tspsypgis3qe        nginx.1             nginx:latest        swarm-manager       Running             Running 34 seconds ago                       
g2f0ytwb2jjg        nginx.2             nginx:latest        swarm-worker-1      Running             Running 34 seconds ago                       
clcmew8bcvom        nginx.3             nginx:latest        swarm-manager       Running             Running 34 seconds ago                       
q293r8zwu692        nginx.4             nginx:latest        swarm-worker-2      Running             Running 34 seconds ago                       
sv7bqa5e08zw        nginx.5             nginx:latest        swarm-worker-1      Running             Running 34 seconds ago                       
r7qg9nk0a9o2        nginx.6             nginx:latest        swarm-worker-2      Running             Running 34 seconds ago   
</code></pre>

<p>Test the Application:</p>

<pre><code>$ curl -i http://192.168.1.10
HTTP/1.1 200 OK
Server: nginx/1.15.0
Date: Thu, 14 Jun 2018 10:01:34 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Tue, 05 Jun 2018 12:00:18 GMT
Connection: keep-alive
ETag: "5b167b52-264"
Accept-Ranges: bytes
</code></pre>

<p>Delete the Service:</p>

<pre><code>
$ docker service rm nginx
nginx
</code></pre>

<h2>Delete the Swarm:</h2>

<pre><code>$ ansible-playbook -i inventory.ini -u root delete-swarm.yml 

PLAY RECAP 
swarm-manager              : ok=2    changed=1    unreachable=0    failed=0   
swarm-worker-1             : ok=2    changed=1    unreachable=0    failed=0   
swarm-worker-2             : ok=2    changed=1    unreachable=0    failed=0   
</code></pre>

<p>Ensure the Nodes is removed from the Swarm, SSH to your Swarm Manager:</p>

<pre><code>$ docker node ls
Error response from daemon: This node is not a swarm manager. Use "docker swarm init" or "docker swarm join" to connect this node to swarm and try again.
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup the Elasticsearch Log Driver on Docker Swarm]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/05/02/setup-the-elasticsearch-log-driver-on-docker-swarm/"/>
    <updated>2018-05-02T15:10:30-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/05/02/setup-the-elasticsearch-log-driver-on-docker-swarm</id>
    <content type="html"><![CDATA[<p><img src="http://obj-cache.cloud.ruanbekker.com/docker-logo.png" alt="" /></p>

<p>Today we will look at a Elasticsearch logging driver for Docker.</p>

<h2>Why a Log Driver?</h2>

<p>By default the log output can be retrieved when using the <code>docker service logs -f service_name</code>, where log output of that service is shown via stdout. When having a lot of services in your swarm, it becomes useful logging all of your log output to a database service.</p>

<p>This is not just for Swarm but Docker stand alone as well.</p>

<p>In this tutorial we will use the Elasticsearch Log Driver, to log our logs for all our docker swarm services to Elasticsearch.</p>

<h2>Installing to Elasticsearch Log Driver:</h2>

<p>If you are running Docker Swarm, run this on all the nodes:</p>

<pre><code class="bash">$ docker plugin install rchicoli/docker-log-elasticsearch:latest --alias elasticsearch_latest
</code></pre>

<p>Verify that the log driver has been installed:</p>

<pre><code class="bash">$ docker plugin ls
ID                  NAME                          DESCRIPTION                          ENABLED
eadf06ad3d2a        elasticsearch_latest:latest   Send log messages to elasticsearch   true
</code></pre>

<h2>Test the Log Driver:</h2>

<p>Run a container of Alpine and echo a string of text:</p>

<pre><code class="bash">$ docker run --rm -ti \
    --log-driver elasticsearch_latest \
    --log-opt elasticsearch-url=http://192.168.0.235:9200 \
    --log-opt elasticsearch-insecure=false \
    --log-opt elasticsearch-sniff=false \
    --log-opt elasticsearch-index=docker-%F \
    --log-opt elasticsearch-type=log \
    --log-opt elasticsearch-timeout=10 \
    --log-opt elasticsearch-version=5 \
    --log-opt elasticsearch-fields=containerID,containerName,containerImageID,containerImageName,containerCreated \
    --log-opt elasticsearch-bulk-workers=1 \
    --log-opt elasticsearch-bulk-actions=1000 \
    --log-opt elasticsearch-bulk-size=1024 \
    --log-opt elasticsearch-bulk-flush-interval=1s \
    --log-opt elasticsearch-bulk-stats=false \
        alpine echo -n "this is a test logging message"
</code></pre>

<p>Have a look at your Elasticsearch indexes, and you will find the index which was specified in the log-options:</p>

<pre><code class="bash">$ curl http://192.168.0.235:9200/_cat/indices?v
health status index             uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   docker-2018.05.01 8FTqWq6nQlSGpYjD9M5qSg   5   1          1            0      8.9kb          8.9kb
</code></pre>

<p>Lets have a look at the Elasticsearch Document which holds the data of the log entry:</p>

<pre><code class="bash">$ curl http://192.168.0.235:9200/docker-2018.05.01/_search?pretty
{
  "took" : 5,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "docker-2018.05.01",
        "_type" : "log",
        "_id" : "hMTUG2MBIFc8kAgSNkYo",
        "_score" : 1.0,
        "_source" : {
          "containerID" : "cee0dc758528",
          "containerName" : "jolly_goodall",
          "containerImageID" : "sha256:3fd9065eaf02feaf94d68376da52541925650b81698c53c6824d92ff63f98353",
          "containerImageName" : "alpine",
          "containerCreated" : "2018-05-01T13:11:20.819447101Z",
          "message" : "this is a test logging message",
          "source" : "stdout",
          "timestamp" : "2018-05-01T13:11:21.119861767Z",
          "partial" : true
        }
      }
    ]
  }
}
</code></pre>

<h2>Using Swarm and Docker Compose:</h2>

<p>We will deploy a stack with a whoami golang web app, which will use the elasticsearch log driver:</p>

<pre><code class="bash docker-compose.yml">version: '3.4'

services:
  whoami:
    image: rbekker87/golang-whoami:latest
    networks:
      - appnet
    deploy:
      labels:
        - "traefik.port=80"
        - "traefik.backend.loadbalancer.swarm=true"
        - "traefik.docker.network=appnet"
        - "traefik.frontend.rule=Host:whoami.homecloud.mydomain.com"
      mode: replicated
      replicas: 10
      restart_policy:
        condition: any
      update_config:
        parallelism: 1
        delay: 70s
        order: start-first
        failure_action: rollback
      placement:
        constraints:
          - 'node.role==worker'
      resources:
        limits:
          cpus: '0.01'
          memory: 128M
        reservations:
          cpus: '0.001'
          memory: 64M
    logging:
      driver: elasticsearch_latest
      options:
        elasticsearch-url: "http://192.168.0.235:9200"
        elasticsearch-sniff: "false"
        elasticsearch-index: "docker-whoami-%F"
        elasticsearch-type: "log"
        elasticsearch-timeout: "10"
        elasticsearch-version: "6"
        elasticsearch-fields: "containerID,containerName,containerImageID,containerImageName,containerCreated"
        elasticsearch-bulk-workers: "1"
        elasticsearch-bulk-actions: "1000"
        elasticsearch-bulk-size: "1024"
        elasticsearch-bulk-flush-interval: "1s"
        elasticsearch-bulk-stats: "false"
networks:
  appnet:
    external: true
</code></pre>

<p>Deploy the Stack:</p>

<pre><code class="bash">$ docker stack deploy -c docker-compose.yml web 
</code></pre>

<p>Give it some time to launch and have a look at your indexes, and you will find the index which it wrote to:</p>

<pre><code class="bash">$ curl http://192.168.0.235:9200/_cat/indices?v
health status index                     uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   docker-2018.05.01         8FTqWq6nQlSGpYjD9M5qSg   5   1          1            0      8.9kb          8.9kb
yellow open   docker-whoami-2018.05.01  YebUtKa1RnCy86iP5_ylgg   5   1         11            0     54.4kb         54.4kb
</code></pre>

<p>Having a look at the data:</p>

<pre><code>$ curl 'http://192.168.0.235:9200/docker-whoami-2018.05.01/_search?pretty&amp;size=1'
{
  "took" : 18,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 11,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "docker-whoami-2018.05.01",
        "_type" : "log",
        "_id" : "acbgG2MBIFc8kAgShQa7",
        "_score" : 1.0,
        "_source" : {
          "containerID" : "97c3b337735f",
          "containerName" : "web_whoami.6.t2prjiexkym14isbx3yfxa99w",
          "containerImageID" : "sha256:0f7762d2ce569fc2ccf95fbc4c7191dde727551a180253fac046daecc580c7e9",
          "containerImageName" : "rbekker87/golang-whoami:latest@sha256:5a55c5de9cc16fbdda376791c90efb7c704c81b8dba949dce21199945c14cc88",
          "containerCreated" : "2018-05-01T13:24:43.089365528Z",
          "message" : "Starting up on port 80",
          "source" : "stdout",
          "timestamp" : "2018-05-01T13:24:48.636773709Z",
          "partial" : false
        }
      }
    ]
  }
}
</code></pre>

<p>For more info about this, have a look at the referenced documentation below.</p>

<h2>Resources:</h2>

<ul>
<li><a href="https://github.com/rchicoli/docker-log-elasticsearch">https://github.com/rchicoli/docker-log-elasticsearch</a></li>
<li><a href="https://github.com/moby/moby/issues/25694">https://github.com/moby/moby/issues/25694</a></li>
<li><a href="https://docs.docker.com/v17.09/engine/admin/logging/view_container_logs/">https://docs.docker.com/v17.09/engine/admin/logging/view_container_logs/</a></li>
<li><a href="https://sysadmins.co.za/how-to-setup-a-2-node-elasticsearch-cluster-on-centos-7-with-some-example-usage/">https://sysadmins.co.za/how-to-setup-a-2-node-elasticsearch-cluster-on-centos-7-with-some-example-usage/</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Forwarding the Docker Socket via a SSH Tunnel to Execute Docker Commands Locally]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/04/30/forwarding-the-docker-socket-via-a-ssh-tunnel-to-execute-docker-commands-locally/"/>
    <updated>2018-04-30T08:30:23-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/04/30/forwarding-the-docker-socket-via-a-ssh-tunnel-to-execute-docker-commands-locally</id>
    <content type="html"><![CDATA[<p>With automation in mind, when you want to execute docker commands remotely, you want to do it in a secure manner, as you don&rsquo;t want to expose your Docker port to the whole world.</p>

<p>One way in doing that, is forwarding the remote docker socket via a local port over a SSH Tunnel. With this way, you can execute docker commands locally on your workstation, as if the swarm is running on your workstation/laptop/node/bastion host etc.</p>

<p>Without the tunnel, I have a swarm on my laptop with no running services:</p>

<pre><code class="bash">$ docker service ls
ID                  NAME                   MODE                REPLICAS            IMAGE                                                               PORTS
</code></pre>

<p>As you can see, we have no services running, but the remote swarm has a couple, so after forwarding the connection, we should see our remote services.</p>

<h2>Setting up the SSH Tunnel:</h2>

<p>Here we will forward the remote docker socket: <code>/var/run/docker.sock</code> to a local port bound to localhost: <code>localhost:2377</code>:</p>

<pre><code class="bash">$ screen -S docker
$ ssh -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -i ~/path/to/key.pem -NL localhost:2377:/var/run/docker.sock root@docker-managers.mydomain.com
</code></pre>

<p>Now the SSH Tunnel will be established, and you can detach your screen session, or open a new shell session. To detach your screen session: <code>'ctrl + a' then d</code></p>

<h2>Verifying that the tunnel is established:</h2>

<p>You can use netstat to verify that the port is listening:</p>

<pre><code class="bash">$ netstat -ant | grep 2377
tcp4       0      0  127.0.0.1.2377         *.*                    LISTEN
</code></pre>

<h2>Inform the Docker Client to use the Port:</h2>

<p>Now we need to inform the docker client, to use the new port to talk to the docker daemon. We do that by setting the <code>DOCKER_HOST</code> environment variable to point to <code>localhost:2377</code>:</p>

<pre><code class="bash">$ export DOCKER_HOST="localhost:2377"
</code></pre>

<p>This will remain for the lifetime of the shell session.</p>

<h2>Testing it Out:</h2>

<p>Now we can run our commands locally, and we should see the output of our remote swarm:</p>

<pre><code class="bash">$ docker service ls
ID                  NAME                   MODE                REPLICAS            IMAGE                                                               PORTS
xjta8e3ek2u2        apps_flask_reminders   replicated          3/3                 rbekker87/flask-reminders:debian
0l7ruktbqj99        apps_kibana            replicated          1/1                 kibana:latest
...
</code></pre>

<h2>Terminating our SSH Tunnel:</h2>

<p>To terminate our SSH Tunnel, reconnect to your shell session, and hit <code>ctrl + c</code>:</p>

<pre><code class="bash">$ screen -ls 
There is a screen on:
    50413.docker    (Detached)
$ screen -r 50413
</code></pre>

<p>Hit <code>ctrl + c</code> :</p>

<pre><code class="bash">CKilled by signal 2.
</code></pre>

<p>And exit the screen session:</p>

<pre><code class="bash">$ exit
</code></pre>

<p>With this way, you can do lots of automation with docker swarm, not limited to swarm, but one of them.</p>
]]></content>
  </entry>
  
</feed>
