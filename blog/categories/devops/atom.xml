<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Devops | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/devops/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2021-03-09T01:21:41-05:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[CICD With DroneCI and Gitea Using Docker Compose]]></title>
    <link href="https://blog.ruanbekker.com/blog/2021/03/09/cicd-with-droneci-and-gitea-using-docker-compose/"/>
    <updated>2021-03-09T01:10:10-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2021/03/09/cicd-with-droneci-and-gitea-using-docker-compose</id>
    <content type="html"><![CDATA[<p>In this post we wil set up a drone-ci and gitea stack using docker-compose and then running a test pipeline.</p>

<p>I have posted a few times about this topic, but this post will be used when I create other examples and wanting to use this post for the ones not having the stack booted yet.</p>

<h2>The Source Code</h2>

<p>All the code will be in my <a href="https://github.com/ruanbekker/drone-gitea-on-docker">github repository</a>.</p>

<p>For our <code>docker-compose.yml</code>:</p>

<pre><code>version: '3.6'

services:
  gitea:
    container_name: gitea
    image: gitea/gitea:${GITEA_VERSION:-1.10.6}
    restart: unless-stopped
    environment:
      # https://docs.gitea.io/en-us/install-with-docker/#environments-variables
      - APP_NAME="Gitea"
      - USER_UID=1000
      - USER_GID=1000
      - RUN_MODE=prod
      - DOMAIN=${IP_ADDRESS}
      - SSH_DOMAIN=${IP_ADDRESS}
      - HTTP_PORT=3000
      - ROOT_URL=http://${IP_ADDRESS}:3000
      - SSH_PORT=222
      - SSH_LISTEN_PORT=22
      - DB_TYPE=sqlite3
    ports:
      - "3000:3000"
      - "222:22"
    networks:
      - cicd_net
    volumes:
      - ./gitea:/data

  drone:
    container_name: drone
    image: drone/drone:${DRONE_VERSION:-1.6.4}
    restart: unless-stopped
    depends_on:
      - gitea
    environment:
      # https://docs.drone.io/server/provider/gitea/
      - DRONE_DATABASE_DRIVER=sqlite3
      - DRONE_DATABASE_DATASOURCE=/data/database.sqlite
      - DRONE_GITEA_SERVER=http://${IP_ADDRESS}:3000/
      - DRONE_GIT_ALWAYS_AUTH=false
      - DRONE_RPC_SECRET=${DRONE_RPC_SECRET}
      - DRONE_SERVER_PROTO=http
      - DRONE_SERVER_HOST=${IP_ADDRESS}:3001
      - DRONE_TLS_AUTOCERT=false
      - DRONE_USER_CREATE=${DRONE_USER_CREATE}
      - DRONE_GITEA_CLIENT_ID=${DRONE_GITEA_CLIENT_ID}
      - DRONE_GITEA_CLIENT_SECRET=${DRONE_GITEA_CLIENT_SECRET}
    ports:
      - "3001:80"
      - "9001:9000"
    networks:
      - cicd_net
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./drone:/data

  drone-runner:
    container_name: drone-runner
    image: drone/drone-runner-docker:${DRONE_RUNNER_VERSION:-1}
    restart: unless-stopped
    depends_on:
      - drone
    environment:
      # https://docs.drone.io/runner/docker/installation/linux/
      # https://docs.drone.io/server/metrics/
      - DRONE_RPC_PROTO=http
      - DRONE_RPC_HOST=drone
      - DRONE_RPC_SECRET=${DRONE_RPC_SECRET}
      - DRONE_RUNNER_NAME="${HOSTNAME}-runner"
      - DRONE_RUNNER_CAPACITY=2
      - DRONE_RUNNER_NETWORKS=cicd_net
      - DRONE_DEBUG=false
      - DRONE_TRACE=false
    ports:
      - "3002:3000"
    networks:
      - cicd_net
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock

networks:
  cicd_net:
    name: cicd_net
</code></pre>

<p>Our <code>boot.sh</code> which we will use to override environment variables:</p>

<pre><code>#!/usr/bin/env bash

export HOSTNAME=$(hostname)
export DRONE_VERSION=1.10.1
export DRONE_RUNNER_VERSION=1.6.3
export GITEA_VERSION=1.13
export IP_ADDRESS=192.168.0.6
export MINIO_ACCESS_KEY="EXAMPLEKEY"
export MINIO_SECRET_KEY="EXAMPLESECRET"
export GITEA_ADMIN_USER="example"
export DRONE_RPC_SECRET="$(echo ${HOSTNAME} | openssl dgst -md5 -hex)"
export DRONE_USER_CREATE="username:${GITEA_ADMIN_USER},machine:false,admin:true,token:${DRONE_RPC_SECRET}"
export DRONE_GITEA_CLIENT_ID=""
export DRONE_GITEA_CLIENT_SECRET=""
docker-compose up -d

echo ""
echo "Gitea: http://${IP_ADDRESS}:3000/"
echo "Drone: http://${IP_ADDRESS}:3001/"
</code></pre>

<h2>Deploy the Stack</h2>

<p>Set the following in your <code>boot.sh</code>:</p>

<pre><code>IP_ADDRESS=192.168.0.6       -&gt; either reachable dns or ip address which will be your clone address and ui addresses.
GITEA_ADMIN_USER="giteauser" -&gt; will be the user you register with in drone
</code></pre>

<p>Now boot the stack:</p>

<pre><code>$ bash boot.sh
</code></pre>

<p><em>Note</em>: Theres a <a href="https://github.com/go-gitea/gitea/issues/7702">current issue</a> where webhooks get fired twice, if you see that just restart gitea with <code>docker restart gitea</code>.</p>

<ul>
<li><p>Head over to: <code>http://${IP_ADDRESS}:3000/user/settings/applications</code> and create a new OAuth2 Application and set the Redirect URI to <code>http://${IP_ADDRESS}:3001/login</code></p></li>
<li><p>Capture the client id and client secret and populate them in the <code>boot.sh</code> in <code>DRONE_GITEA_CLIENT_ID</code> and <code>DRONE_GITEA_CLIENT_SECRET</code> and run <code>bash boot.sh</code> again. This will give drone the correct credentials in order to authenticate with gitea.</p></li>
<li><p>Now when you head over to <code>http://${IP_ADDRESS}:3001/</code> you will be asked to authorize the application and you should be able to access drone.</p></li>
</ul>


<h2>Drone CLI</h2>

<p>Install Drone CLI:
- <a href="https://docs.drone.io/cli/install/">https://docs.drone.io/cli/install/</a></p>

<pre><code>$ curl -L https://github.com/drone/drone-cli/releases/latest/download/drone_darwin_amd64.tar.gz | tar zx
$ sudo mv drone /usr/local/bin/drone
$ chmod +x /usr/local/bin/drone
</code></pre>

<p>Get your Drone Token:
- <a href="http://$">http://$</a>{IP_ADDRESS}:3001/account</p>

<pre><code>$ export DRONE_SERVER=http://${IP_ADDRESS}:3001
$ export DRONE_TOKEN=one-from-the-account-page
drone info
</code></pre>

<h2>Build your first pipeline</h2>

<p>Create a test repo in gitea:</p>

<p><img src="https://user-images.githubusercontent.com/567298/110296470-0ad23800-7ffb-11eb-8428-af49d0ebd62d.png" alt="image" /></p>

<p>Commit a <code>.drone.yml</code> file for drone:</p>

<pre><code>kind: pipeline
type: docker
name: hello-world

trigger:
  branch:
    - master
  event:
    - push

steps:
  - name: say-hello
    image: busybox
    commands:
      - echo hello-world
</code></pre>

<p>Head over to drone and sync your repositories:</p>

<p><img src="https://user-images.githubusercontent.com/567298/110296425-00b03980-7ffb-11eb-9216-76725a62c09e.png" alt="image" /></p>

<p>Activate your repository:</p>

<p><img src="https://user-images.githubusercontent.com/567298/110296623-3523f580-7ffb-11eb-805f-db5db4dab0cb.png" alt="image" /></p>

<p>Push a commit to master and see your pipeline running:</p>

<p><img src="https://user-images.githubusercontent.com/567298/110296747-584ea500-7ffb-11eb-9909-259641a663aa.png" alt="image" /></p>

<h2>More Examples</h2>

<p>For more examples view my example section on the github repository:
- <a href="https://github.com/ruanbekker/drone-gitea-on-docker#more-examples">https://github.com/ruanbekker/drone-gitea-on-docker#more-examples</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Upload Public SSH Keys Using Ansible]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/10/26/upload-public-ssh-keys-using-ansible/"/>
    <updated>2020-10-26T07:44:25+00:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/10/26/upload-public-ssh-keys-using-ansible</id>
    <content type="html"><![CDATA[<p>In this post I will demonstrate how you can use ansible to automate the task of adding one or more ssh public keys to multiple servers authorized_keys file.</p>

<p>This will be focused in a scenario where you have 5 new ssh keys that we would want to copy to our bastion hosts authorized_keys file</p>

<h2>The User Accounts</h2>

<p>We have our bastion server named <code>bastion.mydomain.com</code> where would like to create the following accounts: <code>john, bob, sarah, sam, adam</code> and also upload their personal ssh public keys to those accounts so that they can logon with their ssh private keys.</p>

<p>On my local directory, I have their ssh public keys as:</p>

<pre><code>~/workspace/sshkeys/john.pub
~/workspace/sshkeys/bob.pub
~/workspace/sshkeys/sarah.pub
~/workspace/sshkeys/sam.pub
~/workspace/sshkeys/adam.pub
</code></pre>

<p>They will be referenced in our playbook as <code>key: ".pub') }}"</code> but if they were on github we can reference them as <code>key: https://github.com/.keys</code>, more info on that can be found on the <a href="https://docs.ansible.com/ansible/2.4/authorized_key_module.html">authorized_key_module</a> documentation.</p>

<h2>The Target Server</h2>

<p>Our inventory for the target server only includes one host, but we can add as many as we want, but our inventory will look like this:</p>

<pre><code>$ cat inventory.ini
[bastion]
bastion-host ansible_host=34.x.x.x ansible_user=ubuntu ansible_ssh_private_key_file=~/.ssh/ansible.pem ansible_python_interpreter=/usr/bin/python3
[bastion:vars]
ansible_ssh_common_args='-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null'
</code></pre>

<p>Test if the target server is reachable using the user <code>ubuntu</code> using our admin accounts ssh key <code>ansible.pem</code>:</p>

<pre><code>$ ansible -i inventory.ini -m ping bastion
bastion | SUCCESS =&gt; {
    "changed": false,
    "ping": "pong"
}
</code></pre>

<h2>Our Playbook</h2>

<p>In this playbook, we will reference the users that we want to create and it will loop through those users, creating them on the target server and also use those names to match to the files on our laptop to match the ssh public keys:</p>

<pre><code>$ cat playbook.yml
---
- hosts: bastion
  become: yes
  become_user: root
  become_method: sudo
  tasks:
    - name: create local user account on the target server
      user:
        name: ''
        comment: ''
        shell: /bin/bash
        append: yes
        groups: sudo
        generate_ssh_key: yes
        ssh_key_type: rsa
      with_items:
        - john
        - bob
        - sarah
        - sam
        - adam

    - name: upload ssh public key to users authorized keys file
      authorized_key:
        user: ''
        state: present
        manage_dir: yes
        key: ".pub') }}"
      with_items:
        - john
        - bob
        - sarah
        - sam
        - adam
</code></pre>

<h2>Deploy</h2>

<p>Run the playbook:</p>

<pre><code>$ ansible-playbook -i inventory.ini ssh-setup.yml

PLAY [bastion]

TASK [Gathering Facts]
ok: [bastion-host]

TASK [create local user account on the target server]
changed: [bastion-host] =&gt; (item=john)
changed: [bastion-host] =&gt; (item=bob)
changed: [bastion-host] =&gt; (item=sarah)
changed: [bastion-host] =&gt; (item=sam)
changed: [bastion-host] =&gt; (item=adam)

TASK [upload ssh public key to users authorized keys file]
changed: [bastion-host] =&gt; (item=john)
changed: [bastion-host] =&gt; (item=bob)
changed: [bastion-host] =&gt; (item=sarah)
changed: [bastion-host] =&gt; (item=sam)
changed: [bastion-host] =&gt; (item=adam)

PLAY RECAP
bastion-host                   : ok=6    changed=5    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
</code></pre>

<p>Now when we ask one of the users, adam for example, to authenticate with:</p>

<pre><code>$ ssh -i ~/.ssh/path_to_his_private_key.pem adamin@bastion.mydomain.com
</code></pre>

<p>They should have access to the server.</p>

<h2>Thank You</h2>

<p>Thanks for reading, for more information on this module check out their documentation:</p>

<ul>
<li><a href="https://docs.ansible.com/ansible/2.4/authorized_key_module.html">https://docs.ansible.com/ansible/2.4/authorized_key_module.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Use a SSH Jump Host With Ansible]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/10/26/use-a-ssh-jump-host-with-ansible/"/>
    <updated>2020-10-26T05:25:18+00:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/10/26/use-a-ssh-jump-host-with-ansible</id>
    <content type="html"><![CDATA[<p>In this post we will demonstrate how to use a SSH Bastion or Jump Host with Ansible to reach the target server.</p>

<p>In some scenarios, the target server might be in a private range which is only accessible via a bastion host, and that counts the same for ansible as ansible is using SSH to reach to the target servers.</p>

<h2>SSH Config</h2>

<p>Our bastion host is configured as <code>bastion</code> and the config under <code>~/.ssh/config</code> looks like this:</p>

<pre><code>Host *
    Port 22
    StrictHostKeyChecking no
    UserKnownHostsFile /dev/null
    ServerAliveInterval 60
    ServerAliveCountMax 30

Host bastion
    HostName bastion.mydomain.com
    User bastion
    IdentityFile ~/.ssh/id_rsa
</code></pre>

<p>To verify that our config is working, you should be able to use:</p>

<pre><code>$ ssh bastion
</code></pre>

<h2>Using a Bastion with Ansible</h2>

<p>In order to reach our target server we need to use the bastion, so to test the SSH connection we can use this SSH one-liner. Our target server has a IP address of <code>172.31.81.94</code> and expects us to provide a <code>ansible.pem</code> private key and we need to authenticate with the <code>ubuntu</code> user:</p>

<pre><code>$ ssh -o ProxyCommand="ssh -W %h:%p -q bastion" -i ~/.ssh/ansible.pem ubuntu@172.31.81.94
</code></pre>

<p>If we can reach our server its time to include it in our playbook.</p>

<p>In our inventory:</p>

<pre><code>$ cat inventory.ini
[deployment]
server-a ansible_host=172.31.81.94 ansible_user=ubuntu ansible_ssh_private_key_file=~/.ssh/ansible.pem
[deployment:vars]
ansible_ssh_common_args='-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ProxyCommand="ssh -W %h:%p -q bastion"'
</code></pre>

<p>And our playbook which will use the ping module:</p>

<pre><code>$ cat playbook.yml
- name: Test Ping
  hosts: deployment
  tasks:
  - action: ping
</code></pre>

<p>Test it out:</p>

<pre><code>$ ansible-playbook -i inventory.ini ping.yml

PLAY [Test Ping] ***********************************************************************************************************************************************************

TASK [Gathering Facts] *****************************************************************************************************************************************************
ok: [server-a]

TASK [ping] ****************************************************************************************************************************************************************
ok: [server-a]

PLAY RECAP *****************************************************************************************************************************************************************
server-a                   : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Basic Ping Role With Ansible in a Playbook]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/10/23/basic-ping-role-with-ansible-in-a-playbook/"/>
    <updated>2020-10-23T13:13:16+00:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/10/23/basic-ping-role-with-ansible-in-a-playbook</id>
    <content type="html"><![CDATA[<p>This is a short post on how to create a basic role to reference the ping module in Ansible.</p>

<h2>Directory Structure</h2>

<p>This is our directory strucuture:</p>

<pre><code>$ tree .
.
├── inventory.ini
├── playbooks
│   └── myplaybook.yml
└── roles
    └── ping
        └── tasks
            └── main.yml

4 directories, 3 files
</code></pre>

<p>Create the directories:</p>

<pre><code>$ mkdir -p playbooks
$ mkdir -p roles/ping/tasks
</code></pre>

<p>Our <code>inventory.ini</code> includes the hosts that we will be using, and in this case I will be defining a group named <code>rpifleet</code> with all the host nested under that group and I&rsquo;m using the user <code>pi</code> and my private ssh key <code>~/.ssh/id_rsa</code>:</p>

<pre><code>$ cat inventory.ini
[rpifleet]
rpi-01 ansible_host=rpi-01.local ansible_user=pi ansible_ssh_private_key_file=~/.ssh/id_rsa ansible_python_interpreter=/usr/bin/python3
rpi-02 ansible_host=rpi-02.local ansible_user=pi ansible_ssh_private_key_file=~/.ssh/id_rsa ansible_python_interpreter=/usr/bin/python3
[rpifleet:vars]
ansible_ssh_common_args='-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null'
</code></pre>

<p>Next our role is a basic role that will reference our ping module, so from our main playbook we will reference the role that we are defining:</p>

<pre><code>$ cat roles/ping/tasks/main.yml
---
- name: Test Ping
  action: ping
</code></pre>

<p>Now that we have defined our <code>ping</code> role, we need to include it into our playbook:</p>

<pre><code>$ cat playbooks/myplaybook.yml
---
- name: ping raspberry pi fleet
  hosts: rpifleet
  roles:
    - { role: ../roles/ping }
</code></pre>

<p>You will see due to my playbooks directory being non-default, I defined the path to the role directory.</p>

<h2>Install Ansible</h2>

<p>Next we need to install ansible:</p>

<pre><code>$ pip install ansible
</code></pre>

<h2>Run the Ansible Playbook</h2>

<p>Now run the playbook which will ping the nodes using ssh. Using the ping module is useful when testing the connection to your nodes:</p>

<pre><code>$ ansible-playbook -i inventory.ini playbooks/myplaybook.yml

PLAY [ping raspberry pi fleet] *****************************************************

TASK [Gathering Facts] *************************************************************
ok: [rpi-02]
ok: [rpi-01]

TASK [../roles/ping : Test Ping] ***************************************************
ok: [rpi-02]
ok: [rpi-01]

PLAY RECAP *************************************************************************
rpi-01                     : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
rpi-02                     : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
</code></pre>

<h2>Thank You</h2>

<p>Thanks for reading</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using the Local-exec Provisioner With Terraform]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/09/27/using-the-local-exec-provisioner-with-terraform/"/>
    <updated>2020-09-27T17:08:49+00:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/09/27/using-the-local-exec-provisioner-with-terraform</id>
    <content type="html"><![CDATA[<p>This is a basic example on how to use the <code>local-exec</code> provisioner in terraform, and I will use it to write a environment variable&rsquo;s value to disk.</p>

<h2>Installing Terraform</h2>

<p>Get the latest version of <a href="https://www.terraform.io/downloads.html">terraform</a>, for this post, I will be using the latest version of the time of writing:</p>

<pre><code>$ wget https://releases.hashicorp.com/terraform/0.13.3/terraform_0.13.3_linux_amd64.zip
$ unzip terraform_0.13.3_linux_amd64.zip
$ sudo mv terraform /usr/local/bin/terraform
</code></pre>

<p>Ensure that it&rsquo;s working:</p>

<pre><code>$ terraform -version
Terraform v0.13.3
</code></pre>

<h2>Terraform local-exec</h2>

<p>The local-exec provisioner allows us to run a command locally, so to test that we will write the environment variable <code>owner=ruan</code> to disk.</p>

<p>First setup our <code>main.tf</code>:</p>

<pre><code>resource "null_resource" "this" {
  provisioner "local-exec" {
    command = "echo ${var.owner} &gt; file_${null_resource.this.id}.txt"
  }
}
</code></pre>

<p>As you can see our <code>local-exec</code> provisioner is issuing the command echo to write the environment variable <code>owner</code>&rsquo;s value to a file on disk, and the file name is <code>file_</code> + the null resource&rsquo;s id.</p>

<p>As we are referencing a variable, we need to define the variable, I will define it in <code>variables.tf</code>:</p>

<pre><code>variable "owner" {}
</code></pre>

<p>As you can see, I am not defining the value, as I will define the value at runtime.</p>

<h2>Initialize</h2>

<p>When we initialize terraform, terraform builds up a dependency tree from all the <code>.tf</code> files and downloads any dependencies it requires:</p>

<pre><code>$ terraform init
</code></pre>

<h2>Apply</h2>

<p>Run our deployment and pass our variable at runtime:</p>

<pre><code>$ terraform apply -var 'owner=ruan' -auto-approve

null_resource.this: Creating...
null_resource.this: Provisioning with 'local-exec'...
null_resource.this (local-exec): Executing: ["/bin/sh" "-c" "echo ruan &gt; file_4397943546484635522.txt"]
null_resource.this: Creation complete after 0s [id=4397943546484635522]

Apply complete! Resources: 1 added, 0 changed, 0 destroyed.
</code></pre>

<p>View the written file:</p>

<pre><code>$ cat file_4397943546484635522.txt
ruan
</code></pre>

<p>If we wanted to define the environment variable in the <code>variables.tf</code> file, it will look like this:</p>

<pre><code>variable "owner" {
  description = "the owner of this project"
  default     = "ruan"
}
</code></pre>

<p>The github repository for this code is located at:</p>

<ul>
<li><a href="https://github.com/ruanbekker/terraformfiles/tree/master/variable-via-cli">https://github.com/ruanbekker/terraformfiles/tree/master/variable-via-cli</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
