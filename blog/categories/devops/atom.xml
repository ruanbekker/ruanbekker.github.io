<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Devops | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/devops/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2022-05-11T17:16:43-04:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Publish and Use Your Ansible Role From Git]]></title>
    <link href="https://blog.ruanbekker.com/blog/2022/04/19/publish-and-use-your-ansible-role-from-git/"/>
    <updated>2022-04-19T04:35:09-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2022/04/19/publish-and-use-your-ansible-role-from-git</id>
    <content type="html"><![CDATA[<p>In this tutorial we will be creating a ansible role, publish our ansible role to github, then we will install the role locally and create a ansible playbook to use the ansible role.</p>

<p>The source code for this blog post will be available on my <a href="https://blog.ruanbekker.com/blog/2022/04/19/publish-and-use-your-ansible-role-from-git/">github</a> repository.</p>

<h2>Ansible Installation</h2>

<p>Create a virtual environment with Python:</p>

<pre><code>$ virtualenv .venv -p python3
$ source .venv/bin/activate
</code></pre>

<p>Install ansible with pip:</p>

<pre><code>$ pip install ansible==4.4.0
</code></pre>

<p>Now that we have ansible installed, we can create our role.</p>

<h2>Initialize Ansible Role</h2>

<p>A Ansible Role consists of a couple of files, and using <code>ansible-galaxy</code> makes it easy initializing a boilerplate structure to begin with::</p>

<pre><code>$ ansible-galaxy init --init-path roles ssh_config
- Role ssh_config was created successfully
</code></pre>

<p>The role that we created is named <code>ssh_config</code> and will be placed under the directory <code>roles</code> under our current working directory.</p>

<h2>Define Role Tasks</h2>

<p>Create the dummy task under <code>roles/ssh_config/tasks/main.yml</code>:</p>

<script src="https://gist.github.com/ruanbekker/4971be45476915ba877bb444a9ff1c0b.js"></script>


<p>Then define the defaults environment values in the file <code>roles/ssh_config/defaults/main.yml</code>:</p>

<pre><code class="yaml">---
# defaults file for ssh_config
ssh_port: 22
</code></pre>

<p>The value of <code>ssh_port</code> will default to <code>22</code> if we don&rsquo;t define it in our variables.</p>

<h2>Commit to Git</h2>

<p>The assumption is made here that you already created a git repository and that your access is sorted. Add the files and commit it to git:</p>

<pre><code>$ git add .
$ git commit -m "Your message"
$ git push origin main
</code></pre>

<p>Now your ansible role should be commited and visible in git.</p>

<h2>SSH Config Client Side</h2>

<p>I will be referencing the git source url via SSH, and since I am using my default ssh key, the ssh config isn&rsquo;t really needed, but if you are using a different version control system, with different ports or different ssh keys, the following ssh config snippet may be useful:</p>

<pre><code>$ cat ~/.ssh/config
Host github.com
    User git
    Port 22
    IdentityFile ~/.ssh/id_rsa
</code></pre>

<p>If you won&rsquo;t be using SSH as the source url in your ansible setup for your role, you can skip the SSH setup.</p>

<h2>Installing the Ansible Role from Git</h2>

<p>When installing roles, ansible installs them by default under: <code>~/.ansible/roles</code>, <code>/usr/share/ansible/roles</code> or <code>/etc/ansible/roles</code>.</p>

<p>From our previous steps, we still have the ansible role content locally (not under the default installed directory), so by saying installing the role kinda sounds like we are doing double the work. But the intention is that you have your ansible role centralized and versioned on git, and on new servers or workstations where you want to consume the role from, that specific role, won&rsquo;t be present on that source.</p>

<p>To install the role from Git, we need to populate the <code>requirements.yml</code> file:</p>

<pre><code>$ mkdir ~/my-project
$ cd ~/my-project
</code></pre>

<p>The requirements file is used to define where our role is located, which version and the type of version control, the <code>requirements.yml</code>:</p>

<pre><code class="yaml">---
roles:
  - name: ssh_config
    src: ssh://git@github.com/ruanbekker/ansible-demo-role.git
    version: main
    scm: git
</code></pre>

<p>For other variations of using the requirements file, you can have a look at their <a href="https://galaxy.ansible.com/docs/using/installing.html#installing-multiple-roles-from-a-file">documentation</a></p>

<p>Then install the ansible role from our requirements file (I have used <code>--force</code> to overwrite my current one while testing):</p>

<pre><code>$ ansible-galaxy install -r requirements.yml --force
Starting galaxy role install process
- changing role ssh_config from main to main
- extracting ssh_config to /Users/ruan/.ansible/roles/ssh_config
- ssh_config (main) was installed successfully
</code></pre>

<h2>Ansible Playbook</h2>

<p>Define the ansible playbook to use the role that we installed from git, in a file called <code>playbook.yml</code>:</p>

<pre><code class="yaml">---
- hosts: localhost
  roles:
    - ssh_config
  vars:
    ssh_port: 2202
</code></pre>

<p>Run the ansible playbook:</p>

<pre><code>$ ansible-playbook playbook.yml
PLAY [localhost] *********************************************************************************************

TASK [Gathering Facts] ***************************************************************************************
ok: [localhost]

TASK [ssh_config : Dummy task] *******************************************************************************
ok: [localhost] =&gt; {
    "msg": "This is a dummy task changing ssh port to 2202."
}

PLAY RECAP ***************************************************************************************************
localhost                  : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
</code></pre>

<h2>Thank You</h2>

<p>Thanks for reading, if you like my content, check out my <strong><a href="https://ruan.dev">website</a></strong>, read my <strong><a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a></strong> or follow me at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Provision a AWS EC2 Instance With Terraform]]></title>
    <link href="https://blog.ruanbekker.com/blog/2022/04/16/provision-a-aws-ec2-instance-with-terraform/"/>
    <updated>2022-04-16T19:04:08-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2022/04/16/provision-a-aws-ec2-instance-with-terraform</id>
    <content type="html"><![CDATA[<p>In this tutorial I will demonstrate how to use Terraform (a Infrastructure as Code Tool), to provision a AWS EC2 Instance and the source code that we will be using in this tutorial will be published to my <a href="https://github.com/ruanbekker/terraformfiles/tree/master/aws-ec2-instance">terraformfiles github repository</a>.</p>

<h2>Requirements</h2>

<p>To follow along this tutorial, you will need an AWS Account and Terraform installed</p>

<h2>Terraform</h2>

<p>To install Terraform for your operating system, you can follow <a href="https://learn.hashicorp.com/tutorials/terraform/install-cli">Terraform Installation Documentation</a>, I am using Mac OSx, so for me it will be:</p>

<pre><code class="bash">brew tap hashicorp/tap
brew install hashicorp/tap/terraform
</code></pre>

<p>To verify the installation, we can run <code>terraform version</code> and my output shows:</p>

<pre><code>Terraform v1.1.8
on darwin_amd64
</code></pre>

<h2>Terraform Project Structure</h2>

<p>Create the directory:</p>

<pre><code class="bash">mkdir terraform-aws-ec2
cd terraform-aws-ec2
</code></pre>

<p>Create the following files: <code>main.tf</code>, <code>providers.tf</code>, <code>variables.tf</code>, <code>outputs.tf</code>, <code>locals.tf</code> and <code>terraform.tfvars</code>:</p>

<pre><code class="bash">touch main.tf providers.tf variables.tf outputs.tf locals.tf terraform.tfvars
</code></pre>

<h2>Define Terraform Configuration Code</h2>

<p>First we need to define the aws provider, which we will do in <code>providers.tf</code>:</p>

<pre><code>terraform {
  required_providers {
    aws = {
      version = "~&gt; 3.27"
      source = "hashicorp/aws"
    }
  }
}

provider "aws" {
  region  = "eu-west-1"
  profile = "default"
  shared_credentials_file = "~/.aws/credentials"
}
</code></pre>

<p>You will notice that I am defining my profile name <code>default</code> from the <code>~/.aws/credentials</code> credential provider in order for terraform to authenticate with AWS.</p>

<p>Next I am defining the <code>main.tf</code> which will be the file where we define our aws resources:</p>

<pre><code>data "aws_ami" "latest_ubuntu" {
  most_recent = true
  owners = ["099720109477"]

  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-focal-20.04-*-server-*"]
  }

  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }

  filter {
    name   = "root-device-type"
    values = ["ebs"]
  }

  filter {
    name   = "architecture"
    values = ["x86_64"]
  }

}

data "aws_iam_policy_document" "assume_role_policy" {
  statement {
    actions = ["sts:AssumeRole"]
    principals {
      type        = "Service"
      identifiers = ["ec2.amazonaws.com"]
    }
  }
}

data "aws_iam_policy" "ec2_read_only_access" {
  arn = "arn:aws:iam::aws:policy/AmazonEC2ReadOnlyAccess"
}

resource "aws_iam_role" "ec2_access_role" {
  name               = "${local.project_name}-ec2-role"
  assume_role_policy = data.aws_iam_policy_document.assume_role_policy.json
}

resource "aws_iam_policy_attachment" "readonly_role_policy_attach" {
  name       = "${local.project_name}-ec2-role-attachment"
  roles      = [aws_iam_role.ec2_access_role.name]
  policy_arn = data.aws_iam_policy.ec2_read_only_access.arn
}

resource "aws_iam_instance_profile" "instance_profile" {
  name  = "${local.project_name}-ec2-instance-profile"
  role = aws_iam_role.ec2_access_role.name
}

resource "aws_security_group" "ec2" {
    name        = "${local.project_name}-ec2-sg"
    description = "${local.project_name}-ec2-sg"
    vpc_id      = var.vpc_id

    tags = merge(
      var.default_tags,
      {
       Name = "${local.project_name}-ec2-sg"
      },
    )
}

resource "aws_security_group_rule" "ssh" {
    description       = "allows public ssh access to ec2"
    security_group_id = aws_security_group.ec2.id
    type              = "ingress"
    protocol          = "tcp"
    from_port         = 22
    to_port           = 22
    cidr_blocks       = ["0.0.0.0/0"]
}

resource "aws_security_group_rule" "egress" {
    description       = "allows egress"
    security_group_id = aws_security_group.ec2.id
    type              = "egress"
    protocol          = "-1"
    from_port         = 0
    to_port           = 0
    cidr_blocks       = ["0.0.0.0/0"]
}

resource "aws_instance" "ec2" {
  ami                         = data.aws_ami.latest_ubuntu.id
  instance_type               = var.instance_type
  subnet_id                   = var.subnet_id
  key_name                    = var.ssh_keyname
  vpc_security_group_ids      = [aws_security_group.ec2.id]
  associate_public_ip_address = true
  monitoring                  = true
  iam_instance_profile        = aws_iam_instance_profile.instance_profile.name

  lifecycle {
    ignore_changes            = [subnet_id, ami]
  }

  root_block_device {
      volume_type           = "gp2"
      volume_size           = var.ebs_root_size_in_gb
      encrypted             = false
      delete_on_termination = true
  }

  tags = merge(
    var.default_tags,
    {
     Name = "${local.project_name}"
    },
  )

}
</code></pre>

<p>A couple of things are defined here:</p>

<ul>
<li>A data resource to fetch the latest Ubuntu 20.04 AMI</li>
<li>The IAM Role and Policy that we will use to associate to our EC2 Instance Profile</li>
<li>The EC2 Security Group</li>
<li>The EC2 Instance</li>
<li>The VPC ID and Subnet ID are required variables which we will set in <code>terraform.tfvars</code></li>
</ul>


<p>The next file will be our <code>variables.tf</code> file where we will define all our variable definitions:</p>

<pre><code>variable "default_tags" {
  default = {
    Environment = "test"
    Owner       = "ruan.bekker"
    Project     = "terraform-blogpost"
    CostCenter  = "engineering"
    ManagedBy   = "terraform"
  }
}

variable "aws_region" {
  type        = string
  default     = "eu-west-1"
  description = "the region to use in aws"
}

variable "vpc_id" {
  type        = string
  description = "the vpc to use"
}

variable "ssh_keyname" {
  type        = string
  description = "ssh key to use"
}

variable "subnet_id" {
  type        = string
  description = "the subnet id where the ec2 instance needs to be placed in"
}

variable "instance_type" {
  type        = string
  default     = "t3.nano"
  description = "the instance type to use"
}

variable "project_id" {
  type        = string
  default     = "terraform-blogpost"
  description = "the project name"
}

variable "ebs_root_size_in_gb" {
  type        = number
  default     = 10
  description = "the size in GB for the root disk"
}

variable "environment_name" {
   type    = string
   default = "dev"
   description = "the environment this resource will go to (assumption being made theres one account)"
}
</code></pre>

<p>The next file is our <code>locals.tf</code> which just concatenates our project id and environment name:</p>

<pre><code>locals {
  project_name = "${var.project_id}-${var.environment_name}"
}
</code></pre>

<p>Then our <code>outputs.tf</code> for the values that terraform should output:</p>

<pre><code>output "id" {
  description = "The ec2 instance id"
  value       = aws_instance.ec2.id
  sensitive   = false
}

output "ip" {
  description = "The ec2 instance public ip address"
  value       = aws_instance.ec2.public_ip
  sensitive   = false
}

output "subnet_id" {
  description = "the subnet id which will be used"
  value       = var.subnet_id
  sensitive   = false
}
</code></pre>

<p>Then lastly our <code>terraform.tfvars</code>, which you will need to supply your own values to match your AWS Account:</p>

<pre><code># required
vpc_id = "vpc-063d7xxxxxxxxxxxx"
ssh_keyname = "ireland-key"
subnet_id = "subnet-04b3xxxxxxxxxxxxx"
</code></pre>

<h2>Deploy EC2 Instance</h2>

<p>Now that all our configuration is in place, we need to intialize terraform by downloading the providers:</p>

<pre><code class="bash">terraform init
</code></pre>

<p>Once the terraform init has completed, we can run a <code>terraform plan</code> which will show us what terraform will do. Since the <code>terraform.tfvars</code> are the default file for variables, we don&rsquo;t have to specify the name of the file, but since I want to be excplicit, I will include it (should you want to change the file name):</p>

<pre><code class="bash">terraform plan -var-file="terraform.tfvars"
</code></pre>

<p>Now it&rsquo;s a good time to review what terraform wants to action by viewing the plan output, once you are happy you can deploy the changes by running a <code>terraform apply</code>:</p>

<pre><code class="bash">terraform apply -var-file="terraform.tfvars"
</code></pre>

<p>Optional: You can override variables by either updating the <code>terraform.tfvars</code> or you can append them with terraform apply <code>-var-file="terraform.tfvars" -var="ssh_key=default_key"</code>, a successful output should show something like this:</p>

<pre><code class="bash">Outputs:
id = "i-0dgacxxxxxxxxxxxx"
ip = "18.26.xxx.92"
subnet = "subnet-04b3xxxxxxxxxxxxx"
</code></pre>

<h2>Access your EC2 Instance</h2>

<p>You can access the instance by SSH'ing to the IP that was returned by the output as well as the SSH key name that you provided, or you can make use of the <code>terraform output</code> to access the output value:</p>

<pre><code class="bash">ssh -i ~/.ssh/id_rsa ubuntu@$(terraform output -raw ip)
</code></pre>

<h2>Cleanup</h2>

<p>To delete the infrastructure that Terraform provisioned:</p>

<pre><code class="bash">terraform destroy
</code></pre>

<h2>Thank You</h2>

<p>Thanks for reading, if you like my content, check out my <strong><a href="https://ruan.dev">website</a></strong>, read my <strong><a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a></strong> or follow me at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Create DNS Records With Terraform on Cloudflare]]></title>
    <link href="https://blog.ruanbekker.com/blog/2022/02/20/create-dns-records-with-terraform-on-cloudflare/"/>
    <updated>2022-02-20T13:11:06-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2022/02/20/create-dns-records-with-terraform-on-cloudflare</id>
    <content type="html"><![CDATA[<p>In this tutorial we will use <strong>Terraform</strong> to create DNS records on <strong>Cloudflare</strong>.</p>

<h2>Installing Terraform</h2>

<p>I will be installing terraform for linux, but you can follow terraform&rsquo;s documentation if you are using a different operating system:
- <a href="https://learn.hashicorp.com/tutorials/terraform/install-cli">https://learn.hashicorp.com/tutorials/terraform/install-cli</a></p>

<pre><code class="bash">&gt; curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -
&gt; sudo apt-add-repository "deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"
&gt; sudo apt update &amp;&amp; sudo apt install terraform -y
</code></pre>

<p>Verify that terraform was installed:</p>

<pre><code class="bash">&gt; terraform version
Terraform v1.1.6
on linux_amd64
</code></pre>

<h2>Cloudflare Authentication</h2>

<p>We need to create an API Token in order to authenticate terraform to make the required API calls to create the DNS Record.</p>

<p>They have a great post on this, which you can follow below:
- <a href="https://developers.cloudflare.com/api/tokens/create">https://developers.cloudflare.com/api/tokens/create</a></p>

<p>You will need access to &ldquo;Edit DNS Zones&rdquo; and also include the Domain that you would like to edit.</p>

<p>Ensure that you save the API Token in a safe place.</p>

<h2>Terraform Code</h2>

<p>First we will create a project directory:</p>

<pre><code class="bash">&gt; mkdir terraform-cloudflare-dns
&gt; cd terraform-cloudflare-dns
</code></pre>

<p>First we will create the <code>providers.tf</code> which we define our provider and the required parameters for the provider:</p>

<pre><code>terraform {
  required_providers {
    cloudflare = {
      source = "cloudflare/cloudflare"
      version = "~&gt; 3.0"
    }
  }
}

provider "cloudflare" {
  email   = var.cloudflare_email
  api_token = var.cloudflare_api_token
}
</code></pre>

<p>As you can see, we are referencing <code>email</code> and <code>api_token</code> as variables, therefore we need to define those variables in <code>variables.tf</code>:</p>

<pre><code>variable "cloudflare_email" {
  type        = string
  description = "clouflare email address"
}

variable "cloudflare_api_token" {
  type        = string
  description = "cloudflare api token"
}
</code></pre>

<p>In our <code>main.tf</code>, we are first using a data resource to query cloudflare for our domain <code>rbkr.xyz</code> and then access the attribute <code>id</code> which we will be using in our <code>cloudflare_record</code> resource so that it knows which domain to add the DNS record for.</p>

<p>Then we are going to create the A record <code>foobar</code> and provide the value of <code>127.0.0.1</code>:</p>

<pre><code>data "cloudflare_zone" "this" {
  name = "rbkr.xyz"
}

resource "cloudflare_record" "foobar" {
  zone_id = data.cloudflare_zone.this.id
  name    = "foobar"
  value   = "127.0.0.1"
  type    = "A"
  proxied = false
}
</code></pre>

<p>Then we are defining our outputs in <code>outputs.tf</code>:</p>

<pre><code>output "record" {
  value = cloudflare_record.foobar.hostname
}

output "metadata" {
  value       = cloudflare_record.foobar.metadata
  sensitive   = true
}
</code></pre>

<h2>Creating the Record</h2>

<p>Once our configuration code is in place we can run a <code>init</code> which will download the providers:</p>

<pre><code class="bash">&gt; terraform init
</code></pre>

<p>Once that is done, we can run a <code>plan</code> so we can see what will be deployed, but since our <code>variables.tf</code> has no <code>default</code> values, we will either have to define this in <code>terraform.tfvars</code> or use it in-line.</p>

<p>I will be using it in-line for this demonstration:</p>

<pre><code class="bash">&gt; terraform plan -var "cloudflare_email=$EMAIL" -var "cloudflare_api_token=$API_TOKEN"
</code></pre>

<p>Once you are happy, you can run a <code>apply</code> which will deploy the changes:</p>

<pre><code class="bash">&gt; terraform apply -var "cloudflare_email=$EMAIL" -var "cloudflare_api_token=$API_TOKEN"

Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  # cloudflare_record.foobar will be created
  + resource "cloudflare_record" "foobar" {
      + allow_overwrite = false
      + created_on      = (known after apply)
      + hostname        = (known after apply)
      + id              = (known after apply)
      + metadata        = (known after apply)
      + modified_on     = (known after apply)
      + name            = "foobar"
      + proxiable       = (known after apply)
      + proxied         = false
      + ttl             = (known after apply)
      + type            = "A"
      + value           = "127.0.0.1"
      + zone_id         = "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
    }

Plan: 1 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + metadata = (sensitive value)
  + record   = (known after apply)

Do you want to perform these actions?
  Terraform will perform the actions described above.
  Only 'yes' will be accepted to approve.

  Enter a value: yes

cloudflare_record.foobar: Creating...
cloudflare_record.foobar: Creation complete after 4s [id=xxxxxxxxxxxxxxxxxxxxx]

Apply complete! Resources: 1 added, 0 changed, 0 destroyed.

Outputs:

metadata = &lt;sensitive&gt;
record = "foobar.rbkr.xyz"
</code></pre>

<h2>Test DNS</h2>

<p>We can now test if this is working as expected with a dns utility like dig:</p>

<pre><code class="bash">&gt; dig foobar.rbkr.xyz

; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; foobar.rbkr.xyz
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 20800
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;foobar.rbkr.xyz.       IN      A

;; ANSWER SECTION:
foobar.rbkr.xyz. 300    IN      A       127.0.0.1

;; Query time: 262 msec
;; SERVER: 172.31.0.2#53(172.31.0.2)
;; WHEN: Wed Feb 02 13:57:59 SAST 2022
;; MSG SIZE  rcvd: 68
</code></pre>

<h2>Thank You</h2>

<p>Thanks for reading, if you like my content, check out my <strong><a href="https://ruan.dev">website</a></strong> or follow me at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Install Concourse CI v7.4 on Ubuntu Linux]]></title>
    <link href="https://blog.ruanbekker.com/blog/2021/10/07/install-concourse-ci-v7-dot-4-on-ubuntu-linux/"/>
    <updated>2021-10-07T19:27:05-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2021/10/07/install-concourse-ci-v7-dot-4-on-ubuntu-linux</id>
    <content type="html"><![CDATA[<p><img src="https://i.snag.gy/gzkdu9.jpg?nocache=1511644783495" alt="" /></p>

<p>Concourse is a Pipeline Based Continious Integration system written in Go</p>

<h2>Resources:</h2>

<ul>
<li><a href="https://concourse-ci.org/">https://concourse-ci.org/</a></li>
<li><a href="https://github.com/concourse/concourse">https://github.com/concourse/concourse</a></li>
<li><a href="https://github.com/starkandwayne/concourse-tutorial">https://github.com/starkandwayne/concourse-tutorial</a></li>
</ul>


<h2>Older Version</h2>

<p>An older version is available:</p>

<ul>
<li><a href="https://blog.ruanbekker.com/blog/2021/04/06/install-concourse-ci-v6-on-ubuntu-20-dot-04/">Install Concourse CI V6 on Ubuntu 20.04</a></li>
<li><a href="https://blog.ruanbekker.com/blog/2017/11/07/setup-a-concourse-ci-server-on-ubuntu-16/">Setup Concourse CI v4 on Ubuntu 16.04</a></li>
</ul>


<h2>What is Concourse CI:</h2>

<p>Concourse CI is a Continious Integration Platform. Concourse enables you to construct pipelines with a yaml configuration that can consist out of 3 core concepts, tasks, resources, and jobs that compose them. For more information about this have a look at their <a href="https://concourse.ci/concepts.html">docs</a></p>

<h2>What will we be doing today</h2>

<p>We will setup a Concourse CI Server v6.7.6 (web and worker) on Ubuntu 20.04 and run the traditional <code>Hello, World</code> pipeline</p>

<h2>Setup the Server:</h2>

<p>Concourse needs <code>PostgresSQL</code> server:</p>

<pre><code class="bash">$ apt update &amp;&amp; apt upgrade -y
$ apt install postgresql postgresql-contrib -y
$ systemctl enable postgresql
</code></pre>

<p>Create the Database and User for Concourse on Postgres:</p>

<pre><code class="bash">$ sudo -u postgres createuser concourse
$ sudo -u postgres createdb --owner=concourse atc
</code></pre>

<p>Download the Concourse Binary:</p>

<pre><code class="bash">$ export CONCOURSE_VERSION=7.4.0
$ wget https://github.com/concourse/concourse/releases/download/v${CONCOURSE_VERSION}/concourse-${CONCOURSE_VERSION}-linux-amd64.tgz
$ tar -xvf concourse-${CONCOURSE_VERSION}-linux-amd64.tgz -C /usr/local/
$ rm -rf concourse-*-linux-amd64.tgz
</code></pre>

<p>Create the Encryption Keys:</p>

<pre><code class="bash">$ mkdir /etc/concourse
$ ssh-keygen -t rsa -q -N '' -f /etc/concourse/tsa_host_key -m pem
$ ssh-keygen -t rsa -q -N '' -f /etc/concourse/worker_key -m pem
$ ssh-keygen -t rsa -q -N '' -f /etc/concourse/session_signing_key -m pem
$ cp /etc/concourse/worker_key.pub /etc/concourse/authorized_worker_keys -m pem
</code></pre>

<p>Set the IP Address:</p>

<pre><code class="bash">$ export IP_ADDRESS=$(ifconfig $(route -n | grep '0.0.0.0' | head -1 | rev | awk '{print $1}' | rev) | grep -w 'inet' | awk '{print $2}')
</code></pre>

<p>Concourse Web Process Configuration:</p>

<pre><code class="bash">$ cat &gt; /etc/concourse/web_environment &lt;&lt; EOF
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/concourse/bin
CONCOURSE_ADD_LOCAL_USER=ruan:$(openssl rand -hex 14)
CONCOURSE_SESSION_SIGNING_KEY=/etc/concourse/session_signing_key
CONCOURSE_TSA_HOST_KEY=/etc/concourse/tsa_host_key
CONCOURSE_TSA_AUTHORIZED_KEYS=/etc/concourse/authorized_worker_keys
CONCOURSE_POSTGRES_HOST=127.0.0.1
CONCOURSE_POSTGRES_USER=concourse
CONCOURSE_POSTGRES_PASSWORD=concourse
CONCOURSE_POSTGRES_DATABASE=atc
CONCOURSE_MAIN_TEAM_LOCAL_USER=ruan
CONCOURSE_EXTERNAL_URL=http://$IP_ADDRESS:8080
EOF
</code></pre>

<p>Concourse Worker Process Configuration:</p>

<pre><code class="bash">cat &gt; /etc/concourse/worker_environment &lt;&lt; EOF
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/concourse/bin
CONCOURSE_WORK_DIR=/var/lib/concourse
CONCOURSE_TSA_HOST=127.0.0.1:2222
CONCOURSE_TSA_PUBLIC_KEY=/etc/concourse/tsa_host_key.pub
CONCOURSE_TSA_WORKER_PRIVATE_KEY=/etc/concourse/worker_key
CONCOURSE_GARDEN_DNS_SERVER=8.8.8.8
EOF
</code></pre>

<p>Create a Concourse user:</p>

<pre><code class="bash">$ mkdir /var/lib/concourse
$ sudo adduser --system --group concourse
$ sudo chown -R concourse:concourse /etc/concourse /var/lib/concourse
$ sudo chmod 600 /etc/concourse/*_environment
</code></pre>

<p>Create SystemD Unit Files, first for the Web Service:</p>

<pre><code class="bash">$ cat &gt; /etc/systemd/system/concourse-web.service &lt;&lt; EOF
[Unit]
Description=Concourse CI web process (ATC and TSA)
After=postgresql.service

[Service]
User=concourse
Restart=on-failure
EnvironmentFile=/etc/concourse/web_environment
ExecStart=/usr/local/concourse/bin/concourse web

[Install]
WantedBy=multi-user.target
EOF
</code></pre>

<p>Then the SystemD Unit File for the Worker Service:</p>

<pre><code class="bash">$ cat &gt; /etc/systemd/system/concourse-worker.service &lt;&lt; EOF
[Unit]
Description=Concourse CI worker process
After=concourse-web.service

[Service]
User=root
Restart=on-failure
EnvironmentFile=/etc/concourse/worker_environment
ExecStart=/usr/local/concourse/bin/concourse worker

[Install]
WantedBy=multi-user.target
EOF
</code></pre>

<p>Create a postgres password for the concourse user:</p>

<pre><code class="bash">$ cd /home/concourse/
$ sudo -u concourse psql atc
atc=&gt; ALTER USER concourse WITH PASSWORD 'concourse';
atc=&gt; \q
</code></pre>

<p>Start and Enable the Services:</p>

<pre><code class="bash">$ systemctl start concourse-web concourse-worker
$ systemctl enable concourse-web concourse-worker postgresql
$ systemctl status concourse-web concourse-worker

$ systemctl is-active concourse-worker concourse-web
active
active
</code></pre>

<p>The listening ports should more or less look like the following:</p>

<pre><code class="bash">$ netstat -tulpn

Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 127.0.0.1:7777          0.0.0.0:*               LISTEN      4530/concourse
tcp        0      0 127.0.0.1:7788          0.0.0.0:*               LISTEN      4530/concourse
tcp        0      0 127.0.0.1:8079          0.0.0.0:*               LISTEN      4525/concourse
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1283/sshd
tcp        0      0 127.0.0.1:5432          0.0.0.0:*               LISTEN      4047/postgres
tcp6       0      0 :::36159                :::*                    LISTEN      4525/concourse
tcp6       0      0 :::46829                :::*                    LISTEN      4525/concourse
tcp6       0      0 :::2222                 :::*                    LISTEN      4525/concourse
tcp6       0      0 :::8080                 :::*                    LISTEN      4525/concourse
tcp6       0      0 :::22                   :::*                    LISTEN      1283/sshd
udp        0      0 0.0.0.0:68              0.0.0.0:*                           918/dhclient
udp        0      0 0.0.0.0:42165           0.0.0.0:*                           4530/concourse
</code></pre>

<p>You can check the logs like this:</p>

<pre><code class="bash">$ sudo journalctl -fu concourse-web
$ sudo journalctl -fu concourse-worker
</code></pre>

<p>Make a request using the API:</p>

<pre><code class="bash">$ curl http://${IP_ADDRESS}:8080/api/v1/info
{"version":"7.4.0","worker_version":"2.3","feature_flags":{"across_step":false,"build_rerun":false,"cache_streamed_volumes":false,"global_resources":false,"pipeline_instances":false,"redact_secrets":false,"resource_causality":false},"external_url":"http://x.x.x.x:8080"}
</code></pre>

<h2>Client Side:</h2>

<p>I will be using a the Fly cli from a Mac, so first we need to download the fly-cli for Mac:</p>

<pre><code class="bash">$ export CONCOURSE_VERSION=7.4.0
$ wget https://github.com/concourse/concourse/releases/download/v${CONCOURSE_VERSION}/fly-${CONCOURSE_VERSION}-darwin-amd64.tgz
$ tar -xvf fly-${CONCOURSE_VERSION}-darwin-amd64.tgz
$ sudo mv fly /usr/local/bin/fly
$ rm -rf fly-${CONCOURSE_VERSION}-darwin-amd64.tgz
</code></pre>

<p>Next, we need to setup our Concourse Target by Authenticating against our Concourse Endpoint, lets setup our target with the name <code>ci</code>, and make sure to replace the ip address with the ip of your concourse server:</p>

<pre><code class="bash">$ fly -t ci login -c http://${IP_ADDRESS}:8080
logging in to team 'main'

navigate to the following URL in your browser:

  http://${IP_ADDRESS}:8080/login?fly_port=42181

or enter token manually (input hidden):
target saved
</code></pre>

<p>Lets list our targets:</p>

<pre><code class="bash">$ fly targets
name  url                        team  expiry
ci    http://x.x.x.x:8080        main  Wed, 08 Nov 2021 15:32:59 UTC
</code></pre>

<p>Listing Registered Workers:</p>

<pre><code class="bash">$ fly -t ci workers
name              containers  platform  tags  team  state    version
x.x.x.x           0           linux     none  none  running  1.2
</code></pre>

<p>Listing Active Containers:</p>

<pre><code class="bash">$ fly -t ci containers
handle                                worker            pipeline     job            build #  build id  type   name                  attempt
</code></pre>

<h2>Hello World Pipeline:</h2>

<p>Let&rsquo;s create a basic pipeline, that will print out <code>Hello, World!</code>:</p>

<p>Our <code>hello-world.yml</code></p>

<pre><code class="yml">jobs:
- name: my-job
  plan:
  - task: say-hello
    config:
      platform: linux
      image_resource:
        type: docker-image
        source:
          repository: alpine
          tag: edge
      run:
        path: /bin/sh
        args:
        - -c
        - |
          echo "============="
          echo "Hello, World!"
          echo "============="
</code></pre>

<p>Applying the configuration to our pipeline:</p>

<pre><code class="bash">$ fly -t ci set-pipeline -p yeeehaa -c hello-world.yml
jobs:
  job my-job has been added:
    name: my-job
    plan:
    - task: say-hello
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: alpine
            tag: edge
        run:
          path: /bin/sh
          args:
          - -c
          - |
            echo "============="
            echo "Hello, World!"
            echo "============="

apply configuration? [yN]: y
pipeline created!
you can view your pipeline here: http://x.x.x.x:8080/teams/main/pipelines/yeeehaa

the pipeline is currently paused. to unpause, either:
  - run the unpause-pipeline command
  - click play next to the pipeline in the web ui
</code></pre>

<p>We can browse to the WebUI to unpause the pipeline, but since I like to do everything on cli as far as possible, I will unpause the pipeline via cli:</p>

<pre><code class="bash">$ fly -t ci unpause-pipeline -p yeeehaa
unpaused 'yeeehaa'
</code></pre>

<p>Now our Pipeline is unpaused, but since we did not specify any triggers, we need to manually trigger the pipeline to run, you can either via the WebUI, select your pipeline which in this case will be named <code>yeeehaa</code> and then select the job, which will be <code>my-job</code> then hit the <code>+</code> sign, which will trigger the pipeline.</p>

<p>I will be using the cli:</p>

<pre><code class="bash">$ fly -t ci trigger-job --job yeeehaa/my-job
started yeeehaa/my-job #1
</code></pre>

<p>Via the WebUI on <code>http://x.x.x.x:8080/teams/main/pipelines/yeeehaa/jobs/my-job/builds/1</code> you should see the <code>Hello, World!</code> output, or via the cli, we also have the option to see the output, so let&rsquo;s trigger it again, but this time passing the <code>--watch</code> flag:</p>

<pre><code class="bash">$ fly -t ci trigger-job --job yeeehaa/my-job --watch
started yeeehaa/my-job #2

initializing
running /bin/sh -c echo "============="
echo "Hello, World!"
echo "============="

=============
Hello, World!
=============
succeeded
</code></pre>

<p>Listing our Workers and Containers again:</p>

<pre><code class="bash">$ fly -t ci workers
name              containers  platform  tags  team  state    version
x.x.x.x            2           linux     none  none  running  1.2

$ fly -t ci containers
handle                                worker            pipeline     job         build #  build id  type   name           attempt
46282555-64cd-5h1b-67b8-316486h58eb8  x.x.x.x           yeeehaa      my-job      2        729       task   say-hello      n/a
</code></pre>

<h2>Thank You</h2>

<p>Thanks for reading, if you like my content, check out my <strong><a href="https://ruan.dev">website</a></strong> or follow me at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Install Concourse CI V6 on Ubuntu 20.04]]></title>
    <link href="https://blog.ruanbekker.com/blog/2021/04/06/install-concourse-ci-v6-on-ubuntu-20-dot-04/"/>
    <updated>2021-04-06T17:56:38-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2021/04/06/install-concourse-ci-v6-on-ubuntu-20-dot-04</id>
    <content type="html"><![CDATA[<p><img src="https://i.snag.gy/gzkdu9.jpg?nocache=1511644783495" alt="" /></p>

<p>Concourse is a Pipeline Based Continious Integration system written in Go</p>

<h2>Resources:</h2>

<ul>
<li><a href="https://concourse-ci.org/">https://concourse-ci.org/</a></li>
<li><a href="https://github.com/concourse/concourse">https://github.com/concourse/concourse</a></li>
<li><a href="https://github.com/starkandwayne/concourse-tutorial">https://github.com/starkandwayne/concourse-tutorial</a></li>
</ul>


<h2>Older Version</h2>

<p>An older version is available:</p>

<ul>
<li><a href="https://blog.ruanbekker.com/blog/2017/11/07/setup-a-concourse-ci-server-on-ubuntu-16/">Setup Concourse CI v4 on Ubuntu 16.04</a></li>
</ul>


<h2>What is Concourse CI:</h2>

<p>Concourse CI is a Continious Integration Platform. Concourse enables you to construct pipelines with a yaml configuration that can consist out of 3 core concepts, tasks, resources, and jobs that compose them. For more information about this have a look at their <a href="https://concourse.ci/concepts.html">docs</a></p>

<h2>What will we be doing today</h2>

<p>We will setup a Concourse CI Server v6.7.6 (web and worker) on Ubuntu 20.04 and run the traditional <code>Hello, World</code> pipeline</p>

<h2>Setup the Server:</h2>

<p>Concourse needs <code>PostgresSQL</code> server:</p>

<pre><code class="bash">$ apt update &amp;&amp; apt upgrade -y
$ apt install postgresql postgresql-contrib -y
$ systemctl enable postgresql
</code></pre>

<p>Create the Database and User for Concourse on Postgres:</p>

<pre><code class="bash">$ sudo -u postgres createuser concourse
$ sudo -u postgres createdb --owner=concourse atc
</code></pre>

<p>Download the Concourse and Fly Cli Binaries:</p>

<pre><code class="bash">$ wget https://github.com/concourse/concourse/releases/download/v6.7.6/concourse-6.7.6-linux-amd64.tgz
$ wget https://github.com/concourse/concourse/releases/download/v6.7.6/fly-6.7.6-linux-amd64.tgz
$ tar -xvf concourse-6.7.6-linux-amd64.tgz -C /usr/local/
$ tar -xvf fly-6.7.6-linux-amd64.tgz
$ mv fly /usr/bin/fly
$ rm -rf concourse-6.7.6-linux-amd64.tgz fly-6.7.6-linux-amd64.tgz
</code></pre>

<p>Create the Encryption Keys:</p>

<pre><code class="bash">$ mkdir /etc/concourse
$ ssh-keygen -t rsa -q -N '' -f /etc/concourse/tsa_host_key
$ ssh-keygen -t rsa -q -N '' -f /etc/concourse/worker_key
$ ssh-keygen -t rsa -q -N '' -f /etc/concourse/session_signing_key
$ cp /etc/concourse/worker_key.pub /etc/concourse/authorized_worker_keys
</code></pre>

<p>Concourse Web Process Configuration:</p>

<pre><code class="bash">$ cat /etc/concourse/web_environment

PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/concourse/bin
CONCOURSE_ADD_LOCAL_USER=ruan:pass
CONCOURSE_SESSION_SIGNING_KEY=/etc/concourse/session_signing_key
CONCOURSE_TSA_HOST_KEY=/etc/concourse/tsa_host_key
CONCOURSE_TSA_AUTHORIZED_KEYS=/etc/concourse/authorized_worker_keys
CONCOURSE_POSTGRES_HOST=127.0.0.1
CONCOURSE_POSTGRES_USER=concourse
CONCOURSE_POSTGRES_PASSWORD=concourse
CONCOURSE_POSTGRES_DATABASE=atc
CONCOURSE_MAIN_TEAM_LOCAL_USER=ruan
CONCOURSE_EXTERNAL_URL=http://10.20.30.40:8080 # replace this with your ip address
</code></pre>

<p>Concourse Worker Process Configuration:</p>

<pre><code class="bash">$ cat /etc/concourse/worker_environment

PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/concourse/bin
CONCOURSE_WORK_DIR=/var/lib/concourse
CONCOURSE_TSA_HOST=127.0.0.1:2222
CONCOURSE_TSA_PUBLIC_KEY=/etc/concourse/tsa_host_key.pub
CONCOURSE_TSA_WORKER_PRIVATE_KEY=/etc/concourse/worker_key
CONCOURSE_GARDEN_DNS_SERVER=8.8.8.8
</code></pre>

<p>Create a Concourse user:</p>

<pre><code class="bash">$ mkdir /var/lib/concourse
$ sudo adduser --system --group concourse
$ sudo chown -R concourse:concourse /etc/concourse /var/lib/concourse
$ sudo chmod 600 /etc/concourse/*_environment
</code></pre>

<p>Create SystemD Unit Files, first for the Web Service:</p>

<pre><code class="bash">$ cat /etc/systemd/system/concourse-web.service

[Unit]
Description=Concourse CI web process (ATC and TSA)
After=postgresql.service

[Service]
User=concourse
Restart=on-failure
EnvironmentFile=/etc/concourse/web_environment
ExecStart=/usr/bin/concourse web

[Install]
WantedBy=multi-user.target
</code></pre>

<p>Then the SystemD Unit File for the Worker Service:</p>

<pre><code class="bash">$ cat /etc/systemd/system/concourse-worker.service

[Unit]
Description=Concourse CI worker process
After=concourse-web.service

[Service]
User=root
Restart=on-failure
EnvironmentFile=/etc/concourse/worker_environment
ExecStart=/usr/bin/concourse worker

[Install]
WantedBy=multi-user.target
</code></pre>

<p>Create a postgres password for the concourse user:</p>

<pre><code class="bash">$ cd /home/concourse/
$ sudo -u concourse psql atc
atc=&gt; ALTER USER concourse WITH PASSWORD 'concourse';
atc=&gt; \q
</code></pre>

<p>Start and Enable the Services:</p>

<pre><code class="bash">$ systemctl start concourse-web concourse-worker
$ systemctl enable concourse-web concourse-worker postgresql
$ systemctl status concourse-web concourse-worker

$ systemctl is-active concourse-worker concourse-web
active
active
</code></pre>

<p>The listening ports should more or less look like the following:</p>

<pre><code class="bash">$ netstat -tulpn

Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 127.0.0.1:7777          0.0.0.0:*               LISTEN      4530/concourse
tcp        0      0 127.0.0.1:7788          0.0.0.0:*               LISTEN      4530/concourse
tcp        0      0 127.0.0.1:8079          0.0.0.0:*               LISTEN      4525/concourse
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1283/sshd
tcp        0      0 127.0.0.1:5432          0.0.0.0:*               LISTEN      4047/postgres
tcp6       0      0 :::36159                :::*                    LISTEN      4525/concourse
tcp6       0      0 :::46829                :::*                    LISTEN      4525/concourse
tcp6       0      0 :::2222                 :::*                    LISTEN      4525/concourse
tcp6       0      0 :::8080                 :::*                    LISTEN      4525/concourse
tcp6       0      0 :::22                   :::*                    LISTEN      1283/sshd
udp        0      0 0.0.0.0:68              0.0.0.0:*                           918/dhclient
udp        0      0 0.0.0.0:42165           0.0.0.0:*                           4530/concourse
</code></pre>

<h2>Client Side:</h2>

<p>I will be using a the Fly cli from a Mac, so first we need to download the fly-cli for Mac:</p>

<pre><code class="bash">$ wget https://github.com/concourse/concourse/releases/download/v6.7.6/fly-6.7.6-darwin-amd64.tgz
$ tar -xvf fly-6.7.6-darwin-amd64.tgz
$ sudo mv fly /usr/local/bin/fly
$ rm -rf fly-6.7.6-darwin-amd64.tgz
</code></pre>

<p>Next, we need to setup our Concourse Target by Authenticating against our Concourse Endpoint, lets setup our target with the name <code>ci</code>, and make sure to replace the ip address with the ip of your concourse server:</p>

<pre><code class="bash">$ fly -t ci login -c http://10.20.30.40:8080
logging in to team 'main'

navigate to the following URL in your browser:

  http://10.20.30.40:8080/login?fly_port=42181

or enter token manually (input hidden):
target saved
</code></pre>

<p>Lets list our targets:</p>

<pre><code class="bash">$ fly targets
name  url                        team  expiry
ci    http://10.20.30.40:8080    main  Wed, 08 Nov 2021 15:32:59 UTC
</code></pre>

<p>Listing Registered Workers:</p>

<pre><code class="bash">$ fly -t ci workers
name              containers  platform  tags  team  state    version
10.20.30.40       0           linux     none  none  running  1.2
</code></pre>

<p>Listing Active Containers:</p>

<pre><code class="bash">$ fly -t ci containers
handle                                worker            pipeline     job            build #  build id  type   name                  attempt
</code></pre>

<h2>Hello World Pipeline:</h2>

<p>Let&rsquo;s create a basic pipeline, that will print out <code>Hello, World!</code>:</p>

<p>Our <code>hello-world.yml</code></p>

<pre><code class="yml">jobs:
- name: my-job
  plan:
  - task: say-hello
    config:
      platform: linux
      image_resource:
        type: docker-image
        source:
          repository: alpine
          tag: edge
      run:
        path: /bin/sh
        args:
        - -c
        - |
          echo "============="
          echo "Hello, World!"
          echo "============="
</code></pre>

<p>Applying the configuration to our pipeline:</p>

<pre><code class="bash">$ fly -t ci set-pipeline -p yeeehaa -c hello-world.yml
jobs:
  job my-job has been added:
    name: my-job
    plan:
    - task: say-hello
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: alpine
            tag: edge
        run:
          path: /bin/sh
          args:
          - -c
          - |
            echo "============="
            echo "Hello, World!"
            echo "============="

apply configuration? [yN]: y
pipeline created!
you can view your pipeline here: http://10.20.30.40:8080/teams/main/pipelines/yeeehaa

the pipeline is currently paused. to unpause, either:
  - run the unpause-pipeline command
  - click play next to the pipeline in the web ui
</code></pre>

<p>We can browse to the WebUI to unpause the pipeline, but since I like to do everything on cli as far as possible, I will unpause the pipeline via cli:</p>

<pre><code class="bash">$ fly -t ci unpause-pipeline -p yeeehaa
unpaused 'yeeehaa'
</code></pre>

<p>Now our Pipeline is unpaused, but since we did not specify any triggers, we need to manually trigger the pipeline to run, you can either via the WebUI, select your pipeline which in this case will be named <code>yeeehaa</code> and then select the job, which will be <code>my-job</code> then hit the <code>+</code> sign, which will trigger the pipeline.</p>

<p>I will be using the cli:</p>

<pre><code class="bash">$ fly -t ci trigger-job --job yeeehaa/my-job
started yeeehaa/my-job #1
</code></pre>

<p>Via the WebUI on <code>http://10.20.30.40:8080/teams/main/pipelines/yeeehaa/jobs/my-job/builds/1</code> you should see the <code>Hello, World!</code> output, or via the cli, we also have the option to see the output, so let&rsquo;s trigger it again, but this time passing the <code>--watch</code> flag:</p>

<pre><code class="bash">$ fly -t ci trigger-job --job yeeehaa/my-job --watch
started yeeehaa/my-job #2

initializing
running /bin/sh -c echo "============="
echo "Hello, World!"
echo "============="

=============
Hello, World!
=============
succeeded
</code></pre>

<p>Listing our Workers and Containers again:</p>

<pre><code class="bash">$ fly -t ci workers
name              containers  platform  tags  team  state    version
10.20.30.40       2           linux     none  none  running  1.2

$ fly -t ci containers
handle                                worker            pipeline     job         build #  build id  type   name           attempt
36982955-54fd-4c1b-57b8-216486c58db8  10.20.30.40       yeeehaa      my-job      2        729       task   say-hello      n/a
</code></pre>
]]></content>
  </entry>
  
</feed>
