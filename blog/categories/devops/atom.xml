<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Devops | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/devops/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2022-08-06T03:19:36-04:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Remote Builds With Docker Contexts]]></title>
    <link href="https://blog.ruanbekker.com/blog/2022/07/14/remote-builds-with-docker-contexts/"/>
    <updated>2022-07-14T01:57:34-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2022/07/14/remote-builds-with-docker-contexts</id>
    <content type="html"><![CDATA[<p><img src="https://blog.ruanbekker.com/images/ruanbekker-docker-contexts.png" alt="using-docker-contexts" /></p>

<p>Often you want to save some battery life when you are doing docker builds and leverage a remote host to do the intensive work and we can utilise docker context over ssh to do just that.</p>

<h2>About</h2>

<p>In this tutorial I will show you how to use a remote docker engine to do docker builds, so you still run the docker client locally, but the context of your build will be sent to a remote docker engine via ssh.</p>

<p>We will setup password-less ssh, configure our ssh config, create the remote docker context, then use the remote docker context.</p>

<p><img src="https://user-images.githubusercontent.com/567298/178909518-26f580e9-2b96-41b3-bacd-a5ea5f848ebf.png" alt="image" /></p>

<h2>Password-less SSH</h2>

<p>I will be copying my public key to the remote host:</p>

<pre><code class="bash">$ ssh-copy-id ruan@192.168.2.18
</code></pre>

<p>Setup my ssh config:</p>

<pre><code class="bash">$ cat ~/.ssh/config
Host home-server
    Hostname 192.168.2.18
    User ruan
    IdentityFile ~/.ssh/id_rsa
    StrictHostKeyChecking no
    UserKnownHostsFile /dev/null
</code></pre>

<p>Test:</p>

<pre><code>$ ssh home-server whoami
ruan
</code></pre>

<h2>Docker Context</h2>

<p>On the target host (192.168.2.18) we can verify that docker is installed:</p>

<pre><code class="bash">$ docker version
Client: Docker Engine - Community
 Version:           20.10.12
 API version:       1.41
 Go version:        go1.16.12
 Git commit:        e91ed57
 Built:             Mon Dec 13 11:45:37 2021
 OS/Arch:           linux/amd64
 Context:           default
 Experimental:      true

Server: Docker Engine - Community
 Engine:
  Version:          20.10.12
  API version:      1.41 (minimum version 1.12)
  Go version:       go1.16.12
  Git commit:       459d0df
  Built:            Mon Dec 13 11:43:46 2021
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.4.12
  GitCommit:        7b11cfaabd73bb80907dd23182b9347b4245eb5d
 runc:
  Version:          1.0.2
  GitCommit:        v1.0.2-0-g52b36a2
 docker-init:
  Version:          0.19.0
  GitCommit:        de40ad0
</code></pre>

<p>On the client (my laptop in this example), we will create a docker context called &ldquo;home-server&rdquo; and point it to our target host:</p>

<pre><code class="bash">$ docker context create home-server --docker "host=ssh://home-server"
home-server
Successfully created context "home-server"
</code></pre>

<p>Now we can list our contexts:</p>

<pre><code class="bash">docker context ls
NAME                TYPE                DESCRIPTION                               DOCKER ENDPOINT               KUBERNETES ENDPOINT                                  ORCHESTRATOR
default *           moby                Current DOCKER_HOST based configuration   unix:///var/run/docker.sock   https://k3d-master.127.0.0.1.nip.io:6445 (default)   swarm
home-server         moby                                                          ssh://home-server
</code></pre>

<h2>Using Contexts</h2>

<p>We can verify if this works by listing our cached docker images locally and on our remote host:</p>

<pre><code class="bash">$ docker --context=default images | wc -l
 16
</code></pre>

<p>And listing the remote images by specifying the context:</p>

<pre><code class="bash">$ docker --context=home-server images | wc -l
 70
</code></pre>

<p>We can set the default context to our target host:</p>

<pre><code>$ docker context use home-server
home-server
</code></pre>

<h2>Running Containers over Contexts</h2>

<p>So running containers with remote contexts essentially becomes running containers on remote hosts. In the past, I had to setup a ssh tunnel, point the docker host env var to that endpoint, then run containers on the remote host.</p>

<p>Thats something of the past, we can just point our docker context to our remote host and run the container. If you haven&rsquo;t set the default context, you can specify the context, so running a docker container on a remote host with your docker client locally:</p>

<pre><code class="bash">$ docker --context=home-server run -it -p 8002:8080 ruanbekker/hostname
2022/07/14 05:44:04 Server listening on port 8080
</code></pre>

<p>Now from our client (laptop), we can test our container on our remote host:</p>

<pre><code class="bash">$ curl http://192.168.2.18:8002
Hostname: 8605d292e2b4
</code></pre>

<p>The same way can be used to do remote docker builds, you have your Dockerfile locally, but when you build, you point the context to the remote host, and your context (dockerfile and files referenced in your dockerfile) will be sent to the remote host. This way you can save a lot of battery life as the computation is done on the remote docker engine.</p>

<h2>Thank You</h2>

<p>Thanks for reading, feel free to check out my <a href="https://ruan.dev/">website</a>, feel free to subscribe to my <a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a> or follow me at <a href="https://twitter.com/ruanbekker">@ruanbekker</a> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Prometheus Relabel Config Examples]]></title>
    <link href="https://blog.ruanbekker.com/blog/2022/05/30/prometheus-relabel-config-examples/"/>
    <updated>2022-05-30T03:01:01-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2022/05/30/prometheus-relabel-config-examples</id>
    <content type="html"><![CDATA[<p>This is a quick demonstration on how to use prometheus relabel configs, when you have scenarios for when example, you want to use a part of your hostname and assign it to a prometheus label.</p>

<h2>Prometheus Relabling</h2>

<p>Using a standard prometheus config to scrape two targets:
- <code>ip-192-168-64-29.multipass:9100</code>
- <code>ip-192-168-64-30.multipass:9100</code></p>

<pre><code>global:
  scrape_interval:     15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'local'

scrape_configs:
  - job_name: 'prometheus'
    scrape_interval: 15s
    static_configs:
    - targets: ['localhost:9090']

  - job_name: 'multipass-nodes'
    static_configs:
    - targets: ['ip-192-168-64-29.multipass:9100']
      labels:
        test: 1
    - targets: ['ip-192-168-64-30.multipass:9100']
      labels:
        test: 1
</code></pre>

<p>The Result:</p>

<p><img width="924" alt="image" src="https://user-images.githubusercontent.com/567298/170823370-f2c6b3a3-68a8-4f5a-ad43-2f1b832c95e0.png"></p>

<p>When we want to relabel one of the source the prometheus <a href="https://grafana.com/blog/2022/03/21/how-relabeling-in-prometheus-works/#internal-labels">internal labels</a>, <code>__address__</code> which will be the given target including the port, then we apply regex: <code>(.*)</code> to catch everything from the source label, and since there is only one group we use the <code>replacement</code> as <code>${1}-randomtext</code> and use that value to apply it as the value of the given <code>target_label</code> which in this case is for <code>randomlabel</code>, which will be in this case:</p>

<pre><code>global:
  scrape_interval:     15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'local'

scrape_configs:
  - job_name: 'prometheus'
    scrape_interval: 15s
    static_configs:
    - targets: ['localhost:9090']

  - job_name: 'multipass-nodes'
    static_configs:
    - targets: ['ip-192-168-64-29.multipass:9100']
      labels:
        test: 3
    - targets: ['ip-192-168-64-30.multipass:9100']
      labels:
        test: 3
    relabel_configs:
    - source_labels: [__address__]
      regex: '(.+)'
      replacement: '${1}-randomtext'
      target_label: randomlabel
</code></pre>

<p>The Result:</p>

<p><img width="1107" alt="image" src="https://user-images.githubusercontent.com/567298/170824588-44a79c3d-5131-4311-bcca-f5137d6acdad.png"></p>

<p>In this case we want to relabel the <code>__address__</code> and apply the value to the <code>instance</code> label, but we want to exclude the <code>:9100</code> from the <code>__address__</code> label:</p>

<pre><code># Config: https://github.com/prometheus/prometheus/blob/release-2.36/config/testdata/conf.good.yml
global:
  scrape_interval:     15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'local'

scrape_configs:
  - job_name: 'prometheus'
    scrape_interval: 15s
    static_configs:
    - targets: ['localhost:9090']

  - job_name: 'multipass-nodes'
    static_configs:
    - targets: ['ip-192-168-64-29.multipass:9100']
      labels:
        test: 4
    - targets: ['ip-192-168-64-30.multipass:9100']
      labels:
        test: 4
    relabel_configs:
    - source_labels: [__address__]
      separator: ':'
      regex: '(.*):(.*)'
      replacement: '${1}'
      target_label: instance
</code></pre>

<p>The Result:</p>

<p><img width="950" alt="image" src="https://user-images.githubusercontent.com/567298/170824806-45f0f243-5fe7-4635-9e9a-335616a322da.png"></p>

<h2>AWS EC2 SD Configs</h2>

<p>On AWS EC2 you can make use of the <a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config">ec2_sd_config</a> where you can make use of EC2 Tags, to set the values of your tags to prometheus label values.</p>

<p>In this scenario, on my EC2 instances I have 3 tags:
- Key: PrometheusScrape, Value: Enabled
- Key: Name, Value: pdn-server-1
- Key: Environment, Value: dev</p>

<p>In our config, we only apply a node-exporter scrape config to instances which are tagged <code>PrometheusScrape=Enabled</code>, then we use the <code>Name</code> tag, and assign it&rsquo;s value to the <code>instance</code> tag, and the similarly we assign the <code>Environment</code> tag value to the <code>environment</code> promtheus label value.</p>

<p>Because this prometheus instance resides in the same VPC, I am using the <code>__meta_ec2_private_ip</code> which is the private ip address of the EC2 instance to assign it to the address where it needs to scrape the node exporter metrics endpoint:</p>

<pre><code class="yaml">scrape_configs:
  - job_name: node-exporter
    scrape_interval: 15s
    ec2_sd_configs:
    - region: eu-west-1
      port: 9100
      filters:
        - name: tag:PrometheusScrape
          values:
            - Enabled
    relabel_configs:
    - source_labels: [__meta_ec2_private_ip]
      replacement: '${1}:9100'
      target_label: __address__
    - source_labels: [__meta_ec2_tag_Name]
      target_label: instance
    - source_labels: [__meta_ec2_tag_Environment]
      target_label: environment
</code></pre>

<p>You will need a EC2 Ready Only instance role (or access keys on the configuration) in order for prometheus to read the EC2 tags on your account.</p>

<p>See their <a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config">documentation</a> for more info.</p>

<h2>Stack</h2>

<p>The docker-compose used:</p>

<pre><code class="yaml">version: '3.8'

services:
  prometheus:
    image: prom/prometheus
    container_name: 'prometheus'
    user: root
    restart: unless-stopped
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention=14d'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.external-url=http://prometheus.127.0.0.1.nip.io'
    ports:
      - 9090:9090
    networks:
      - public
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

networks:
  public:
    name: public

volumes:
  prometheus-data: {}
</code></pre>

<h2>References</h2>

<p>Usful docs:</p>

<ul>
<li><a href="https://grafana.com/blog/2022/03/21/how-relabeling-in-prometheus-works/#internal-labels">https://grafana.com/blog/2022/03/21/how-relabeling-in-prometheus-works/#internal-labels</a></li>
<li><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config">https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config</a></li>
<li><a href="https://regexr.com/">https://regexr.com/</a></li>
</ul>


<h2>Thank You</h2>

<p>Thanks for reading, if you like my content, check out my <strong><a href="https://ruan.dev">website</a></strong>, read my <strong><a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a></strong> or follow me at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Create a AWS Lambda Layer With Docker]]></title>
    <link href="https://blog.ruanbekker.com/blog/2022/05/27/create-a-aws-lambda-layer-with-docker/"/>
    <updated>2022-05-27T06:19:05-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2022/05/27/create-a-aws-lambda-layer-with-docker</id>
    <content type="html"><![CDATA[<p>In this tutorial we will be creating a AWS Lambda Python <a href="https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html">Layer</a> that will include the Python Requests package and we will compile the package with Docker and the LambCI image.</p>

<h2>Getting Started</h2>

<p>First we will create the directory where we will store the intermediate data:</p>

<pre><code class="bash">$ mkdir lambda-layers
$ cd lambda-layers
</code></pre>

<p>Then we will create the directory structure, as you can see I will be using the python 3.8 runtime:</p>

<pre><code class="bash">$ mkdir -p requests/python/lib/python3.8
$ cd requests
</code></pre>

<p>Write the dependencies to the requirements file:</p>

<pre><code class="bash">$ echo "requests" &gt; requirements.txt
</code></pre>

<p>Install dependencies locally using docker, where we will be using the <code>lambci/lambda:build-python3.8</code> iamge and we are mounting our current working directory to <code>/var/task</code> inside the container, and then we will be running the command <code>pip install -r requirements.txt -t python/lib/python3.7/site-packages/; exit</code> inside the container, which will essentially dump the content to our working directory:</p>

<pre><code class="bash">$ docker run -v $PWD:/var/task \
   lambci/lambda:build-python3.8 \
   sh -c "pip install -r requirements.txt -t python/lib/python3.8/site-packages/; exit"
</code></pre>

<p>Zip up the deployment package that we will push to AWS Lambda Layers:</p>

<pre><code class="bash">$ zip -r package.zip python &gt; /dev/null
</code></pre>

<p>Publish the layer using the aws cli tools, by specifying the deployment package, the compatible runtime and a identifier:</p>

<pre><code class="bash">$ aws --profile dev lambda \
   publish-layer-version --layer-name python-requests \
   --description "Python Requests using 3.8 Runtime" \
   --zip-file fileb://package.zip \
   --compatible-runtime "python3.8"
</code></pre>

<p>Then when you want to reference the layer on the functio that you want to create, you can do it like this:</p>

<pre><code class="bash">$ aws lambda create-function --function-name test-requests \
   --runtime python3.8 \
   --handler lambda_function.lambda_handler \
   --role "" --layers "arn:aws:lambda:eu-west-1:xxxxxxxxxxxx:layer:test-requests" \
   --code "S3Bucket=string,S3Key=string"
</code></pre>

<h2>Thank You</h2>

<p>Thanks for reading, if you like my content, check out my <strong><a href="https://ruan.dev">website</a></strong>, read my <strong><a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a></strong> or follow me at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong> on Twitter.</p>

<p>Credit to <a href="https://oznetnerd.com/2020/11/11/lambda-packaging-the-right-way/">oznetnerd.com</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Publish and Use Your Ansible Role From Git]]></title>
    <link href="https://blog.ruanbekker.com/blog/2022/04/19/publish-and-use-your-ansible-role-from-git/"/>
    <updated>2022-04-19T04:35:09-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2022/04/19/publish-and-use-your-ansible-role-from-git</id>
    <content type="html"><![CDATA[<p>In this tutorial we will be creating a ansible role, publish our ansible role to github, then we will install the role locally and create a ansible playbook to use the ansible role.</p>

<p>The source code for this blog post will be available on my <a href="https://blog.ruanbekker.com/blog/2022/04/19/publish-and-use-your-ansible-role-from-git/">github</a> repository.</p>

<h2>Ansible Installation</h2>

<p>Create a virtual environment with Python:</p>

<pre><code>$ virtualenv .venv -p python3
$ source .venv/bin/activate
</code></pre>

<p>Install ansible with pip:</p>

<pre><code>$ pip install ansible==4.4.0
</code></pre>

<p>Now that we have ansible installed, we can create our role.</p>

<h2>Initialize Ansible Role</h2>

<p>A Ansible Role consists of a couple of files, and using <code>ansible-galaxy</code> makes it easy initializing a boilerplate structure to begin with::</p>

<pre><code>$ ansible-galaxy init --init-path roles ssh_config
- Role ssh_config was created successfully
</code></pre>

<p>The role that we created is named <code>ssh_config</code> and will be placed under the directory <code>roles</code> under our current working directory.</p>

<h2>Define Role Tasks</h2>

<p>Create the dummy task under <code>roles/ssh_config/tasks/main.yml</code>:</p>

<script src="https://gist.github.com/ruanbekker/4971be45476915ba877bb444a9ff1c0b.js"></script>


<p>Then define the defaults environment values in the file <code>roles/ssh_config/defaults/main.yml</code>:</p>

<pre><code class="yaml">---
# defaults file for ssh_config
ssh_port: 22
</code></pre>

<p>The value of <code>ssh_port</code> will default to <code>22</code> if we don&rsquo;t define it in our variables.</p>

<h2>Commit to Git</h2>

<p>The assumption is made here that you already created a git repository and that your access is sorted. Add the files and commit it to git:</p>

<pre><code>$ git add .
$ git commit -m "Your message"
$ git push origin main
</code></pre>

<p>Now your ansible role should be commited and visible in git.</p>

<h2>SSH Config Client Side</h2>

<p>I will be referencing the git source url via SSH, and since I am using my default ssh key, the ssh config isn&rsquo;t really needed, but if you are using a different version control system, with different ports or different ssh keys, the following ssh config snippet may be useful:</p>

<pre><code>$ cat ~/.ssh/config
Host github.com
    User git
    Port 22
    IdentityFile ~/.ssh/id_rsa
</code></pre>

<p>If you won&rsquo;t be using SSH as the source url in your ansible setup for your role, you can skip the SSH setup.</p>

<h2>Installing the Ansible Role from Git</h2>

<p>When installing roles, ansible installs them by default under: <code>~/.ansible/roles</code>, <code>/usr/share/ansible/roles</code> or <code>/etc/ansible/roles</code>.</p>

<p>From our previous steps, we still have the ansible role content locally (not under the default installed directory), so by saying installing the role kinda sounds like we are doing double the work. But the intention is that you have your ansible role centralized and versioned on git, and on new servers or workstations where you want to consume the role from, that specific role, won&rsquo;t be present on that source.</p>

<p>To install the role from Git, we need to populate the <code>requirements.yml</code> file:</p>

<pre><code>$ mkdir ~/my-project
$ cd ~/my-project
</code></pre>

<p>The requirements file is used to define where our role is located, which version and the type of version control, the <code>requirements.yml</code>:</p>

<pre><code class="yaml">---
roles:
  - name: ssh_config
    src: ssh://git@github.com/ruanbekker/ansible-demo-role.git
    version: main
    scm: git
</code></pre>

<p>For other variations of using the requirements file, you can have a look at their <a href="https://galaxy.ansible.com/docs/using/installing.html#installing-multiple-roles-from-a-file">documentation</a></p>

<p>Then install the ansible role from our requirements file (I have used <code>--force</code> to overwrite my current one while testing):</p>

<pre><code>$ ansible-galaxy install -r requirements.yml --force
Starting galaxy role install process
- changing role ssh_config from main to main
- extracting ssh_config to /Users/ruan/.ansible/roles/ssh_config
- ssh_config (main) was installed successfully
</code></pre>

<h2>Ansible Playbook</h2>

<p>Define the ansible playbook to use the role that we installed from git, in a file called <code>playbook.yml</code>:</p>

<pre><code class="yaml">---
- hosts: localhost
  roles:
    - ssh_config
  vars:
    ssh_port: 2202
</code></pre>

<p>Run the ansible playbook:</p>

<pre><code>$ ansible-playbook playbook.yml
PLAY [localhost] *********************************************************************************************

TASK [Gathering Facts] ***************************************************************************************
ok: [localhost]

TASK [ssh_config : Dummy task] *******************************************************************************
ok: [localhost] =&gt; {
    "msg": "This is a dummy task changing ssh port to 2202."
}

PLAY RECAP ***************************************************************************************************
localhost                  : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
</code></pre>

<h2>Thank You</h2>

<p>Thanks for reading, if you like my content, check out my <strong><a href="https://ruan.dev">website</a></strong>, read my <strong><a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a></strong> or follow me at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Provision a AWS EC2 Instance With Terraform]]></title>
    <link href="https://blog.ruanbekker.com/blog/2022/04/16/provision-a-aws-ec2-instance-with-terraform/"/>
    <updated>2022-04-16T19:04:08-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2022/04/16/provision-a-aws-ec2-instance-with-terraform</id>
    <content type="html"><![CDATA[<p>In this tutorial I will demonstrate how to use Terraform (a Infrastructure as Code Tool), to provision a AWS EC2 Instance and the source code that we will be using in this tutorial will be published to my <a href="https://github.com/ruanbekker/terraformfiles/tree/master/aws-ec2-instance">terraformfiles github repository</a>.</p>

<h2>Requirements</h2>

<p>To follow along this tutorial, you will need an AWS Account and Terraform installed</p>

<h2>Terraform</h2>

<p>To install Terraform for your operating system, you can follow <a href="https://learn.hashicorp.com/tutorials/terraform/install-cli">Terraform Installation Documentation</a>, I am using Mac OSx, so for me it will be:</p>

<pre><code class="bash">brew tap hashicorp/tap
brew install hashicorp/tap/terraform
</code></pre>

<p>To verify the installation, we can run <code>terraform version</code> and my output shows:</p>

<pre><code>Terraform v1.1.8
on darwin_amd64
</code></pre>

<h2>Terraform Project Structure</h2>

<p>Create the directory:</p>

<pre><code class="bash">mkdir terraform-aws-ec2
cd terraform-aws-ec2
</code></pre>

<p>Create the following files: <code>main.tf</code>, <code>providers.tf</code>, <code>variables.tf</code>, <code>outputs.tf</code>, <code>locals.tf</code> and <code>terraform.tfvars</code>:</p>

<pre><code class="bash">touch main.tf providers.tf variables.tf outputs.tf locals.tf terraform.tfvars
</code></pre>

<h2>Define Terraform Configuration Code</h2>

<p>First we need to define the aws provider, which we will do in <code>providers.tf</code>:</p>

<pre><code>terraform {
  required_providers {
    aws = {
      version = "~&gt; 3.27"
      source = "hashicorp/aws"
    }
  }
}

provider "aws" {
  region  = "eu-west-1"
  profile = "default"
  shared_credentials_file = "~/.aws/credentials"
}
</code></pre>

<p>You will notice that I am defining my profile name <code>default</code> from the <code>~/.aws/credentials</code> credential provider in order for terraform to authenticate with AWS.</p>

<p>Next I am defining the <code>main.tf</code> which will be the file where we define our aws resources:</p>

<pre><code>data "aws_ami" "latest_ubuntu" {
  most_recent = true
  owners = ["099720109477"]

  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-focal-20.04-*-server-*"]
  }

  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }

  filter {
    name   = "root-device-type"
    values = ["ebs"]
  }

  filter {
    name   = "architecture"
    values = ["x86_64"]
  }

}

data "aws_iam_policy_document" "assume_role_policy" {
  statement {
    actions = ["sts:AssumeRole"]
    principals {
      type        = "Service"
      identifiers = ["ec2.amazonaws.com"]
    }
  }
}

data "aws_iam_policy" "ec2_read_only_access" {
  arn = "arn:aws:iam::aws:policy/AmazonEC2ReadOnlyAccess"
}

resource "aws_iam_role" "ec2_access_role" {
  name               = "${local.project_name}-ec2-role"
  assume_role_policy = data.aws_iam_policy_document.assume_role_policy.json
}

resource "aws_iam_policy_attachment" "readonly_role_policy_attach" {
  name       = "${local.project_name}-ec2-role-attachment"
  roles      = [aws_iam_role.ec2_access_role.name]
  policy_arn = data.aws_iam_policy.ec2_read_only_access.arn
}

resource "aws_iam_instance_profile" "instance_profile" {
  name  = "${local.project_name}-ec2-instance-profile"
  role = aws_iam_role.ec2_access_role.name
}

resource "aws_security_group" "ec2" {
    name        = "${local.project_name}-ec2-sg"
    description = "${local.project_name}-ec2-sg"
    vpc_id      = var.vpc_id

    tags = merge(
      var.default_tags,
      {
       Name = "${local.project_name}-ec2-sg"
      },
    )
}

resource "aws_security_group_rule" "ssh" {
    description       = "allows public ssh access to ec2"
    security_group_id = aws_security_group.ec2.id
    type              = "ingress"
    protocol          = "tcp"
    from_port         = 22
    to_port           = 22
    cidr_blocks       = ["0.0.0.0/0"]
}

resource "aws_security_group_rule" "egress" {
    description       = "allows egress"
    security_group_id = aws_security_group.ec2.id
    type              = "egress"
    protocol          = "-1"
    from_port         = 0
    to_port           = 0
    cidr_blocks       = ["0.0.0.0/0"]
}

resource "aws_instance" "ec2" {
  ami                         = data.aws_ami.latest_ubuntu.id
  instance_type               = var.instance_type
  subnet_id                   = var.subnet_id
  key_name                    = var.ssh_keyname
  vpc_security_group_ids      = [aws_security_group.ec2.id]
  associate_public_ip_address = true
  monitoring                  = true
  iam_instance_profile        = aws_iam_instance_profile.instance_profile.name

  lifecycle {
    ignore_changes            = [subnet_id, ami]
  }

  root_block_device {
      volume_type           = "gp2"
      volume_size           = var.ebs_root_size_in_gb
      encrypted             = false
      delete_on_termination = true
  }

  tags = merge(
    var.default_tags,
    {
     Name = "${local.project_name}"
    },
  )

}
</code></pre>

<p>A couple of things are defined here:</p>

<ul>
<li>A data resource to fetch the latest Ubuntu 20.04 AMI</li>
<li>The IAM Role and Policy that we will use to associate to our EC2 Instance Profile</li>
<li>The EC2 Security Group</li>
<li>The EC2 Instance</li>
<li>The VPC ID and Subnet ID are required variables which we will set in <code>terraform.tfvars</code></li>
</ul>


<p>The next file will be our <code>variables.tf</code> file where we will define all our variable definitions:</p>

<pre><code>variable "default_tags" {
  default = {
    Environment = "test"
    Owner       = "ruan.bekker"
    Project     = "terraform-blogpost"
    CostCenter  = "engineering"
    ManagedBy   = "terraform"
  }
}

variable "aws_region" {
  type        = string
  default     = "eu-west-1"
  description = "the region to use in aws"
}

variable "vpc_id" {
  type        = string
  description = "the vpc to use"
}

variable "ssh_keyname" {
  type        = string
  description = "ssh key to use"
}

variable "subnet_id" {
  type        = string
  description = "the subnet id where the ec2 instance needs to be placed in"
}

variable "instance_type" {
  type        = string
  default     = "t3.nano"
  description = "the instance type to use"
}

variable "project_id" {
  type        = string
  default     = "terraform-blogpost"
  description = "the project name"
}

variable "ebs_root_size_in_gb" {
  type        = number
  default     = 10
  description = "the size in GB for the root disk"
}

variable "environment_name" {
   type    = string
   default = "dev"
   description = "the environment this resource will go to (assumption being made theres one account)"
}
</code></pre>

<p>The next file is our <code>locals.tf</code> which just concatenates our project id and environment name:</p>

<pre><code>locals {
  project_name = "${var.project_id}-${var.environment_name}"
}
</code></pre>

<p>Then our <code>outputs.tf</code> for the values that terraform should output:</p>

<pre><code>output "id" {
  description = "The ec2 instance id"
  value       = aws_instance.ec2.id
  sensitive   = false
}

output "ip" {
  description = "The ec2 instance public ip address"
  value       = aws_instance.ec2.public_ip
  sensitive   = false
}

output "subnet_id" {
  description = "the subnet id which will be used"
  value       = var.subnet_id
  sensitive   = false
}
</code></pre>

<p>Then lastly our <code>terraform.tfvars</code>, which you will need to supply your own values to match your AWS Account:</p>

<pre><code># required
vpc_id = "vpc-063d7xxxxxxxxxxxx"
ssh_keyname = "ireland-key"
subnet_id = "subnet-04b3xxxxxxxxxxxxx"
</code></pre>

<h2>Deploy EC2 Instance</h2>

<p>Now that all our configuration is in place, we need to intialize terraform by downloading the providers:</p>

<pre><code class="bash">terraform init
</code></pre>

<p>Once the terraform init has completed, we can run a <code>terraform plan</code> which will show us what terraform will do. Since the <code>terraform.tfvars</code> are the default file for variables, we don&rsquo;t have to specify the name of the file, but since I want to be excplicit, I will include it (should you want to change the file name):</p>

<pre><code class="bash">terraform plan -var-file="terraform.tfvars"
</code></pre>

<p>Now it&rsquo;s a good time to review what terraform wants to action by viewing the plan output, once you are happy you can deploy the changes by running a <code>terraform apply</code>:</p>

<pre><code class="bash">terraform apply -var-file="terraform.tfvars"
</code></pre>

<p>Optional: You can override variables by either updating the <code>terraform.tfvars</code> or you can append them with terraform apply <code>-var-file="terraform.tfvars" -var="ssh_key=default_key"</code>, a successful output should show something like this:</p>

<pre><code class="bash">Outputs:
id = "i-0dgacxxxxxxxxxxxx"
ip = "18.26.xxx.92"
subnet = "subnet-04b3xxxxxxxxxxxxx"
</code></pre>

<h2>Access your EC2 Instance</h2>

<p>You can access the instance by SSH'ing to the IP that was returned by the output as well as the SSH key name that you provided, or you can make use of the <code>terraform output</code> to access the output value:</p>

<pre><code class="bash">ssh -i ~/.ssh/id_rsa ubuntu@$(terraform output -raw ip)
</code></pre>

<h2>Cleanup</h2>

<p>To delete the infrastructure that Terraform provisioned:</p>

<pre><code class="bash">terraform destroy
</code></pre>

<h2>Thank You</h2>

<p>Thanks for reading, if you like my content, check out my <strong><a href="https://ruan.dev">website</a></strong>, read my <strong><a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a></strong> or follow me at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong> on Twitter.</p>
]]></content>
  </entry>
  
</feed>
