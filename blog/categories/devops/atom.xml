<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Devops | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/devops/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2023-07-15T17:47:08-04:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Manage Helm Releases With Terraform]]></title>
    <link href="https://blog.ruanbekker.com/blog/2023/03/09/manage-helm-releases-with-terraform/"/>
    <updated>2023-03-09T16:15:47-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2023/03/09/manage-helm-releases-with-terraform</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/224163430-34e18f11-2182-4d2b-b7ab-f4683c187719.png" alt="helm-releases-with-terraform" /></p>

<p>In this post we will use terraform to deploy a helm release to kubernetes.</p>

<h2>Kubernetes</h2>

<p>For this demonstration I will be using <a href="https://kind.sigs.k8s.io/">kind</a> to deploy a local Kubernetes cluster to the operating system that I am running this on, which will be Ubuntu Linux. For a more in-depth tutorial on Kind, you can see my post on <a href="https://blog.ruanbekker.com/blog/2022/09/20/kind-for-local-kubernetes-clusters/">Kind for Local Kubernetes Clusters</a>.</p>

<h2>Installing the Pre-Requirements</h2>

<p>We will be installing terraform, docker, kind and kubectl on Linux.</p>

<p>Install terraform:</p>

<pre><code class="bash">wget https://releases.hashicorp.com/terraform/1.3.0/terraform_1.3.0_linux_amd64.zip
unzip terraform_1.3.0_linux_amd64.zip
rm terraform_1.3.0_linux_amd64.zip
mv terraform /usr/bin/terraform
</code></pre>

<p>Verify that terraform has been installed:</p>

<pre><code class="bash">terraform -version
</code></pre>

<p>Which in my case returns:</p>

<pre><code class="bash">Terraform v1.3.0
on linux_amd64
</code></pre>

<p>Install Docker on Linux (be careful to curl pipe bash - trust the scripts that you are running):</p>

<pre><code class="bash">curl https://get.docker.com | bash
</code></pre>

<p>Then running <code>docker ps</code> should return:</p>

<pre><code class="bash">CONTAINER ID   IMAGE        COMMAND         CREATED          STATUS          PORTS       NAMES
</code></pre>

<p>Install kind on Linux:</p>

<pre><code class="bash">apt update
curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.17.0/kind-linux-amd64
chmod +x ./kind
sudo mv ./kind /usr/local/bin/kind
</code></pre>

<p>Then to verify that kind was installed with <code>kind --version</code> should return:</p>

<pre><code class="bash">kind version 0.17.0
</code></pre>

<p>Create a kubernetes cluster using kind:</p>

<pre><code class="bash">kind create cluster --name rbkr --image kindest/node:v1.24.0
</code></pre>

<p>Now install <a href="https://kubernetes.io/docs/tasks/tools/">kubectl</a>:</p>

<pre><code class="bash">curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
</code></pre>

<p>Then to verify that kubectl was installed:</p>

<pre><code class="bash">kubectl version --client
</code></pre>

<p>Which in my case returns:</p>

<pre><code class="bash">Client Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.1", GitCommit:"8f94681cd294aa8cfd3407b8191f6c70214973a4", GitTreeState:"clean", BuildDate:"2023-01-18T15:58:16Z", GoVersion:"go1.19.5", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
</code></pre>

<p>Now we can test if kubectl can communicate with the kubernetes api server:</p>

<pre><code class="bash">kubectl get nodes
</code></pre>

<p>In my case it returns:</p>

<pre><code class="bash">NAME                 STATUS   ROLES           AGE     VERSION
rbkr-control-plane   Ready    control-plane   6m20s   v1.24.0
</code></pre>

<h2>Terraform</h2>

<p>Now that our pre-requirements are sorted we can configure terraform to communicate with kubernetes. For that to happen, we need to consult the <a href="https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs">terraform kubernetes provider</a>&rsquo;s documentation.</p>

<p>As per their documentation they provide us with this snippet:</p>

<pre><code>terraform {
  required_providers {
    kubernetes = {
      source = "hashicorp/kubernetes"
      version = "2.18.0"
    }
  }
}

provider "kubernetes" {
  # Configuration options
}
</code></pre>

<p>And from their <a href="https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs">main</a> page, it gives us a couple of options to configure the provider and the easiest is probably to read the <code>~/.kube/config</code> configuration file.</p>

<p>But in cases where you have multiple configurations in your kube config file, this might not be ideal, and I like to be precise, so I will extract the client certificate, client key and cluster ca certificate and endpoint from our <code>~/.kube/config</code> file.</p>

<p>If we run <code>cat ~/.kube/config</code> we will see something like this:</p>

<pre><code class="yaml">apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRU......FURS0tLS0tCg==
    server: https://127.0.0.1:40305
  name: kind-rbkr
contexts:
- context:
    cluster: kind-rbkr
    user: kind-rbkr
  name: kind-rbkr
current-context: kind-rbkr
kind: Config
preferences: {}
users:
- name: kind-rbkr
  user:
    client-certificate-data: LS0tLS1CRX......FURS0tLS0tCg==
    client-key-data: LS0tLS1CRUejhKWUk2N2.....S0tCg==
</code></pre>

<p>First we will create a directory for our certificates:</p>

<pre><code class="bash">mkdir ~/certs
</code></pre>

<p>I have truncated my kube config for readability, but for our first file <code>certs/client-cert.pem</code> we will copy the value of <code>client-certificate-data:</code>, which will look something like this:</p>

<pre><code class="bash">cat certs/client-cert.pem
LS0tLS1CRX......FURS0tLS0tCg==
</code></pre>

<p>Then we will copy the contents of <code>client-key-data:</code> into <code>certs/client-key.pem</code> and then lastly the content of <code>certificate-authority-data:</code> into <code>certs/cluster-ca-cert.pem</code>.</p>

<p>So then we should have the following files inside our <code>certs/</code> directory:</p>

<pre><code class="bash">tree certs/
certs/
├── client-cert.pem
├── client-key.pem
└── cluster-ca-cert.pem

0 directories, 3 files
</code></pre>

<p>Now make them read only:</p>

<pre><code class="bash">chmod 400 ~/certs/*
</code></pre>

<p>Now that we have that we can start writing our terraform configuration. In <code>providers.tf</code>:</p>

<pre><code>terraform {
  required_providers {
    kubernetes = {
      source = "hashicorp/kubernetes"
      version = "2.18.0"
    }
  }
}

provider "kubernetes" {
  host                   = "https://127.0.0.1:40305"
  client_certificate     = base64decode(file("~/certs/client-cert.pem"))
  client_key             = base64decode(file("~/certs/client-key.pem"))
  cluster_ca_certificate = base64decode(file("~/certs/cluster-ca-cert.pem"))
}
</code></pre>

<p>Your host might look different to mine, but you can find your host endpoint in <code>~/.kube/config</code>.</p>

<p>For a simple test we can list all our namespaces to ensure that our configuration is working. In a file called <code>namespaces.tf</code>, we can populate the following:</p>

<pre><code>data "kubernetes_all_namespaces" "allns" {}

output "all-ns" {
  value = data.kubernetes_all_namespaces.allns.namespaces
}
</code></pre>

<p>Now we need to initialize terraform so that it can download the providers:</p>

<pre><code class="bash">terraform init
</code></pre>

<p>Then we can run a plan which will reveal our namespaces:</p>

<pre><code class="bash">terraform plan

data.kubernetes_all_namespaces.allns: Reading...
data.kubernetes_all_namespaces.allns: Read complete after 0s [id=a0ff7e83ffd7b2d9953abcac9f14370e842bdc8f126db1b65a18fd09faa3347b]

Changes to Outputs:
  + all-ns = [
      + "default",
      + "kube-node-lease",
      + "kube-public",
      + "kube-system",
      + "local-path-storage",
    ]
</code></pre>

<p>We can now remove our <code>namespaces.tf</code> as our test worked:</p>

<pre><code class="bash">rm namespaces.tf
</code></pre>

<h2>Helm Releases with Terraform</h2>

<p>We will need two things, we need to consult the <a href="https://registry.terraform.io/providers/hashicorp/helm/latest/docs/resources/release">terraform helm release provider</a> documentation and we also need to consult the helm chart documentation which we are interested in.</p>

<p>In my previous post I wrote about <a href="https://blog.ruanbekker.com/blog/2023/01/24/everything-you-need-to-know-about-helm/">Everything you need to know about Helm</a> and I used the <a href="https://artifacthub.io/packages/helm/bitnami/nginx">Bitnami Nginx Helm Chart</a>, so we will use that one again.</p>

<p>As we are working with helm releases, we need to configure the helm provider, I will just extend my configuration from my previous provider config in <code>providers.tf</code>:</p>

<pre><code>terraform {
  required_providers {
    kubernetes = {
      source = "hashicorp/kubernetes"
      version = "2.18.0"
    }
    helm = {
      source = "hashicorp/helm"
      version = "2.9.0"
    }
  }
}

provider "kubernetes" {
  host                   = "https://127.0.0.1:40305"
  client_certificate     = base64decode(file("~/certs/client-cert.pem"))
  client_key             = base64decode(file("~/certs/client-key.pem"))
  cluster_ca_certificate = base64decode(file("~/certs/cluster-ca-cert.pem"))
}

provider "helm" {
  kubernetes {
    host                   = "https://127.0.0.1:40305"
    client_certificate     = base64decode(file("~/certs/client-cert.pem"))
    client_key             = base64decode(file("~/certs/client-key.pem"))
    cluster_ca_certificate = base64decode(file("~/certs/cluster-ca-cert.pem"))
  }
}
</code></pre>

<p>We will create three terraform files:</p>

<pre><code class="bash">touch {main,outputs,variables}.tf
</code></pre>

<p>And our values yaml in <code>helm-chart/nginx/values.yaml</code>:</p>

<pre><code class="bash">mkdir -p helm-chart/nginx
</code></pre>

<p>Then you can copy the values file from <a href="https://artifacthub.io/packages/helm/bitnami/nginx?modal=values">https://artifacthub.io/packages/helm/bitnami/nginx?modal=values</a> into <code>helm-chart/nginx/values.yaml</code>.</p>

<p>In our <code>main.tf</code> I will use two ways to override values in our <code>values.yaml</code> using <code>set</code> and <code>templatefile</code>. The reason for the templatefile, is when we want to fetch a value and want to replace the content with our values file, it could be used when we retrieve a value from a data source as an example. In my example im just using a variable.</p>

<p>We will have the following:</p>

<pre><code>resource "helm_release" "nginx" {
  name             = var.release_name
  version          = var.chart_version
  namespace        = var.namespace
  create_namespace = var.create_namespace
  chart            = var.chart_name
  repository       = var.chart_repository_url
  dependency_update = true
  reuse_values      = true
  force_update      = true
  atomic              = var.atomic

  set {
    name  = "image.tag"
    value = "1.23.3-debian-11-r3"
  }

  set {
    name  = "service.type"
    value = "ClusterIP"
  }

  values = [
    templatefile("${path.module}/helm-chart/nginx/values.yaml", {
      NAME_OVERRIDE   = var.release_name
    }
  )]

}
</code></pre>

<p>As you can see we are referencing a <code>NAME_OVERRIDE</code> in our <code>values.yaml</code>, I have cleaned up the values file to the following:</p>

<pre><code class="yaml">nameOverride: "${NAME_OVERRIDE}"

## ref: https://hub.docker.com/r/bitnami/nginx/tags/
image:
  registry: docker.io
  repository: bitnami/nginx
  tag: 1.23.3-debian-11-r3
</code></pre>

<p>The <code>NAME_OVERRIDE</code> must be in a <code>${}</code> format.</p>

<p>In our <code>variables.tf</code> we will have the following:</p>

<pre><code>variable "release_name" {
  type        = string
  default     = "nginx"
  description = "The name of our release."
}

variable "chart_repository_url" {
  type        = string
  default     = "https://charts.bitnami.com/bitnami"
  description = "The chart repository url."
}

variable "chart_name" {
  type        = string
  default     = "nginx"
  description = "The name of of our chart that we want to install from the repository."
}

variable "chart_version" {
  type        = string
  default     = "13.2.20"
  description = "The version of our chart."
}

variable "namespace" {
  type        = string
  default     = "apps"
  description = "The namespace where our release should be deployed into."
}

variable "create_namespace" {
  type        = bool
  default     = true
  description = "If it should create the namespace if it doesnt exist."
}

variable "atomic" {
  type        = bool
  default     = false
  description = "If it should wait until release is deployed."
}
</code></pre>

<p>And lastly our <code>outputs.tf</code>:</p>

<pre><code>output "metadata" {
  value = helm_release.nginx.metadata
}
</code></pre>

<p>Now that we have all our configuration ready, we can initialize terraform:</p>

<pre><code class="bash">terraform init
</code></pre>

<p>Then we can run a plan to see what terraform wants to deploy:</p>

<pre><code class="bash">terraform plan
</code></pre>

<p>The plan output shows the following:</p>

<pre><code class="bash">Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  # helm_release.nginx will be created
  + resource "helm_release" "nginx" {
      + atomic                     = false
      + chart                      = "nginx"
      + cleanup_on_fail            = false
      + create_namespace           = true
      + dependency_update          = false
      + disable_crd_hooks          = false
      + disable_openapi_validation = false
      + disable_webhooks           = false
      + force_update               = false
      + id                         = (known after apply)
      + lint                       = false
      + manifest                   = (known after apply)
      + max_history                = 0
      + metadata                   = (known after apply)
      + name                       = "nginx"
      + namespace                  = "apps"
      + pass_credentials           = false
      + recreate_pods              = false
      + render_subchart_notes      = true
      + replace                    = false
      + repository                 = "https://charts.bitnami.com/bitnami"
      + reset_values               = false
      + reuse_values               = false
      + skip_crds                  = false
      + status                     = "deployed"
      + timeout                    = 300
      + values                     = [
          + &lt;&lt;-EOT
                nameOverride: "nginx"

                ## ref: https://hub.docker.com/r/bitnami/nginx/tags/
                image:
                  registry: docker.io
                  repository: bitnami/nginx
                  tag: 1.23.3-debian-11-r3
            EOT,
        ]
      + verify                     = false
      + version                    = "13.2.20"
      + wait                       = false
      + wait_for_jobs              = false

      + set {
          + name  = "image.tag"
          + value = "1.23.3-debian-11-r3"
        }
    }

Plan: 1 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + metadata = (known after apply)
</code></pre>

<p>Once we are happy with our plan, we can run a apply:</p>

<pre><code class="bash">terraform apply 

Plan: 1 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + metadata = (known after apply)

Do you want to perform these actions?
  Terraform will perform the actions described above.
  Only 'yes' will be accepted to approve.

  Enter a value: yes

helm_release.nginx: Creating...
helm_release.nginx: Still creating... [10s elapsed]

metadata = tolist([
  {
    "app_version" = "1.23.3"
    "chart" = "nginx"
    "name" = "nginx"
    "namespace" = "apps"
    "revision" = 1
    "values" = "{\"image\":{\"registry\":\"docker.io\",\"repository\":\"bitnami/nginx\",\"tag\":\"1.23.3-debian-11-r3\"},\"nameOverride\":\"nginx\"}"
    "version" = "13.2.20"
  },
])
</code></pre>

<p>Then we can verify if the pod is running:</p>

<pre><code class="bash">kubectl get pods -n apps
NAME                    READY   STATUS    RESTARTS   AGE
nginx-59bdc6465-xdbfh   1/1     Running   0          2m35s
</code></pre>

<h2>Importing Helm Releases into Terraform State</h2>

<p>If you have an existing helm release that was deployed with helm and you want to transfer the ownership to terraform, you first need to write the terraform code, then import the resources into terraform state using:</p>

<pre><code class="bash">terraform import helm_release.nginx apps/nginx
</code></pre>

<p>Where the last argument is <code>&lt;namespace&gt;/&lt;release-name&gt;</code>. Once that is imported you can run terraform plan and apply.</p>

<p>If you want to discover all helm releases managed by helm you can use:</p>

<pre><code class="bash">kubectl get all -A -l app.kubernetes.io/managed-by=Helm
</code></pre>

<h2>Thank You</h2>

<p>Thanks for reading, feel free to check out my <a href="https://ruan.dev/">website</a>, feel free to subscribe to my <a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a> or follow me at <a href="https://twitter.com/ruanbekker">@ruanbekker</a> on Twitter.</p>

<ul>
<li>Linktree: <a href="https://go.ruan.dev/links">https://go.ruan.dev/links</a></li>
<li>Patreon: <a href="https://go.ruan.dev/patreon">https://go.ruan.dev/patreon</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Persisting Terraform Remote State in Gitlab]]></title>
    <link href="https://blog.ruanbekker.com/blog/2023/03/05/persisting-terraform-remote-state-in-gitlab/"/>
    <updated>2023-03-05T01:43:54-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2023/03/05/persisting-terraform-remote-state-in-gitlab</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/222946002-7cd88466-c584-4ea0-b190-54b1c3052865.png" alt="terraform-state-gitlab" /></p>

<p>In this tutorial we will demonstrate how to persist your terraform state in gitlab managed terraform state, using the terraform http backend.</p>

<p>For detailed information about this consult <a href="https://docs.gitlab.com/ee/user/infrastructure/iac/terraform_state.html">their documentation</a></p>

<h2>What are we doing?</h2>

<p>We will create a terraform pipeline which will run the plan step automatically and a manual step to run the apply step.</p>

<p>During these steps and different pipelines we need to persist our terraform state remotely so that new pipelines can read from our state what we last stored.</p>

<p>Gitlab offers a <a href="https://docs.gitlab.com/ee/user/infrastructure/iac/terraform_state.html">remote backend</a> for our terraform state which we can use, and we will use a basic example of using the random resource.</p>

<h2>Prerequisites</h2>

<p>If you don&rsquo;t see the &ldquo;Infrastructure&rdquo; menu on your left, you need to enable it at &ldquo;Settings&rdquo;, &ldquo;General&rdquo;, &ldquo;Visibility&rdquo;, &ldquo;Project features&rdquo;, &ldquo;Permissions&rdquo; and under &ldquo;Operations&rdquo;, turn on the toggle.</p>

<p>For more information on this see their <a href="https://docs.gitlab.com/ee/user/infrastructure/iac/terraform_state.html#prerequisites">documentation</a></p>

<h2>Authentication</h2>

<p>For this demonstration I created a token which is only scoped for this one project, for this we need a to create a token under, &ldquo;Settings&rdquo;, &ldquo;Access Tokens&rdquo;:</p>

<p><img src="https://user-images.githubusercontent.com/567298/222896148-6b0121fe-fceb-470e-a096-5db03ae0eab9.png" alt="image" /></p>

<p>Select the <code>api</code> under scope:</p>

<p><img src="https://user-images.githubusercontent.com/567298/222896298-fee26e1f-6bcf-4d7c-80eb-ed48ded33bf2.png" alt="image" /></p>

<p>Store the token name and token value as <code>TF_USERNAME</code> and <code>TF_PASSWORD</code> as a CICD variable under &ldquo;Settings&rdquo;, &ldquo;CI/CD&rdquo;, &ldquo;Variables&rdquo;.</p>

<h2>Terraform Code</h2>

<p>We will use a basic <code>random_uuid</code> resource for this demonstration, our <code>main.tf</code>:</p>

<pre><code>resource "random_uuid" "uuid" {}

output "uuid" {
  value       = random_uuid.uuid.result
  sensitive   = false
}
</code></pre>

<p>Our <code>providers.tf</code>, you will notice the <code>backend "http" {}</code> is what is required for our gitlab remote state:</p>

<pre><code>terraform {
  required_providers {
    random = {
      source = "hashicorp/random"
      version = "3.4.3"
    }
  }
  backend "http" {}
  required_version = "~&gt; 1.3.6"
}

provider "random" {}
</code></pre>

<p>Push that up to gitlab for now.</p>

<h2>Gitlab Pipeline</h2>

<p>Our <code>.gitlab-ci.yml</code> consists of a plan step and a apply step which is a manual step as we first want to review our plan step before we apply.</p>

<p>Our pipeline will only run on the default branch, which in my case is main:</p>

<pre><code class="yaml">image:
  name: hashicorp/terraform:1.3.6
  entrypoint: [""]

cache:
  paths:
    - .terraform

workflow:
  rules:
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
    - when: never

variables:
  TF_ADDRESS: "https://gitlab.com/api/v4/projects/${CI_PROJECT_ID}/terraform/state/default-terraform.tfstate"

stages:
  - plan
  - apply

.terraform_init: &amp;terraform_init
  - terraform init
      -backend-config=address=${TF_ADDRESS}
      -backend-config=lock_address=${TF_ADDRESS}/lock
      -backend-config=unlock_address=${TF_ADDRESS}/lock
      -backend-config=username=${TF_USERNAME}
      -backend-config=password=${TF_PASSWORD}
      -backend-config=lock_method=POST
      -backend-config=unlock_method=DELETE
      -backend-config=retry_wait_min=5

terraform:plan:
  stage: plan
  artifacts:
    paths:
      - '**/*.tfplan'
      - '**/.terraform.lock.hcl'
  before_script:
    - *terraform_init
  script:
    - terraform validate
    - terraform plan -input=false -out default.tfplan

terraform:apply:
  stage: apply
  artifacts:
    paths:
      - '**/*.tfplan'
      - '**/.terraform.lock.hcl'
  before_script:
    - *terraform_init
  script:
    - terraform apply -input=false -auto-approve default.tfplan
  when: manual
</code></pre>

<p>Where the magic happens is in the <code>terraform init</code> step, that is where we will initialize the terraform state in gitlab, and as you can see we are taking the <code>TF_ADDRESS</code> variable to define the path of our state and in this case our state file will be named <code>default-terraform.tfstate</code>.</p>

<p>If it was a case where you are deploying multiple environments, you can use something like <code>${ENVIRONMENT}-terraform.tfstate</code>.</p>

<p>When we run our pipeline, we can look at our plan step:</p>

<p><img src="https://user-images.githubusercontent.com/567298/222947389-9d9d8d4f-a114-44b5-b183-a2b126ba82b8.png" alt="image" /></p>

<p>Once we are happy with this we can run the manual step and do the apply step, then our pipeline should look like this:</p>

<p><img src="https://user-images.githubusercontent.com/567298/222930015-6445a5da-7887-47a6-989e-f33a33b9451a.png" alt="image" /></p>

<p>When we inspect our terraform state in the infrastructure menu, we can see the state file was created:</p>

<p><img src="https://user-images.githubusercontent.com/567298/222901200-2cd0a0f9-6e81-438f-bc74-286778b648d4.png" alt="image" /></p>

<h2>Thank You</h2>

<p>Thanks for reading, feel free to check out my <a href="https://ruan.dev/">website</a>, feel free to subscribe to my <a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a> or follow me at <a href="https://twitter.com/ruanbekker">@ruanbekker</a> on Twitter.</p>

<ul>
<li>Linktree: <a href="https://go.ruan.dev/links">https://go.ruan.dev/links</a></li>
<li>Patreon: <a href="https://go.ruan.dev/patreon">https://go.ruan.dev/patreon</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started With Wiremock]]></title>
    <link href="https://blog.ruanbekker.com/blog/2023/01/14/getting-started-with-wiremock/"/>
    <updated>2023-01-14T17:03:12-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2023/01/14/getting-started-with-wiremock</id>
    <content type="html"><![CDATA[<p>In this tutorial we will use docker to run an instance of wiremock to setup a mock api for us to test our api&rsquo;s.</p>

<h2>Wiremock</h2>

<p><a href="https://wiremock.org/">Wiremock</a> is a tool for building mock API&rsquo;s which enables us to build stable development environments.</p>

<h2>Docker and Wiremock</h2>

<p>Run a wiremock instance with docker:</p>

<pre><code class="bash">docker run -it --rm -p 8080:8080 --name wiremock wiremock/wiremock:2.34.0
</code></pre>

<p>Then our wiremock instance will be exposed on port 8080 locally, which we can use to make a request against to create a api mapping:</p>

<pre><code class="bash">curl -XPOST -H "Content-Type: application/json" \
  http://localhost:8080/__admin/mappings
  -d '{"request": {"url": "/testapi","method": "GET"}, "response": {"status": 200, "body": "{\"result\": \"ok\"
}", "headers": {"Content-Type": "application/json"}}}'
</code></pre>

<p>The response should be something like this:</p>

<pre><code class="json">{
    "id" : "223a2c0a-8b43-42dc-8ba6-fe973da1e420",
    "request" : {
      "url" : "/testapi",
      "method" : "GET"
    },
    "response" : {
      "status" : 200,
      "body" : "{\"result\": \"ok\"}",
      "headers" : {
        "Content-Type" : "application/json"
      }
    },
    "uuid" : "223a2c0a-8b43-42dc-8ba6-fe973da1e420"
}
</code></pre>

<h2>Test Wiremock</h2>

<p>If we make a GET request against our API:</p>

<pre><code class="bash">curl http://localhost:8080/testapi
</code></pre>

<p>Our response should be:</p>

<pre><code class="json">{
  "result": "ok"
}
</code></pre>

<h2>Export Wiremock Mappings</h2>

<p>We can export our mappings to a local file named <code>stubs.json</code> with:</p>

<pre><code class="bash">curl -s http://localhost:8080/__admin/mappings --output stubs.json
</code></pre>

<h2>Import Wiremock Mappings</h2>

<p>We can import our mappings from our <code>stubs.json</code> file with:</p>

<pre><code class="bash">curl -XPOST -v --data-binary @stubs.json http://localhost:8080/__admin/mappings/import
</code></pre>

<h2>Resources</h2>

<ul>
<li><a href="https://wiremock.org/docs/docker/">https://wiremock.org/docs/docker/</a></li>
<li><a href="https://github.com/WireMock-Net/WireMock.Net/wiki/Admin-API-Reference">https://github.com/WireMock-Net/WireMock.Net/wiki/Admin-API-Reference</a></li>
</ul>


<h2>Thank You</h2>

<p>Thanks for reading, feel free to check out my <a href="https://ruan.dev/">website</a>, feel free to subscribe to my <a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a> or follow me at <a href="https://twitter.com/ruanbekker">@ruanbekker</a> on Twitter.</p>

<ul>
<li>Linktree: <a href="https://go.ruan.dev/links">https://go.ruan.dev/links</a></li>
<li>Patreon: <a href="https://go.ruan.dev/patreon">https://go.ruan.dev/patreon</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ansible Playbook for Your Macbook Homebrew Packages]]></title>
    <link href="https://blog.ruanbekker.com/blog/2022/08/28/ansible-playbook-for-your-macbook-homebrew-packages/"/>
    <updated>2022-08-28T19:14:54-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2022/08/28/ansible-playbook-for-your-macbook-homebrew-packages</id>
    <content type="html"><![CDATA[<p><img src="https://blog.ruanbekker.com/images/ansible-macbook.png" alt="ansible-macbook-homebrew" /></p>

<p>In this tutorial I will demonstrate how to use Ansible for Homebrew Configuration Management. The aim for using Ansible to manage your homebrew packages helps you to have a consistent list of packages on your macbook.</p>

<p>For me personally, when I get a new laptop it&rsquo;s always a mission to get the same packages installed as what I had before, and ansible solves that for us to have all our packages defined in configuration management.</p>

<h2>Install Ansible</h2>

<p>Install ansible with python and pip:</p>

<pre><code class="bash">python3 -m pip install ansible==4.9.0
</code></pre>

<h2>Ansible Configuration</h2>

<p>Create the <code>ansible.cfg</code> configuration file:</p>

<pre><code>[defaults]
inventory = inventory.ini
deprecation_warnings = False
</code></pre>

<p>Our <code>inventory.ini</code> will define the information about our target host, which will be localhost as we are using ansible to run against our local target which is our macbook:</p>

<pre><code>[localhost]
my.laptop  ansible_connection=local

[localhost:vars]
ansible_python_interpreter = /usr/bin/python3
</code></pre>

<h2>Ansible Playbook</h2>

<p>Our playbook <code>homebrew.yaml</code> will define the tasks to add the homebrew taps, cask packages and homebrew packages. You can change the packages as you desire, but these are the ones that I use:</p>

<pre><code class="yaml">- hosts: localhost
  name: Macbook Playbook
  gather_facts: False
  vars:
    TFENV_ARCH: amd64
  tasks:
    - name: Ensures taps are present via homebrew
      community.general.homebrew_tap:
        name: "{{ item }}"
        state: present
      with_items:
        - hashicorp/tap

    - name: Ensures packages are present via homebrew cask
      community.general.homebrew_cask:
        name: "{{ item }}"
        state: present
        install_options: 'appdir=/Applications'
      with_items:
        - visual-studio-code
        - multipass
        - spotify

    - name: Ensures packages are present via homebrew
      community.general.homebrew:
        name: "{{ item }}"
        path: "/Applications"
        state: present
      with_items:
        - openssl
        - readline
        - sqlite3
        - xz
        - zlib
        - jq
        - yq
        - wget
        - go
        - kubernetes-cli
        - fzf
        - sshuttle
        - hugo
        - helm
        - kind
        - awscli
        - gnupg
        - kubectx
        - helm
        - stern
        - terraform
        - tfenv
        - pyenv 
        - jsonnet
      ignore_errors: yes
      tags:
        - packages
</code></pre>

<h2>Deploy Playbook</h2>

<p>Now you can run the playbook using:</p>

<pre><code class="bash">ansible-playbook homebrew.yaml
</code></pre>

<h2>Source Code</h2>

<p>The code can be found in my github repository:
- <a href="https://github.com/ruanbekker/ansible-macbook-setup">https://github.com/ruanbekker/ansible-macbook-setup</a></p>

<h2>Thanks</h2>

<p>Thanks for reading, feel free to check out my <a href="https://ruan.dev/">website</a>, feel free to subscribe to my <a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a> or follow me at <a href="https://twitter.com/ruanbekker">@ruanbekker</a> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Remote Builds With Docker Contexts]]></title>
    <link href="https://blog.ruanbekker.com/blog/2022/07/14/remote-builds-with-docker-contexts/"/>
    <updated>2022-07-14T01:57:34-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2022/07/14/remote-builds-with-docker-contexts</id>
    <content type="html"><![CDATA[<p><img src="https://blog.ruanbekker.com/images/ruanbekker-docker-contexts.png" alt="using-docker-contexts" /></p>

<p>Often you want to save some battery life when you are doing docker builds and leverage a remote host to do the intensive work and we can utilise docker context over ssh to do just that.</p>

<h2>About</h2>

<p>In this tutorial I will show you how to use a remote docker engine to do docker builds, so you still run the docker client locally, but the context of your build will be sent to a remote docker engine via ssh.</p>

<p>We will setup password-less ssh, configure our ssh config, create the remote docker context, then use the remote docker context.</p>

<p><img src="https://user-images.githubusercontent.com/567298/178909518-26f580e9-2b96-41b3-bacd-a5ea5f848ebf.png" alt="image" /></p>

<h2>Password-less SSH</h2>

<p>I will be copying my public key to the remote host:</p>

<pre><code class="bash">$ ssh-copy-id ruan@192.168.2.18
</code></pre>

<p>Setup my ssh config:</p>

<pre><code class="bash">$ cat ~/.ssh/config
Host home-server
    Hostname 192.168.2.18
    User ruan
    IdentityFile ~/.ssh/id_rsa
    StrictHostKeyChecking no
    UserKnownHostsFile /dev/null
</code></pre>

<p>Test:</p>

<pre><code>$ ssh home-server whoami
ruan
</code></pre>

<h2>Docker Context</h2>

<p>On the target host (192.168.2.18) we can verify that docker is installed:</p>

<pre><code class="bash">$ docker version
Client: Docker Engine - Community
 Version:           20.10.12
 API version:       1.41
 Go version:        go1.16.12
 Git commit:        e91ed57
 Built:             Mon Dec 13 11:45:37 2021
 OS/Arch:           linux/amd64
 Context:           default
 Experimental:      true

Server: Docker Engine - Community
 Engine:
  Version:          20.10.12
  API version:      1.41 (minimum version 1.12)
  Go version:       go1.16.12
  Git commit:       459d0df
  Built:            Mon Dec 13 11:43:46 2021
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.4.12
  GitCommit:        7b11cfaabd73bb80907dd23182b9347b4245eb5d
 runc:
  Version:          1.0.2
  GitCommit:        v1.0.2-0-g52b36a2
 docker-init:
  Version:          0.19.0
  GitCommit:        de40ad0
</code></pre>

<p>On the client (my laptop in this example), we will create a docker context called &ldquo;home-server&rdquo; and point it to our target host:</p>

<pre><code class="bash">$ docker context create home-server --docker "host=ssh://home-server"
home-server
Successfully created context "home-server"
</code></pre>

<p>Now we can list our contexts:</p>

<pre><code class="bash">docker context ls
NAME                TYPE                DESCRIPTION                               DOCKER ENDPOINT               KUBERNETES ENDPOINT                                  ORCHESTRATOR
default *           moby                Current DOCKER_HOST based configuration   unix:///var/run/docker.sock   https://k3d-master.127.0.0.1.nip.io:6445 (default)   swarm
home-server         moby                                                          ssh://home-server
</code></pre>

<h2>Using Contexts</h2>

<p>We can verify if this works by listing our cached docker images locally and on our remote host:</p>

<pre><code class="bash">$ docker --context=default images | wc -l
 16
</code></pre>

<p>And listing the remote images by specifying the context:</p>

<pre><code class="bash">$ docker --context=home-server images | wc -l
 70
</code></pre>

<p>We can set the default context to our target host:</p>

<pre><code>$ docker context use home-server
home-server
</code></pre>

<h2>Running Containers over Contexts</h2>

<p>So running containers with remote contexts essentially becomes running containers on remote hosts. In the past, I had to setup a ssh tunnel, point the docker host env var to that endpoint, then run containers on the remote host.</p>

<p>Thats something of the past, we can just point our docker context to our remote host and run the container. If you haven&rsquo;t set the default context, you can specify the context, so running a docker container on a remote host with your docker client locally:</p>

<pre><code class="bash">$ docker --context=home-server run -it -p 8002:8080 ruanbekker/hostname
2022/07/14 05:44:04 Server listening on port 8080
</code></pre>

<p>Now from our client (laptop), we can test our container on our remote host:</p>

<pre><code class="bash">$ curl http://192.168.2.18:8002
Hostname: 8605d292e2b4
</code></pre>

<p>The same way can be used to do remote docker builds, you have your Dockerfile locally, but when you build, you point the context to the remote host, and your context (dockerfile and files referenced in your dockerfile) will be sent to the remote host. This way you can save a lot of battery life as the computation is done on the remote docker engine.</p>

<h2>Thank You</h2>

<p>Thanks for reading, feel free to check out my <a href="https://ruan.dev/">website</a>, feel free to subscribe to my <a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a> or follow me at <a href="https://twitter.com/ruanbekker">@ruanbekker</a> on Twitter.</p>
]]></content>
  </entry>
  
</feed>
