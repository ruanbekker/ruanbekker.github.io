<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Docker-compose | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/docker-compose/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2018-10-30T15:09:33-04:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Running a 3 Node Elasticsearch Cluster With Docker Compose on Your Laptop for Testing]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/04/29/running-a-3-node-elasticsearch-cluster-with-docker-compose-on-your-laptop-for-testing/"/>
    <updated>2018-04-29T13:43:35-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/04/29/running-a-3-node-elasticsearch-cluster-with-docker-compose-on-your-laptop-for-testing</id>
    <content type="html"><![CDATA[<p>Having a Elasticsearch cluster on your laptop with Docker for testing is great. And in this post I will show you how quick and easy it is, to have a 3 node elasticsearch cluster running on docker for testing.</p>

<h2>Pre-Requisites</h2>

<p>We need to set the <code>vm.max_map_count</code> kernel parameter:</p>

<pre><code class="bash">$ sudo sysctl -w vm.max_map_count=262144
</code></pre>

<p>To set this permanently, add it to <code>/etc/sysctl.conf</code> and reload with <code>sudo sysctl -p</code></p>

<h2>Docker Compose:</h2>

<p>The docker compose file that we will reference:</p>

<pre><code class="yaml">version: '2.2'
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.2.4
    container_name: elasticsearch
    environment:
      - cluster.name=docker-cluster
      - bootstrap.memory_lock=true
      - http.cors.enabled=true
      - http.cors.allow-origin=*
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - esdata1:/home/ruan/workspace/docker/elasticsearch/data
    ports:
      - 9200:9200
    networks:
      - esnet
  elasticsearch2:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.2.4
    container_name: elasticsearch2
    environment:
      - cluster.name=docker-cluster
      - bootstrap.memory_lock=true
      - http.cors.enabled=true
      - http.cors.allow-origin=*
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - "discovery.zen.ping.unicast.hosts=elasticsearch"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - esdata2:/home/ruan/workspace/docker/elasticsearch/data
    networks:
      - esnet
  elasticsearch3:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.2.4
    container_name: elasticsearch3
    environment:
      - cluster.name=docker-cluster
      - bootstrap.memory_lock=true
      - http.cors.enabled=true
      - http.cors.allow-origin=*
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - "discovery.zen.ping.unicast.hosts=elasticsearch"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - esdata3:/home/ruan/workspace/docker/elasticsearch/data
    networks:
      - esnet

  kibana:
    image: 'docker.elastic.co/kibana/kibana:6.3.2'
    container_name: kibana
    environment:
      SERVER_NAME: kibana.local
      ELASTICSEARCH_URL: http://elasticsearch:9200
    ports:
      - '5601:5601'
    networks:
      - esnet

  headPlugin:
    image: 'mobz/elasticsearch-head:5'
    container_name: head
    ports:
      - '9100:9100'
    networks:
      - esnet

volumes:
  esdata1:
    driver: local
  esdata2:
    driver: local
  esdata3:
    driver: local

networks:
  esnet:
</code></pre>

<p>Now make sure the paths exist that we referenced in the compose file, in my case <code>/home/ruan/workspace/docker/elasticsearch/data</code></p>

<h2>Deploy</h2>

<p>Deploy your elasticsearch cluster with docker compose:</p>

<pre><code class="bash">$ docker-compose up
</code></pre>

<p>This will run in the foreground, and you should see console output.</p>

<h2>Testing Elasticsearch</h2>

<p>Let&rsquo;s run a couple of queries, first up, check the cluster health api:</p>

<pre><code class="bash">$ curl http://127.0.0.1:9200/_cluster/health?pretty
{
  "cluster_name" : "docker-cluster",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 3,
  "number_of_data_nodes" : 3,
  "active_primary_shards" : 1,
  "active_shards" : 2,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 100.0
}
</code></pre>

<p>Create a index with replication count of 2:</p>

<pre><code class="bash">$ curl -H "Content-Type: application/json" -XPUT http://127.0.0.1:9200/test -d '{"number_of_replicas": 2}'
</code></pre>

<p>Ingest a document to elasticsearch:</p>

<pre><code class="bash">$ curl -H "Content-Type: application/json" -XPUT http://127.0.0.1:9200/test/docs/1 -d '{"name": "ruan"}'
{"_index":"test","_type":"docs","_id":"1","_version":1,"result":"created","_shards":{"total":3,"successful":3,"failed":0},"_seq_no":0,"_primary_term":1}
</code></pre>

<p>View the indices:</p>

<pre><code class="bash">$ curl http://127.0.0.1:9200/_cat/indices?v
health status index                       uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   test                        w4p2Q3fTR4uMSYBfpNVPqw   5   2          1            0      3.3kb          1.1kb
green  open   .monitoring-es-6-2018.04.29 W69lql-rSbORVfHZrj4vug   1   1       1601           38        4mb            2mb
</code></pre>

<h2>Kibana</h2>

<p>Kibana is also included in the stack and is accessible via <a href="http://localhost:5601/">http://localhost:5601/</a> and you it should look more or less like:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/kibana-local-home.png" alt="" /></p>

<h2>Elasticsearch Head UI</h2>

<p>I always prefer working directly with the RESTFul API, but if you would like to use a UI to interact with Elasticsearch, you can access it via <a href="http://localhost:9100/">http://localhost:9100/</a> and should look like this:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elasticsearch-head-ui.png" alt="" /></p>

<h2>Deleting the Cluster:</h2>

<p>As its running in the foreground, you can just hit ctrl + c and as we persisted data in our compose, when you spin up the cluster again, the data will still be there.</p>

<h2>Resources:</h2>

<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Use Docker Secrets With MySQL on Docker Swarm]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/11/23/use-docker-secrets-with-mysql-on-docker-swarm/"/>
    <updated>2017-11-23T16:55:15-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/11/23/use-docker-secrets-with-mysql-on-docker-swarm</id>
    <content type="html"><![CDATA[<p><img src="http://obj-cache.cloud.ruanbekker.com/docker-logo.png" alt="" /></p>

<p>Today we will use Docker Secrets, more specifically store our MySQL Passwords in Secrets, which will be passed to our containers, so that we don&rsquo;t use clear text passwords in our Compose files.</p>

<h2>What is Docker Secrets:</h2>

<p>In Docker, Docker Secrets are encrypted during transit and at rest in a Docker Swarm Cluster. The great thing about Docker Secrets is that you can manage these secrets from a central place, and the fact that it encrypts the data and transfers the data securely to the containers that needs the secrets. So you authorize which containers needs access to these secrets.</p>

<p>So instead of setting the MySQL Root Passwords in clear text, you will create the secrets, then in your docker-compose file, you will reference the secret name.</p>

<h2>Deploy MySQL with Docker Secrets</h2>

<p>We will deploy a Stack that contains MySQL and Adminer (WebUI for MySQL).</p>

<p>We will make the MySQL Service Persistent by setting a constraint to only run on the Manager node, as we will create the volume path on the host, and then map the host to the container so that the container can have persistent data. We will also create secrets for our MySQL Service so that we dont expose any plaintext passwords in our compose file.</p>

<p>Our Docker Compose file:</p>

<pre><code class="yaml docker-compose.yml">version: '3.3'

services:
  db:
    image: mysql
    secrets:
      - db_root_password
      - db_dba_password
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
      resources:
        reservations:
          memory: 128M
        limits:
          memory: 256M
    ports:
      - 3306:3306
    environment:
      MYSQL_USER: dba
      MYSQL_DATABASE: mydb
      MYSQL_ROOT_PASSWORD_FILE: /run/secrets/db_root_password
      MYSQL_PASSWORD_FILE: /run/secrets/db_dba_password
    networks:
      - appnet
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - type: bind
        source: /opt/docker/volumes/mysql
        target: /var/lib/mysql

  adminer:
    image: adminer
    ports:
      - 8080:8080
    networks:
      - appnet

secrets:
  db_root_password:
    external: true
  db_dba_password:
    external: true

networks:
  appnet:
    external: true
</code></pre>

<h2>Dependencies:</h2>

<p>As we specified our secrets and networks as external resources, it needs to exist before we deploy our stack. We also need to create the directory for our mysql data, as the data will be mapped from our host to our container.</p>

<p>Create the Overlay Network:</p>

<pre><code class="bash">$ docker network create --driver overlay appnet
</code></pre>

<p>Create the Secrets:</p>

<pre><code class="bash">$ openssl rand -base64 12 | docker secret create db_root_password -
$ openssl rand -base64 12 | docker secret create db_dba_password -
</code></pre>

<p>List the Secrets:</p>

<pre><code class="bash">$ docker secret ls
ID                          NAME                CREATED             UPDATED
jzhrwyxkiqt8v81ow0xjktqnw   db_root_password    12 seconds ago      12 seconds ago
plr6rbrqkqy7oplrd21pja3ol   db_dba_password     4 seconds ago       4 seconds ago
</code></pre>

<p>Inspect the secret, so that we can see that theres not value exposed:</p>

<pre><code class="bash">$ docker secret inspect db_root_password
[
    {
        "ID": "jzhrwyxkiqt8v81ow0xjktqnw",
        "Version": {
            "Index": 982811
        },
        "CreatedAt": "2017-11-23T14:33:17.005968748Z",
        "UpdatedAt": "2017-11-23T14:33:17.005968748Z",
        "Spec": {
            "Name": "db_root_password",
            "Labels": {}
        }
    }
]
</code></pre>

<p>Create the Directory for MySQL:</p>

<pre><code class="bash">$ mkdir -p /opt/docker/volumes/mysql
</code></pre>

<h2>Deployment Time!</h2>

<p>Deploy the stack:</p>

<pre><code>$ docker stack deploy -c docker-compose.yml apps
Creating service apps_adminer
Creating service apps_db
</code></pre>

<p>As you can see the data of our MySQL container resides on our host, which makes the data persistent for the container:</p>

<pre><code class="bash">$ ls /opt/docker/volumes/mysql/
auto.cnf  ca-key.pem  ca.pem  client-cert.pem  client-key.pem  ib_buffer_pool  ibdata1  ib_logfile0  ib_logfile1  ibtmp1  mydb  mysql  performance_schema  private_key.pem  public_key.pem  server-cert.pem  server-key.pem  sys
</code></pre>

<h2>Connect to MySQL</h2>

<p>The value of our secrets will reside under <code>/run/secrets/</code> in our container, as we have mapped it to our mysql container, lets have a look at them:</p>

<pre><code class="bash">$ docker exec -it $(docker ps -f name=apps_db -q) ls /run/secrets/
db_dba_password  db_root_password
</code></pre>

<p>View the actual value of the <code>db_root_password</code>:</p>

<pre><code class="bash">$ docker exec -it $(docker ps -f name=apps_db -q) cat /run/secrets/db_root_password
mRpcY1eY2+wimf10
</code></pre>

<p>Connecting to MySQL:</p>

<pre><code class="bash">$ docker exec -it $(docker ps -f name=apps_db -q) mysql -u root -p
Enter password:
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 8
Server version: 5.7.20 MySQL Community Server (GPL)

Copyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql&gt; show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mydb               |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
5 rows in set (0.00 sec)
</code></pre>

<p>As we have deployed adminer, you can access the Adminer WebUI on the Host&rsquo;s IP and the Defined Port.</p>

<h2>Testing Data Persistance:</h2>

<pre><code class="bash">$ docker exec -it $(docker ps -f name=apps_db -q) mysql -u root -p
Enter password:
Welcome to the MySQL monitor.  Commands end with ; or \g.

mysql&gt; create database ruan;
Query OK, 1 row affected (0.00 sec)

mysql&gt; show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mydb               |
| mysql              |
| performance_schema |
| ruan               |
| sys                |
+--------------------+
6 rows in set (0.00 sec)

mysql&gt; exit;
Bye
</code></pre>

<p>Verify the hostname of our container, before we kill the container:</p>

<pre><code class="bash">$ docker exec -it $(docker ps -f name=apps_db -q) hostname
bdedb54bbc2b
</code></pre>

<p>Kill the container:</p>

<pre><code>$ docker kill $(docker ps -f name=apps_db -q)
bdedb54bbc2b
</code></pre>

<p>Verify the status of the MySQL Service, as we can see the service count is 0, so the container was succesfully killed.</p>

<pre><code class="bash">$ docker service ls -f name=apps_db
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
nzf96q05fktm        apps_db             replicated          0/1                 mysql:latest        *:3306-&gt;3306/tcp
</code></pre>

<p>After waiting for a couple of seconds, we can see the service is in service again, then check the hostname so that we can confirm that its a new container:</p>

<pre><code>$ docker service ls -f name=apps_db
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
nzf96q05fktm        apps_db             replicated          1/1                 mysql:latest        *:3306-&gt;3306/tcp

$ docker exec -it $(docker ps -f name=apps_db -q) hostname
95c15c89f891
</code></pre>

<p>Logong to MySQL again and verify if our perviously created database is still there:</p>

<pre><code class="bash">$ docker exec -it $(docker ps -f name=apps_db -q) mysql -u root -p
Enter password:
Welcome to the MySQL monitor.  Commands end with ; or \g.

mysql&gt; show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mydb               |
| mysql              |
| performance_schema |
| ruan               |
| sys                |
+--------------------+
6 rows in set (0.01 sec)
</code></pre>

<p>By design docker is stateless, but as we mapped the host&rsquo;s path to the container our data is persistent. As we have set a constraint so that the container must only spin up on this node, the container will always have access to the data path.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Local Dev Environment With Docker MySQL and Adminer WebUI With Docker Compose]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/11/13/local-dev-environment-with-docker-mysql-and-adminer-webui-with-docker-compose/"/>
    <updated>2017-11-13T16:15:34-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/11/13/local-dev-environment-with-docker-mysql-and-adminer-webui-with-docker-compose</id>
    <content type="html"><![CDATA[<p>Let&rsquo;s setup a local development environment with Docker, MySQL and Adminer WebUI using Docker Compose</p>

<h2>Docker Compose File:</h2>

<p>Let&rsquo;s look at our docker-compose file:</p>

<pre><code class="yml">version: '3.2'

services:
  mysql-client:
    image: alpine:edge
    volumes:
      - type: bind
        source: ./workspace
        target: /root/workspace
    networks:
      - docknet
    command: ping 127.0.0.1

  db:
    image: mysql
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: example
    networks:
      - docknet
    volumes:
      - type: volume
        source: dbdata
        target: /var/lib/mysql

  adminer:
    image: adminer
    restart: always
    ports:
      - 8080:8080
    networks:
      - docknet

networks:
    docknet:
        external: true

volumes:
  dbdata:
    external: true
</code></pre>

<p>Environment Variables for the MySQL Docker image is:</p>

<pre><code>- MYSQL_ROOT_PASSWORD
- MYSQL_DATABASE
- MYSQL_USER, MYSQL_PASSWORD
- MYSQL_ALLOW_EMPTY_PASSWORD
- MYSQL_RANDOM_ROOT_PASSWORD
- MYSQL_ONETIME_PASSWORD
</code></pre>

<p>More info can be viewed on this resource: <a href="https://hub.docker.com/_/mysql/">hub.docker.com/_/mysql/</a></p>

<h2>Pre-Requirements:</h2>

<p>Let&rsquo;s create our pre-requirement:</p>

<ol>
<li>Networks:</li>
</ol>


<pre><code class="bash">$ docker network create docknet
</code></pre>

<ol>
<li>Volumes:</li>
</ol>


<p>Our Volume for MySQL so that we have persistent data:</p>

<pre><code class="bash">$ docker volume create dbdata
</code></pre>

<p>Our <code>workspace</code> directory that will be persistent in our <code>debug-client</code> alpine container:</p>

<pre><code class="bash">$ mkdir -p workspace/python
</code></pre>

<h2>Launching our Services:</h2>

<p>Let&rsquo;s launch our services:</p>

<pre><code class="bash ">$ docker-compose -f mysql-compose.yml up -d
Creating mysql_db_1 ...
Creating mysql_adminer_1
Creating mysql_debug-client_1
</code></pre>

<p>Listing our Containers:</p>

<pre><code class="bash">$ docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                      NAMES
e05804ab6d64        alpine:edge         "ping 127.0.0.1"         21 seconds ago      Up 4 seconds                                   mysql_debug-client_1
c052ceeb6d3b        mysql               "docker-entrypoint..."   21 seconds ago      Up 5 seconds        3306/tcp                   mysql_db_1
2b0446daab4c        adminer             "entrypoint.sh doc..."   26 seconds ago      Up 5 seconds        0.0.0.0:8080-&gt;8080/tcp     mysql_adminer_1
</code></pre>

<h2>Using the Debug Container:</h2>

<p>I will use the debug container as the client to connect to the internal services, for example, the mysql-client:</p>

<pre><code class="bash">$ apk update
$ apk add mysql-client
$ mysql -h db -u root -ppassword
MySQL [(none)]&gt;
</code></pre>

<p>Also, you will find the persistent data directory for our workspace:</p>

<pre><code class="bash">$ ls /root/workspace/
python
</code></pre>

<h2>Accessing the MySQL WebUI: Adminer</h2>

<p>Access the service via the exposed endpoint:</p>

<pre><code class="bash">+ http://localhost:8080/
</code></pre>

<p>The login view:</p>

<p><img src="https://i.snag.gy/m8dUxe.jpg" alt="" /></p>

<p>Creating the Table:</p>

<p><img src="https://i.snag.gy/tPVbg6.jpg" alt="" /></p>

<h2>Deleting the Environment:</h2>

<p>The External Resources will not be deleted:</p>

<pre><code class="bash">$ docker-compose -f mysql-compose.yml down
Removing mysql_debug-client_1 ... done
Removing mysql_db_1           ... done
Removing mysql_adminer_1      ... done
Network docknet is external, skipping
</code></pre>

<h2>Resources:</h2>

<ul>
<li><a href="https://hub.docker.com/_/mysql/">https://hub.docker.com/_/mysql/</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Creating a Nodejs Hostname App With Docker Stacks on Swarm]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/09/24/creating-a-nodejs-hostname-app-with-docker-stacks-on-swarm/"/>
    <updated>2017-09-24T17:52:51-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/09/24/creating-a-nodejs-hostname-app-with-docker-stacks-on-swarm</id>
    <content type="html"><![CDATA[<p>Create a Nodejs Application that responds GET requests with its Hostname.</p>

<p>Our nodejs application will sit beind a HAProxy Load Balancer, we are mounting the <code>docker.sock</code> from the host to the container, so as we scale our web application, our load balancer is aware of the changes, and scales as we scale our web application.</p>

<h2>Creating the Application:</h2>

<p>Our nodejs application:</p>

<pre><code class="javascript app.js">var http = require('http');
var os = require('os');
http.createServer(function (req, res) {
    res.writeHead(200, {'Content-Type': 'text/html'});
    res.end(`My Hostname: ${os.hostname()}\n`);
}).listen(8080);
</code></pre>

<p>Our Dockerfile:</p>

<pre><code class="docker Dockerfile">FROM node:alpine
ADD app.js /app.js
CMD ["node", "/app.js"]
</code></pre>

<p>Build and Push to your registry, or you could use my image on Dockerhub: <a href="https://hub.docker.com/r/rbekker87/node-containername/">hub.docker.com/r/rbekker87/node-containername</a></p>

<pre><code class="bash Build and Push">$ docker login
$ docker build -t &lt;username&gt;/&lt;repo&gt;:&lt;tag&gt; .
$ docker push  &lt;username&gt;/&lt;repo&gt;:&lt;tag&gt;
</code></pre>

<h2>Creating the Compose file</h2>

<p>Create the compose file that will define our services:</p>

<pre><code class="bash docker-compose.yml">version: '3'

services:
  node-app:
    image: rbekker87/node-containername
    networks:
      - nodenet
    environment:
      - SERVICE_PORTS=8080
    deploy:
      replicas: 20
      update_config:
        parallelism: 5
        delay: 10s
      restart_policy:
        condition: on-failure
        max_attempts: 3
        window: 120s

  loadbalancer:
    image: dockercloud/haproxy:latest
    depends_on:
      - node-app
    environment:
      - BALANCE=leastconn
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - 80:80
    networks:
      - nodenet
    deploy:
      placement:
        constraints: [node.role == manager]

networks:
  nodenet:
    driver: overlay
</code></pre>

<h2>Create the Stack:</h2>

<p>Deploy the Stack by specifying the compose file and name of our stack:</p>

<pre><code class="bash Deploy our Stack">$ docker stack deploy -c docker-compose.yml node
</code></pre>

<p>List the Services in the Stack:</p>

<pre><code class="bash List Services in our Stack">$ docker stack ls
NAME                SERVICES
node                2
</code></pre>

<p>List the Tasks in the Stack:</p>

<pre><code class="bash Tasks in our Stack">$ docker stack ps node
ID                  NAME                  IMAGE                                 NODE     DESIRED STATE       CURRENT STATE            ERROR               PORTS
l5ryfaedzzaq        node_loadbalancer.1   dockercloud/haproxy:latest            dsm-01   Running             Running 40 minutes ago
c8nrrcvek79h        node_node-app.5       rbekker87/node-containername:latest   dsm-01   Running             Running 40 minutes ago
dqii18b2q5nn        node_node-app.10      rbekker87/node-containername:latest   dsm-01   Running             Running 40 minutes ago
vkpw2rugy0ah        node_node-app.11      rbekker87/node-containername:latest   dsm-01   Running             Running 40 minutes ago
mm88nvnvy5lg        node_node-app.12      rbekker87/node-containername:latest   dsm-01   Running             Running 40 minutes ago
oyx8rfqc1xl2        node_node-app.16      rbekker87/node-containername:latest   dsm-01   Running             Running 41 minutes ago
</code></pre>

<h2>Test out our Application</h2>

<p>Test out the Service:</p>

<pre><code class="bash GET Requests">$ curl -XGET http://127.0.0.1/
My Hostname: a6e34246e73b

$ curl -XGET http://127.0.0.1/
My Hostname: 5de71278be38

$ curl -XGET http://127.0.0.1/
My Hostname: e0b7316fdd51
</code></pre>

<h2>Scaling Out:</h2>

<p>Scale our Application out to 30 replica&rsquo;s</p>

<pre><code class="bash Scaling Up">$ docker service scale node-app=30
</code></pre>

<p>Scale our Application down to 10 replica&rsquo;s</p>

<pre><code class="bash Scaling Down">$ docker service scale node-app=10
</code></pre>

<h2>Cleanup</h2>

<p>Remove the Stack:</p>

<pre><code class="bash Delete the Stack">$ docker stack rm node
Removing service node_loadbalancer
Removing service node_node-app
Removing network node_nodenet
</code></pre>

<h2>Resources:</h2>

<ul>
<li><a href="https://github.com/ruanbekker/docker-node-containername">https://github.com/ruanbekker/docker-node-containername</a></li>
<li><a href="https://hub.docker.com/r/rbekker87/node-containername/">https://hub.docker.com/r/rbekker87/node-containername/</a></li>
<li><a href="https://medium.com/@nirgn/load-balancing-applications-with-haproxy-and-docker-d719b7c5b231">Resource 1</a> + <a href="http://anokun7.github.io/microservices-demo/">Resource 2</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Create a 3 Node Elasticsearch Stack With HAProxy on Docker Swarm]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/09/24/create-a-3-node-elasticsearch-stack-with-haproxy-on-docker-swarm/"/>
    <updated>2017-09-24T15:40:19-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/09/24/create-a-3-node-elasticsearch-stack-with-haproxy-on-docker-swarm</id>
    <content type="html"><![CDATA[<p>Tried out creating a 3 node elasticsearch stack on docker swarm using docker-compose, that sits behind a haproxy service.</p>

<h2>Environment:</h2>

<p>Images:</p>

<ul>
<li><a href="https://hub.docker.com/r/dockercloud/haproxy/">dockercloud/haproxy</a></li>
<li><a href="https://github.com/ruanbekker/docker-elasticsearch">rbekker87/elasticsearch:master-5.6-alpine</a></li>
</ul>


<p>Stack:</p>

<ul>
<li>1 x haproxy</li>
<li>1 x elasticsearch master (haproxy wont send requests to this one)</li>
<li>2 x elasticsearch master/data</li>
<li>1 x esnet overlay network</li>
</ul>


<h2>Defining our Stack</h2>

<p>First we will create our compose file, which we will call <code>es-compose.yml</code>:</p>

<pre><code class="yaml">version: '3'

services:
  es-master:
    image: rbekker87/elasticsearch:master-5.6-alpine
    networks:
      - esnet
    deploy:
      replicas: 1

  es-data-1:
    image: rbekker87/elasticsearch:master-5.6-alpine
    environment:
     - SERVICE_PORTS=9200
    networks:
      - esnet
    deploy:
      replicas: 2

  es-data-2:
    image: rbekker87/elasticsearch:master-5.6-alpine
    environment:
     - SERVICE_PORTS=9200
    networks:
      - esnet
    deploy:
      replicas: 2

  loadbalancer:
    image: dockercloud/haproxy:latest
    depends_on:
      - es-data-1
      - es-data-2
    environment:
      - BALANCE=leastconn
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - 9200:80
    networks:
      - esnet
    deploy:
      placement:
        constraints: [node.role == manager]

networks:
  esnet:
    driver: overlay
</code></pre>

<p>The above compose file defines that we want a overlay network, which we will associate with all our services, 3 elasticsearch services, haproxy service which will expose port 9200, then from haproxy it has a container port of 80, which sends to the backend <code>SERVICE_PORTS</code> of each elasticsearch service.</p>

<p>We have only defined <code>SERVICE_PORTS=9200</code> on our es-data services, as I just want to proxy client connections to them.</p>

<h2>Creating our Elasticsearch Stack</h2>

<p>Now that we have our compose file ready, let&rsquo;s create our stack using <code>docker stack deploy</code>:</p>

<pre><code class="bash Create the Stack">$ docker stack deploy -c es-compose.yml analytics

Creating network analytics_esnet
Creating service analytics_loadbalancer
Creating service analytics_es-master
Creating service analytics_es-data-1
Creating service analytics_es-data-2
</code></pre>

<p>Let&rsquo;s have a look at our stack:</p>

<pre><code class="bash Docker Stack Status ">$ docker stack ps analytics
ID                  NAME                       IMAGE                                       NODE                  DESIRED STATE       CURRENT STATE            ERROR               PORTS
4t3ukxl2kch3        analytics_loadbalancer.1   dockercloud/haproxy:latest                  scw-swarm-master-01   Running             Running 27 seconds ago
jgbxtgqkg9jp        analytics_es-data-2.1      rbekker87/elasticsearch:master-5.6-alpine   scw-swarm-master-01   Running             Running 33 seconds ago
x5cq6pm7u7mn        analytics_es-data-1.1      rbekker87/elasticsearch:master-5.6-alpine   scw-swarm-master-01   Running             Running 36 seconds ago
5v22w1hvtdvm        analytics_es-master.1      rbekker87/elasticsearch:master-5.6-alpine   scw-swarm-master-01   Running             Running 38 seconds ago
</code></pre>

<p>View the logs of our haproxy service:</p>

<pre><code class="bash HAProxy Service Logs">$ docker service logs -f analytics_loadbalancer
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | INFO:haproxy:dockercloud/haproxy 1.6.7 is running outside Docker Cloud
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | INFO:haproxy:Haproxy is running in SwarmMode, loading HAProxy definition through docker api
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | INFO:haproxy:dockercloud/haproxy PID: 7
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | INFO:haproxy:=&gt; Add task: Initial start - Swarm Mode
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | INFO:haproxy:=&gt; Executing task: Initial start - Swarm Mode
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | INFO:haproxy:==========BEGIN==========
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | INFO:haproxy:Linked service: analytics_es-data-1, analytics_es-data-2, analytics_es-master
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | INFO:haproxy:Linked container: analytics_es-data-1.1.u641c5bq5vkjklk8sb1scnnlc, analytics_es-data-2.1.ic9an6bzj6aejs0lx0vzfpia6, analytics_es-master.1.h4erlgwzit509p0zehzmozy3u
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | INFO:haproxy:HAProxy configuration:
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | global
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   log 127.0.0.1 local0
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   log 127.0.0.1 local1 notice
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   log-send-hostname
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   maxconn 4096
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   pidfile /var/run/haproxy.pid
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   user haproxy
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   group haproxy
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   daemon
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   stats socket /var/run/haproxy.stats level admin
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   ssl-default-bind-options no-sslv3
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   ssl-default-bind-ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:AES128-GCM-SHA256:AES128-SHA256:AES128-SHA:AES256-GCM-SHA384:AES256-SHA256:AES256-SHA:DHE-DSS-AES128-SHA:DES-CBC3-SHA
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | defaults
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   balance leastconn
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   log global
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   mode http
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   option redispatch
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   option httplog
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   option dontlognull
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   option forwardfor
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   timeout connect 5000
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   timeout client 50000
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   timeout server 50000
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | listen stats
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   bind :1936
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   mode http
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   stats enable
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   timeout connect 10s
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   timeout client 1m
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   timeout server 1m
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   stats hide-version
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   stats realm Haproxy\ Statistics
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   stats uri /
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   stats auth stats:stats
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | frontend default_port_80
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   bind :80
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   reqadd X-Forwarded-Proto:\ http
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   maxconn 4096
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   default_backend default_service
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | backend default_service
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   server analytics_es-data-1.1.u641c5bq5vkjklk8sb1scnnlc 10.0.7.5:9200 check inter 2000 rise 2 fall 3
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   server analytics_es-data-2.1.ic9an6bzj6aejs0lx0vzfpia6 10.0.7.7:9200 check inter 2000 rise 2 fall 3
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | INFO:haproxy:Launching HAProxy
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | INFO:haproxy:HAProxy has been launched(PID: 10)
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | INFO:haproxy:===========END===========
</code></pre>

<h2>Testing Elasticsearch:</h2>

<p>Do a GET request on our HAProxy&rsquo;s Expose port: 9200</p>

<pre><code class="bash Test Elasticsearch on port 9200">$ curl -XGET http://127.0.0.1:9200
{
  "name" : "5306a0c2ee24",
  "cluster_name" : "es-cluster",
  "cluster_uuid" : "FUJmMekFQVq6zXofPCin2A",
  "version" : {
    "number" : "5.6.0",
    "build_hash" : "781a835",
    "build_date" : "2017-09-07T03:09:58.087Z",
    "build_snapshot" : false,
    "lucene_version" : "6.6.0"
  },
  "tagline" : "You Know, for Search"
}
</code></pre>

<p>Have a look at the <code>/_cat/nodes</code> API:</p>

<pre><code class="bash Get the Node Info">$  curl -XGET http://127.0.0.1:9200/_cat/nodes?v
ip       heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
10.0.7.6           28          84  14    3.09    2.28     1.49 mdi       -      56c1b0aebc5f
10.0.7.2           27          84  15    3.09    2.28     1.49 mdi       *      572c68bca904
10.0.7.4           29          84  15    3.09    2.28     1.49 mdi       -      5306a0c2ee24
</code></pre>
]]></content>
  </entry>
  
</feed>
