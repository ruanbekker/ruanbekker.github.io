<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Docker | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/docker/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2021-08-14T15:52:33-04:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A Tour With Vagrant and Virtualbox on Mac]]></title>
    <link href="https://blog.ruanbekker.com/blog/2021/08/14/a-tour-with-vagrant-and-virtualbox-on-mac/"/>
    <updated>2021-08-14T13:41:32-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2021/08/14/a-tour-with-vagrant-and-virtualbox-on-mac</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/58658188-37cec280-8320-11e9-90ca-1226b3ccb292.png" alt="vagrant" /></p>

<p><a href="https://www.vagrantup.com/">Vagrant</a>, yet another amazing product from <a href="https://www.hashicorp.com/">Hashicorp</a>.</p>

<p>Vagrant makes it really easy to provision virtual servers for local development (not limited to), which they refer as &ldquo;boxes&rdquo;, that enables developers to run their jobs/tasks/applications in a really easy and fast way. Vagrant utilizes a declarative configuration model, so you can describe which OS you want, bootstrap them with installation instructions as soon as it boots, etc.</p>

<h2>What are we doing today?</h2>

<p>When completing this tutorial, you will have Vagrant and Virtualbox installed on your Mac and should be able to launch a Ubuntu Virtual Server locally with Vagrant and using the Virtualbox provider which will be responsible for running our VM&rsquo;s.</p>

<p>We will also look at different configuration options to configure the VM, bootstrapping software, using the shell, docker and ansible provisioner.</p>

<p>For this demonstration, I am using a Mac OSX, but you can run this on Mac, Windows or Linux. First we will use Homebrew to install Virtualbox, then Vagrant, then we will provision a Ubuntu box and I will also show how to inject shell commands into your Vagrantfile so that you can provision software to your VM, and also forward traffic to a web server from the host to the guest.</p>

<p>If you are looking for a Linux version instead of mac, you can look at this post:
* <a href="https://blog.ruanbekker.com/blog/2019/05/30/use-vagrant-to-setup-a-local-development-environment-on-linux/">Use Vagrant to Setup a Local Development Environment on Linux</a></p>

<h2>Pre-Requisites</h2>

<p>I will be installing Vagrant and Virtualbox with Homebrew, if you do not have homebrew installed, you can install homebrew with:</p>

<pre><code class="bash">$ ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"
</code></pre>

<p>Once homebrew is installed, it&rsquo;s a good thing to update the indexes:</p>

<pre><code class="bash">$ brew update
</code></pre>

<h2>Virtualbox</h2>

<p>Install <a href="https://www.virtualbox.org/">VirtualBox</a> using homebrew:</p>

<pre><code class="bash">$ brew install --cask virtualbox
</code></pre>

<h2>Vagrant</h2>

<p>Install <a href="https://www.vagrantup.com/">Vagrant</a> using homebrew:</p>

<pre><code class="bash">$ brew install --cask vagrant
</code></pre>

<p>Install the virtualbox guest additions plugin for vagrant:</p>

<pre><code class="bash">$ vagrant plugin install vagrant-vbguest
</code></pre>

<p>If you would like a vagrant manager utility to help you manage your vagrant boxes, you can install <a href="http://www.vagrantmanager.com/">vagrant-manager</a> using homebrew:</p>

<pre><code class="bash">$ brew install --cask vagrant-manager
</code></pre>

<h2>Create your first Vagrant Box</h2>

<p>From <a href="https://app.vagrantup.com/boxes/search">app.vagrantup.com/boxes/search</a> you can search for any box, such as ubuntu, centos, alpine etc and for this demonstration I am going with <a href="https://app.vagrantup.com/ubuntu/boxes/focal64">ubuntu/focal64</a>.</p>

<p>I am creating a new directory for my devbox:</p>

<pre><code class="bash">$ mkdir devbox 
$ cd devbox
</code></pre>

<p>Then initialize the Vagrantfile by running:</p>

<pre><code class="bash">$ vagrant init ubuntu/focal64
</code></pre>

<p>A <code>Vagrantfile</code> has been created in the current working directory:</p>

<pre><code>$ cat Vagrantfile | grep -v "#"

Vagrant.configure("2") do |config|
  config.vm.box = "ubuntu/focal64"
end
</code></pre>

<p>Boot the VM:</p>

<pre><code class="bash">$ vagrant up
</code></pre>

<p>The box should now be in a started state, and we can verify that by running:</p>

<pre><code class="bash">$ vagrant status
Current machine states:

default                   running (virtualbox)
</code></pre>

<p>We can now SSH to our VM by running:</p>

<pre><code class="bash">$ vagrant ssh
vagrant@ubuntu-focal:~$
</code></pre>

<h2>Installing Software with Vagrant</h2>

<p>First let&rsquo;s destroy the VM that we created:</p>

<pre><code class="bash">$ vagrant destroy --force
</code></pre>

<p>Then edit the <code>Vagrantfile</code> and add the commands that we want to be executed when the VM boots, in our case, installing Nginx:</p>

<pre><code class="ruby">Vagrant.configure("2") do |config|
  config.vm.box = "ubuntu/focal64"
  config.vm.network "forwarded_port", guest: 80, host: 8080
  config.vm.provision "shell", inline: &lt;&lt;-SHELL
     apt update
     apt install nginx -y
  SHELL
end
</code></pre>

<p>You will also notice that we are forwarding port 8080 from our host, to port 80 on the VM so that we can access the webserver on port 8080 from our laptop. Then boot the VM:</p>

<pre><code>$ vagrant up
</code></pre>

<p>Once the VM has booted and installed our software, we should be able to access the index document served by Nginx on our VM:</p>

<pre><code class="bash">$ curl -I http://localhost:8080/

HTTP/1.1 200 OK
Server: nginx/1.18.0 (Ubuntu)
Date: Sat, 14 Aug 2021 18:11:59 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Sat, 14 Aug 2021 18:11:10 GMT
Connection: keep-alive
ETag: "6118073e-264"
Accept-Ranges: bytes
</code></pre>

<h2>Shared Folders</h2>

<p>Let&rsquo;s say you want to map your local directory to your VM, in a scenario where you want to store your <code>index.html</code> on your laptop and map it to the VM, we can use <code>config.vm.synced_folder</code>.</p>

<p>On our laptop, create a <code>html</code> directory where we will store our <code>index.hml</code>:</p>

<pre><code>$ mkdir html
</code></pre>

<p>Now create the content in the <code>index.html</code> under the <code>html</code> directory:</p>

<pre><code>$ echo "Hello, World" &gt; html/index.html
</code></pre>

<p>Now we need to make vagrant aware of the folder that we are mapping to the VM, so we need to edit the <code>Vagrantfile</code> and it will now look like this:</p>

<pre><code class="ruby"># -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure("2") do |config|
  config.vm.box = "ubuntu/focal64"
  config.vm.network "forwarded_port", guest: 80, host: 8080
  config.vm.provision "shell", inline: &lt;&lt;-SHELL
     apt update
     apt install nginx -y
  SHELL
  config.vm.synced_folder "html", "/var/www/html"
end
</code></pre>

<p>To reload the VM with our changes, we need to use:</p>

<pre><code class="bash">$ vagrant reload --provision
</code></pre>

<p>Once the VM is up, we can verify the changes:</p>

<pre><code>$ curl http://localhost:8080/
Hello, World
</code></pre>

<p>Now we can edit our content locally which is synced to our VM.</p>

<h2>Setting Hostname and Configure Memory</h2>

<p>We can also configure the hostname of our VM and configure the amount of memory that we want to allocate to our VM using:</p>

<ul>
<li><code>config.vm.hostname</code></li>
<li><code>vb.memory</code></li>
</ul>


<p>An example of that will look like the following:</p>

<pre><code class="ruby"># -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure("2") do |config|
  config.vm.box = "ubuntu/focal64"
  config.vm.hostname = "mydevbox"
  config.vm.network "forwarded_port", guest: 80, host: 8080
  config.vm.provision "shell", inline: &lt;&lt;-SHELL
     apt update
     apt install nginx -y
  SHELL
  config.vm.synced_folder "html", "/var/www/html"
  config.vm.provider "virtualbox" do |vb|
    vb.memory = "1024"
  end
end
</code></pre>

<p>In this example our VM&rsquo;s hostname is <code>mydevbox</code> and we assigned 1024MB of memory to our VM.</p>

<h2>Provisioners: Shell</h2>

<p>We can also run scripts from our local directory on our laptop on our VM using the <a href="https://www.vagrantup.com/docs/provisioning/shell">shell provisioner</a>.</p>

<p>First we need to create the script on our local directory:</p>

<pre><code class="bash">$ cat bootstrap.sh
#!/usr/bin/env bash
set -x
echo "my hostname is $(hostname)"
</code></pre>

<p>Then in our <code>Vagrantfile</code> we inform vagrant to execute the shell script:</p>

<pre><code class="ruby"># -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure("2") do |config|
  config.vm.box = "ubuntu/focal64"
  config.vm.hostname = "mydevbox"
  config.vm.provision :shell, :path =&gt; "bootstrap.sh"
end
</code></pre>

<p>Since my VM is already running, I will be doing a <code>reload</code>:</p>

<pre><code class="bash">$ vagrant reload --provision
...
==&gt; default: Running provisioner: shell...
    default: Running: /var/folders/04/r10yvb8d5dgfvd167jz5z23w0000gn/T/vagrant-shell20210814-70233-1p9dump.sh
    default: ++ hostname
    default: my hostname is mydevbox
    default: + echo 'my hostname is mydevbox'
</code></pre>

<p>As you can see the shell script from our local directory was executed on our VM, you can use this method to automate installations as well, etc.</p>

<h2>Provisioners: Docker</h2>

<p>Vagrant offers a <a href="https://www.vagrantup.com/docs/provisioning/docker">docker provisioner</a>, and for this example we will be hosting a mysql server using docker container in our VM.</p>

<p>Our <code>Vagrantfile</code>:</p>

<pre><code class="ruby"># -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure("2") do |config|
  config.vm.box = "ubuntu/focal64"
  config.vm.hostname = "mydevbox"
  config.vm.network "forwarded_port", guest: 3306, host: 3306
  config.vm.provision "docker" do |d|
    d.run "mysql", image: "mysql:8.0",
      args: "-p 3306:3306 -e MYSQL_ROOT_PASSWORD=password"
  end
end
</code></pre>

<p>Since I don&rsquo;t have port <code>3306</code> listening locally, I have mapped port <code>3306</code> from my laptop to port <code>3306</code> on my VM and I am using the <code>mysql:8.0</code> container image from docker hub and passing the arguments which is specific to the container.</p>

<p>The convenient thing about the docker provisioner, is that it will install docker onto the VM for you.</p>

<p>Once the config has been set in your <code>Vagrantfile</code> do a reload:</p>

<pre><code class="bash">$ vagrant reload --provision
...
    default: /vagrant =&gt; /Users/ruanbekker/workspace/vagrant/devbox
==&gt; default: Running provisioner: docker...
    default: Installing Docker onto machine...
==&gt; default: Starting Docker containers...
==&gt; default: -- Container: mysql
</code></pre>

<p>From our laptop we should be able to communicate with our mysql server:</p>

<pre><code class="bash">$ nc -vz localhost 3306
found 0 associations
found 1 connections:
     1: flags=82&lt;CONNECTED,PREFERRED&gt;
    outif lo0
    src 127.0.0.1 port 58745
    dst 127.0.0.1 port 3306
    rank info not available
    TCP aux info available

Connection to localhost port 3306 [tcp/mysql] succeeded!
</code></pre>

<p>We can also SSH to our VM and verify if the container is running:</p>

<pre><code class="bash">$ vagrant ssh
</code></pre>

<p>And then list the containers:</p>

<pre><code class="bash">$  docker ps
CONTAINER ID   IMAGE       COMMAND                  CREATED         STATUS         PORTS                                                  NAMES
30a843a486ae   mysql:8.0   "docker-entrypoint.sh    2 minutes ago   Up 2 minutes   0.0.0.0:3306-&gt;3306/tcp, :::3306-&gt;3306/tcp, 33060/tcp   mysql
</code></pre>

<h2>Provisioners: Ansible</h2>

<p>We can also execute <a href="https://www.ansible.com/">Ansible</a> playbooks on our VM using the <a href="https://www.vagrantup.com/docs/provisioning/ansible">Ansible Provisioner</a>.</p>

<p>Something to note is that we use <code>ansible</code> to execute the playbook on the host, and <code>ansible_local</code> to execute the playbook on the VM.</p>

<p>First we will create our <a href="https://docs.ansible.com/playbooks_best_practices.html#directory-layout">project structure</a> for ansible, so that we have the following in place:</p>

<pre><code class="bash">.
Vagrantfile
provisioning/playbook.yml
provisioning/group_vars/all
</code></pre>

<p>Create the <code>provisioning</code> directory:</p>

<pre><code class="bash">$ mkdir provisioning
</code></pre>

<p>Then the content for our <code>provisioning/playbook.yml</code> playbook:</p>

<pre><code class="yaml">---
- hosts: all
  become: yes
  tasks:
    - name: ensure ntpd is at the latest version
      apt:
        pkg: ntp
        state: ""
      notify:
      - restart ntpd
  handlers:
    - name: restart ntpd
      service:
        name: ntp
        state: restarted
</code></pre>

<p>Our <code>provisioning/group_vars/all</code> file that will contain the variables for the all group:</p>

<pre><code class="yaml">desired_state: "latest"
</code></pre>

<p>In our <code>Vagrantfile</code>:</p>

<pre><code class="ruby"># -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure("2") do |config|
  config.vm.box = "ubuntu/focal64"
  config.vm.hostname = "mydevbox"
  config.vm.provision :ansible do |ansible|
    ansible.playbook = "provisioning/playbook.yml"
  end
end
</code></pre>

<p>When using ansible with vagrant the inventory is <a href="https://www.vagrantup.com/docs/provisioning/ansible_intro#auto-generated-inventory">auto-generated</a> when then inventory is not specified. Vagrant will store the inventory on the host at <code>.vagrant/provisioners/ansible/inventory/vagrant_ansible_inventory</code>.</p>

<p>To execute playbooks with ansible, we need ansible installed on our host machine, for this demonstration I will be using virtualenv and then install ansible using pip:</p>

<pre><code class="bash">$ python3 -m pip install virtualenv
$ virtualenv -p $(which python3) .venv
$ source .venv/bin/activate
$ pip install ansible
</code></pre>

<p>Now that we have ansible installed, reload the VM to execute the playbook on our VM:</p>

<pre><code class="bash">$ vagrant reload --provision
...
==&gt; default: Running provisioner: ansible...
    default: Running ansible-playbook...

PLAY [all] *********************************************************************

TASK [Gathering Facts] *********************************************************
ok: [default]

TASK [ensure ntpd is at the latest version] ************************************
ok: [default]

PLAY RECAP *********************************************************************
default                    : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
</code></pre>

<p>Pretty neat right?</p>

<h2>Tear Down</h2>

<p>To destroy the VM:</p>

<pre><code class="bash">$ vagrant destroy --force
</code></pre>

<h2>Resources</h2>

<p>For more information on vagrant, check out their documentation:</p>

<ul>
<li><a href="https://www.vagrantup.com/docs">https://www.vagrantup.com/docs</a></li>
</ul>


<p>On provisioning documentation:</p>

<ul>
<li><a href="https://www.vagrantup.com/docs/provisioning/shell">https://www.vagrantup.com/docs/provisioning/shell</a></li>
<li><a href="https://www.vagrantup.com/docs/provisioning/docker">https://www.vagrantup.com/docs/provisioning/docker</a></li>
<li><a href="https://www.vagrantup.com/docs/provisioning/ansible_intro">https://www.vagrantup.com/docs/provisioning/ansible_intro</a></li>
</ul>


<p>I have a couple of example <code>Vagrantfile</code>s available on my github repository:</p>

<ul>
<li><a href="https://github.com/ruanbekker/vagrantfiles">https://github.com/ruanbekker/vagrantfiles</a></li>
</ul>


<h2>Thank You</h2>

<p>Thanks for reading, if you like my content, check out my <strong><a href="https://ruan.dev">website</a></strong> or follow me at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CICD With DroneCI and Gitea Using Docker Compose]]></title>
    <link href="https://blog.ruanbekker.com/blog/2021/03/09/cicd-with-droneci-and-gitea-using-docker-compose/"/>
    <updated>2021-03-09T01:10:10-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2021/03/09/cicd-with-droneci-and-gitea-using-docker-compose</id>
    <content type="html"><![CDATA[<p>In this post we wil set up a drone-ci and gitea stack using docker-compose and then running a test pipeline.</p>

<p>I have posted a few times about this topic, but this post will be used when I create other examples and wanting to use this post for the ones not having the stack booted yet.</p>

<h2>The Source Code</h2>

<p>All the code will be in my <a href="https://github.com/ruanbekker/drone-gitea-on-docker">github repository</a>.</p>

<p>For our <code>docker-compose.yml</code>:</p>

<pre><code>version: '3.6'

services:
  gitea:
    container_name: gitea
    image: gitea/gitea:${GITEA_VERSION:-1.10.6}
    restart: unless-stopped
    environment:
      # https://docs.gitea.io/en-us/install-with-docker/#environments-variables
      - APP_NAME="Gitea"
      - USER_UID=1000
      - USER_GID=1000
      - RUN_MODE=prod
      - DOMAIN=${IP_ADDRESS}
      - SSH_DOMAIN=${IP_ADDRESS}
      - HTTP_PORT=3000
      - ROOT_URL=http://${IP_ADDRESS}:3000
      - SSH_PORT=222
      - SSH_LISTEN_PORT=22
      - DB_TYPE=sqlite3
    ports:
      - "3000:3000"
      - "222:22"
    networks:
      - cicd_net
    volumes:
      - ./gitea:/data

  drone:
    container_name: drone
    image: drone/drone:${DRONE_VERSION:-1.6.4}
    restart: unless-stopped
    depends_on:
      - gitea
    environment:
      # https://docs.drone.io/server/provider/gitea/
      - DRONE_DATABASE_DRIVER=sqlite3
      - DRONE_DATABASE_DATASOURCE=/data/database.sqlite
      - DRONE_GITEA_SERVER=http://${IP_ADDRESS}:3000/
      - DRONE_GIT_ALWAYS_AUTH=false
      - DRONE_RPC_SECRET=${DRONE_RPC_SECRET}
      - DRONE_SERVER_PROTO=http
      - DRONE_SERVER_HOST=${IP_ADDRESS}:3001
      - DRONE_TLS_AUTOCERT=false
      - DRONE_USER_CREATE=${DRONE_USER_CREATE}
      - DRONE_GITEA_CLIENT_ID=${DRONE_GITEA_CLIENT_ID}
      - DRONE_GITEA_CLIENT_SECRET=${DRONE_GITEA_CLIENT_SECRET}
    ports:
      - "3001:80"
      - "9001:9000"
    networks:
      - cicd_net
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./drone:/data

  drone-runner:
    container_name: drone-runner
    image: drone/drone-runner-docker:${DRONE_RUNNER_VERSION:-1}
    restart: unless-stopped
    depends_on:
      - drone
    environment:
      # https://docs.drone.io/runner/docker/installation/linux/
      # https://docs.drone.io/server/metrics/
      - DRONE_RPC_PROTO=http
      - DRONE_RPC_HOST=drone
      - DRONE_RPC_SECRET=${DRONE_RPC_SECRET}
      - DRONE_RUNNER_NAME="${HOSTNAME}-runner"
      - DRONE_RUNNER_CAPACITY=2
      - DRONE_RUNNER_NETWORKS=cicd_net
      - DRONE_DEBUG=false
      - DRONE_TRACE=false
    ports:
      - "3002:3000"
    networks:
      - cicd_net
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock

networks:
  cicd_net:
    name: cicd_net
</code></pre>

<p>Our <code>boot.sh</code> which we will use to override environment variables:</p>

<pre><code>#!/usr/bin/env bash

export HOSTNAME=$(hostname)
export DRONE_VERSION=1.10.1
export DRONE_RUNNER_VERSION=1.6.3
export GITEA_VERSION=1.13
export IP_ADDRESS=192.168.0.6
export MINIO_ACCESS_KEY="EXAMPLEKEY"
export MINIO_SECRET_KEY="EXAMPLESECRET"
export GITEA_ADMIN_USER="example"
export DRONE_RPC_SECRET="$(echo ${HOSTNAME} | openssl dgst -md5 -hex)"
export DRONE_USER_CREATE="username:${GITEA_ADMIN_USER},machine:false,admin:true,token:${DRONE_RPC_SECRET}"
export DRONE_GITEA_CLIENT_ID=""
export DRONE_GITEA_CLIENT_SECRET=""
docker-compose up -d

echo ""
echo "Gitea: http://${IP_ADDRESS}:3000/"
echo "Drone: http://${IP_ADDRESS}:3001/"
</code></pre>

<h2>Deploy the Stack</h2>

<p>Set the following in your <code>boot.sh</code>:</p>

<pre><code>IP_ADDRESS=192.168.0.6       -&gt; either reachable dns or ip address which will be your clone address and ui addresses.
GITEA_ADMIN_USER="giteauser" -&gt; will be the user you register with in drone
</code></pre>

<p>Now boot the stack:</p>

<pre><code>$ bash boot.sh
</code></pre>

<p><em>Note</em>: Theres a <a href="https://github.com/go-gitea/gitea/issues/7702">current issue</a> where webhooks get fired twice, if you see that just restart gitea with <code>docker restart gitea</code>.</p>

<ul>
<li><p>Head over to: <code>http://${IP_ADDRESS}:3000/user/settings/applications</code> and create a new OAuth2 Application and set the Redirect URI to <code>http://${IP_ADDRESS}:3001/login</code></p></li>
<li><p>Capture the client id and client secret and populate them in the <code>boot.sh</code> in <code>DRONE_GITEA_CLIENT_ID</code> and <code>DRONE_GITEA_CLIENT_SECRET</code> and run <code>bash boot.sh</code> again. This will give drone the correct credentials in order to authenticate with gitea.</p></li>
<li><p>Now when you head over to <code>http://${IP_ADDRESS}:3001/</code> you will be asked to authorize the application and you should be able to access drone.</p></li>
</ul>


<h2>Drone CLI</h2>

<p>Install Drone CLI:
- <a href="https://docs.drone.io/cli/install/">https://docs.drone.io/cli/install/</a></p>

<pre><code>$ curl -L https://github.com/drone/drone-cli/releases/latest/download/drone_darwin_amd64.tar.gz | tar zx
$ sudo mv drone /usr/local/bin/drone
$ chmod +x /usr/local/bin/drone
</code></pre>

<p>Get your Drone Token:
- <a href="http://$">http://$</a>{IP_ADDRESS}:3001/account</p>

<pre><code>$ export DRONE_SERVER=http://${IP_ADDRESS}:3001
$ export DRONE_TOKEN=one-from-the-account-page
drone info
</code></pre>

<h2>Build your first pipeline</h2>

<p>Create a test repo in gitea:</p>

<p><img src="https://user-images.githubusercontent.com/567298/110296470-0ad23800-7ffb-11eb-8428-af49d0ebd62d.png" alt="image" /></p>

<p>Commit a <code>.drone.yml</code> file for drone:</p>

<pre><code>kind: pipeline
type: docker
name: hello-world

trigger:
  branch:
    - master
  event:
    - push

steps:
  - name: say-hello
    image: busybox
    commands:
      - echo hello-world
</code></pre>

<p>Head over to drone and sync your repositories:</p>

<p><img src="https://user-images.githubusercontent.com/567298/110296425-00b03980-7ffb-11eb-9216-76725a62c09e.png" alt="image" /></p>

<p>Activate your repository:</p>

<p><img src="https://user-images.githubusercontent.com/567298/110296623-3523f580-7ffb-11eb-805f-db5db4dab0cb.png" alt="image" /></p>

<p>Push a commit to master and see your pipeline running:</p>

<p><img src="https://user-images.githubusercontent.com/567298/110296747-584ea500-7ffb-11eb-9909-259641a663aa.png" alt="image" /></p>

<h2>More Examples</h2>

<p>For more examples view my example section on the github repository:
- <a href="https://github.com/ruanbekker/drone-gitea-on-docker#more-examples">https://github.com/ruanbekker/drone-gitea-on-docker#more-examples</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ship Your Docker Logs to Loki Using Fluentbit]]></title>
    <link href="https://blog.ruanbekker.com/blog/2021/02/26/ship-your-docker-logs-to-loki-using-fluentbit/"/>
    <updated>2021-02-26T15:26:34-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2021/02/26/ship-your-docker-logs-to-loki-using-fluentbit</id>
    <content type="html"><![CDATA[<p>In this tutorial, I will show you how to ship your docker containers logs to <a href="https://grafana.com/oss/loki/">Grafana Loki</a> via <a href="https://fluentbit.io/">Fluent Bit</a>.</p>

<h2>Grafana and Loki</h2>

<p>First we need to get Grafana and Loki up and running and we will be using docker and docker-compose to do that.</p>

<p>Our <code>docker-compose-loki.yml</code>:</p>

<pre><code>version: "3.7"

services:
  grafana:
    image: grafana/grafana:7.4.2
    container_name: 'grafana'
    restart: unless-stopped
    volumes:
      - ./data/grafana/data:/var/lib/grafana
      - ./configs/grafana/datasource.yml:/etc/grafana/provisioning/datasources/datasource.yml
    networks:
      - public
    ports:
      - 3000:3000
    depends_on:
      - loki
    logging:
      driver: "json-file"
      options:
        max-size: "1m"  

  loki:
    image: grafana/loki:2.1.0
    container_name: loki
    command: -config.file=/mnt/loki-local-config.yaml
    user: root
    restart: unless-stopped
    volumes:
      - ./data/loki/data:/tmp/loki
      - ./configs/loki/loki.yml:/mnt/loki-local-config.yaml
    ports:
      - 3100:3100
    networks:
      - public
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

networks:
  public:
    name: public
</code></pre>

<p>We are referencing 2 config files, first our loki datasource defined by <code>./configs/grafana/datasource.yml</code>:</p>

<pre><code>apiVersion: 1

datasources:
- name: loki
  type: loki
  access: proxy
  orgId: 1
  url: http://loki:3100
  basicAuth: false
  isDefault: true
  version: 1
  editable: true
</code></pre>

<p>And our second config is our loki config <code>./configs/loki/loki.yml</code>:</p>

<pre><code>auth_enabled: false

server:
  http_listen_port: 3100

ingester:
  lifecycler:
    address: 127.0.0.1
    ring:
      kvstore:
        store: inmemory
      replication_factor: 1
    final_sleep: 0s
  chunk_idle_period: 5m
  chunk_retain_period: 30s
  max_transfer_retries: 0

schema_config:
  configs:
    - from: 2018-04-15
      store: boltdb
      object_store: filesystem
      schema: v11
      index:
        prefix: index_
        period: 168h

storage_config:
  boltdb:
    directory: /tmp/loki/index

  filesystem:
    directory: /tmp/loki/chunks

limits_config:
  enforce_metric_name: false
  reject_old_samples: true
  reject_old_samples_max_age: 168h

chunk_store_config:
  max_look_back_period: 0s

table_manager:
  retention_deletes_enabled: false
  retention_period: 0s
</code></pre>

<p>Once you have everything in place, boot the grafana and loki containers:</p>

<pre><code>$ docker-compose -f docker-compose-loki.yml up -d
</code></pre>

<h2>Fluent Bit</h2>

<p>Next we need to boot our log processor and forwarder, fluent bit. In our <code>docker-compose-fluentbit.yml</code>:</p>

<pre><code>version: "3.7"

services:
  fluent-bit:
    image: grafana/fluent-bit-plugin-loki:latest
    container_name: fluent-bit
    environment:
      - LOKI_URL=http://loki:3100/loki/api/v1/push
    volumes:
      - ./fluent-bit.conf:/fluent-bit/etc/fluent-bit.conf
    ports:
      - "24224:24224"
      - "24224:24224/udp"
    networks:
      - public

networks:
  public:
    name: public
</code></pre>

<p>And as you can see we are referencing a config <code>./configs/fluentbit/fluent-bit.conf</code>:</p>

<pre><code>[INPUT]
    Name        forward
    Listen      0.0.0.0
    Port        24224
[Output]
    Name grafana-loki
    Match *
    Url ${LOKI_URL}
    RemoveKeys source,container_id
    Labels {job="fluent-bit"}
    LabelKeys container_name
    BatchWait 1s
    BatchSize 1001024
    LineFormat json
    LogLevel info
</code></pre>

<p>Once you have your configs in place, boot fluent-bit:</p>

<pre><code>$ docker-compose -f docker-compose-fluentbit.yml up -d
</code></pre>

<h2>Nginx App</h2>

<p>Now to configure our docker container to ship its logs to fluent-bit, which will forward the logs to Loki.</p>

<p>In our <code>docker-compose-app.yml</code>:</p>

<pre><code>version: "3"

services:
  nginx-json:
    image: ruanbekker/nginx-demo:json
    container_name: nginx-app
    ports:
      - 8080:80
    logging:
      driver: fluentd
      options:
        fluentd-address: 127.0.0.1:24224
</code></pre>

<p>The fluent-bit container listens on port 24224 locally on our docker host and is not reachable via its container network, so let&rsquo;s boot our application:</p>

<pre><code>$ docker-compose -f docker-compose-app.yml up -d
</code></pre>

<p>Once our application is up, let&rsquo;s make a request to our nginx-app:</p>

<pre><code>$ curl http://localhost:8080/
ok
</code></pre>

<p>Now head over to Grafana at <a href="http://localhost:3000/explore">http://localhost:3000/explore</a> and query: <code>{job="fluent-bit", container_name="/nginx-app"}</code> and you should see something like this:</p>

<p><img src="https://user-images.githubusercontent.com/567298/109366000-03908900-789b-11eb-952e-36ff23657517.png" alt="image" /></p>

<p>Beautiful right? I know.</p>

<h2>Github Repo</h2>

<p>The source code for this can be found on:</p>

<ul>
<li><a href="https://github.com/ruanbekker/docker-logging-loki-fuentbit">https://github.com/ruanbekker/docker-logging-loki-fuentbit</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Reduce Docker Log Size on Disk]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/12/23/reduce-docker-log-size-on-disk/"/>
    <updated>2020-12-23T04:11:35-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/12/23/reduce-docker-log-size-on-disk</id>
    <content type="html"><![CDATA[<p>In cases where you are using the defaults for logging and your application logs a lot you can consume a lot of disk space and you can run out of disk space quite quickly.</p>

<p>If it&rsquo;s a case where you already ran out of disk space, we can investigate the disk space consumed by docker logs:</p>

<pre><code>$ cd /var/lib/docker/containers
$ du -sh *
6.0G    14052251a0f13f46f65bc73d10c01408130ee8ae71529600ba5bd6bee76af4ee
1.2G    e6b40b1d30c5cf05e8cb201ca9abf6bd283d7cf7ceaa3be2a0422be7cd750a33
</code></pre>

<p>Referenced from <a href="https://blog.birkhoff.me/devops-truncate-docker-container-logs-periodically-to-free-up-server-disk-space/">https://blog.birkhoff.me/devops-truncate-docker-container-logs-periodically-to-free-up-server-disk-space/</a> you can truncate those files:</p>

<pre><code>$ sh -c 'truncate -s 0 /var/lib/docker/containers/*/*-json.log'
</code></pre>

<p>Check the size again:</p>

<pre><code>$ du -sh *
40K 14052251a0f13f46f65bc73d10c01408130ee8ae71529600ba5bd6bee76af4ee
36K e6b40b1d30c5cf05e8cb201ca9abf6bd283d7cf7ceaa3be2a0422be7cd750a33
</code></pre>

<p>To overcome this issue you can use this in logging options in your compose:</p>

<pre><code>...
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
...
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HTTPS for Local Development With MiniCA]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/12/23/https-for-local-development-with-minica/"/>
    <updated>2020-12-23T03:11:08-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/12/23/https-for-local-development-with-minica</id>
    <content type="html"><![CDATA[<p>In this tutorial we will use <a href="https://github.com/jsha/minica">minica</a> to enable us to run our web applications over HTTPS for local development.</p>

<p>To read more about about <a href="https://github.com/jsha/minica">minica</a> check out their website.</p>

<h2>Generate Certificates</h2>

<p>You can use their binary from their github page or use my docker image to generate the certificates to a <code>./certs</code> directory:</p>

<pre><code>$ docker run --user "$(id -u):$(id -g)" -it -v $PWD/certs:/output ruanbekker/minica --domains 192.168.0.20.nip.io
</code></pre>

<p>In the case from above, we are generating certificates for the FQDN <code>192.168.0.20.nip.io</code>. You will find the generated certificates under <code>./certs/</code>.</p>

<h2>Application Stack</h2>

<p>We will use docker to create a nginx webserver to serve our content via https using the generated vertificates.</p>

<p>Our <code>docker-compose.yml</code>:</p>

<pre><code>version: '3.7'
services:
  nginx:
    image: nginx
    container_name: nginx
    ports:
      - 80:80
      - 443:443
    volumes:
      - ~/personal/docker-minica-nginx/nginx.conf:/etc/nginx/nginx.conf
      - ~/personal/docker-minica-nginx/ssl.conf:/etc/nginx/conf.d/ssl.conf
      - ~/personal/docker-minica-nginx/certs/192.168.0.6.nip.io:/etc/nginx/certs
      - ~/personal/docker-minica-nginx/html/index.html:/usr/share/nginx/html/index.html
</code></pre>

<p>Our <code>nginx.conf</code>:</p>

<pre><code>user  nginx;
worker_processes  1;
error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;

events {
    worker_connections  1024;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

    access_log  /var/log/nginx/access.log  main;

    sendfile        on;
    keepalive_timeout  65;
    include /etc/nginx/conf.d/ssl.conf;
}
</code></pre>

<p>Our <code>ssl.conf</code>:</p>

<pre><code>server {
    listen 80;
    server_name 192.168.0.6.nip.io;
    return 301 https://$host$request_uri;
}

server {
    listen 443 ssl;
    server_name 192.168.0.6.nip.io;

    ssl_certificate /etc/nginx/certs/cert.pem;
    ssl_certificate_key /etc/nginx/certs/key.pem;

    location / {
        root   /usr/share/nginx/html;
        index  index.html;
    }
}
</code></pre>

<p>Our <code>html/index.html</code>:</p>

<pre><code>&lt;!DOCTYPE html&gt;
&lt;html lang="en-us"&gt;
&lt;head&gt;
    &lt;meta charset="utf-8"&gt;
    &lt;link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous"&gt;
    &lt;script src="https://code.jquery.com/jquery-3.1.1.min.js" crossorigin="anonymous"&gt;&lt;/script&gt;
    &lt;title&gt;Sample Page&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;div class="container-fluid"&gt;
        &lt;div class="row"&gt;
            &lt;div class="bitProcessor"&gt;&lt;/div&gt;
            &lt;div class="col-md-12" style="background-color: white; position: absolute; top: 40%;width: 80%;left: 10%;"&gt;
                &lt;center&gt;
                    &lt;h1&gt;Hello, World!&lt;/h1&gt;
                    &lt;p&gt;This is sample text.&lt;/p&gt;
                &lt;/center&gt;
            &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>

<h2>Import Certificates</h2>

<p>We have a certificate <code>./certs/minica.pem</code> which we need to import and trust on our local workstation, I am using a Mac so it will be Keychain Access.</p>

<p><img src="https://user-images.githubusercontent.com/567298/101961866-5a2ee500-3c13-11eb-9f89-03fa1bd4670d.png" alt="image" /></p>

<p>Once you open Keychain Access, select &ldquo;file&rdquo;, &ldquo;import items&rdquo; and browse and import <code>./certs/minica.pem</code>, once you are done search for minica:</p>

<p><img src="https://user-images.githubusercontent.com/567298/101962064-d4f80000-3c13-11eb-9479-c043ba3ced2c.png" alt="image" /></p>

<p>Select the item, file -> get info, expand trust, change &ldquo;when using this certificate&rdquo; to Always trust and close.</p>

<p>You will now see the root ca is trusted:</p>

<p><img src="https://user-images.githubusercontent.com/567298/101962197-2dc79880-3c14-11eb-8d26-49874c9703fa.png" alt="image" /></p>

<h2>Boot the Application Stack</h2>

<p>As we have <code>docker-compose.yml</code> in our current working directory, we can use docker-compose to boot our application:</p>

<pre><code>$ docker-compose up
Creating network "docker-minica-nginx_default" with the default driver
Creating nginx ... done
Attaching to nginx
</code></pre>

<p>Now when we browse to <code>https://192.168.0.6.nip.io</code> we will see:</p>

<p><img src="https://user-images.githubusercontent.com/567298/101962367-a9c1e080-3c14-11eb-898b-688b50c1b9db.png" alt="image" /></p>

<p>And when we inspect the certificate, we can see its valid:</p>

<p><img src="https://user-images.githubusercontent.com/567298/101962411-c78f4580-3c14-11eb-80cd-cf8e449eca95.png" alt="image" /></p>

<h2>Thank You</h2>

<p>Thank you for reading.</p>
]]></content>
  </entry>
  
</feed>
