<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Docker | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/docker/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2020-10-29T08:24:36+00:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Setup a NFS Server With Docker]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/09/20/setup-a-nfs-server-with-docker/"/>
    <updated>2020-09-20T16:07:09+00:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/09/20/setup-a-nfs-server-with-docker</id>
    <content type="html"><![CDATA[<p>In this tutorial we will setup a <strong>NFS Server</strong> using <strong>Docker</strong> for our development environment.</p>

<h2>Host Storage Path</h2>

<p>In this example we will be using our host path <code>/data/nfs-storage</code> which will host our storage for our NFS server, which will will mount to the container:</p>

<pre><code>$ mkdir -p /data/nfs-storage
</code></pre>

<h2>NFS Server</h2>

<p>Create the NFS Server with docker:</p>

<pre><code>$ docker run -itd --privileged \
  --restart unless-stopped \
  -e SHARED_DIRECTORY=/data \
  -v /data/nfs-storage:/data \
  -p 2049:2049 \
  itsthenetwork/nfs-server-alpine:12
</code></pre>

<p>We can do the same using docker-compose, for our <code>docker-compose.yml</code>:</p>

<pre><code>version: "2.1"
services:
  # https://hub.docker.com/r/itsthenetwork/nfs-server-alpine
  nfs:
    image: itsthenetwork/nfs-server-alpine:12
    container_name: nfs
    restart: unless-stopped
    privileged: true
    environment:
      - SHARED_DIRECTORY=/data
    volumes:
      - /data/nfs-storage:/data
    ports:
      - 2049:2049
</code></pre>

<p>To deploy using docker-compose:</p>

<pre><code>$ docker-compose up -d
</code></pre>

<h2>NFS Client</h2>

<p>To use a NFS Client to mount this to your filesystem, you can look at <a href="https://blog.ruanbekker.com/blog/2017/12/05/setup-a-nfs-server-on-a-raspberrypi/" rel="nofollow" target="_blank">this blogpost></a></p>

<p>In summary:</p>

<pre><code>$ sudo apt install nfs-client -y
$ sudo mount -v -o vers=4,loud 192.168.0.4:/ /mnt
</code></pre>

<p>Verify that the mount is showing:</p>

<pre><code>$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda2       109G   53G   51G  52% /
192.168.0.4:/   4.5T  2.2T  2.1T  51% /mnt
</code></pre>

<p>Now, create a test file on our NFS export:</p>

<pre><code>$ touch /mnt/file.txt
</code></pre>

<p>Verify that the test file is on the local path:</p>

<pre><code>$ ls /data/nfs-storage/
file.txt
</code></pre>

<p>If you want to load this into other client&rsquo;s <code>/etc/fstab</code>:</p>

<pre><code>192.168.0.4:/   /mnt   nfs4    _netdev,auto  0  0
</code></pre>

<h2>NFS Docker Volume Plugin</h2>

<p>You can use a NFS Volume Plugin for Docker or Docker Swarm for persistent container storage.</p>

<p>To use the NFS Volume plugin, we need to download <a href="https://github.com/ContainX/docker-volume-netshare/releases" target="_blank" rel="nofollow">docker-volume-netshare</a> from their github releases page.</p>

<pre><code>$ wget https://github.com/ContainX/docker-volume-netshare/releases/download/v0.36/docker-volume-netshare_0.36_amd64.deb
$ dpkg -i docker-volume-netshare_0.36_amd64.deb
$ service docker-volume-netshare start
</code></pre>

<p>Then your <code>docker-compose.yml</code>:</p>

<pre><code>version: '3.7'

services:
  mysql:
    image: mariadb:10.1
    networks:
      - private
    environment:
      - MYSQL_ROOT_PASSWORD=${DATABASE_PASSWORD:-admin}
      - MYSQL_DATABASE=testdb
      - MYSQL_USER=${DATABASE_USER:-admin}
      - MYSQL_PASSWORD=${DATABASE_PASSWORD:-admin}
    volumes:
      - mysql_data.vol:/var/lib/mysql

volumes:
  mysql_data.vol:
    driver: nfs
    driver_opts:
      share: 192.168.69.1:/mysql_data_vol
</code></pre>

<h2>Thank You</h2>

<p>That&rsquo;s it. Thanks for reading, follow me on Twitter and say hi! <a href="https://twitter.com/ruanbekker" rel="nofollow" target="_blank"><strong>@ruanbekker</strong></a></p><p><a href="https://saythanks.io/to/ruan.ru.bekker@gmail.com" rel="nofollow" target="_blank"><img src="https://svgshare.com/i/Pfy.svg" alt="Say Thanks!"></a></p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started on Logging With Loki Using Docker]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/08/13/getting-started-on-logging-with-loki-using-docker/"/>
    <updated>2020-08-13T13:39:28+00:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/08/13/getting-started-on-logging-with-loki-using-docker</id>
    <content type="html"><![CDATA[<p>Logging with Loki is AMAZING!</p>

<p>In the past couple of months i&rsquo;ve been working a lot with logging, but more specifically logging with loki. As most of my metrics reside in prometheus, I use grafana quite extensively and logging was always the one that stood out a bit as I pushed my logs to elasticsearch and consumed them from grafana. Which worked pretty well, but the maintenance and resource costs was a bit too much for what I was looking for.</p>

<p>And then grafana released Loki, which is like prometheus, but for logs. And that was just super, exactly what I was looking for. For my use case, I was looking for something that can be consumed by grafana as a presentation layer, central based so I can push all sorts of logs, and want a easy way to grep for logs and a bonus would be to have a cli tool.</p>

<p>And Loki checked all those boxes!</p>

<div class="tenor-gif-embed" data-postid="7644619" data-share-method="host" data-width="100%" data-aspect-ratio="1.1971153846153846"><a href="https://tenor.com/view/oh-yeah-gif-7644619">Oh Yeah Parks And Recreation GIF</a> from <a href="https://tenor.com/search/ohyeah-gifs">Ohyeah GIFs</a></div>


<script type="text/javascript" async src="https://tenor.com/embed.js"></script>


<h2>What can you expect from this blog</h2>

<p>In this post will be a getting started guide to Loki, we will provision Loki, Grafana and Nginx using Docker to get our environment up and running, so that we can push our nginx container logs to the loki datasource, and access the logs via grafana.</p>

<p>We will then generate some logs so that we can show a couple of query examples using the log query language (LogQL) and use the LogCLI to access our logs via cli.</p>

<p>In a <a href="">future post</a>, I will demonstrate how to setup Loki for a non-docker deployment.</p>

<h2>Some useful information about Loki</h2>

<p>Let&rsquo;s first talk about Loki compared with Elasticsearch, as they are not the same:</p>

<ol>
<li>Loki does not index the text of the logs, instead grouping entries into streams and index those with labels</li>
<li>Things like full text search engines tokenizes your text into k/v pairs and gets written to an inverted index, which over time in my opinion gets complex to maintain, expensive to scale, storage retention, etc.</li>
<li>Loki is advertised as easy to scale, affordable to operate as it uses DynamoDB for Indexing and S3 for Storage</li>
<li>When using Loki, you may need to forget what you know and look to see how the problem can be solved differently with parallelization. Lokiâ€™s superpower is breaking up queries into small pieces and dispatching them in parallel so that you can query huge amounts of log data in small amounts of time.</li>
</ol>


<p>If we look at the <strong>Loki Log Model</strong>, we can see that the timestamp and the labels are indexed and the content of the logs are not indexed:</p>

<p><img src="https://img.sysadmins.co.za/cpr6n7.png" alt="loki" /></p>

<p>A <strong>log stream</strong> is a stream of log entries with the same exact label set:</p>

<p><img src="https://img.sysadmins.co.za/el6djk.png" alt="loki" /></p>

<p>For the <strong>storage</strong> side, inside each chunk, log entries are sorted by timestamp. Loki only indexes minimum and maximum timestamps of a chunk. Storage options support local storage, AWS S3, Google Cloud Storage and Azure</p>

<p><img src="https://img.sysadmins.co.za/959pjw.png" alt="loki" /></p>

<p>For <strong>chunks and querying</strong>, chunks are filled per stream and they are flushed of a few criterias such as age and size:</p>

<p><img src="https://img.sysadmins.co.za/ekm8cy.png" alt="loki" /></p>

<p>And one of the most important parts are the <strong>labels</strong>, labels define the stream and therefore its very important.</p>

<p>High cardinality is bad for labels, as something like a IP address can reduce your performance a lot, as it will create a stream for every unique IP label.</p>

<p>Static defined labels such as environment, hostnames are good, you can read more up about it <a href="https://grafana.com/blog/2020/04/21/how-labels-in-loki-can-make-log-queries-faster-and-easier/">here</a></p>

<p>Here is a info graphic on how one log line can be split up into 36 streams:</p>

<p><img src="https://img.sysadmins.co.za/g119oq.png" alt="" /></p>

<p>So with that being said, <strong>good labels</strong> will be considered as cluster, job, namespace, environment, etc where as <strong>bad labels</strong> will be things such as userid, ip address, url path, etc</p>

<h2>Selecting logstreams with Loki</h2>

<p>Selecting logstreams, is done by using <strong>label matchers</strong> and <strong>filter expressions</strong>, such as this example:</p>

<pre><code>{job="dockerlogs", environment="development"} |= "POST" |~ "196.35.64.+"
</code></pre>

<p>Label Matchers and Filter Expressions support:</p>

<ul>
<li><code>=</code> Contains string</li>
<li><code>!=</code> Does not contain string</li>
<li><code>=~</code> Matches regular expression</li>
<li><code>!~</code> Does not match regular expression</li>
</ul>


<h2>Supported Clients</h2>

<p>At the moment of writing, loki supports the following log clients:</p>

<ul>
<li>Promtail (tails logs and ships to Loki)</li>
<li>Docker Driver</li>
<li>Fluentd</li>
<li>Fluent Bit</li>
<li>Logstash</li>
</ul>


<p>We will be going into more detail on using promtail in a <a href="">future post</a>, but you can read more up about it <a href="https://github.com/grafana/loki/tree/master/cmd">here</a></p>

<h2>Loki in Action</h2>

<p>Time to get to the fun part, clone my <a href="https://github.com/ruanbekker/loki-docker-nginx-example">github repo</a>:</p>

<pre><code>$ git clone https://github.com/ruanbekker/loki-docker-nginx-example
$ cd loki-docker-nginx-example
</code></pre>

<p>You can inspect the docker-compose.yml:</p>

<pre><code>$ cat docker-compose.yml
version: "3.4"

services:
  my-nginx-service:
    image: nginx
    container_name: my-nginx-service
    ports:
      - 8000:80
    environment:
      - FOO=bar
    logging:
      driver: loki
      options:
        loki-url: http://localhost:3100/loki/api/v1/push
        loki-external-labels: job=dockerlogs,owner=ruan,environment=development

  grafana:
    image: grafana/grafana:7.1.1
    volumes:
    - ./config/datasource.yml:/etc/grafana/provisioning/datasources/datasource.yml
    ports:
    - "3000:3000"

  loki:
   image: grafana/loki:v1.3.0
   volumes:
     - ./config/loki.yaml:/etc/config/loki.yaml
   entrypoint:
     - /usr/bin/loki
     - -config.file=/etc/config/loki.yaml
   ports:
     - "3100:3100"
</code></pre>

<p>As you can see loki will be the datasource where we will be pushing our logs to from our nginx container and we are defining our logging section where it should find loki and we are also setting labels to that log stream using <code>loki-external-labels</code>. Then we are using grafana to auto configure the loki datasource from the <code>./config/datasource.yml</code> section so that we can visualize our logs.</p>

<p>If you don&rsquo;t want to define the logging section per container, you can always set the defaults using the <code>/etc/docker/daemon.json</code> by following <a href="https://grafana.com/docs/loki/latest/clients/docker-driver/configuration/#change-the-default-logging-driver">this guide</a></p>

<p>Let&rsquo;s boot up our stack:</p>

<pre><code>$ docker-compose up
</code></pre>

<p>After everything is up, you should be able to access nginx by visiting: <code>http://nginx.localdns.xyz:8000/</code>, after you received a response, visit Grafana on <code>http://grafana.localdns.xyz:3000</code> using the username and password: <code>admin/admin</code>.</p>

<p>If you head over to datasources, you should see the loki datasource which was provisioned for you:</p>

<p><img src="https://img.sysadmins.co.za/tyn0ny.png" alt="loki-grafana" /></p>

<p>When you head to the left on explore and you select the loki datasource on <code>http://grafana.localdns.xyz:3000/explore</code> you should see the following:</p>

<p><img src="https://img.sysadmins.co.za/5kp07m.png" alt="loki-grafana" /></p>

<p>You will see that grafana discovers logstreams with the label <code>job</code> as you can see that our <code>job="dockerlogs"</code> can be seen there. We can either click on it, select the log labels from the left and browse the label we want to select or manually enter the query.</p>

<p>I will be using the query manually:</p>

<pre><code>{job="dockerlogs"}
</code></pre>

<p>So now we will get all the logs that has that label associated and as you can see, we see our request that we made:</p>

<p><img src="https://img.sysadmins.co.za/gra0oe.png" alt="nginx-grafana-loki" /></p>

<p>We can see one error due to the favicon.ico that it could not find, but let&rsquo;s first inspect our first log line:</p>

<p><img src="https://img.sysadmins.co.za/6dbuqn.png" alt="loki" /></p>

<p>Here we can see the labels assigned to that log event, which we can include in our query, like if we had multiple services and different environments, we can use a query like the following to only see logs for a specific service and environment:</p>

<pre><code>{job="dockerlogs", environment="development", compose_service="my-nginx-service"}
</code></pre>

<p>In the example above we used the selectors to select the logs we want to see, now we can use our filter expressions, to &ldquo;grep&rdquo; our logs.</p>

<p>Let&rsquo;s say we want to focus only on one service, and we want to filter for any logs with GET requests, so first we select to service then apply the filter expression:</p>

<pre><code>{compose_service="my-nginx-service"} |= "GET"
</code></pre>

<p><img src="https://img.sysadmins.co.za/vv609g.png" alt="loki-logs" /></p>

<p>As you can see we can see the ones we were looking for, we can also chain them, so we want to se GET&rsquo;s and errors:</p>

<pre><code>{compose_service="my-nginx-service"} |= "GET" |= "error"
</code></pre>

<p>And lets say for some reason we only want to see the logs that comes from a 192.168.32 subnet:</p>

<pre><code>{compose_service="my-nginx-service"} |= "GET" |= "error" |~ "192.168.32."
</code></pre>

<p>But we dont want to see requests from &ldquo;nginx.localdns.xyz&rdquo;:</p>

<pre><code>{compose_service="my-nginx-service"} |= "GET" |= "error" |~ "192.168.32." != "nginx.localdns.xyz"
</code></pre>

<p>Make two extra get requests to &ldquo;foo.localdns.xyz:8000&rdquo; and &ldquo;bar.localdns.xyz:8000&rdquo; and then we change the query to say that we only want to see errors and hostnames coming from the 2 requests that we made:</p>

<pre><code>{compose_service="my-nginx-service"} |= "error" |~ "(foo|bar).localdns.xyz"
</code></pre>

<p>If we expand one of the log lines, we can do a ad-hoc analysis to see the percentage of logs by source for example:</p>

<p><img src="https://img.sysadmins.co.za/9ctz6d.png" alt="loki-logs" /></p>

<h2>LogCLI</h2>

<p>If you prefer the cli to query logs, logcli is the command line client for loki, allows you to query logs from your terminal and has clients for linux, mac and windows.</p>

<p>Check the releases for the latest version:</p>

<ul>
<li><a href="https://github.com/grafana/loki/releases">https://github.com/grafana/loki/releases</a></li>
</ul>


<pre><code>$ wget https://github.com/grafana/loki/releases/download/v1.5.0/logcli-darwin-amd64.zip
$ unzip logcli-darwin-amd64.zip
$ mv logcli-darwin-amd64 /usr/local/bin/logcli
</code></pre>

<p>Set your environment details, in our case we dont have a username and password for loki:</p>

<pre><code>$ #export LOKI_USERNAME=${MYUSER}
$ #export LOKI_PASSWORD=${MYPASS}
$ export LOKI_ADDR=http://localhost:3001
</code></pre>

<p>We can view all our labels, letâ€™s view all the job labels:</p>

<pre><code>$ logcli labels job
http://localhost:3001/loki/api/v1/label/job/values
dockerlogs
</code></pre>

<p>Letâ€™s look at family apps nginx logs:</p>

<pre><code>$ logcli query '{job="dockerlogs"}'
http://localhost:3001/loki/api/v1/query_range?direction=BACKWARD&amp;end=1587727924005496000&amp;limit=30&amp;query=%7Bjob%3D%22dockerlogs%22%2C&amp;start=1587724324005496000
Common labels: {environment="development", owner="ruan", compose_service="my-nginx-service", job="dockerlogs", host="docker-desktop", compose_project="loki-nginx-docker"}
2020-08-13 17:08:40 192.168.32.1 - - [13/Aug/2020:15:08:40 +0000] "GET / HTTP/1.1" 200 612 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:79.0) Gecko/20100101 Firefox/79.0" "-"
</code></pre>

<p>We can also pipe that output to grep, awk, etc:</p>

<pre><code>$ logcli query '{job="dockerlogs"}' | grep GREP | awk -F 'X' '{print  $1}'
</code></pre>

<p>Supported arguments:</p>

<pre><code>$ logcli query --help
usage: logcli query [&lt;flags&gt;] &lt;query&gt;


Run a LogQL query.


Flags:
      --help             Show context-sensitive help (also try --help-long and --help-man).
      --version          Show application version.
  -q, --quiet            suppress everything but log lines
      --stats            show query statistics
  -o, --output=default   specify output mode [default, raw, jsonl]
  -z, --timezone=Local   Specify the timezone to use when formatting output timestamps [Local, UTC]
      --addr="http://localhost:3100"
                         Server address. Can also be set using LOKI_ADDR env var.
      --username=""      Username for HTTP basic auth. Can also be set using LOKI_USERNAME env var.
      --password=""      Password for HTTP basic auth. Can also be set using LOKI_PASSWORD env var.
      --ca-cert=""       Path to the server Certificate Authority. Can also be set using LOKI_CA_CERT_PATH env var.
      --tls-skip-verify  Server certificate TLS skip verify.
      --cert=""          Path to the client certificate. Can also be set using LOKI_CLIENT_CERT_PATH env var.
      --key=""           Path to the client certificate key. Can also be set using LOKI_CLIENT_KEY_PATH env var.
      --org-id=ORG-ID    org ID header to be substituted for auth
      --limit=30         Limit on number of entries to print.
      --since=1h         Lookback window.
      --from=FROM        Start looking for logs at this absolute time (inclusive)
      --to=TO            Stop looking for logs at this absolute time (exclusive)
      --step=STEP        Query resolution step width
      --forward          Scan forwards through logs.
      --no-labels        Do not print any labels
      --exclude-label=EXCLUDE-LABEL ...
                         Exclude labels given the provided key during output.
      --include-label=INCLUDE-LABEL ...
                         Include labels given the provided key during output.
      --labels-length=0  Set a fixed padding to labels
  -t, --tail             Tail the logs
      --delay-for=0      Delay in tailing by number of seconds to accumulate logs for re-ordering


Args:
  &lt;query&gt;  eg '{foo="bar",baz=~".*blip"} |~ ".*error.*"'
</code></pre>

<h2>Thank you</h2>

<p>I hope this was useful</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Increase Performance With Your Ghost Blog on Docker]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/06/08/build-a-ghost-blog-with-nginx-cache-on-docker/"/>
    <updated>2020-06-08T23:28:07+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/06/08/build-a-ghost-blog-with-nginx-cache-on-docker</id>
    <content type="html"><![CDATA[<p><img src="https://img.sysadmins.co.za/wngib2.png" alt="nginx-blog-ghost-caching" /></p>

<p>Nginx Caching + Ghost == Great Performance.</p>

<p>In this post we will build a nginx reverse proxy with caching enabled for our static content such as images, which will be our frontend and therefore we will have port 80 exposed, and run our ghost blog as our backend, which we will proxy traffic through from our nginx container.</p>

<h2>But why would you want caching?</h2>

<p>Returning data from memory is a lot faster than returning data from disk, and in this case where a request is being made against nginx, then it proxy passes the request to ghost, gets the data that you requested and returns the data to the client.</p>

<p>So for items that rarely changes like images, we can benefit from caching, so the images can be returned from the nginx service, where the first request will be made to ghost and then it will be loaded into nginx cache, so then the next time when you request the same image it will be returned from cache instead of making that same request to ghost again.</p>

<h2>Caching Info</h2>

<p>For this demonstration once we define the size of our chache which will be 500MB and we specify that if an object has not been accessed for 24 hours, we can expire the object from the cache.</p>

<h2>Nginx</h2>

<p>We will build our nginx container by adding our custom nginx config to our dockerfile.</p>

<p>Our <code>Dockerfile</code> will look like the following:</p>

<pre><code>ROM nginx:stable
ADD nginx.conf /etc/nginx/nginx.conf
</code></pre>

<p>Our <code>nginx.conf</code> configuration file:</p>

<pre><code>events {
  worker_connections  1024;
}

http {
  default_type       text/html;
  access_log         /dev/stdout;
  sendfile           on;
  keepalive_timeout  65;

  #proxy_cache_path /tmp/ghostcache levels=1:2 keys_zone=ghostcache:500m max_size=2g inactive=30d;
  proxy_cache_path /tmp/ghostcache levels=1:2 keys_zone=ghostcache:60m max_size=500m inactive=24h;
  proxy_cache_key "$scheme$request_method$host$request_uri";
  proxy_cache_methods GET HEAD;

  server {
    listen 80;

    location / {
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header Host $http_host;
        proxy_pass http://ghost:2368;
    }

    location ~* \.(?:css|js|ico)$ {
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header Host $http_host;
        proxy_pass http://ghost:2368;
        access_log off;
    }

    location ^~ /content/images/ {
        proxy_cache ghostcache;
        proxy_cache_valid 60m;
        proxy_cache_valid 404 1m;
        proxy_ignore_headers Set-Cookie;
        proxy_hide_header Set-Cookie;
        proxy_cache_use_stale error timeout invalid_header updating http_500 http_502 http_503 http_504;
        proxy_ignore_headers Cache-Control;
        add_header X-Cache-Status $upstream_cache_status;

        proxy_set_header Host $http_host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_pass http://ghost:2368;
        access_log off;
    }
  }
}
</code></pre>

<p>Then our <code>docker-compose.yml</code> where we will add our nginx and ghost container to run together:</p>

<pre><code>version: '3.4'

services:
  ghost:
    image: ghost:3.15.1
    container_name: 'ghost'
    environment:
      - NODE_ENV=production
      - url=http://localhost:80
    networks:
      - ghost
    volumes:
      - ghost_content:/var/lib/ghost/content/data

  proxy:
    build: .
    container_name: 'proxy'
    depends_on:
      - ghost
    ports:
      - 80:80
    networks:
      - ghost

networks:
  ghost: {}

volumes:
  ghost_content: {}
</code></pre>

<p>To boot our stack:</p>

<pre><code>$ docker-compose up
</code></pre>

<h2>Test Caching</h2>

<p>Once your containers are in a running state, open your browsers devloper tools and look at the networking tab, then access your ghost blog on <code>http://localhost:80/</code>, the first time a image is opened you should see the cache shows <code>MISS</code> when you refresh again you should see a <code>HIT</code>, which means that the object is being returned from your cache.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setting the Correct Service Name in Datadog Logging for Docker Swarm]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/12/11/setting-the-correct-service-name-in-datadog-logging-for-docker-swarm/"/>
    <updated>2019-12-11T23:35:53+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/12/11/setting-the-correct-service-name-in-datadog-logging-for-docker-swarm</id>
    <content type="html"><![CDATA[<p>For some reason, when logging to datadog from your applications running on docker swarm, the service names in datadog appears to have the names on the docker image. The application talks to the datadog agent which runs in global mode on swarm.</p>

<p>Setting <code>DATADOG_SERVICE_NAME</code> or <code>DD_SERVICE_NAME</code> as environment variables on the swarm service has zero affect, as they keep showing the service name as the docker image name, as example:</p>

<p><img width="1268" alt="08496333-01C4-4492-807E-FAC40826AFDE" src="https://user-images.githubusercontent.com/567298/70661591-49007080-1c6d-11ea-8230-0dbe086bd168.png"></p>

<p>If we inspect the tags, we can see that the docker image shows as the source and maps through as the docker service name. As you can see the swarm service name is what we want to be the service name (not alpine):</p>

<p><img width="1269" alt="783C6D52-62B2-4F2B-A6D4-28150CC58005" src="https://user-images.githubusercontent.com/567298/70661651-65041200-1c6d-11ea-858b-90034099c319.png"></p>

<p>One way how to fix this is to setup a pipeline processor, head over to Logs -> Configuration:</p>

<p><img width="267" alt="93CEE277-55A6-4DE1-8AE6-A02C64B0ACAD" src="https://user-images.githubusercontent.com/567298/70661767-adbbcb00-1c6d-11ea-8274-ad5da6ddfdd7.png"></p>

<p>Select &ldquo;Pipelines&rdquo; and add a new pipeline, select the filter <code>source:alpine</code> to limit down the results to the alpine image, and name your processor:</p>

<p><img width="763" alt="0BF3D6A6-9646-442D-A494-8DF489C5217F" src="https://user-images.githubusercontent.com/567298/70661837-cdeb8a00-1c6d-11ea-8fb4-2c272fda596f.png"></p>

<p>Next add a new processor and set the type to remapper, select the tag group as &ldquo;swarm_service&rdquo; and set the attribute to service and name the processor:</p>

<p><img width="762" alt="C02092F4-0EEC-4AF9-9E2A-F7A126560CD8" src="https://user-images.githubusercontent.com/567298/70662081-3a668900-1c6e-11ea-9ea9-9f80dfc669f3.png"></p>

<p>Add a new processor:</p>

<p><img width="1151" alt="5C2F7FB9-8948-4588-A283-86E94BC07513" src="https://user-images.githubusercontent.com/567298/70661901-e6f43b00-1c6d-11ea-9dbc-8c4c3a24b51b.png"></p>

<p>Select a service remapper, set the attribute to service and name the processor:</p>

<p><img width="761" alt="852904AE-9395-4B4B-B1F4-54427D88C970" src="https://user-images.githubusercontent.com/567298/70661986-0ab78100-1c6e-11ea-9edc-5fd748d73d0c.png"></p>

<p>Now when you go back to logs, you will find that the service name is being set to the correct service name in datadog:</p>

<p><img width="1159" alt="0F11DDC4-E99C-4A2F-B6AB-7409B4E7546C" src="https://user-images.githubusercontent.com/567298/70662290-95987b80-1c6e-11ea-8d8c-bec4d44cde60.png"></p>

<p>When you inspect one of the logs, you will see that the attribute is being set to the log:</p>

<p><img width="633" alt="4B098970-6345-40B9-9F90-411D8FE6A9E6" src="https://user-images.githubusercontent.com/567298/70662330-a9dc7880-1c6e-11ea-8b48-51900161cf01.png"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Deploy a Webapp on a AWS EKS Kubernetes Cluster]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/11/17/how-to-deploy-a-webapp-on-a-aws-eks-kubernetes-cluster/"/>
    <updated>2019-11-17T00:21:19+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/11/17/how-to-deploy-a-webapp-on-a-aws-eks-kubernetes-cluster</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/68999897-f59a3d00-08cf-11ea-83c7-8624e6048106.png" alt="kubernetes-eks-deploy-webapp" /></p>

<p><a href="https://saythanks.io/to/ruanbekker"><img src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" alt="Say Thanks!" /></a> <a href="https://linux-hackers-slack.herokuapp.com/"><img src="https://linux-hackers-slack.herokuapp.com/badge.svg" alt="Slack Status" /></a> <a href="https://linux-hackers.slack.com/"><img src="https://img.shields.io/badge/chat-on_slack-orange.svg" alt="Chat on Slack" /></a> <a href="https://github.com/ruanbekker"><img src="https://img.shields.io/github/followers/ruanbekker.svg?label=Follow&amp;style=social" alt="GitHub followers" /></a></p>

<p><a href="https://twitter.com/ruanbekker?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @ruanbekker</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>

<p>In our previous post, <a href="https://blog.ruanbekker.com/blog/2019/11/16/how-to-setup-a-aws-eks-kubernetes-cluster/">Part 1 - Setup a EKS Cluster</a> we went through the steps on how to Setup a EKS Cluster.</p>

<h2>What are we doing today</h2>

<p>In this post, we will deploy a sample web application to EKS and access our application using a ELB that EKS provides us.</p>

<h2>Deployment Manifests</h2>

<p>We will have two manifests that we will deploy to Kubernetes, a deployment manifest that will hold the information about our application and a service manifest that will hold the information about the service load balancer.</p>

<p>The deployment manifest, you will notice that we are specifying that we want 3 containers, we are using labels so that our service and deployment can find each other and we are using a basic http web application that will listen on port 8000 inside the container:</p>

<pre><code class="bash">$ cat deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-hostname-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: my-app-container
          image: ruanbekker/hostname
          ports:
          - name: http
            containerPort: 8000
</code></pre>

<p>The service manifest, you will notice that we are specifying <code>type: LoadBalancer</code> in our service manifest, this will tell EKS to provision a ELB for your application so that we can access our application from the internet.</p>

<p>You will see that the selector is specifying <code>my-app</code> which we also provided in our deployment.yml so that our service know where to find our backend application. We are also stating that the service is listening on port 80, and will forward its traffic to our deployment on port 8000:</p>

<pre><code class="bash">$ cat service.yml
apiVersion: v1
kind: Service
metadata:
  name: my-hostname-app-service
  labels:
    app: my-app
spec:
  ports:
  - port: 80
    targetPort: 8000
  selector:
    app: my-app
  type: LoadBalancer
</code></pre>

<h2>Deployment Time</h2>

<p>Deploy our application:</p>

<pre><code class="bash">$ kubectl apply -f deployment.yml
deployment.apps/my-hostname-app created
</code></pre>

<p>Deploy our service:</p>

<pre><code class="bash">$ kubectl apply -f service.yml
service/my-hostname-app-service created
</code></pre>

<p>Now when we look at our deployment, we should see that 3 replicas of our application is running:</p>

<pre><code class="bash">$ kubectl get deployments
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
my-hostname-app   3/3     3            3           4m38s
</code></pre>

<p>To see the pods of that deployment, look at the pods:</p>

<pre><code class="bash">$ kubectl get pods
NAME                               READY   STATUS    RESTARTS   AGE
my-hostname-app-5dcd48dfc5-2j8zm   1/1     Running   0          24s
my-hostname-app-5dcd48dfc5-58vkc   1/1     Running   0          24s
my-hostname-app-5dcd48dfc5-cmjwj   1/1     Running   0          24s
</code></pre>

<p>As we have more than one service in our EKS cluster, we can specify the labels that we have applied on our manifests to filter what we want to see (<code>app: my-app</code>):</p>

<pre><code class="bash">$ kubectl get service --selector app=my-app
NAME                      TYPE           CLUSTER-IP       EXTERNAL-IP                                                              PORT(S)        AGE
my-hostname-app-service   LoadBalancer   10.100.114.166   a460661ce089b11ea97cd06dd7513db6-669054126.eu-west-1.elb.amazonaws.com   80:30648/TCP   2m29s
</code></pre>

<p>As we can see EKS provisioned a ELB for us, and we can access the application by making a HTTP request:</p>

<pre><code class="bash">$ curl -i http://a460661ce089b11ea97cd06dd7513db6-669054126.eu-west-1.elb.amazonaws.com
HTTP/1.1 200 OK
Date: Sat, 16 Nov 2019 18:05:27 GMT
Content-Length: 43
Content-Type: text/plain; charset=utf-8

Hostname: my-hostname-app-5dcd48dfc5-2j8zm
</code></pre>

<h2>Scaling our Deployment</h2>

<p>Let&rsquo;s scale our deployment to 5 replicas:</p>

<pre><code class="bash">$ kubectl scale deployment/my-hostname-app --replicas 5
deployment.extensions/my-hostname-app scaled
</code></pre>

<p>After all the pods has been deployed, you should be able to see the 5 out of 5 pods that we provisioned, should be running:</p>

<pre><code class="bash">$ kubectl get deployments
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
my-hostname-app   5/5     5            5           5m7s
</code></pre>

<p>We can then also see the pods that our deployment is referencing:</p>

<pre><code class="bash">$ kubectl get pods
NAME                               READY   STATUS    RESTARTS   AGE
my-hostname-app-5dcd48dfc5-2j8zm   1/1     Running   0          6m8s
my-hostname-app-5dcd48dfc5-58vkc   1/1     Running   0          6m8s
my-hostname-app-5dcd48dfc5-cmjwj   1/1     Running   0          6m8s
my-hostname-app-5dcd48dfc5-m4xcq   1/1     Running   0          67s
my-hostname-app-5dcd48dfc5-zf6xl   1/1     Running   0          68s
</code></pre>

<h2>Further Reading on Kubernetes</h2>

<p>This is one amazing resource that covers a lot of kubernetes topics and will help you throughout your EKS journey:</p>

<ul>
<li><a href="https://eksworkshop.com/introduction/">EKSWorkshop</a></li>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/worker.html">Worker Nodes Documentation</a></li>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-guestbook.html">Guestbook Kubernetes Sample Application</a></li>
</ul>


<h2>Thank You</h2>

<p>Let me know what you think. If you liked my content, feel free to checkout my content on <strong><a href="https://ruan.dev/">ruan.dev</a></strong> or follow me on twitter at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong></p>

<center><script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Buy Me a Coffee', '#46b798', 'A6423ZIQ');kofiwidget2.draw();</script></center>



]]></content>
  </entry>
  
</feed>
