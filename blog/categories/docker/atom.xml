<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Docker | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/docker/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2019-03-21T03:02:04-04:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[How to Deploy a Docker Swarm Cluster on Scaleway With Terraform]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/03/21/how-to-deploy-a-docker-swarm-cluster-on-scaleway-with-terraform/"/>
    <updated>2019-03-21T02:15:07-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/03/21/how-to-deploy-a-docker-swarm-cluster-on-scaleway-with-terraform</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/54737111-09fa2e80-4bb7-11e9-97f4-a94a31fc9a3a.png" alt="" /></p>

<p>We will deploy a 3 node docker swarm cluster with terraform on scaleway. I have used the base source code from <a href="https://github.com/stefanprodan/scaleway-swarm-terraform">this</a> repository but tweaked the configuration to my needs.</p>

<h2>Pre-Requisites</h2>

<p>Ensure terraform and jq is instaled:</p>

<pre><code class="bash">$ brew install terraform
$ brew install jq
</code></pre>

<h2>Terraform</h2>

<p>You can have a look at the linked source at the top for the source code, but below I will provide each file that will make up our terraform deployment.</p>

<p>Ource <code>main.tf</code></p>

<pre><code>provider "scaleway" {
  region = "${var.region}"
}

data "scaleway_bootscript" "debian" {
  architecture = "x86_64"
  name = "x86_64 mainline 4.15.11 rev1"
}

data "scaleway_image" "debian_stretch" {
  architecture = "x86_64"
  name         = "Debian Stretch"
}

data "template_file" "docker_conf" {
  template = "${file("conf/docker.tpl")}"

  vars {
    ip = "${var.docker_api_ip}"
  }
}
</code></pre>

<p>The <code>outputs.tf</code></p>

<pre><code>output "swarm_manager_public_ip" {
  value = "${scaleway_ip.swarm_manager_ip.0.ip}"
}

output "swarm_manager_private_ip" {
  value = "${scaleway_server.swarm_manager.0.private_ip}"
}

output "swarm_workers_public_ip" {
  value = "${concat(scaleway_server.swarm_worker.*.name, scaleway_server.swarm_worker.*.public_ip)}"
}

output "swarm_workers_private_ip" {
  value = "${concat(scaleway_server.swarm_worker.*.name, scaleway_server.swarm_worker.*.private_ip)}"
}

output "workspace" {
  value = "${terraform.workspace}"
}
</code></pre>

<p>Our <code>security-groups.tf</code></p>

<pre><code>resource "scaleway_security_group" "swarm_managers" {
  name        = "swarm_managers"
  description = "Allow HTTP/S and SSH traffic"
}

resource "scaleway_security_group_rule" "ssh_accept" {
  security_group = "${scaleway_security_group.swarm_managers.id}"

  action    = "accept"
  direction = "inbound"
  ip_range  = "0.0.0.0/0"
  protocol  = "TCP"
  port      = 22
}

resource "scaleway_security_group_rule" "http_accept" {
  security_group = "${scaleway_security_group.swarm_managers.id}"

  action    = "accept"
  direction = "inbound"
  ip_range  = "0.0.0.0/0"
  protocol  = "TCP"
  port      = 80
}

resource "scaleway_security_group_rule" "https_accept" {
  security_group = "${scaleway_security_group.swarm_managers.id}"

  action    = "accept"
  direction = "inbound"
  ip_range  = "0.0.0.0/0"
  protocol  = "TCP"
  port      = 443
}

resource "scaleway_security_group" "swarm_workers" {
  name        = "swarm_workers"
  description = "Allow SSH traffic"
}

resource "scaleway_security_group_rule" "ssh_accept_workers" {
  security_group = "${scaleway_security_group.swarm_workers.id}"

  action    = "accept"
  direction = "inbound"
  ip_range  = "0.0.0.0/0"
  protocol  = "TCP"
  port      = 22
}
</code></pre>

<p>Our <code>variables.tf</code></p>

<pre><code>variable "docker_version" {
  default = "18.06.3~ce~3-0~debian"
}

variable "region" {
  default = "ams1"
}

variable "manager_instance_type" {
  default = "START1-M"
}

variable "worker_instance_type" {
  default = "START1-M"
}

variable "worker_instance_count" {
  default = 2
}

variable "docker_api_ip" {
  default = "127.0.0.1"
}
</code></pre>

<p>Our <code>managers.tf</code></p>

<pre><code>resource "scaleway_ip" "swarm_manager_ip" {
  count = 1
}

resource "scaleway_server" "swarm_manager" {
  count          = 1
  name           = "${terraform.workspace}-manager-${count.index + 1}"
  image          = "${data.scaleway_image.debian_stretch.id}"
  type           = "${var.manager_instance_type}"
  bootscript     = "${data.scaleway_bootscript.debian.id}"
  security_group = "${scaleway_security_group.swarm_managers.id}"
  public_ip      = "${element(scaleway_ip.swarm_manager_ip.*.ip, count.index)}"

  volume {
    size_in_gb = 50
    type       = "l_ssd"
  }

  provisioner "remote-exec" {
    script = "scripts/mount-disk.sh"
  }

  connection {
    type = "ssh"
    user = "root"
    private_key = "${file("~/.ssh/id_rsa")}"
  }

  provisioner "remote-exec" {
    inline = [
      "mkdir -p /etc/systemd/system/docker.service.d",
    ]
  }

  provisioner "file" {
    content     = "${data.template_file.docker_conf.rendered}"
    destination = "/etc/systemd/system/docker.service.d/docker.conf"
  }

  provisioner "file" {
    source      = "scripts/install-docker-ce.sh"
    destination = "/tmp/install-docker-ce.sh"
  }

  provisioner "file" {
    source      = "scripts/local-persist-plugin.sh"
    destination = "/tmp/local-persist-plugin.sh"
  }

  provisioner "remote-exec" {
    inline = [
      "chmod +x /tmp/install-docker-ce.sh",
      "/tmp/install-docker-ce.sh ${var.docker_version}",
      "docker swarm init --advertise-addr ${self.private_ip}",
      "chmod +x /tmp/local-persist-plugin.sh",
      "/tmp/local-persist-plugin.sh"
    ]
  }
}
</code></pre>

<p>Our <code>workers.tf</code></p>

<pre><code>resource "scaleway_ip" "swarm_worker_ip" {
  count = "${var.worker_instance_count}"
}

resource "scaleway_server" "swarm_worker" {
  count          = "${var.worker_instance_count}"
  name           = "${terraform.workspace}-worker-${count.index + 1}"
  image          = "${data.scaleway_image.debian_stretch.id}"
  type           = "${var.worker_instance_type}"
  bootscript     = "${data.scaleway_bootscript.debian.id}"
  security_group = "${scaleway_security_group.swarm_workers.id}"
  public_ip      = "${element(scaleway_ip.swarm_worker_ip.*.ip, count.index)}"

  volume {
    size_in_gb = 50
    type       = "l_ssd"
  }

  provisioner "remote-exec" {
    script = "scripts/mount-disk.sh"
  }

  connection {
    type = "ssh"
    user = "root"
    private_key = "${file("~/.ssh/id_rsa")}"
  }

  provisioner "remote-exec" {
    inline = [
      "mkdir -p /etc/systemd/system/docker.service.d",
    ]
  }

  provisioner "file" {
    content     = "${data.template_file.docker_conf.rendered}"
    destination = "/etc/systemd/system/docker.service.d/docker.conf"
  }

  provisioner "file" {
    source      = "scripts/install-docker-ce.sh"
    destination = "/tmp/install-docker-ce.sh"
  }

  provisioner "file" {
    source      = "scripts/local-persist-plugin.sh"
    destination = "/tmp/local-persist-plugin.sh"
  }

  provisioner "remote-exec" {
    inline = [
      "chmod +x /tmp/install-docker-ce.sh",
      "/tmp/install-docker-ce.sh ${var.docker_version}",
      "docker swarm join --token ${data.external.swarm_tokens.result.worker} ${scaleway_server.swarm_manager.0.private_ip}:2377",
      "chmod +x /tmp/local-persist-plugin.sh",
      "/tmp/local-persist-plugin.sh",
    ]
  }

  provisioner "remote-exec" {
    when = "destroy"

    inline = [
      "docker node update --availability drain ${self.name}",
    ]

    on_failure = "continue"

    connection {
      type = "ssh"
      user = "root"
      host = "${scaleway_ip.swarm_manager_ip.0.ip}"
    }
  }

  provisioner "remote-exec" {
    when = "destroy"

    inline = [
      "docker swarm leave",
    ]

    on_failure = "continue"
  }

  provisioner "remote-exec" {
    when = "destroy"

    inline = [
      "docker node rm --force ${self.name}",
    ]

    on_failure = "continue"

    connection {
      type = "ssh"
      user = "root"
      host = "${scaleway_ip.swarm_manager_ip.0.ip}"
    }
  }
}

data "external" "swarm_tokens" {
  program = ["./scripts/fetch-tokens.sh"]

  query = {
    host = "${scaleway_ip.swarm_manager_ip.0.ip}"
  }

  depends_on = ["scaleway_server.swarm_manager"]
}
</code></pre>

<p>Our config for the docker daemon: <code>conf/docker.tpl</code></p>

<pre><code>[Service]
ExecStart=
ExecStart=/usr/bin/dockerd -H fd:// \
  -H tcp://${ip}:2375 \
  --storage-driver=overlay2 \
  --dns 8.8.4.4 --dns 8.8.8.8 \
  --log-driver json-file \
  --log-opt max-size=50m --log-opt max-file=10 \
  --experimental=true \
  --metrics-addr 172.17.0.1:9323
</code></pre>

<p>Our script to mount our additional disk: <code>scripts/mount-disk.sh</code></p>

<pre><code class="bash">#!/bin/bash
apt update
apt install xfsprogs attr -y
mkfs -t xfs /dev/vdb
echo "/dev/vdb /mnt xfs defaults 0 0" &gt;&gt; /etc/fstab
mount -a
</code></pre>

<p>Our script to install docker: <code>scripts/install-docker-ce.sh</code></p>

<pre><code class="bash">#!/usr/bin/env bash

DOCKER_VERSION=$1
DEBIAN_FRONTEND=noninteractive apt-get -qq update
apt-get -qq install apt-transport-https ca-certificates curl software-properties-common
curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -
add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable"

apt-get -q update -y
apt-get -q install -y docker-ce=$DOCKER_VERSION containerd.io
</code></pre>

<p>Our script that retrieves the swarm tokens: <code>scripts/fetch-tokens.sh</code></p>

<pre><code class="bash">#!/usr/bin/env bash

# Processing JSON in shell scripts
# https://www.terraform.io/docs/providers/external/data_source.html#processing-json-in-shell-scripts

set -e

# Extract "host" argument from the input into HOST shell variable
eval "$(jq -r '@sh "HOST=\(.host)"')"

MANAGER=$(ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null root@$HOST docker swarm join-token manager -q)
WORKER=$(ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null root@$HOST docker swarm join-token worker -q)

# produce a json object containing the tokens
jq -n --arg manager "$MANAGER" --arg worker "$WORKER" '{"manager":$manager,"worker":$worker}'
</code></pre>

<p>Our script to install the <a href="https://github.com/CWSpear/local-persist">local-persist docker volume</a> plugin: <code>scripts/local-persist-plugin.sh</code></p>

<pre><code>#!/usr/bin/env bash
set -e
curl -fsSL https://raw.githubusercontent.com/CWSpear/local-persist/master/scripts/install.sh | bash
</code></pre>

<h2>Deploy your Swarm</h2>

<p>Note that we will be deploying 3x SMART1-M servers with Debian Stretch. At this moment the image id is the one of debian stretch but may change in the future. If you want to change the distro, update the install script, and the terraform files.</p>

<p><a href="https://www.scaleway.com/docs/generate-an-api-token/">Generate API Token on Scaleway</a> then export it to your current shell:</p>

<pre><code class="bash">export SCALEWAY_ORGANIZATION="&lt;organization-id&gt;"
export SCALEWAY_TOKEN="&lt;secret&gt;"
</code></pre>

<p>Make sure that your ssh private key is the intended one as in the config, in my example: <code>~/.ssh/id_rsa</code> and that they are allowed in your servers <code>authorized_keys</code> file</p>

<p>Create a new workspace:</p>

<pre><code class="bash">$ terraform new workspace swarm
</code></pre>

<p>Pull down the providers and initialize:</p>

<pre><code class="bash">$ terraform init
</code></pre>

<p>Deploy!</p>

<pre><code class="bash">$ terraform apply
...
...
scaleway_server.swarm_worker[0]: Creation complete after 4m55s (ID: xx-xx-xx-xx-xx)

Apply complete! Resources: 14 added, 0 changed, 0 destroyed.
Outputs:

swarm_manager_private_ip = 10.21.x.x
swarm_manager_public_ip = 51.xx.xx.xx
swarm_workers_private_ip = [
    swarm-worker-1,
    swarm-worker-2,
    10.20.xx.xx,
    10.20.xx.xx,
]
swarm_workers_public_ip = [
    swarm-worker-1,
    swarm-worker-2,
    51.xx.xx.xx,
    51.xx.xx.xx,
]
workspace = swarm
</code></pre>

<p>Once your deployment is done you will be prompted with the public/private ip addresses of your nodes as seen above, you can also manually retrieve them:</p>

<pre><code>$ terraform terraform output
</code></pre>

<p>Or for a specific node, such as the manager:</p>

<pre><code>$ terraform terraform output swarm-manager
51.xx.xx.xx
</code></pre>

<p>Go ahead and ssh to your manager nodes and list the swarm nodes, boom, easy right.</p>

<pre><code>$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
2696o0vrt93x8qf2gblbfc8pf *   swarm-manager       Ready               Active              Leader              18.09.3
72ava7rrp2acnyadisg52n7ym     swarm-worker-1      Ready               Active                                  18.09.3
sy2otqn20qe9jc2v9io3a21jm     swarm-worker-2      Ready               Active                                  18.09.3
</code></pre>

<p>When you want to destroy the environment:</p>

<pre><code>$ terraform destroy -force
</code></pre>

<h2>References:</h2>

<p>Big thanks goes to <a href="https://github.com/stefanprodan">@stefanprodan</a></p>

<ul>
<li><a href="https://www.terraform.io/docs/index.html">https://www.terraform.io/docs/index.html</a></li>
<li><a href="https://docs.docker.com/engine/swarm/">https://docs.docker.com/engine/swarm/</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Container Persistent Storage for Docker Swarm Using a GlusterFS Volume Plugin]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/03/05/container-persistent-storage-for-docker-swarm-using-a-glusterfs-volume-plugin/"/>
    <updated>2019-03-05T13:18:30-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/03/05/container-persistent-storage-for-docker-swarm-using-a-glusterfs-volume-plugin</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/53351889-85572000-392a-11e9-9720-464e9318206e.jpg" alt="" /></p>

<p>From one of my previous posts I demonstrated how to provide persistent storage for your containers by using a <a href="https://blog.ruanbekker.com/blog/2018/02/16/guide-to-setup-docker-convoy-volume-driver-for-docker-swarm-with-nfs/">Convoy NFS Plugin</a>.</p>

<p>I&rsquo;ve stumbled upon one AWESOME GlusterFS Volume Plugin for Docker by <a href="https://github.com/trajano/docker-volume-plugins/tree/master/glusterfs-volume-plugin">@trajano</a>, please have a look at his repository. I&rsquo;ve been waiting for some time for one solid glusterfs volume plugin, and it works great.</p>

<h2>What we will be doing today</h2>

<p>We will setup a 3 node replicated glusterfs volume and show how easy it is to install the volume plugin and then demonstrate how storage from our swarms containers are persisted.</p>

<p>Our servers that we will be using will have the private ip&rsquo;s as shown below:</p>

<pre><code>10.22.125.101
10.22.125.102
10.22.125.103
</code></pre>

<h2>Setup GlusterFS</h2>

<p>Have a look at <a href="https://blog.ruanbekker.com/blog/2019/03/05/setup-a-3-node-replicated-storage-volume-with-glusterfs/?referral=github.com">this</a> post to setup the glusterfs volume.</p>

<h2>Install the GlusterFS Volume Plugin</h2>

<p>Below I&rsquo;m installing the plugin and setting the alias name as <code>glusterfs</code>, granting all permissions and keeping the plugin in a disabled state.</p>

<pre><code class="bash">$ docker plugin install --alias glusterfs trajano/glusterfs-volume-plugin --grant-all-permissions --disable
</code></pre>

<p>Set the glusterfs servers:</p>

<pre><code>$ docker plugin set glusterfs SERVERS=10.22.125.101,10.22.125.102,10.22.125.103
</code></pre>

<p>Enable the glusterfs plugin:</p>

<pre><code>$ docker plugin enable glusterfs
</code></pre>

<h2>Create a Service in Docker Swarm</h2>

<p>Deploy a sample service on docker swarm with a volume backed by glusterfs. Note that my glusterfs volume is called <code>gfs</code></p>

<pre><code class="yaml">version: "3.4"

services:
  foo:
    image: alpine
    command: ping localhost
    networks:
      - net
    volumes:
      - vol1:/tmp

networks:
  net:
    driver: overlay

volumes:
  vol1:
    driver: glusterfs
    name: "gfs/vol1"
</code></pre>

<p>Deploy the stack:</p>

<pre><code class="bash">$ docker stack deploy -c docker-compose.yml test
Creating service test_foo
</code></pre>

<p>Have a look on which node is your container running:</p>

<pre><code class="bash">$ docker service ps test_foo
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
jfwzb7yxnrxx        test_foo.1          alpine:latest       swarm-worker-1      Running             Running 37 seconds ago
</code></pre>

<p>Now jump to the <code>swarm-worker-1</code> node and verify that the container is running on that node:</p>

<pre><code class="bash">$ docker ps
CONTAINER ID        IMAGE                                          COMMAND                  CREATED             STATUS                  PORTS               NAMES
d469f341d836        alpine:latest                                  "ping localhost"           59 seconds ago      Up 57 seconds                               test_foo.1.jfwzb7yxnrxxnd0qxtcjex8lu
</code></pre>

<p>Now since the container is running on this node, we will also see that the volume defined in our task configuration will also be present:</p>

<pre><code class="bash">$ docker volume ls
DRIVER                       VOLUME NAME
glusterfs:latest             gfs/vol1
</code></pre>

<p>Exec into the container and look at the disk layout:</p>

<pre><code class="bash">$ docker exec -it d469f341d836 sh
/ # df -h
Filesystem                Size      Used Available Use% Mounted on
overlay                  45.6G      3.2G     40.0G   7% /
10.22.125.101:gfs/vol1   45.6G      3.3G     40.0G   8% /tmp
</code></pre>

<p>While you are in the container, write the hostname&rsquo;s value into a file which is mapped to the glusterfs volume:</p>

<pre><code class="bash">$ echo $HOSTNAME &gt; /tmp/data.txt
$ cat /tmp/data.txt
d469f341d836
</code></pre>

<h2>Testing Data Persistence</h2>

<p>Time to test the data persistence. Scale the service to 3 replicas, then hop onto a new node where a replica resides and check if the data was persisted.</p>

<pre><code class="bash">$ docker service scale test_foo=3
test_foo scaled to 3
overall progress: 3 out of 3 tasks
1/3: running   [==================================================&gt;]
2/3: running   [==================================================&gt;]
3/3: running   [==================================================&gt;]
verify: Service converged
</code></pre>

<p>Check where the containers are running:</p>

<pre><code class="bash">$ docker service ps test_foo
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
jfwzb7yxnrxx        test_foo.1          alpine:latest       swarm-worker-1      Running             Running 2 minutes ago
mdsg6c5b2nqb        test_foo.2          alpine:latest       swarm-worker-3      Running             Running 15 seconds ago
iybat57t4lha        test_foo.3          alpine:latest       swarm-worker-2      Running             Running 15 seconds ago
</code></pre>

<p>Hop onto the <code>swarm-worker-2</code> node and check if the data is persisted from our previous write:</p>

<pre><code class="bash">$ docker exec -it 4228529aba29 sh
$ cat /tmp/data.txt
d469f341d836
</code></pre>

<p>Now let&rsquo;s append data to that file, then delete the stack and recreate to test if the data is still persisted:</p>

<pre><code class="bash">$ echo $HOSTNAME &gt;&gt; /tmp/data.txt
$ cat /tmp/data.txt
d469f341d836
4228529aba29
</code></pre>

<p>On the manager delete the stack:</p>

<pre><code class="bash">$ docker stack rm test
Removing service test_foo
</code></pre>

<p>The deploy the stack again:</p>

<pre><code class="bash">$ docker stack deploy -c docker-compose.yml test
Creating service test_foo
</code></pre>

<p>Check where the container is running:</p>

<pre><code class="bash">$ docker service ps test_foo
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS
9d6z02m123jk        test_foo.1          alpine:latest       swarm-worker-1      Running             Running 2 seconds ago
</code></pre>

<p>Exec into the container and read the data:</p>

<pre><code class="bash">$ docker exec -it 3008b1e1bba1 cat /tmp/data.txt
d469f341d836
4228529aba29
</code></pre>

<p>And as you can see the data is persisted.</p>

<h2>Resources</h2>

<p>Please have a look and star <a href="https://github.com/trajano/docker-volume-plugins">@trajano&rsquo;s</a> repository:</p>

<ul>
<li><a href="https://github.com/trajano/docker-volume-plugins">https://github.com/trajano/docker-volume-plugins</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Use Swarm Managed Configs in Docker Swarm to Store Your Application Configs]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/02/28/use-swarm-managed-configs-in-docker-swarm-to-store-your-application-configs/"/>
    <updated>2019-02-28T09:48:28-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/02/28/use-swarm-managed-configs-in-docker-swarm-to-store-your-application-configs</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/53351889-85572000-392a-11e9-9720-464e9318206e.jpg" alt="" /></p>

<p>Docker version 17.06 introduced Swarm Service Configs, which allows you to store data like configuration files, note that this is for non-sensitive information.</p>

<p>In this tutorial we will store the data of our <code>index.html</code> in a service config, then attach the config to our service.</p>

<h2>Creating the Config</h2>

<p>Create the <code>index.html</code> file and store it as a config:</p>

<pre><code class="bash">$ cat &gt; index.html &lt;&lt; EOF
&lt;html&gt;
  &lt;body&gt;
    Hello, World!
  &lt;/body&gt;
&lt;/html&gt;
EOF
</code></pre>

<p>Store the config as <code>nginx_root_doc</code>:</p>

<pre><code class="bash">$ docker config create nginx_root_doc index.html
</code></pre>

<h2>Create the Service</h2>

<p>Create the swarm service and associate the config with the service and set the target path where the config will reside:</p>

<pre><code class="bash">$ docker service create --name web \
  --config source=nginx_root_doc,target=/usr/share/nginx/html/index.html \
  --publish 8080:80 nginx:alpine
</code></pre>

<p>Once the service is up, test it:</p>

<pre><code class="bash">$ curl -i http://localhost:8080/
&lt;html&gt;
HTTP/1.1 200 OK
Server: nginx/1.15.9
Date: Thu, 28 Feb 2019 12:00:19 GMT
Content-Type: text/html
Content-Length: 52
Last-Modified: Thu, 28 Feb 2019 11:59:37 GMT
Connection: keep-alive
ETag: "5c77cd29-34"
Accept-Ranges: bytes

&lt;html&gt;
  &lt;body&gt;
    Hello, World!
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>Delete the service:</p>

<pre><code class="bash">$ docker service rm web
</code></pre>

<p>Delete the config:</p>

<pre><code class="bash">$ docker config rm nginx_root_doc
</code></pre>

<h2>Create the Service using Compose:</h2>

<p>Doing the same with a docker-compose file, will look like the following. The first example will be where we will explicitly define our path of our secret, and will create on deploy time. Our compose file:</p>

<pre><code class="yaml">services:
  web:
    image: nginx:alpine
    ports:
      - 8080:80
    networks:
      - net
    configs:
      - source: nginx_root_doc
        target: /usr/share/nginx/html/index.html

configs:
  nginx_root_doc:
    file: ./index.html

networks:
  net:
    driver: overlay
</code></pre>

<p>Deploying our stack:</p>

<pre><code class="bash">$ docker stack deploy -c docker-compose.yml apps
Creating network apps_net
Creating config apps_nginx_root_doc
Creating service apps_web
</code></pre>

<p>Testing our our service:</p>

<pre><code class="bash">$ curl -i http://localhost:8080/
HTTP/1.1 200 OK
Server: nginx/1.15.9
Date: Thu, 28 Feb 2019 12:20:52 GMT
Content-Type: text/html
Content-Length: 56
Last-Modified: Thu, 28 Feb 2019 12:20:47 GMT
Connection: keep-alive
ETag: "5c77d21f-38"
Accept-Ranges: bytes

&lt;html&gt;
  &lt;body&gt;
    Hello, World!
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>Note, that configs cant be updated, if you want to rotate a config you will create a new config and update the target in your task definition to point to your new config.</p>

<p>Delete the stack:</p>

<pre><code class="bash">$ docker stack rm apps
Removing service apps_web
Removing config apps_nginx_root_doc
Removing network apps_net
</code></pre>

<p>Another example will be to point to a external config which already exists in swarm. The only change will be that we need to set the config as a external type.</p>

<p>Create the config:</p>

<pre><code class="bash">$ docker config create nginx_root_doc index.html
</code></pre>

<p>Now that the config exists, create this compose file:</p>

<pre><code class="yaml">version: "3.3"

services:
  web:
    image: nginx:alpine
    ports:
      - 8080:80
    networks:
      - net
    configs:
      - source: nginx_root_doc
        target: /usr/share/nginx/html/index.html

configs:
  nginx_root_doc:
    external: true

networks:
  net:
    driver: overlay
</code></pre>

<p>Then deploy the stack:</p>

<pre><code class="bash">$ docker stack deploy -c docker-compose.yml apps
Creating network apps_net
Creating service apps_web
</code></pre>

<p>Then testing:</p>

<pre><code class="bash">$ curl -i http://localhost:8080/
HTTP/1.1 200 OK
Server: nginx/1.15.9
Date: Thu, 28 Feb 2019 12:28:11 GMT
Content-Type: text/html
Content-Length: 56
Last-Modified: Thu, 28 Feb 2019 12:28:09 GMT
Connection: keep-alive
ETag: "5c77d3d9-38"
Accept-Ranges: bytes

&lt;html&gt;
  &lt;body&gt;
    Hello, World!
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>

<h2>Resources:</h2>

<p>For more information on docker swarm configs have a look at <a href="https://docs.docker.com/engine/swarm/configs/#example-rotate-a-config">docker&rsquo;s documentation</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a 3 Node Docker Swarm Cluster on Ubuntu 16.04]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/01/10/setup-a-3-node-docker-swarm-cluster-on-ubuntu-16-dot-04/"/>
    <updated>2019-01-10T09:52:07-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/01/10/setup-a-3-node-docker-swarm-cluster-on-ubuntu-16-dot-04</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/53351889-85572000-392a-11e9-9720-464e9318206e.jpg" alt="" /></p>

<p>Docker Swarm is a Clustering and Orchestration Framework for the Docker ecosystem. Have a look at their <a href="https://docs.docker.com/engine/swarm/">official documentation</a> for detailed information.</p>

<p>In this Tutorial we will Setup a 3 Node Docker Swarm Cluster and to Demonstrate How Easy it is to Deploy a Web Application with 2 Replicas from a Docker Image.</p>

<p><br></p>

<script type="text/javascript">
  ( function() {
    if (window.CHITIKA === undefined) { window.CHITIKA = { 'units' : [] }; };
    var unit = {"calltype":"async[2]","publisher":"rbekker87","width":728,"height":90,"sid":"Chitika Default"};
    var placement_id = window.CHITIKA.units.length;
    window.CHITIKA.units.push(unit);
    document.write('<div id="chitikaAdBlock-' + placement_id + '"></div>');
}());
</script>


<script type="text/javascript" src="//cdn.chitika.net/getads.js" async></script>


<p><br></p>

<h2>Overview of What we will be Doing</h2>

<ul>
<li>Install Docker on 3 Servers with Ubuntu 16.04</li>
<li>Initialize the Swarm and Join the Worker Nodes</li>
<li>Create a Nginx Service with 2 Replicas</li>
<li>Do some Inspection: View some info on the Service</li>
</ul>


<h2>Prerequisites</h2>

<p>3 Fresh Deployed Ubuntu 16.04 Servers. ( 1GB Memory Servers will be good for development )</p>

<h2>What is Docker</h2>

<p>Docker is a Open Source Technology that allows you to create lightweight, isolated, reproducible application instances which is called Containers. Docker is built on top of the LXC technology, so it uses Linux Containers and as mentioned, it&rsquo;s lightweight compared to a traditional VM.</p>

<p>A Container is isolated and uses the Kernel of the Docker host, it also utilizes Kernel features such as cgroups and namespaces in order to make them isolated.</p>

<h2>Installing Docker Community Edition</h2>

<p>Remove any older versions of Docker that might be present and install the dependencies:</p>

<pre><code class="bash">$ sudo apt remove docker docker-engine -y
$ sudo apt install linux-image-extra-$(uname -r) linux-image-extra-virtual python-setuptools -y
$ sudo apt install apt-transport-https ca-certificates curl software-properties-common -y
</code></pre>

<p>Get the needed repository to setup Docker Community Edition:</p>

<pre><code class="bash">$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
$ sudo apt-key fingerprint 0EBFCD88
$ sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
</code></pre>

<p>Update the repository index and Install Docker Community Edition:</p>

<pre><code class="bash">$ sudo apt update
$ sudo apt install docker-ce -y
$ sudo easy_install pip
$ sudo pip install docker-compose
</code></pre>

<p>Enable Docker on Startup and Start the Docker Engine:</p>

<pre><code class="bash">$ sudo systemctl enable docker
$ sudo systemctl restart docker
</code></pre>

<p>If you would like to execute your docker commands without sudo, add your user to the docker group:</p>

<pre><code class="bash">$ sudo usermod -aG docker $(whoami)
</code></pre>

<p>Test your Setup by Running a Hello World Container. You will see that if the image is not in the local docker image cache, it will pull the image from docker hub (or the respective docker registry), then once the image is saved locally, docker will then instantiate the container from that image:</p>

<pre><code class="bash">$ docker run hello-world
Unable to find image 'hello-world:latest' locally
latest: Pulling from library/hello-world
78445dd45222: Pull complete
Digest: sha256:c5515758d4c5e1e838e9cd307f6c6a0d620b5e07e6f927b07d05f6d12a1ac8d7
Status: Downloaded newer image for hello-world:latest

Hello from Docker!
This message shows that your installation appears to be working correctly.
</code></pre>

<h2>DNS Configuration</h2>

<p>If you have a DNS Server you can configure the A Records for these hosts on DNS, but for simplicity, I will add the noted IP Addresses from the previous step into my <code>/etc/hosts</code> file so we can resolve names to IP&rsquo;s</p>

<p>Open up the the hosts file:</p>

<pre><code class="bash">$ sudo vim /etc/hosts
</code></pre>

<p>In my example, my IP Addresses:</p>

<pre><code>192.0.2.41  manager
192.0.2.42  worker-1
192.0.2.43  worker-2
</code></pre>

<p>Repeat the above steps on the other 2 Servers and make note of the IP Addresses of each node. You should be able to ping and reach the nodes that was configured. Make sure to allow all traffic between these nodes.</p>

<h2>Initialize the Swarm:</h2>

<p>Now we will initialize the swarm on the manager node and as we have more than one network interface, we will specify the &ndash;advertise-addr option:</p>

<pre><code class="bash">$ docker swarm init --advertise-addr 192.0.2.41
Swarm initialized: current node (siqyf3yricsvjkzvej00a9b8h) is now a manager.

    To add a worker to this swarm, run the following command:

    docker swarm join \
    --token SWMTKN-1-0eith07xkcg93lzftuhjmxaxwfa6mbkjsmjzb3d3sx9cobc2zp-97s6xzdt27y2gk3kpm0cgo6y2 \
    192.0.2.41:2377

    To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
</code></pre>

<p>From the response above, we received the join token that allows the workers to register with the manager node. If its a scenario where you want to have more than one manager node, you can run <code>docker swarm join-token manager</code> to receive the join token for additional manager.</p>

<p>Let&rsquo;s add the two worker nodes to the manager:</p>

<pre><code class="bash">$ [worker-1] docker swarm join --token SWMTKN-1-0eith07xkcg93lzftuhjmxaxwfa6mbkjsmjzb3d3sx9cobc2zp-97s6xzdt27y2gk3kpm0cgo6y2 192.0.2.41:2377
This node joined a swarm as a worker.
</code></pre>

<pre><code class="bash">$ [worker-2] docker swarm join --token SWMTKN-1-0eith07xkcg93lzftuhjmxaxwfa6mbkjsmjzb3d3sx9cobc2zp-97s6xzdt27y2gk3kpm0cgo6y2 192.0.2.41:2377
This node joined a swarm as a worker.
</code></pre>

<p>To see the node status, so that we can determine if the nodes are active/available etc, from the manager node, list all the nodes in the swarm:</p>

<pre><code class="bash">[manager] $ docker node ls
ID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS
j14mte3v1jhtbm3pb2qrpgwp6    worker-1  Ready   Active 
siqyf3yricsvjkzvej00a9b8h *  master    Ready   Active        Leader
srl5yzme5hxnzxal2t1efmwje    worker-2  Ready   Active
</code></pre>

<h2>Reobtaining the Join Tokens</h2>

<p>If at any time, you lost your join token, it can be retrieved by running the following for the manager token:</p>

<pre><code class="bash">$ docker swarm join-token manager -q SWMTKN-1-67chzvi4epx28ii18gizcia8idfar5hokojz660igeavnrltf0-09ijujbnnh4v960b8xel58pmj
</code></pre>

<p>And the following to retrieve the worker token:</p>

<pre><code class="bash">$ docker swarm join-token worker -q SWMTKN-1-67chzvi4epx28ii18gizcia8idfar5hokojz660igeavnrltf0-acs21nn28v17uwhw0oqg5ibwx
</code></pre>

<p>Swarm Services in Docker uses a declarative model which means that you define the desired state of the service, and rely on Docker to maintain this state. More information on this can be found on their <a href="https://docs.docker.com/engine/swarm/how-swarm-mode-works/services/">Documentation</a></p>

<p>At this moment, we will see that we have no services running in our swarm:</p>

<pre><code class="bash">[manager] $ docker service ls
ID  NAME  MODE  REPLICAS  IMAGE
</code></pre>

<h2>Deploying our First Service</h2>

<p>Now onto the creation of a standard nginx service with 2 replicas, which means that there will be 2 containers of nginx running in our swarm.</p>

<p>But first, we need to create a overlay network, which is a network driver that creates a distributed network among multiple Docker daemon hosts. Swarm takes care of the routing automatically, which is routed via the port mappings. So you can have that your container sits on worker-2, when you hit your manager node on the published port, it will route the request to the desired application that resides on the respective container.</p>

<p>To create a overlay network called mynet:</p>

<pre><code class="bash">[manager] $ docker network create --driver overlay mynet
</code></pre>

<p>Now onto creating the Service. If any of these containers fail, they will handled by the manager node and will be spawned again to have the desired number that we set on the replica option:</p>

<pre><code class="bash">[manager] $ docker service create --name my-web --publish 8080:80 --replicas 2 --network mynet nginx
</code></pre>

<p>Let&rsquo;s have a look at our nginx service:</p>

<pre><code class="bash">[manager] $ docker service ls
ID            NAME    MODE        REPLICAS  IMAGE
1okycpshfusq  my-web  replicated  2/2       nginx:latest
</code></pre>

<p>After we see that the replica count is 2/2 our service is ready.</p>

<p>To see on which nodes our containers are running that makes up our service:</p>

<pre><code class="bash">[manager] $ docker service ps my-web
ID            NAME      IMAGE         NODE      DESIRED STATE  CURRENT STATE           ERROR  PORTS
k0qqrh8s0c2d  my-web.1  nginx:latest  worker-1  Running        Running 30 seconds ago
nku9wer6tmll  my-web.2  nginx:latest  worker-2  Running        Running 30 seconds ago
</code></pre>

<p>From the above output, we can see that worker-1 and worker-2 are serving our containers for our service. We can also retrieve more information of our service by using the inspect option, which will give you a detailed response in json format of the service:</p>

<pre><code class="bash">[manager] $ docker service inspect my-web
</code></pre>

<p>We can get the Endpoint Port info by using inspect and using the &ndash;format parameter to filter the output:</p>

<pre><code class="bash">[manager] $ docker service inspect --format="" my-web  | python -m json.tool
</code></pre>

<p>From the output we will find the PublishedPort is the Port that we Expose, which will be the listener. Our TargetPort will be the port that is listening on the container:</p>

<pre><code class="json">[
    {
        "Protocol": "tcp",
        "PublishMode": "ingress",
        "PublishedPort": 8080,
        "TargetPort": 80
    }
]
</code></pre>

<p>Now that we went through the inspection of our service, its time to test our base nginx service.</p>

<h2>Testing Nginx in our Swarm</h2>

<p>Make a request against your docker node manager address on the port that was exposed, in this case 8080:</p>

<pre><code class="bash">$ curl -I http://docker-node-manager-ip:8080

HTTP/1.1 200 OK
Server: nginx/1.15.5
Date: Thu, 10 Jan 2019 14:48:40 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Tue, 02 Oct 2018 14:49:27 GMT
Connection: keep-alive
ETag: "5bb38577-264"
Accept-Ranges: bytes
</code></pre>

<p>Now we have successfull setup a 3 node docker swarm cluster and deployed a basic nginx service to our swarm. Please have a look at my other <a href="https://blog.ruanbekker.com/blog/categories/docker/">Docker Swarm Tutorials</a> for other content.</p>

<h2>Thank You</h2>

<p>Please feel free to show support by, <strong>sharing</strong> this post, making a <strong>donation</strong>, <strong>subscribing</strong> or <strong>reach out to me</strong> if you want me to demo and write up on any specific tech topic.</p>

<center>
<form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_top">
<input type="hidden" name="cmd" value="_s-xclick" />
<input type="hidden" name="hosted_button_id" value="W7CBGYTCWGANQ" />
<input type="image" src="https://user-images.githubusercontent.com/567298/49853901-461c3700-fdf1-11e8-9d80-8a424a3173af.png" border="0" name="submit" title="PayPal - The safer, easier way to pay online!" alt="Donate with PayPal button" />
</form>
</center>




<p><p></p>

<p>Thanks for reading!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tutorial on Using Gitlab CI/CD Pipelines to Deploy Your Python Flask Restful API With Postgres on Heroku]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/01/05/tutorial-on-using-gitlab-cicd-pipelines-to-deploy-your-python-flask-restful-api-with-postgres-on-heroku/"/>
    <updated>2019-01-05T10:27:31-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/01/05/tutorial-on-using-gitlab-cicd-pipelines-to-deploy-your-python-flask-restful-api-with-postgres-on-heroku</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/50217968-0629f680-0393-11e9-8387-ad69937eb891.png" alt="" /></p>

<p>Today we will build a Restful API using Python Flask, SQLAlchemy using Postgres as our Database, testing using Python Unittest, a CI/CD Pipeline on Gitlab, and Deployment to Heroku.</p>

<p>From our previous post, we demonstrated setting up a <a href="https://blog.ruanbekker.com/blog/2018/12/20/setup-a-gitlab-runner-on-your-own-server-to-run-your-jobs-that-gets-triggered-from-gitlab-ci/">Custom Gitlab Runner on Your Own Server for  Gitlab CI</a>.</p>

<h2>Heroku</h2>

<p>If you don&rsquo;t have an account already, <a href="https://dashboard.heroku.com/account">Heroku</a> offer&rsquo;s 5 free applications in their free tier account. Once you have created your account, create 2 applications. I named mine flask-api-staging and flask-api-prod.</p>

<p>You can create the applications via cli or the ui, from the ui it will look more or less like this:</p>

<p><img src="https://user-images.githubusercontent.com/567298/50719120-69f53480-10a0-11e9-8720-aa3dc007b6fc.png" alt="" /></p>

<p>Select an app name and check if the name is available then select create. Note down the name and config as we will use it in our <code>.gitlab-ci.yml</code> config:</p>

<p><img src="https://user-images.githubusercontent.com/567298/50719134-8db87a80-10a0-11e9-80f4-cccd9a86752f.png" alt="" /></p>

<h2>Heroku API Key</h2>

<p>To allow the deployment of applications to Heroku from Gitlab, we need to generate a API Key on Heroku and save the config in Gitlab.</p>

<p>Head over to your <a href="https://dashboard.heroku.com/account">Heroku Dashboard</a>, select Account Settings, scroll to the API Key section and generate a API Key.</p>

<p>Head over to your Gitlab Repository, select Settings, CI/CD, then select Variables enter the Key: HEROKU_API_KEY and the Secret of the API Key into the Value and select Save Variable.</p>

<p>We will reference this variable from our deploy steps.</p>

<h2>Heroku Postgres Add-on</h2>

<p>Heroku offers a free Postgres Add-On, to activate: Select your application, select Resources, search for the Add-on <code>Heroku Postgres</code>, select and select the Hobby Dev Free version and select provision.</p>

<h2>Our Application Code</h2>

<p>Clone your repository then let&rsquo;s start by creating our Flask API. Note this is more on Gitlab CI/CD than going into detail into the Flask Application.</p>

<p>Create the files that we will need:</p>

<pre><code class="bash">$ touch app.py config.cfg requirements.txt tests.py Procfile
</code></pre>

<p>Let&rsquo;s start by populating our configuration for our flask app: <code>config.cfg</code></p>

<pre><code class="bash">#SQLALCHEMY_DATABASE_URI='sqlite:///database.db'
SQLALCHEMY_TRACK_MODIFICATIONS=False
</code></pre>

<p>Our Flask Application: <code>app.py</code></p>

<p>Note that we are using flask-heroku, with this package Heroku will automatically discover your configuration for your database using environment variables. So if you have a postgres add-on, you don&rsquo;t need to specify the location of your database.</p>

<p>If you want to use sqlite, you can remove the heroku instantiation and uncomment the <code>SQLALCHEMY_DATABASE_URI</code> property in your <code>config.cfg</code></p>

<pre><code class="python">from flask import Flask, jsonify, request
from flask_sqlalchemy import SQLAlchemy
from flask_marshmallow import Marshmallow
from flask_heroku import Heroku
from passlib.hash import sha256_crypt
from datetime import datetime

app = Flask(__name__)
app.config.from_pyfile('config.cfg')
heroku = Heroku(app)
db = SQLAlchemy(app)
ma = Marshmallow(app)

## --Database Models--
class Member(db.Model):
    __tablename__ = 'members'

    id = db.Column(db.Integer, primary_key=True, autoincrement=True)
    email = db.Column(db.String(255), unique=True, nullable=False)
    username = db.Column(db.String(50), unique=True)
    password_hash = db.Column(db.String(100))
    firstname = db.Column(db.String(50), unique=False)
    lastname = db.Column(db.String(50), unique=False)
    registered_on = db.Column(db.DateTime, nullable=False)

class MemberSchema(ma.ModelSchema):
    class Meta:
        model = Member
        fields = ('id', 'username', 'email')

member_schema = MemberSchema(strict=True, only=('id', 'username'))
members_schema = MemberSchema(strict=True, many=True)

## --Views--
@app.route('/')
def index():
    return jsonify({'message': 'ok'}), 200

# list users
@app.route('/api/user', methods=['GET'])
def list_users():
    all_users = Member.query.all()
    result = members_schema.dump(all_users)
    return jsonify(result.data)

# get user
@app.route('/api/user/&lt;int:id&gt;', methods=['GET'])
def get_user(id):
    user = Member.query.get(id)
    result = member_schema.dump(user)
    return jsonify(result.data)

# add user
@app.route('/api/user', methods=['POST'])
def add_user():
    email = request.json['email']
    username = request.json['username']
    password_hash = sha256_crypt.encrypt(request.json['password'])
    firstname = request.json['firstname']
    lastname = request.json['lastname']
    new_user = Member(email=email, username=username, password_hash=password_hash, firstname=firstname, lastname=lastname, registered_on=datetime.utcnow())
    try:
        db.session.add(new_user)
        db.session.commit()
        result = member_schema.dump(Member.query.get(new_user.id))
        return jsonify({'member': result.data})
    except:
        db.session.rollback()
        result = {'message': 'error'}
        return jsonify(result)

# update user
@app.route('/api/user/&lt;int:id&gt;', methods=['PUT'])
def update_user(id):
    user = Member.query.get(id)
    username = request.json['username']
    email = request.json['email']
    user.email = email
    user.username = username
    db.session.commit()
    return member_schema.jsonify(user)

# delete user
@app.route('/api/user/&lt;int:id&gt;', methods=['DELETE'])
def delete_user(id):
    user = Member.query.get(id)
    db.session.delete(user)
    db.session.commit()
    return jsonify({'message': '{} has been deleted'.format(user.username)})

if __name__ == '__main__':
    app.run()
</code></pre>

<p>Our tests: <code>tests.py</code></p>

<pre><code class="python">import unittest
import app as myapi
import json
import sys

class TestFlaskApi(unittest.TestCase):
    def setUp(self):
        self.app = myapi.app.test_client()

    def test_hello_world(self):
        response = self.app.get('/')
        self.assertEqual(
            json.loads(response.get_data().decode(sys.getdefaultencoding())),
            {"message": "ok"}
        )

if __name__ == '__main__':
    unittest.main()
</code></pre>

<p>Our requirements file: <code>requirements.txt</code></p>

<pre><code>Click==7.0
Flask==1.0.2
flask-heroku==0.1.9
flask-marshmallow==0.9.0
Flask-SQLAlchemy==2.3.2
gunicorn==19.9.0
itsdangerous==1.1.0
Jinja2==2.10
MarkupSafe==1.1.0
marshmallow==2.17.0
marshmallow-sqlalchemy==0.15.0
passlib==1.7.1
psycopg2-binary==2.7.6.1
six==1.12.0
SQLAlchemy==1.2.15
Werkzeug==0.14.1
</code></pre>

<p>Our Procfile for Heroku: <code>Procfile</code></p>

<pre><code>web: gunicorn app:app
</code></pre>

<p>And lastly, our gitlab-ci configuration which will include our build, test and deploy steps. As soon as a commit to master is received the pipeline will be acticated. Note that our production deploy step is a manual trigger.</p>

<p>Our config for <code>.gitlab-ci.yml</code>. Note to replace your Heroku app names.</p>

<pre><code class="yaml">image: rbekker87/build-tools:latest

stages:
  - ver
  - init
  - tests
  - deploy

ver:
  stage: ver
  script:
    - python --version
    - whoami

init:
  stage: init
  script:
    - apk add postgresql-dev --no-cache
    - pip install psycopg2-binary
    - pip install -r requirements.txt

run_tests:
  stage: tests
  script:
    - apk add postgresql-dev --no-cache
    - pip install psycopg2-binary
    - pip install -r requirements.txt
    - python tests.py

deploy_staging:
  stage: deploy
  script:
    - git remote add heroku https://heroku:$HEROKU_API_KEY@git.heroku.com/flask-api-staging.git
    - git push heroku master
    - echo "Deployed to Staging Server https://flask-api-staging.herokuapp.com"
  environment:
    name: staging
    url: https://flask-api-staging.herokuapp.com/
  only:
    - master

deploy_production:
  stage: deploy
  script:
    - git remote add heroku https://heroku:$HEROKU_API_KEY@git.heroku.com/flask-api-prod.git
    - git push heroku master
    - echo "Deployed to Production Server https://flask-api-prod.herokuapp.com"
  environment:
    name: production
    url: https://flask-api-prod.herokuapp.com/
  when: manual
  only:
    - master
</code></pre>

<h2>Send to Gitlab:</h2>

<p>Once everything is populated, stage your changes, commit your work and push to master:</p>

<pre><code class="bash">$ git add .
$ git commit -m "blogpost demo commit"
$ git push origin master
</code></pre>

<p>Once the code has been pushed to master, gitlab will pick it up and trigger the pipeline to run.</p>

<h2>Gitlab Pipelines</h2>

<p>Head over to Gitlab, select CI/CD -> Pipelines, you should see a running pipeline, select it, then you should see the overview of all your jobs:</p>

<p><img src="https://user-images.githubusercontent.com/567298/50728734-83958b00-1137-11e9-8c6f-4f7d9aaab46d.png" alt="" /></p>

<p>If everything has passed you should see the <code>Passed</code> status as shown above.</p>

<p>You will notice that the staging environment has been deployed. Now you can do some testing and when you are happy with it, you can select the play button which will deploy to production on the pipelines dashboard.</p>

<h2>Creating the Tables on Postgres</h2>

<p>Before we can interact with our API, we need to provision the postgres tables from the database models that we wrote in our application.</p>

<p>Open up a Python shell on Heroku and initialize the tables:</p>

<pre><code class="bash">$ heroku run python -a flask-api-prod
&gt;&gt;&gt; from app import db
&gt;&gt;&gt; db.create_all()
&gt;&gt;&gt; exit()
</code></pre>

<h2>Testing the API:</h2>

<p>Now that everything is up and running, its time to test our API.</p>

<p>List the users:</p>

<pre><code class="bash">$ curl https://flask-api-staging.herokuapp.com/api/user
[]
</code></pre>

<p>Create a User:</p>

<pre><code class="bash">$ curl -H 'Content-Type: application/json' -XPOST https://flask-api-staging.herokuapp.com/api/user -d '{"username": "ruanb", "password": "pass", "email": "r@r.com", "firstname": "ruan", "lastname": "bekker"}'
{
  "member": {
    "id": 1,
    "username": "ruanb"
  }
}
</code></pre>

<p>List Users:</p>

<pre><code class="bash">$ curl -H 'Content-Type: application/json' -XGET https://flask-api-staging.herokuapp.com/api/user
[
  {
    "email": "ruan@r.com",
    "id": 1,
    "username": "ruanb"
  }
]
</code></pre>

<p>Update a User&rsquo;s email address:</p>

<pre><code class="bash">$ curl -H 'Content-Type: application/json' -XPUT https://flask-api-staging.herokuapp.com/api/user/1 -d '{"username": "ruanb", "email": "ruan@r.com"}'
{
  "id": 1,
  "username": "ruanb"
}
</code></pre>

<p>Retrieve a single user:</p>

<pre><code class="bash">$ curl -H 'Content-Type: application/json' -XGET https://flask-api-staging.herokuapp.com/api/user/1
{
  "email": "ruan@r.com",
  "id": 1,
  "username": "ruanb"
}
</code></pre>

<p>Delete User:</p>

<pre><code class="bash">$ curl -H 'Content-Type: application/json' -XDELETE https://flask-api-staging.herokuapp.com/api/user/1
{
  "message": "ruanb has been deleted"
}
</code></pre>

<h2>Troubleshooting</h2>

<p>I had some issues with Heroku, where one was after I deployed, I received this error in Heroku&rsquo;s logs:</p>

<pre><code class="bash">code=H14 desc="No web processes running" method=GET path="/"
</code></pre>

<p>I just had to scale my web dyno to 1:</p>

<pre><code class="bash">$ heroku ps:scale web=1 -a flask-api-staging
Scaling dynos... done, now running web at 1:Free
</code></pre>

<p>Have a look at their <a href="https://devcenter.heroku.com/articles/heroku-cli">documentation</a> if you need help with the heroku cli.</p>

<p>And to troubleshoot within the dyno, you can exec into it by running this:</p>

<pre><code>heroku ps:exec -a flask-api-staging
</code></pre>

<p>I seriously dig Gitlab-CI and with this demonstration you can see how easy it is to setup a CI/CD Pipeline on Gitlab and Deploy them to Heroku.</p>

<h2>Resources:</h2>

<p>The code for this demo is available at: <a href="https://gitlab.com/rbekker87/demo-cicd-flask-heroku">gitlab.com/rbekker87/demo-cicd-flask-heroku</a></p>

<p>For more blog posts on Gitlab, have a look at my <a href="https://blog.ruanbekker.com/blog/categories/gitlab/">gitlab</a> category on <a href="https://blog.ruanbekker.com/">blog.ruanbekker.com</a></p>

<h2>Thank You</h2>

<p>Please feel free to show support by, <strong>sharing</strong> this post, making a <strong>donation</strong>, <strong>subscribing</strong> or <strong>reach out to me</strong> if you want me to demo and write up on any specific tech topic.</p>

<center>
<form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_top">
<input type="hidden" name="cmd" value="_s-xclick" />
<input type="hidden" name="hosted_button_id" value="W7CBGYTCWGANQ" />
<input type="image" src="https://user-images.githubusercontent.com/567298/49853901-461c3700-fdf1-11e8-9d80-8a424a3173af.png" border="0" name="submit" title="PayPal - The safer, easier way to pay online!" alt="Donate with PayPal button" />
</form>
</center>


<p><br></p>

<p>Ad space:</p>

<script type="text/javascript">
  ( function() {
    if (window.CHITIKA === undefined) { window.CHITIKA = { 'units' : [] }; };
    var unit = {"calltype":"async[2]","publisher":"rbekker87","width":728,"height":90,"sid":"Chitika Default"};
    var placement_id = window.CHITIKA.units.length;
    window.CHITIKA.units.push(unit);
    document.write('<div id="chitikaAdBlock-' + placement_id + '"></div>');
}());
</script>


<script type="text/javascript" src="//cdn.chitika.net/getads.js" async></script>




<p><p></p>

<p>Thanks for reading!</p>
]]></content>
  </entry>
  
</feed>
