<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Docker | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/docker/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2023-05-22T02:46:18-04:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Running a Multi-Broker Kafka Cluster on Docker]]></title>
    <link href="https://blog.ruanbekker.com/blog/2023/05/17/running-a-multi-broker-kafka-cluster-on-docker/"/>
    <updated>2023-05-17T10:50:57-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2023/05/17/running-a-multi-broker-kafka-cluster-on-docker</id>
    <content type="html"><![CDATA[<p>In this post we will run a kakfa cluster with 3 kafka brokers on docker compose and using a producer to send messages to our topics and a consumer that will receive the messages from the topics.</p>

<h2>What is Kafka?</h2>

<p>Kafka is a distributed event store and stream processing platform. Kafka is used to build real-time streaming data pipelines and real-time streaming applications.</p>

<p>This is a fantastic resource if you want to understand the components better in detail:
- <a href="https://www.upsolver.com/blog/apache-kafka-architecture-what-you-need-to-know">https://www.upsolver.com/blog/apache-kafka-architecture-what-you-need-to-know</a></p>

<p>But on a high level, the components of a typical Kafka setup:</p>

<ol>
<li>Zookeeper: Kafka relies on Zookeeper to do leadership election of Kafka Brokers and Topic Partitions.</li>
<li>Broker: Kafka server that receives messages from producers, assigns them to offsets and commit the messages to disk storage. A offset is used for data consistency in a event of failure, so that consumers know from where to consume from their last message.</li>
<li>Topic: A topic can be thought of categories to organize messages. Producers writes messages to topics, consumers reads from those topics.</li>
<li>Partitions: A topic is split into multiple partitions. This improves scalability through parallelism (not just one broker). Kafka also does replication</li>
</ol>


<p>For great in detail information about kafka and its components, I encourage you to visit the <a href="https://www.upsolver.com/blog/apache-kafka-architecture-what-you-need-to-know">mentioned post</a> from above.</p>

<h2>Launch Kafka</h2>

<p>This is the <code>docker-compose.yaml</code> that we will be using to run a kafka cluster with 3 broker containers, 1 zookeeper container, 1 producer, 1 consumer and a kafka-ui.</p>

<p>All the source code is available on my <a href="https://github.com/ruanbekker/quick-starts/tree/main/docker/kafka">quick-starts github repository</a> .</p>

<pre><code class="yaml">version: "3.9"

services:
  zookeeper:
    platform: linux/amd64
    image: confluentinc/cp-zookeeper:${CONFLUENT_PLATFORM_VERSION:-7.4.0}
    container_name: zookeeper
    restart: unless-stopped
    ports:
      - '32181:32181'
      - '2888:2888'
      - '3888:3888'
    environment:
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_CLIENT_PORT: 32181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_SERVERS: zookeeper:2888:3888
    healthcheck:
      test: echo stat | nc localhost 32181
      interval: 10s
      timeout: 10s
      retries: 3
    networks:
      - kafka
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

  kafka-ui: 
    container_name: kafka-ui 
    image: provectuslabs/kafka-ui:latest
    ports:
      - 8080:8080
    depends_on:
      - broker-1
      - broker-2
      - broker-3
    environment:
      KAFKA_CLUSTERS_0_NAME: broker-1
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: broker-1:29091
      KAFKA_CLUSTERS_0_METRICS_PORT: 19101
      KAFKA_CLUSTERS_1_NAME: broker-2
      KAFKA_CLUSTERS_1_BOOTSTRAPSERVERS: broker-2:29092
      KAFKA_CLUSTERS_1_METRICS_PORT: 19102
      KAFKA_CLUSTERS_2_NAME: broker-3
      KAFKA_CLUSTERS_2_BOOTSTRAPSERVERS: broker-3:29093
      KAFKA_CLUSTERS_2_METRICS_PORT: 19103
      DYNAMIC_CONFIG_ENABLED: 'true'
    networks:
      - kafka
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

  broker-1:
    platform: linux/amd64
    image: confluentinc/cp-kafka:${CONFLUENT_PLATFORM_VERSION:-7.4.0}
    container_name: broker-1
    restart: unless-stopped
    ports:
      - '9091:9091'
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker-1:29091,EXTERNAL://localhost:9091
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_JMX_PORT: 19101
      KAFKA_JMX_HOSTNAME: localhost
    healthcheck:
      test: nc -vz localhost 9091
      interval: 10s
      timeout: 10s
      retries: 3
    networks:
      - kafka
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

  broker-2:
    platform: linux/amd64
    image: confluentinc/cp-kafka:${CONFLUENT_PLATFORM_VERSION:-7.4.0}
    container_name: broker-2
    restart: unless-stopped
    ports:
      - '9092:9092'
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker-2:29092,EXTERNAL://localhost:9092
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_JMX_PORT: 19102
      KAFKA_JMX_HOSTNAME: localhost
    healthcheck:
      test: nc -vz localhost 9092
      interval: 10s
      timeout: 10s
      retries: 3
    networks:
      - kafka
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

  broker-3:
    platform: linux/amd64
    image: confluentinc/cp-kafka:${CONFLUENT_PLATFORM_VERSION:-7.4.0}
    container_name: broker-3
    restart: unless-stopped
    ports:
      - '9093:9093'
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker-3:29093,EXTERNAL://localhost:9093
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_JMX_PORT: 19103
      KAFKA_JMX_HOSTNAME: localhost
    healthcheck:
      test: nc -vz localhost 9093
      interval: 10s
      timeout: 10s
      retries: 3
    networks:
      - kafka
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

  producer:
    platform: linux/amd64
    container_name: producer
    build: ./python-client/
    restart: always
    environment:
      - ACTION=producer
      - BOOTSTRAP_SERVERS=broker-1:29091,broker-2:29092,broker-3:29093
      - TOPIC=my-topic
      - PYTHONUNBUFFERED=1 # https://github.com/docker/compose/issues/4837#issuecomment-302765592
    networks:
      - kafka
    depends_on:
      - zookeeper
      - broker-1
      - broker-2
      - broker-3
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

  consumer:
    platform: linux/amd64
    container_name: consumer
    build: ./python-client/
    restart: always
    environment:
      - ACTION=consumer
      - BOOTSTRAP_SERVERS=broker-1:29091,broker-2:29092,broker-3:29093
      - TOPIC=my-topic
      - CONSUMER_GROUP=cg-group-id
      - PYTHONUNBUFFERED=1 # https://github.com/docker/compose/issues/4837#issuecomment-302765592
    networks:
      - kafka
    depends_on:
      - zookeeper
      - broker-1
      - broker-2
      - broker-3
      - producer
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

networks:
  kafka:
    name: kafka
</code></pre>

<p>In our compose file we defined our core stack:</p>

<ul>
<li>1 Zookeeper Container</li>
<li>3 Kafka Broker Containers</li>
<li>1 Kafka UI</li>
</ul>


<p>Then we have our clients:</p>

<ul>
<li>1 Producer that will send messages to our topics (source code: <a href="https://github.com/ruanbekker/quick-starts/blob/main/docker/kafka/python-client/produce.py">https://github.com/ruanbekker/quick-starts/blob/main/docker/kafka/python-client/produce.py</a> )</li>
<li>1 Consumer that will read the messages from our topics (source code: <a href="https://github.com/ruanbekker/quick-starts/blob/main/docker/kafka/python-client/consume.py">https://github.com/ruanbekker/quick-starts/blob/main/docker/kafka/python-client/consume.py</a> )</li>
</ul>


<p>We can boot the stack with:</p>

<pre><code class="bash">docker-compose up --build -d
</code></pre>

<p>You can verify that the brokers are passing their health checks with:</p>

<pre><code class="bash">docker-compose ps
</code></pre>

<h2>Producers and Consumers</h2>

<p>The producer generates random data and sends it to a topic, where the consumer will listen on the same topic and read messages from that topic.</p>

<p>To view the output of what the <code>producer</code> is doing, you can tail the logs:</p>

<pre><code class="bash">docker logs -f producer
</code></pre>

<p>And to view the output of what the <code>consumer</code> is doing, you can tail the logs:</p>

<pre><code class="bash">docker logs -f consumer
</code></pre>

<h2>Kafka UI</h2>

<p>The Kafka UI will be available on <a href="http://localhost:8080">http://localhost:8080</a></p>

<h2>Thank You</h2>

<p>Thanks for reading, feel free to check out my <a href="https://ruan.dev/">website</a>, feel free to subscribe to my <a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a> or follow me at <a href="https://twitter.com/ruanbekker">@ruanbekker</a> on Twitter.</p>

<ul>
<li>Linktree: <a href="https://go.ruan.dev/links">https://go.ruan.dev/links</a></li>
<li>Patreon: <a href="https://go.ruan.dev/patreon">https://go.ruan.dev/patreon</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started With Wiremock]]></title>
    <link href="https://blog.ruanbekker.com/blog/2023/01/14/getting-started-with-wiremock/"/>
    <updated>2023-01-14T17:03:12-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2023/01/14/getting-started-with-wiremock</id>
    <content type="html"><![CDATA[<p>In this tutorial we will use docker to run an instance of wiremock to setup a mock api for us to test our api&rsquo;s.</p>

<h2>Wiremock</h2>

<p><a href="https://wiremock.org/">Wiremock</a> is a tool for building mock API&rsquo;s which enables us to build stable development environments.</p>

<h2>Docker and Wiremock</h2>

<p>Run a wiremock instance with docker:</p>

<pre><code class="bash">docker run -it --rm -p 8080:8080 --name wiremock wiremock/wiremock:2.34.0
</code></pre>

<p>Then our wiremock instance will be exposed on port 8080 locally, which we can use to make a request against to create a api mapping:</p>

<pre><code class="bash">curl -XPOST -H "Content-Type: application/json" \
  http://localhost:8080/__admin/mappings
  -d '{"request": {"url": "/testapi","method": "GET"}, "response": {"status": 200, "body": "{\"result\": \"ok\"
}", "headers": {"Content-Type": "application/json"}}}'
</code></pre>

<p>The response should be something like this:</p>

<pre><code class="json">{
    "id" : "223a2c0a-8b43-42dc-8ba6-fe973da1e420",
    "request" : {
      "url" : "/testapi",
      "method" : "GET"
    },
    "response" : {
      "status" : 200,
      "body" : "{\"result\": \"ok\"}",
      "headers" : {
        "Content-Type" : "application/json"
      }
    },
    "uuid" : "223a2c0a-8b43-42dc-8ba6-fe973da1e420"
}
</code></pre>

<h2>Test Wiremock</h2>

<p>If we make a GET request against our API:</p>

<pre><code class="bash">curl http://localhost:8080/testapi
</code></pre>

<p>Our response should be:</p>

<pre><code class="json">{
  "result": "ok"
}
</code></pre>

<h2>Export Wiremock Mappings</h2>

<p>We can export our mappings to a local file named <code>stubs.json</code> with:</p>

<pre><code class="bash">curl -s http://localhost:8080/__admin/mappings --output stubs.json
</code></pre>

<h2>Import Wiremock Mappings</h2>

<p>We can import our mappings from our <code>stubs.json</code> file with:</p>

<pre><code class="bash">curl -XPOST -v --data-binary @stubs.json http://localhost:8080/__admin/mappings/import
</code></pre>

<h2>Resources</h2>

<ul>
<li><a href="https://wiremock.org/docs/docker/">https://wiremock.org/docs/docker/</a></li>
<li><a href="https://github.com/WireMock-Net/WireMock.Net/wiki/Admin-API-Reference">https://github.com/WireMock-Net/WireMock.Net/wiki/Admin-API-Reference</a></li>
</ul>


<h2>Thank You</h2>

<p>Thanks for reading, feel free to check out my <a href="https://ruan.dev/">website</a>, feel free to subscribe to my <a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a> or follow me at <a href="https://twitter.com/ruanbekker">@ruanbekker</a> on Twitter.</p>

<ul>
<li>Linktree: <a href="https://go.ruan.dev/links">https://go.ruan.dev/links</a></li>
<li>Patreon: <a href="https://go.ruan.dev/patreon">https://go.ruan.dev/patreon</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Logging With Docker Promtail and Grafana Loki]]></title>
    <link href="https://blog.ruanbekker.com/blog/2022/11/18/logging-with-docker-promtail-and-grafana-loki/"/>
    <updated>2022-11-18T00:42:49-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2022/11/18/logging-with-docker-promtail-and-grafana-loki</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/202631247-4ee94f01-b34a-471f-b428-6aba80b31e8c.png" alt="grafana-loki-promtail" /></p>

<p>In this post we will use Grafana Promtail to collect all our logs and ship it to Grafana Loki.</p>

<h2>About</h2>

<p>We will be using Docker Compose and mount the docker socket to Grafana Promtail so that it is aware of all the docker events and configure it that only containers with docker labels <code>logging=promtail</code> needs to be enabled for logging, which will then scrape those logs and send it to Grafana Loki where we will visualize it in Grafana.</p>

<h2>Promtail</h2>

<p>In our promtail configuration <code>config/promtail.yaml</code>:</p>

<pre><code class="yaml"># https://grafana.com/docs/loki/latest/clients/promtail/configuration/
# https://docs.docker.com/engine/api/v1.41/#operation/ContainerList
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  - job_name: flog_scrape 
    docker_sd_configs:
      - host: unix:///var/run/docker.sock
        refresh_interval: 5s
        filters:
          - name: label
            values: ["logging=promtail"] 
    relabel_configs:
      - source_labels: ['__meta_docker_container_name']
        regex: '/(.*)'
        target_label: 'container'
      - source_labels: ['__meta_docker_container_log_stream']
        target_label: 'logstream'
      - source_labels: ['__meta_docker_container_label_logging_jobname']
        target_label: 'job'
</code></pre>

<p>You can see we are using the <code>docker_sd_configs</code> provider and filter only docker containers with the docker labels <code>logging=promtail</code> and once we have those logs we relabel our labels to have the container name and we also use docker labels like <code>log_stream</code> and <code>logging_jobname</code> to add labels to our logs.</p>

<h2>Grafana Config</h2>

<p>We would like to auto configure our datasources for Grafana and in <code>config/grafana-datasources.yml</code> we have:</p>

<pre><code class="yaml">apiVersion: 1

datasources:
  - name: Loki
    type: loki
    access: proxy
    url: http://loki:3100
    version: 1
    editable: false
    isDefault: true
</code></pre>

<h2>Docker Compose</h2>

<p>Then lastly we have our <code>docker-compose.yml</code> that wire up all our containers:</p>

<pre><code class="yaml">version: '3.8'

services:
  nginx-app:
    container_name: nginx-app
    image: nginx
    labels:
      logging: "promtail"
      logging_jobname: "containerlogs"
    ports:
      - 8080:80
    networks:
      - app

  grafana:
    image: grafana/grafana:latest
    ports:
      - 3000:3000
    volumes:
      - ./config/grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yaml
    environment:
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_DISABLE_LOGIN_FORM=true
    networks:
      - app

  loki:
    image: grafana/loki:latest
    ports:
      - 3100:3100
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - app

  promtail:
    image:  grafana/promtail:latest
    container_name: promtail
    volumes:
      - ./config/promtail.yaml:/etc/promtail/docker-config.yaml
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock
    command: -config.file=/etc/promtail/docker-config.yaml
    depends_on:
      - loki
    networks:
      - app

networks:
  app:
    name: app
</code></pre>

<p>As you can see with our nginx container we define our labels:</p>

<pre><code class="yaml">  nginx-app:
    container_name: nginx-app
    image: nginx
    labels:
      logging: "promtail"
      logging_jobname: "containerlogs"
</code></pre>

<p>Which uses <code>logging: "promtail"</code> to let promtail know this log container&rsquo;s log to be scraped and <code>logging_jobname: "containerlogs"</code> which will assign containerlogs to the job label.</p>

<h2>Start the stack</h2>

<p>If you are following along all this configuration is available in my github repository <a href="https://github.com/ruanbekker/docker-promtail-loki">https://github.com/ruanbekker/docker-promtail-loki</a> .</p>

<p>Once you have everything in place you can start it with:</p>

<pre><code class="bash">docker-compose up -d
</code></pre>

<p>Access nginx on <a href="http://localhost:8080">http://localhost:8080</a></p>

<p><img width="1113" alt="image" src="https://user-images.githubusercontent.com/567298/202505252-3cbc2d03-d1d2-48e6-bea7-5db54233b9a2.png"></p>

<p>Then navigate to grafana on <a href="http://localhost:3000">http://localhost:3000</a> and select explore on the left and select the container:</p>

<p><img width="560" alt="image" src="https://user-images.githubusercontent.com/567298/202504989-e05a08a2-eb2f-41a1-85f4-9a11a8affd7c.png"></p>

<p>And you will see the logs:</p>

<p><img width="1425" alt="image" src="https://user-images.githubusercontent.com/567298/202505099-c47b76cc-3090-4eb9-8459-db659d0aac18.png"></p>

<h2>Thank You</h2>

<p>Thanks for reading, feel free to check out my <a href="https://ruan.dev/">website</a>, feel free to subscribe to my <a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a> or follow me at <a href="https://twitter.com/ruanbekker">@ruanbekker</a> on Twitter.</p>

<ul>
<li>Linktree: <a href="https://go.ruan.dev/links">https://go.ruan.dev/links</a></li>
<li>Patreon: <a href="https://go.ruan.dev/patreon">https://go.ruan.dev/patreon</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[KinD for Local Kubernetes Clusters]]></title>
    <link href="https://blog.ruanbekker.com/blog/2022/09/20/kind-for-local-kubernetes-clusters/"/>
    <updated>2022-09-20T02:18:16-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2022/09/20/kind-for-local-kubernetes-clusters</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/191189852-44f2fd39-7ad7-4d0a-a36b-c2889a838649.png" alt="kubernetes-kind" /></p>

<p>In this tutorial we will demonstrate how to use KinD (Kubernetes in Docker) to provision local kubernetes clusters for local development.</p>

<h2>About</h2>

<p>KinD uses container images to run as &ldquo;nodes&rdquo;, so spinning up and tearing down clusters becomes really easy or running multiple or different versions, is as easy as pointing to a different container image.</p>

<p>Configuration such as node count, ports, volumes, image versions can either be controlled via the command line or via configuration, more information on that can be found on their documentation:</p>

<ul>
<li><a href="https://kind.sigs.k8s.io/docs/user/quick-start/">https://kind.sigs.k8s.io/docs/user/quick-start/</a></li>
<li><a href="https://kind.sigs.k8s.io/docs/user/configuration/">https://kind.sigs.k8s.io/docs/user/configuration/</a></li>
</ul>


<h2>Installation</h2>

<p>Follow the <a href="https://kind.sigs.k8s.io/docs/user/quick-start/#installing-with-a-package-manager">docs</a> for more information, but for mac:</p>

<pre><code class="bash">brew install kind
</code></pre>

<p>To verify if kind was installed, you can run:</p>

<pre><code class="bash">kind version
</code></pre>

<h2>Create a Cluster</h2>

<p>Create the cluster with command line arguments, such as cluster name, the container image:</p>

<pre><code class="bash">kind create cluster --name cluster-1 --image kindest/node:v1.24.0
</code></pre>

<p>And the output will look something like this:</p>

<pre><code class="bash">Creating cluster "cluster-1" ...
 ✓ Ensuring node image (kindest/node:v1.24.0) 🖼
 ✓ Preparing nodes 📦
 ✓ Writing configuration 📜
 ✓ Starting control-plane 🕹️
 ✓ Installing CNI 🔌
 ✓ Installing StorageClass 💾
Set kubectl context to "kind-cluster-1"
You can now use your cluster with:

kubectl cluster-info --context kind-cluster-1

Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community 🙂
</code></pre>

<p>I <strong>highly recommend</strong> installing <a href="https://github.com/ahmetb/kubectx">kubectx</a>, which makes it easy to switch between kubernetes contexts.</p>

<h2>Create a Cluster with Config</h2>

<p>If you would like to define your cluster configuration as config, you can create a file <code>default-config.yaml</code> with the following as a 2 node cluster, and specifying version 1.24.0:</p>

<pre><code class="yaml">---
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  image: kindest/node:v1.24.0@sha256:0866296e693efe1fed79d5e6c7af8df71fc73ae45e3679af05342239cdc5bc8e
- role: worker
  image: kindest/node:v1.24.0@sha256:0866296e693efe1fed79d5e6c7af8df71fc73ae45e3679af05342239cdc5bc8e
</code></pre>

<p>Then create the cluster and point the config:</p>

<pre><code class="bash">kind create cluster --name kind-cluster --config default-config.yaml
</code></pre>

<h2>Interact with the Cluster</h2>

<p>View the cluster info:</p>

<pre><code class="bash">kubectl cluster-info --context kind-kind-cluster
</code></pre>

<p>View cluster contexts:</p>

<pre><code class="bash">kubectl config get-contexts
</code></pre>

<p>Use context:</p>

<pre><code class="bash">kubectl config use-context kind-kind-cluster
</code></pre>

<p>View nodes:</p>

<pre><code class="bash">kubectl get nodes -o wide

NAME                         STATUS   ROLES           AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION      CONTAINER-RUNTIME
kind-cluster-control-plane   Ready    control-plane   2m11s   v1.24.0   172.20.0.5    &lt;none&gt;        Ubuntu 21.10   5.10.104-linuxkit   containerd://1.6.4
kind-cluster-worker          Ready    &lt;none&gt;          108s    v1.24.0   172.20.0.4    &lt;none&gt;        Ubuntu 21.10   5.10.104-linuxkit   containerd://1.6.4
</code></pre>

<h2>Deploy Sample Application</h2>

<p>We will create a deployment, a service and port-forward to our service to access our application. You can also specify port configuration to your cluster so that you don&rsquo;t need to port-forward, which you can find in their <a href="https://kind.sigs.k8s.io/docs/user/configuration/#extra-port-mappings">port mappings documentation</a></p>

<p>I will be using the following commands to generate the manifests, but will also add them to this post:</p>

<pre><code class="bash">kubectl create deployment hostname --namespace default --replicas 2 --image ruanbekker/containers:hostname --port 8080 --dry-run=client -o yaml &gt; hostname-deployment.yaml
kubectl expose deployment hostname --namespace default --port=80 --target-port=8080 --name=hostname-http --dry-run=client -o yaml &gt; hostname-service.yaml
</code></pre>

<p>The manifest:</p>

<pre><code class="yaml">---
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: hostname
  name: hostname
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: hostname
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: hostname
    spec:
      containers:
      - image: ruanbekker/containers:hostname
        name: containers
        ports:
        - containerPort: 8080
        resources: {}
status: {}
---
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: hostname
  name: hostname-http
  namespace: default
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: hostname
status:
  loadBalancer: {}
</code></pre>

<p>Then apply them with:</p>

<pre><code class="bash">kubectl apply -f &lt;name-of-manifest&gt;.yaml
</code></pre>

<p>Or if you used kubectl to create them:</p>

<pre><code class="bash">kubectl apply -f hostname-deployment.yaml
kubectl apply -f hostname-service.yaml
</code></pre>

<p>You can then view your resources with:</p>

<pre><code class="bash">kubectl get deployment,pod,service

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/hostname   2/2     2            2           9m27s

NAME                            READY   STATUS    RESTARTS   AGE
pod/hostname-7ff58c5644-67vhq   1/1     Running   0          9m27s
pod/hostname-7ff58c5644-wjjbw   1/1     Running   0          9m27s

NAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
service/hostname-http   ClusterIP   10.96.218.58   &lt;none&gt;        80/TCP    5m48s
service/kubernetes      ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP   24m
</code></pre>

<p>Port forward to your service:</p>

<pre><code class="bash">kubectl port-forward svc/hostname-http 8080:80
</code></pre>

<p>Then access your application:</p>

<pre><code class="bash">curl http://localhost:8080/

Hostname: hostname-7ff58c5644-wjjbw
</code></pre>

<h2>Delete Kind Cluster</h2>

<p>View the clusters:</p>

<pre><code class="bash">kind get clusters
</code></pre>

<p>Delete a cluster:</p>

<pre><code class="bash">kind delete cluster --name kind-cluster
</code></pre>

<h2>Extras</h2>

<p>I highly recommend using <code>kubectx</code> to switch contexts and <code>kubens</code> to set the default namespace, and aliases:</p>

<pre><code class="bash">alias k=kubectl
alias kx=kubectx
alias kns=kubens
</code></pre>

<h2>Thank You</h2>

<p>Thanks for reading, feel free to check out my <a href="https://ruan.dev/">website</a>, feel free to subscribe to my <a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a> or follow me at <a href="https://twitter.com/ruanbekker">@ruanbekker</a> on Twitter.</p>

<ul>
<li>Linktree: <a href="https://go.ruan.dev/links">https://go.ruan.dev/links</a></li>
<li>Patreon: <a href="https://go.ruan.dev/patreon">https://go.ruan.dev/patreon</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker Multistage Builds for Hugo]]></title>
    <link href="https://blog.ruanbekker.com/blog/2022/07/31/docker-multistage-builds-for-hugo/"/>
    <updated>2022-07-31T02:23:51-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2022/07/31/docker-multistage-builds-for-hugo</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/182013196-aff6e76f-2cf3-4ec2-bfcc-3e977915a6aa.png" alt="blog-ruanbekker-multistage-builds" /></p>

<p>In this tutorial I will demonstrate how to keep your docker <strong>container images</strong> nice and <strong>slim</strong> with the use of <strong>multistage builds</strong> for a <strong>hugo</strong> documentation project.</p>

<p>Hugo is a static content generator so essentially that means that it will <strong>generate your markdown files into html</strong>. Therefore we don&rsquo;t need to include all the content from our project repository as we only need the static content (html, css, javascript) to reside on our <strong>final container image</strong>.</p>

<h2>What are we doing today</h2>

<p>We will use the <strong><a href="https://github.com/h-enk/doks">DOKS</a></strong> Modern Documentation theme for <strong><a href="https://gohugo.io/">Hugo</a></strong> as our project example, where we will build and run our documentation website on a docker container, but more importantly make use of multistage builds to <strong>optimize the size</strong> of our <strong>container image</strong>.</p>

<h2>Our Build Strategy</h2>

<p>Since hugo is a static content generator, we will use a <strong><a href="https://hub.docker.com/_/node">node</a></strong> container image as our base. We will then build and generate the content using <code>npm run build</code> which will generate the static content to <code>/src/public</code> in our build stage.</p>

<p>Since we then have static content, we can utilize a second stage using a <strong><a href="https://hub.docker.com/_/nginx">nginx</a></strong> container image with the purpose of a <strong>web server</strong> to host our <strong>static content</strong>. We will copy the static content from our <code>build</code> stage into our second stage and place it under our defined path in our nginx config.</p>

<p>This way we only include the required content on our final container image.</p>

<h2>Building our Container Image</h2>

<p>First clone the <a href="https://github.com/h-enk/doks">docs github repository</a> and change to the directory:</p>

<pre><code class="bash">git clone https://github.com/h-enk/doks
cd doks
</code></pre>

<p>Now create a <code>Dockerfile</code> in the root path with the following content:</p>

<pre><code class="dockerfile">FROM node:16.15.1 as build
WORKDIR /src
ADD . .
RUN npm install
RUN npm run build

FROM  nginx:alpine
LABEL demonstration.by Ruan Bekker &lt;@ruanbekker&gt;
COPY  nginx/config/nginx.conf /etc/nginx/nginx.conf
COPY  nginx/config/app.conf /etc/nginx/conf.d/app.conf
COPY  --from=build /src/public /usr/share/nginx/app
</code></pre>

<p>As we can see we are copying two nginx config files to our final image, which we will need to create.</p>

<p>Create the nginx config directory:</p>

<pre><code class="bash">mkdir -p nginx/config
</code></pre>

<p>The content for our main nginx config <code>nginx/config/nginx.conf</code>:</p>

<pre><code>user  nginx;
worker_processes  auto;
error_log  /var/log/nginx/error.log notice;
pid        /var/run/nginx.pid;

events {
    worker_connections  1024;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

    access_log  /var/log/nginx/access.log  main;

    sendfile        on;

    # timeouts
    client_body_timeout 12;
    client_header_timeout 12;
    keepalive_timeout  25;
    send_timeout 10;

    # buffer size
    client_body_buffer_size 10K;
    client_header_buffer_size 1k;
    client_max_body_size 8m;
    large_client_header_buffers 4 4k;

    # gzip compression
    gzip  on;
    gzip_vary on;
    gzip_min_length 10240;
    gzip_proxied expired no-cache no-store private auth;
    gzip_types text/plain text/css text/xml text/javascript application/x-javascript application/xml;
    gzip_disable "MSIE [1-6]\.";

    include /etc/nginx/conf.d/app.conf;
}
</code></pre>

<p>And in our main nginx config we are including a virtual host config <code>app.conf</code>, which we will create locally, and the content of <code>nginx/config/app.conf</code>:</p>

<pre><code>server {
    listen       80;
    server_name  localhost;

    location / {
        root   /usr/share/nginx/app;
        index  index.html index.htm;
    }

    #error_page  404              /404.html;

    # redirect server error pages to the static page /50x.html
    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
        root   /usr/share/nginx/html;
    }

}
</code></pre>

<p>Now that we have our docker config in place, we can build our container image:</p>

<pre><code class="bash">docker build -t ruanbekker/hashnode-docs-blogpost:latest .
</code></pre>

<p>Then we can review the <strong>size</strong> of our container image, which is only <code>27.4MB</code> in size, pretty neat right.</p>

<pre><code class="bash">docker images --filter reference=ruanbekker/hashnode-docs-blogpost

REPOSITORY                          TAG       IMAGE ID       CREATED          SIZE
ruanbekker/hashnode-docs-blogpost   latest    5b60f30f40e6   21 minutes ago   27.4MB
</code></pre>

<h2>Running our Container</h2>

<p>Now that we&rsquo;ve built our container image, we can run our documentation site, by specifying our host port on the left to map to our container port on the right in <code>80:80</code>:</p>

<pre><code class="bash">docker run -it -p 80:80 ruanbekker/hashnode-docs-blogpost:latest
</code></pre>

<p>When you don&rsquo;t have port 80 already listening prior to running the previous command, when you head to <a href="http://localhost">http://localhost</a> (if you are running this locally), you should see our documentation site up and running:</p>

<p><img src="https://user-images.githubusercontent.com/567298/182018773-ecf3cd6c-ce2c-487a-a1bf-4a84fe1b6a09.png" alt="image" /></p>

<h2>Thank You</h2>

<p>I have published this container image to <a href="https://hub.docker.com/r/ruanbekker/hashnode-docs-blogpost">ruanbekker/hashnode-docs-blogpost</a>.</p>

<p>Thanks for reading, feel free to check out my <strong><a href="https://ruan.dev">website</a></strong>, feel free to subscribe to my <strong><a href="http://digests.ruanbekker.com/?via=hashnode">newsletter</a></strong> or follow me at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong> on Twitter.</p>
]]></content>
  </entry>
  
</feed>
