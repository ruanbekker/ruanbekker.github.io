<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Docker | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/docker/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2022-09-20T03:07:55-04:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[KinD for Local Kubernetes Clusters]]></title>
    <link href="https://blog.ruanbekker.com/blog/2022/09/20/kind-for-local-kubernetes-clusters/"/>
    <updated>2022-09-20T02:18:16-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2022/09/20/kind-for-local-kubernetes-clusters</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/191189852-44f2fd39-7ad7-4d0a-a36b-c2889a838649.png" alt="kubernetes-kind" /></p>

<p>In this tutorial we will demonstrate how to use KinD (Kubernetes in Docker) to provision local kubernetes clusters for local development.</p>

<h2>About</h2>

<p>KinD uses container images to run as &ldquo;nodes&rdquo;, so spinning up and tearing down clusters becomes really easy or running multiple or different versions, is as easy as pointing to a different container image.</p>

<p>Configuration such as node count, ports, volumes, image versions can either be controlled via the command line or via configuration, more information on that can be found on their documentation:</p>

<ul>
<li><a href="https://kind.sigs.k8s.io/docs/user/quick-start/">https://kind.sigs.k8s.io/docs/user/quick-start/</a></li>
<li><a href="https://kind.sigs.k8s.io/docs/user/configuration/">https://kind.sigs.k8s.io/docs/user/configuration/</a></li>
</ul>


<h2>Installation</h2>

<p>Follow the <a href="https://kind.sigs.k8s.io/docs/user/quick-start/#installing-with-a-package-manager">docs</a> for more information, but for mac:</p>

<pre><code class="bash">brew install kind
</code></pre>

<p>To verify if kind was installed, you can run:</p>

<pre><code class="bash">kind version
</code></pre>

<h2>Create a Cluster</h2>

<p>Create the cluster with command line arguments, such as cluster name, the container image:</p>

<pre><code class="bash">kind create cluster --name cluster-1 --image kindest/node:v1.24.0
</code></pre>

<p>And the output will look something like this:</p>

<pre><code class="bash">Creating cluster "cluster-1" ...
 ‚úì Ensuring node image (kindest/node:v1.24.0) üñº
 ‚úì Preparing nodes üì¶
 ‚úì Writing configuration üìú
 ‚úì Starting control-plane üïπÔ∏è
 ‚úì Installing CNI üîå
 ‚úì Installing StorageClass üíæ
Set kubectl context to "kind-cluster-1"
You can now use your cluster with:

kubectl cluster-info --context kind-cluster-1

Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community üôÇ
</code></pre>

<p>I <strong>highly recommend</strong> installing <a href="https://github.com/ahmetb/kubectx">kubectx</a>, which makes it easy to switch between kubernetes contexts.</p>

<h2>Create a Cluster with Config</h2>

<p>If you would like to define your cluster configuration as config, you can create a file <code>default-config.yaml</code> with the following as a 2 node cluster, and specifying version 1.24.0:</p>

<pre><code class="yaml">---
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  image: kindest/node:v1.24.0@sha256:0866296e693efe1fed79d5e6c7af8df71fc73ae45e3679af05342239cdc5bc8e
- role: worker
  image: kindest/node:v1.24.0@sha256:0866296e693efe1fed79d5e6c7af8df71fc73ae45e3679af05342239cdc5bc8e
</code></pre>

<p>Then create the cluster and point the config:</p>

<pre><code class="bash">kind create cluster --name kind-cluster --config default-config.yaml
</code></pre>

<h2>Interact with the Cluster</h2>

<p>View the cluster info:</p>

<pre><code class="bash">kubectl cluster-info --context kind-kind-cluster
</code></pre>

<p>View cluster contexts:</p>

<pre><code class="bash">kubectl config get-contexts
</code></pre>

<p>Use context:</p>

<pre><code class="bash">kubectl config use-context kind-kind-cluster
</code></pre>

<p>View nodes:</p>

<pre><code class="bash">kubectl get nodes -o wide

NAME                         STATUS   ROLES           AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION      CONTAINER-RUNTIME
kind-cluster-control-plane   Ready    control-plane   2m11s   v1.24.0   172.20.0.5    &lt;none&gt;        Ubuntu 21.10   5.10.104-linuxkit   containerd://1.6.4
kind-cluster-worker          Ready    &lt;none&gt;          108s    v1.24.0   172.20.0.4    &lt;none&gt;        Ubuntu 21.10   5.10.104-linuxkit   containerd://1.6.4
</code></pre>

<h2>Deploy Sample Application</h2>

<p>We will create a deployment, a service and port-forward to our service to access our application. You can also specify port configuration to your cluster so that you don&rsquo;t need to port-forward, which you can find in their <a href="https://kind.sigs.k8s.io/docs/user/configuration/#extra-port-mappings">port mappings documentation</a></p>

<p>I will be using the following commands to generate the manifests, but will also add them to this post:</p>

<pre><code class="bash">kubectl create deployment hostname --namespace default --replicas 2 --image ruanbekker/containers:hostname --port 8080 --dry-run=client -o yaml &gt; hostname-deployment.yaml
kubectl expose deployment hostname --namespace default --port=80 --target-port=8080 --name=hostname-http --dry-run=client -o yaml &gt; hostname-service.yaml
</code></pre>

<p>The manifest:</p>

<pre><code class="yaml">---
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: hostname
  name: hostname
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: hostname
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: hostname
    spec:
      containers:
      - image: ruanbekker/containers:hostname
        name: containers
        ports:
        - containerPort: 8080
        resources: {}
status: {}
---
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: hostname
  name: hostname-http
  namespace: default
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: hostname
status:
  loadBalancer: {}
</code></pre>

<p>Then apply them with:</p>

<pre><code class="bash">kubectl apply -f &lt;name-of-manifest&gt;.yaml
</code></pre>

<p>Or if you used kubectl to create them:</p>

<pre><code class="bash">kubectl apply -f hostname-deployment.yaml
kubectl apply -f hostname-service.yaml
</code></pre>

<p>You can then view your resources with:</p>

<pre><code class="bash">kubectl get deployment,pod,service

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/hostname   2/2     2            2           9m27s

NAME                            READY   STATUS    RESTARTS   AGE
pod/hostname-7ff58c5644-67vhq   1/1     Running   0          9m27s
pod/hostname-7ff58c5644-wjjbw   1/1     Running   0          9m27s

NAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
service/hostname-http   ClusterIP   10.96.218.58   &lt;none&gt;        80/TCP    5m48s
service/kubernetes      ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP   24m
</code></pre>

<p>Port forward to your service:</p>

<pre><code class="bash">kubectl port-forward svc/hostname-http 8080:80
</code></pre>

<p>Then access your application:</p>

<pre><code class="bash">curl http://localhost:8080/

Hostname: hostname-7ff58c5644-wjjbw
</code></pre>

<h2>Delete Kind Cluster</h2>

<p>View the clusters:</p>

<pre><code class="bash">kind get clusters
</code></pre>

<p>Delete a cluster:</p>

<pre><code class="bash">kind delete cluster --name kind-cluster
</code></pre>

<h2>Extras</h2>

<p>I highly recommend using <code>kubectx</code> to switch contexts and <code>kubens</code> to set the default namespace, and aliases:</p>

<pre><code class="bash">alias k=kubectl
alias kx=kubectx
alias kns=kubens
</code></pre>

<h2>Thank You</h2>

<p>Thanks for reading, feel free to check out my <a href="https://ruan.dev/">website</a>, feel free to subscribe to my <a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a> or follow me at <a href="https://twitter.com/ruanbekker">@ruanbekker</a> on Twitter.</p>

<ul>
<li>Linktree: <a href="https://go.ruan.dev/links">https://go.ruan.dev/links</a></li>
<li>Patreon: <a href="https://go.ruan.dev/patreon">https://go.ruan.dev/patreon</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker Multistage Builds for Hugo]]></title>
    <link href="https://blog.ruanbekker.com/blog/2022/07/31/docker-multistage-builds-for-hugo/"/>
    <updated>2022-07-31T02:23:51-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2022/07/31/docker-multistage-builds-for-hugo</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/182013196-aff6e76f-2cf3-4ec2-bfcc-3e977915a6aa.png" alt="blog-ruanbekker-multistage-builds" /></p>

<p>In this tutorial I will demonstrate how to keep your docker <strong>container images</strong> nice and <strong>slim</strong> with the use of <strong>multistage builds</strong> for a <strong>hugo</strong> documentation project.</p>

<p>Hugo is a static content generator so essentially that means that it will <strong>generate your markdown files into html</strong>. Therefore we don&rsquo;t need to include all the content from our project repository as we only need the static content (html, css, javascript) to reside on our <strong>final container image</strong>.</p>

<h2>What are we doing today</h2>

<p>We will use the <strong><a href="https://github.com/h-enk/doks">DOKS</a></strong> Modern Documentation theme for <strong><a href="https://gohugo.io/">Hugo</a></strong> as our project example, where we will build and run our documentation website on a docker container, but more importantly make use of multistage builds to <strong>optimize the size</strong> of our <strong>container image</strong>.</p>

<h2>Our Build Strategy</h2>

<p>Since hugo is a static content generator, we will use a <strong><a href="https://hub.docker.com/_/node">node</a></strong> container image as our base. We will then build and generate the content using <code>npm run build</code> which will generate the static content to <code>/src/public</code> in our build stage.</p>

<p>Since we then have static content, we can utilize a second stage using a <strong><a href="https://hub.docker.com/_/nginx">nginx</a></strong> container image with the purpose of a <strong>web server</strong> to host our <strong>static content</strong>. We will copy the static content from our <code>build</code> stage into our second stage and place it under our defined path in our nginx config.</p>

<p>This way we only include the required content on our final container image.</p>

<h2>Building our Container Image</h2>

<p>First clone the <a href="https://github.com/h-enk/doks">docs github repository</a> and change to the directory:</p>

<pre><code class="bash">git clone https://github.com/h-enk/doks
cd doks
</code></pre>

<p>Now create a <code>Dockerfile</code> in the root path with the following content:</p>

<pre><code class="dockerfile">FROM node:16.15.1 as build
WORKDIR /src
ADD . .
RUN npm install
RUN npm run build

FROM  nginx:alpine
LABEL demonstration.by Ruan Bekker &lt;@ruanbekker&gt;
COPY  nginx/config/nginx.conf /etc/nginx/nginx.conf
COPY  nginx/config/app.conf /etc/nginx/conf.d/app.conf
COPY  --from=build /src/public /usr/share/nginx/app
</code></pre>

<p>As we can see we are copying two nginx config files to our final image, which we will need to create.</p>

<p>Create the nginx config directory:</p>

<pre><code class="bash">mkdir -p nginx/config
</code></pre>

<p>The content for our main nginx config <code>nginx/config/nginx.conf</code>:</p>

<pre><code>user  nginx;
worker_processes  auto;
error_log  /var/log/nginx/error.log notice;
pid        /var/run/nginx.pid;

events {
    worker_connections  1024;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

    access_log  /var/log/nginx/access.log  main;

    sendfile        on;

    # timeouts
    client_body_timeout 12;
    client_header_timeout 12;
    keepalive_timeout  25;
    send_timeout 10;

    # buffer size
    client_body_buffer_size 10K;
    client_header_buffer_size 1k;
    client_max_body_size 8m;
    large_client_header_buffers 4 4k;

    # gzip compression
    gzip  on;
    gzip_vary on;
    gzip_min_length 10240;
    gzip_proxied expired no-cache no-store private auth;
    gzip_types text/plain text/css text/xml text/javascript application/x-javascript application/xml;
    gzip_disable "MSIE [1-6]\.";

    include /etc/nginx/conf.d/app.conf;
}
</code></pre>

<p>And in our main nginx config we are including a virtual host config <code>app.conf</code>, which we will create locally, and the content of <code>nginx/config/app.conf</code>:</p>

<pre><code>server {
    listen       80;
    server_name  localhost;

    location / {
        root   /usr/share/nginx/app;
        index  index.html index.htm;
    }

    #error_page  404              /404.html;

    # redirect server error pages to the static page /50x.html
    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
        root   /usr/share/nginx/html;
    }

}
</code></pre>

<p>Now that we have our docker config in place, we can build our container image:</p>

<pre><code class="bash">docker build -t ruanbekker/hashnode-docs-blogpost:latest .
</code></pre>

<p>Then we can review the <strong>size</strong> of our container image, which is only <code>27.4MB</code> in size, pretty neat right.</p>

<pre><code class="bash">docker images --filter reference=ruanbekker/hashnode-docs-blogpost

REPOSITORY                          TAG       IMAGE ID       CREATED          SIZE
ruanbekker/hashnode-docs-blogpost   latest    5b60f30f40e6   21 minutes ago   27.4MB
</code></pre>

<h2>Running our Container</h2>

<p>Now that we&rsquo;ve built our container image, we can run our documentation site, by specifying our host port on the left to map to our container port on the right in <code>80:80</code>:</p>

<pre><code class="bash">docker run -it -p 80:80 ruanbekker/hashnode-docs-blogpost:latest
</code></pre>

<p>When you don&rsquo;t have port 80 already listening prior to running the previous command, when you head to <a href="http://localhost">http://localhost</a> (if you are running this locally), you should see our documentation site up and running:</p>

<p><img src="https://user-images.githubusercontent.com/567298/182018773-ecf3cd6c-ce2c-487a-a1bf-4a84fe1b6a09.png" alt="image" /></p>

<h2>Thank You</h2>

<p>I have published this container image to <a href="https://hub.docker.com/r/ruanbekker/hashnode-docs-blogpost">ruanbekker/hashnode-docs-blogpost</a>.</p>

<p>Thanks for reading, feel free to check out my <strong><a href="https://ruan.dev">website</a></strong>, feel free to subscribe to my <strong><a href="http://digests.ruanbekker.com/?via=hashnode">newsletter</a></strong> or follow me at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Remote Builds With Docker Contexts]]></title>
    <link href="https://blog.ruanbekker.com/blog/2022/07/14/remote-builds-with-docker-contexts/"/>
    <updated>2022-07-14T01:57:34-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2022/07/14/remote-builds-with-docker-contexts</id>
    <content type="html"><![CDATA[<p><img src="https://blog.ruanbekker.com/images/ruanbekker-docker-contexts.png" alt="using-docker-contexts" /></p>

<p>Often you want to save some battery life when you are doing docker builds and leverage a remote host to do the intensive work and we can utilise docker context over ssh to do just that.</p>

<h2>About</h2>

<p>In this tutorial I will show you how to use a remote docker engine to do docker builds, so you still run the docker client locally, but the context of your build will be sent to a remote docker engine via ssh.</p>

<p>We will setup password-less ssh, configure our ssh config, create the remote docker context, then use the remote docker context.</p>

<p><img src="https://user-images.githubusercontent.com/567298/178909518-26f580e9-2b96-41b3-bacd-a5ea5f848ebf.png" alt="image" /></p>

<h2>Password-less SSH</h2>

<p>I will be copying my public key to the remote host:</p>

<pre><code class="bash">$ ssh-copy-id ruan@192.168.2.18
</code></pre>

<p>Setup my ssh config:</p>

<pre><code class="bash">$ cat ~/.ssh/config
Host home-server
    Hostname 192.168.2.18
    User ruan
    IdentityFile ~/.ssh/id_rsa
    StrictHostKeyChecking no
    UserKnownHostsFile /dev/null
</code></pre>

<p>Test:</p>

<pre><code>$ ssh home-server whoami
ruan
</code></pre>

<h2>Docker Context</h2>

<p>On the target host (192.168.2.18) we can verify that docker is installed:</p>

<pre><code class="bash">$ docker version
Client: Docker Engine - Community
 Version:           20.10.12
 API version:       1.41
 Go version:        go1.16.12
 Git commit:        e91ed57
 Built:             Mon Dec 13 11:45:37 2021
 OS/Arch:           linux/amd64
 Context:           default
 Experimental:      true

Server: Docker Engine - Community
 Engine:
  Version:          20.10.12
  API version:      1.41 (minimum version 1.12)
  Go version:       go1.16.12
  Git commit:       459d0df
  Built:            Mon Dec 13 11:43:46 2021
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.4.12
  GitCommit:        7b11cfaabd73bb80907dd23182b9347b4245eb5d
 runc:
  Version:          1.0.2
  GitCommit:        v1.0.2-0-g52b36a2
 docker-init:
  Version:          0.19.0
  GitCommit:        de40ad0
</code></pre>

<p>On the client (my laptop in this example), we will create a docker context called &ldquo;home-server&rdquo; and point it to our target host:</p>

<pre><code class="bash">$ docker context create home-server --docker "host=ssh://home-server"
home-server
Successfully created context "home-server"
</code></pre>

<p>Now we can list our contexts:</p>

<pre><code class="bash">docker context ls
NAME                TYPE                DESCRIPTION                               DOCKER ENDPOINT               KUBERNETES ENDPOINT                                  ORCHESTRATOR
default *           moby                Current DOCKER_HOST based configuration   unix:///var/run/docker.sock   https://k3d-master.127.0.0.1.nip.io:6445 (default)   swarm
home-server         moby                                                          ssh://home-server
</code></pre>

<h2>Using Contexts</h2>

<p>We can verify if this works by listing our cached docker images locally and on our remote host:</p>

<pre><code class="bash">$ docker --context=default images | wc -l
 16
</code></pre>

<p>And listing the remote images by specifying the context:</p>

<pre><code class="bash">$ docker --context=home-server images | wc -l
 70
</code></pre>

<p>We can set the default context to our target host:</p>

<pre><code>$ docker context use home-server
home-server
</code></pre>

<h2>Running Containers over Contexts</h2>

<p>So running containers with remote contexts essentially becomes running containers on remote hosts. In the past, I had to setup a ssh tunnel, point the docker host env var to that endpoint, then run containers on the remote host.</p>

<p>Thats something of the past, we can just point our docker context to our remote host and run the container. If you haven&rsquo;t set the default context, you can specify the context, so running a docker container on a remote host with your docker client locally:</p>

<pre><code class="bash">$ docker --context=home-server run -it -p 8002:8080 ruanbekker/hostname
2022/07/14 05:44:04 Server listening on port 8080
</code></pre>

<p>Now from our client (laptop), we can test our container on our remote host:</p>

<pre><code class="bash">$ curl http://192.168.2.18:8002
Hostname: 8605d292e2b4
</code></pre>

<p>The same way can be used to do remote docker builds, you have your Dockerfile locally, but when you build, you point the context to the remote host, and your context (dockerfile and files referenced in your dockerfile) will be sent to the remote host. This way you can save a lot of battery life as the computation is done on the remote docker engine.</p>

<h2>Thank You</h2>

<p>Thanks for reading, feel free to check out my <a href="https://ruan.dev/">website</a>, feel free to subscribe to my <a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a> or follow me at <a href="https://twitter.com/ruanbekker">@ruanbekker</a> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup Linkding Bookmarks Manager on Docker]]></title>
    <link href="https://blog.ruanbekker.com/blog/2022/05/31/setup-linkding-bookmarks-manager-on-docker/"/>
    <updated>2022-05-31T15:50:24-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2022/05/31/setup-linkding-bookmarks-manager-on-docker</id>
    <content type="html"><![CDATA[<p><strong>Note</strong>:  <em>Originally posted on <a href="https://containers.fan/posts/setup-linkding-bookmarks-manager/">containers.fan</a></em></p>

<p>I&rsquo;ve stumbled upon a great bookmarks manager service called <strong><a href="https://github.com/sissbruecker/linkding/blob/master/README.md">Linkding</a></strong>. What I really like about it, it allows you to save your bookmarks, assign tags to it to search for it later, it has chrome and firefox browser extensions, and comes with an API.</p>

<h2>Installing Linkding</h2>

<p>We will be using Traefik to do SSL termination and host based routing, if you don‚Äôt have Traefik running already, you can follow this post to get that set up:</p>

<ul>
<li><a href="https://containers.fan/posts/setup-traefik-v2-docker-compose/">https://containers.fan/posts/setup-traefik-v2-docker-compose/</a></li>
</ul>


<p>You can follow the <a href="https://github.com/sissbruecker/linkding/blob/master/README.md">linkding documentation</a> for more detailed information.</p>

<p>The <code>docker-compose.yml</code> that I will be use:</p>

<pre><code class="yaml">version: "3.8"

services:
  linkding:
    image: sissbruecker/linkding:latest
    container_name: linkding
    volumes:
      - ./data:/etc/linkding/data
    environment:
      - LD_DISABLE_BACKGROUND_TASKS=False
      - LD_DISABLE_URL_VALIDATION=False
    restart: unless-stopped
    cpus: 0.5
    mem_limit: 512m
    networks:
      - public
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.linkding-app.rule=Host(`linkding.yourdomain.net`)"
      - "traefik.http.routers.linkding-app.entrypoints=https"
      - "traefik.http.routers.linkding-app.tls.certresolver=letsencrypt"
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

networks:
  public:
    name: public
</code></pre>

<p>Make sure to replace the FQDN of your choice, as I used <code>linkding.yourdomain.net</code> as an example.</p>

<p>Once everything is in place, boot the stack:</p>

<pre><code class="bash">docker-compose up -d
</code></pre>

<h2>Admin Account Registration</h2>

<p>Once your linkding container has booted, you can create a admin user with the following command (ensure to replace where needed):</p>

<pre><code class="bash">docker-compose exec linkding python manage.py createsuperuser --username=admin --email=root@localhost
</code></pre>

<p>Once you head over to the linkding url that you provided and you logon, you should be able to see something like this:</p>

<p><img src="https://user-images.githubusercontent.com/567298/171265323-2b23515f-8535-4c89-a195-6ab9b63eab68.png" alt="linkding" /></p>

<h2>Creating Bookmarks</h2>

<p>When you select &ldquo;Add Bookmark&rdquo; and you provide the URL, linkding will retrieve the title and the description and populate it for you, and you can provide the tags (seperated by spaces):</p>

<p><img src="https://user-images.githubusercontent.com/567298/171266278-ab31afc0-4aca-48fc-9795-4d49ae9b3508.png" alt="linkding-bookmark" /></p>

<h2>Browser Extensions</h2>

<p>To add a browser extension, select &ldquo;Settings&rdquo;, then &ldquo;Integrations&rdquo;, then you will find the link to the browser extension for Chrome and Firefox:</p>

<p><img src="https://user-images.githubusercontent.com/567298/171266713-3e2b2e5d-2ff0-43be-9713-5dd69a15d0cd.png" alt="linkding-browser-extension" /></p>

<p>After you install the browser extension and click on it for the first time, it will ask you to set the Linkding Base URL and API Authentication Token:</p>

<p><img src="https://user-images.githubusercontent.com/567298/171267455-123cad06-3758-4991-bb7e-40dc43a62996.png" alt="linkding-configuration" /></p>

<p>You can find that at the bottom of the &ldquo;Integrations&rdquo; section:</p>

<p><img src="https://user-images.githubusercontent.com/567298/171269639-45e65ab0-b413-4879-9c8f-0b82f5884096.png" alt="linkding-rest-api-access" /></p>

<h2>REST API</h2>

<p>You can follow the <a href="https://github.com/sissbruecker/linkding/blob/master/docs/API.md">API Docs</a> for more information, using an example to search for bookmarks with the term &ldquo;docker&rdquo;:</p>

<pre><code class="bash">curl -sL -H "Authorization: Token ${LINKDING_API_TOKEN}" "https://linkding.${DOMAIN}/api/bookmarks?q=docker" | python3 -m json.tool
</code></pre>

<p>In my case returns a response like the following:</p>

<pre><code class="json">{
    "count": 1,
    "next": null,
    "previous": null,
    "results": [
        {
            "id": 6,
            "url": "https://www.docker.com/blog/deploying-web-applications-quicker-and-easier-with-caddy-2/",
            "title": "",
            "description": "",
            "website_title": "Deploying Web Applications Quicker and Easier with Caddy 2 - Docker",
            "website_description": "Deploying web apps can be tough, even with leading server technologies. Learn how you can use Caddy 2 and Docker simplify this process.",
            "is_archived": false,
            "tag_names": [
                "caddy",
                "docker"
            ],
            "date_added": "2022-05-31T19:11:53.739002Z",
            "date_modified": "2022-05-31T19:11:53.739016Z"
        }
    ]
}
</code></pre>

<h2>Thank You</h2>

<p>Thanks for reading, feel free to check out my <strong><a href="https://ruan.dev">website</a></strong>, read my <strong><a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a></strong> or follow me at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Prometheus Relabel Config Examples]]></title>
    <link href="https://blog.ruanbekker.com/blog/2022/05/30/prometheus-relabel-config-examples/"/>
    <updated>2022-05-30T03:01:01-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2022/05/30/prometheus-relabel-config-examples</id>
    <content type="html"><![CDATA[<p>This is a quick demonstration on how to use prometheus relabel configs, when you have scenarios for when example, you want to use a part of your hostname and assign it to a prometheus label.</p>

<h2>Prometheus Relabling</h2>

<p>Using a standard prometheus config to scrape two targets:
- <code>ip-192-168-64-29.multipass:9100</code>
- <code>ip-192-168-64-30.multipass:9100</code></p>

<pre><code>global:
  scrape_interval:     15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'local'

scrape_configs:
  - job_name: 'prometheus'
    scrape_interval: 15s
    static_configs:
    - targets: ['localhost:9090']

  - job_name: 'multipass-nodes'
    static_configs:
    - targets: ['ip-192-168-64-29.multipass:9100']
      labels:
        test: 1
    - targets: ['ip-192-168-64-30.multipass:9100']
      labels:
        test: 1
</code></pre>

<p>The Result:</p>

<p><img width="924" alt="image" src="https://user-images.githubusercontent.com/567298/170823370-f2c6b3a3-68a8-4f5a-ad43-2f1b832c95e0.png"></p>

<p>When we want to relabel one of the source the prometheus <a href="https://grafana.com/blog/2022/03/21/how-relabeling-in-prometheus-works/#internal-labels">internal labels</a>, <code>__address__</code> which will be the given target including the port, then we apply regex: <code>(.*)</code> to catch everything from the source label, and since there is only one group we use the <code>replacement</code> as <code>${1}-randomtext</code> and use that value to apply it as the value of the given <code>target_label</code> which in this case is for <code>randomlabel</code>, which will be in this case:</p>

<pre><code>global:
  scrape_interval:     15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'local'

scrape_configs:
  - job_name: 'prometheus'
    scrape_interval: 15s
    static_configs:
    - targets: ['localhost:9090']

  - job_name: 'multipass-nodes'
    static_configs:
    - targets: ['ip-192-168-64-29.multipass:9100']
      labels:
        test: 3
    - targets: ['ip-192-168-64-30.multipass:9100']
      labels:
        test: 3
    relabel_configs:
    - source_labels: [__address__]
      regex: '(.+)'
      replacement: '${1}-randomtext'
      target_label: randomlabel
</code></pre>

<p>The Result:</p>

<p><img width="1107" alt="image" src="https://user-images.githubusercontent.com/567298/170824588-44a79c3d-5131-4311-bcca-f5137d6acdad.png"></p>

<p>In this case we want to relabel the <code>__address__</code> and apply the value to the <code>instance</code> label, but we want to exclude the <code>:9100</code> from the <code>__address__</code> label:</p>

<pre><code># Config: https://github.com/prometheus/prometheus/blob/release-2.36/config/testdata/conf.good.yml
global:
  scrape_interval:     15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'local'

scrape_configs:
  - job_name: 'prometheus'
    scrape_interval: 15s
    static_configs:
    - targets: ['localhost:9090']

  - job_name: 'multipass-nodes'
    static_configs:
    - targets: ['ip-192-168-64-29.multipass:9100']
      labels:
        test: 4
    - targets: ['ip-192-168-64-30.multipass:9100']
      labels:
        test: 4
    relabel_configs:
    - source_labels: [__address__]
      separator: ':'
      regex: '(.*):(.*)'
      replacement: '${1}'
      target_label: instance
</code></pre>

<p>The Result:</p>

<p><img width="950" alt="image" src="https://user-images.githubusercontent.com/567298/170824806-45f0f243-5fe7-4635-9e9a-335616a322da.png"></p>

<h2>AWS EC2 SD Configs</h2>

<p>On AWS EC2 you can make use of the <a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config">ec2_sd_config</a> where you can make use of EC2 Tags, to set the values of your tags to prometheus label values.</p>

<p>In this scenario, on my EC2 instances I have 3 tags:
- Key: PrometheusScrape, Value: Enabled
- Key: Name, Value: pdn-server-1
- Key: Environment, Value: dev</p>

<p>In our config, we only apply a node-exporter scrape config to instances which are tagged <code>PrometheusScrape=Enabled</code>, then we use the <code>Name</code> tag, and assign it&rsquo;s value to the <code>instance</code> tag, and the similarly we assign the <code>Environment</code> tag value to the <code>environment</code> promtheus label value.</p>

<p>Because this prometheus instance resides in the same VPC, I am using the <code>__meta_ec2_private_ip</code> which is the private ip address of the EC2 instance to assign it to the address where it needs to scrape the node exporter metrics endpoint:</p>

<pre><code class="yaml">scrape_configs:
  - job_name: node-exporter
    scrape_interval: 15s
    ec2_sd_configs:
    - region: eu-west-1
      port: 9100
      filters:
        - name: tag:PrometheusScrape
          values:
            - Enabled
    relabel_configs:
    - source_labels: [__meta_ec2_private_ip]
      replacement: '${1}:9100'
      target_label: __address__
    - source_labels: [__meta_ec2_tag_Name]
      target_label: instance
    - source_labels: [__meta_ec2_tag_Environment]
      target_label: environment
</code></pre>

<p>You will need a EC2 Ready Only instance role (or access keys on the configuration) in order for prometheus to read the EC2 tags on your account.</p>

<p>See their <a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config">documentation</a> for more info.</p>

<h2>Stack</h2>

<p>The docker-compose used:</p>

<pre><code class="yaml">version: '3.8'

services:
  prometheus:
    image: prom/prometheus
    container_name: 'prometheus'
    user: root
    restart: unless-stopped
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention=14d'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.external-url=http://prometheus.127.0.0.1.nip.io'
    ports:
      - 9090:9090
    networks:
      - public
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

networks:
  public:
    name: public

volumes:
  prometheus-data: {}
</code></pre>

<h2>References</h2>

<p>Usful docs:</p>

<ul>
<li><a href="https://grafana.com/blog/2022/03/21/how-relabeling-in-prometheus-works/#internal-labels">https://grafana.com/blog/2022/03/21/how-relabeling-in-prometheus-works/#internal-labels</a></li>
<li><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config">https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config</a></li>
<li><a href="https://regexr.com/">https://regexr.com/</a></li>
</ul>


<h2>Thank You</h2>

<p>Thanks for reading, if you like my content, check out my <strong><a href="https://ruan.dev">website</a></strong>, read my <strong><a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a></strong> or follow me at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong> on Twitter.</p>
]]></content>
  </entry>
  
</feed>
