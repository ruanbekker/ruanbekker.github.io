<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Docker | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/docker/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2018-04-07T10:05:11-04:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Set Docker Environment Variables During Build Time]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/04/07/set-docker-environment-variables-during-build-time/"/>
    <updated>2018-04-07T09:51:35-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/04/07/set-docker-environment-variables-during-build-time</id>
    <content type="html"><![CDATA[<p>When using that <code>ARG</code> option in your Dockerfile, you can specify the <code>--build-args</code> option to define the value for the key that you specify in your Dockerfile to use for a environment variable as an example.</p>

<p>Today we will use the <code>arg</code> and <code>env</code> to set environment variables at build time.</p>

<h2>The Dockerfile:</h2>

<p>Our Dockerfile</p>

<pre><code class="dockerfile">FROM alpine:edge
ARG NAME
ENV OWNER=${NAME:-NOT_DEFINED}
CMD ["sh", "-c", "echo env var: ${OWNER}"]
</code></pre>

<p>Building our Image, we will pass the value to our NAME argument:</p>

<pre><code class="bash">$ docker build --build-arg NAME=james -t ruan:test .
</code></pre>

<p>Now when we run our container, we will notice that the build time argument has passed through to our environment variable from the running container:</p>

<pre><code class="bash">$ docker run -it ruan:test 
env var: james
</code></pre>

<p>When we build the image without specifying build arguments, and running the container:</p>

<pre><code class="bash">$ docker build -t ruan:test .
$ docker run -it ruan:test 
env var: NOT_DEFINED
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker Environment Substitution With Dockerfile]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/04/07/docker-environment-substitution-with-dockerfile/"/>
    <updated>2018-04-07T09:18:20-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/04/07/docker-environment-substitution-with-dockerfile</id>
    <content type="html"><![CDATA[<p>The <a href="https://12factor.net/">12 Factor</a> way, is a general guideline that provides best practices when building applications. One of them is using environment variables to store application configuration.</p>

<h2>What will we be doing:</h2>

<p>In this post we will build a simple docker application that returns the environment variable&rsquo;s value to standard out. We are using environment substitution, so if the environment variable is not provided, we will set a default value of <code>NOT_DEFINED</code>.</p>

<p>We will have the environment variable <code>OWNER</code> and when no value is set for that Environment Variable, the <code>NOT_DEFINED</code> value will be returned.</p>

<h2>The Dockerfile</h2>

<p>Our Dockerfile:</p>

<pre><code class="dockerfile">FROM alpine:edge
ENV OWNER=${OWNER:-NOT_DEFINED}
CMD ["sh", "-c", "echo env var: ${OWNER}"]
</code></pre>

<p>Building the image:</p>

<pre><code class="bash">$ docker build -t test:envs .
</code></pre>

<h2>Putting it to action:</h2>

<p>Now we will run a container and pass the <code>OWNER</code> environment variable as an option:</p>

<pre><code class="bash">$ docker run -it -e OWNER=ruan test:envs . 
env var: ruan
</code></pre>

<p>When we run a container without specifying the environment variable:</p>

<pre><code class="bash">$ docker run -it ruan:test 
env var: NOT_DEFINED
</code></pre>

<h2>Resources:</h2>

<ul>
<li><a href="https://tryolabs.com/blog/2015/03/26/configurable-docker-containers-for-multiple-environments/">https://tryolabs.com/blog/2015/03/26/configurable-docker-containers-for-multiple-environments/</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Populate Environment Variables From Docker Secrets With a Flask Demo App]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/03/12/populate-environment-variables-from-docker-secrets-with-a-flask-demo-app/"/>
    <updated>2018-03-12T18:16:42-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/03/12/populate-environment-variables-from-docker-secrets-with-a-flask-demo-app</id>
    <content type="html"><![CDATA[<p><img src="http://obj-cache.cloud.ruanbekker.com/docker-logo.png" alt="" /></p>

<p>In this post we will create a basic Python Flask WebApp on Docker Swarm, but we will read our Flask Host, and Flask Port from Environment Variables, which will be populated from Docker Secrets, which we will read in from a python script.</p>

<h2>Our Directory Setup:</h2>

<p>This can be retrieved from <a href="https://github.com/ruanbekker/docker-swarm-apps/tree/master/tools-secrets-env-exporter">github.com/ruanbekker/docker-swarm-apps/tool-secrets-env-exporter</a>, but I will place the code in here as well.</p>

<pre><code class="bash Dockerfile:">FROM alpine:edge
RUN apk add --no-cache python2 py2-pip &amp;&amp; pip install flask
ADD exporter.py /exporter.py
ADD boot.sh /boot.sh
ADD app.py /app.py
CMD ["/bin/sh", "/boot.sh"]
</code></pre>

<pre><code class="python exporter.py">import os
from glob import glob

for var in glob('/run/secrets/*'):
    k=var.split('/')[-1]
    v=open(var).read().rstrip('\n')
    os.environ[k] = v
    print("export {key}={value}".format(key=k,value=v))
</code></pre>

<pre><code class="python app.py">import os
from flask import Flask

flask_host = str(os.environ['flask_host'])
flask_port = int(os.environ['flask_port'])

app = Flask(__name__)

@app.route('/')
def index():
    return 'ok\n'

if __name__ == '__main__':
    app.run(host=flask_host, port=flask_port)
</code></pre>

<pre><code class="bash boot.sh">#!/bin/sh
set -e
eval $(python /exporter.py)
python /app.py
</code></pre>

<h2>Flow Information:</h2>

<p>The exporter script checks all the secrets that is mounted to the container, then formats the secrets to a key/value pair, which then exports the environment variables to the current shell, which thereafter gets read by the flask application.</p>

<h2>Usage:</h2>

<p>Create Docker Secrets:</p>

<pre><code>$ echo 5001 | docker secret create flask_port -
$ echo 0.0.0.0 | docker secret create flask_host -
</code></pre>

<p>Build and Push the Image:</p>

<pre><code>$ docker build -t registry.gitlab.com/&lt;user&gt;/&lt;repo&gt;/&lt;image&gt;:&lt;tag&gt;
$ docker push registry.gitlab.com/&lt;user&gt;/&lt;repo&gt;/&lt;image&gt;:&lt;tag&gt;
</code></pre>

<p>Create the Service, and specify the secrets that we created earlier:</p>

<pre><code>$ docker service create --name webapp \
--secret source=flask_host,target=flask_host \
--secret source=flask_port,target=flask_port \
registry.gitlab.com/&lt;user&gt;/&lt;repo&gt;/&lt;image&gt;:&lt;tag&gt;
</code></pre>

<p>Exec into the container, list to see where the secrets got populated:</p>

<pre><code>$ ls /run/secrets/
flask_host  flask_port
</code></pre>

<p>Do a netstat, to see that the value from the created secret is listening:</p>

<pre><code>$ netstat -tulpn
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:5001            0.0.0.0:*               LISTEN      7/python
</code></pre>

<p>Do a GET request on the Flask Application:</p>

<pre><code>$ curl http://0.0.0.0:5001/
ok
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Guide to Setup Docker Convoy Volume Driver for Docker Swarm With NFS]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/02/16/guide-to-setup-docker-convoy-volume-driver-for-docker-swarm-with-nfs/"/>
    <updated>2018-02-16T08:51:59-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/02/16/guide-to-setup-docker-convoy-volume-driver-for-docker-swarm-with-nfs</id>
    <content type="html"><![CDATA[<p><img src="http://obj-cache.cloud.ruanbekker.com/docker-logo.png" alt="" /></p>

<p>In this post we will setup <a href="https://github.com/rancher/convoy">Rancher&rsquo;s Convoy Storage Plugin</a> with NFS, to provide data persistence in Docker Swarm.</p>

<h2>The Overview:</h2>

<p>This essentially means that we will have a NFS Volume, when the service gets created on Docker Swarm, the cluster creates these volumes with path mapping, so when a container gets spawned, restarted, scaled etc, the container that gets started on the new node will be aware of the volume, and will get the data that its expecting.</p>

<p>Its also good to note that our NFS Server will be a single point of failure, therefore its also good to look at a Distributed Volume like <a href="https://sysadmins.co.za/tag/glusterfs">GlusterFS</a>, <a href="https://sysadmins.co.za/tag/xtreemfs/">XtreemFS</a>, <a href="">Ceph</a>, etc.</p>

<ul>
<li>NFS Server (10.8.133.83)</li>
<li>Rancher Convoy Plugin on Each Docker Node in the Swarm (10.8.133.83, 10.8.166.19, 10.8.142.195)</li>
</ul>


<h2>Setup NFS:</h2>

<p>Setup the NFS Server</p>

<pre><code class="bash">$ sudo apt-get install nfs-kernel-server nfs-common -y
$ mkdir /vol
$ chown -R nobody:nogroup /vol
$ echo '/vol 10.8.133.83(rw,sync,no_subtree_check) 10.8.166.19(rw,sync,no_subtree_check) 10.8.142.195(rw,sync,no_subtree_check)' &gt;&gt; /etc/exports
$ sudo systemctl restart nfs-kernel-server
$ sudo systemctl enable nfs-kernel-server
</code></pre>

<p>Setup the NFS Clients on each Docker Node:</p>

<pre><code class="bash">$ sudo apt-get install nfs-common -y
$ mount 10.8.133.83:/vol /mnt
$ umount /mnt
$ df -h
</code></pre>

<p>If you can see tht the volume is mounted, unmount it and add it to the <code>fstab</code> so the volume can be mounted on boot:</p>

<pre><code class="bash">$ sudo bash -c "echo '10.8.133.83:/vol /mnt nfs auto,nofail,noatime,nolock,intr,tcp,actimeo=1800 0 0' &gt;&gt; /etc/fstab"
$ sudo mount -a
</code></pre>

<h2>Install Rancher Convoy Plugin:</h2>

<p>The Plugin needs to be installed on each docker node that will be part of the swarm:</p>

<pre><code class="bash">$ cd /tmp
$ wget https://github.com/rancher/convoy/releases/download/v0.5.0/convoy.tar.gz
$ tar xzf convoy.tar.gz
$ sudo cp convoy/convoy convoy/convoy-pdata_tools /usr/local/bin/
$ sudo mkdir -p /etc/docker/plugins/
$ sudo bash -c 'echo "unix:///var/run/convoy/convoy.sock" &gt; /etc/docker/plugins/convoy.spec'
</code></pre>

<h2>Create the init script:</h2>

<p>Thanks to <a href="https://gist.github.com/deviantony/557984d62e867e6f505577b207db6ffc">deviantony</a></p>

<pre><code class="bash">#!/bin/sh
### BEGIN INIT INFO
# Provides:
# Required-Start:    $remote_fs $syslog
# Required-Stop:     $remote_fs $syslog
# Default-Start:     2 3 4 5
# Default-Stop:      0 1 6
# Short-Description: Start daemon at boot time
# Description:       Enable service provided by daemon.
### END INIT INFO

dir="/usr/local/bin"
cmd="convoy daemon --drivers vfs --driver-opts vfs.path=/mnt/docker/volumes"
user="root"
name="convoy"

pid_file="/var/run/$name.pid"
stdout_log="/var/log/$name.log"
stderr_log="/var/log/$name.err"

get_pid() {
    cat "$pid_file"
}

is_running() {
    [ -f "$pid_file" ] &amp;&amp; ps `get_pid` &gt; /dev/null 2&gt;&amp;1
}

case "$1" in
    start)
    if is_running; then
        echo "Already started"
    else
        echo "Starting $name"
        cd "$dir"
        if [ -z "$user" ]; then
            sudo $cmd &gt;&gt; "$stdout_log" 2&gt;&gt; "$stderr_log" &amp;
        else
            sudo -u "$user" $cmd &gt;&gt; "$stdout_log" 2&gt;&gt; "$stderr_log" &amp;
        fi
        echo $! &gt; "$pid_file"
        if ! is_running; then
            echo "Unable to start, see $stdout_log and $stderr_log"
            exit 1
        fi
    fi
    ;;
    stop)
    if is_running; then
        echo -n "Stopping $name.."
        kill `get_pid`
        for i in {1..10}
        do
            if ! is_running; then
                break
            fi

            echo -n "."
            sleep 1
        done
        echo

        if is_running; then
            echo "Not stopped; may still be shutting down or shutdown may have failed"
            exit 1
        else
            echo "Stopped"
            if [ -f "$pid_file" ]; then
                rm "$pid_file"
            fi
        fi
    else
        echo "Not running"
    fi
    ;;
    restart)
    $0 stop
    if is_running; then
        echo "Unable to stop, will not attempt to start"
        exit 1
    fi
    $0 start
    ;;
    status)
    if is_running; then
        echo "Running"
    else
        echo "Stopped"
        exit 1
    fi
    ;;
    *)
    echo "Usage: $0 {start|stop|restart|status}"
    exit 1
    ;;
esac

exit 0
</code></pre>

<p>Make the script executable:</p>

<pre><code class="bash">$ chmod +x /etc/init.d/convoy
</code></pre>

<p>Enable the service on boot:</p>

<pre><code class="bash">$ sudo systemctl enable convoy
</code></pre>

<p>Start the service:</p>

<pre><code class="bash">$ sudo /etc/init.d/convoy start
</code></pre>

<p>This should be done on all the nodes.</p>

<h2>Externally Managed Convoy Volumes</h2>

<p>One thing to note is that, after your delete a volume, you will still need to delete the directory from the path where its hosted, as the application does not do that by itself.</p>

<p>Creating the Volume Before hand:</p>

<pre><code class="bash">$ convoy create test1
test1

$ docker volume ls
DRIVER              VOLUME NAME
convoy              test1

$ cat /mnt/docker/volumes/config/vfs_volume_test1.json
{"Name":"test1","Size":0,"Path":"/mnt/docker/volumes/test1","MountPoint":"","PrepareForVM":false,"CreatedTime":"Mon Feb 05 13:07:05 +0000 2018","Snapshots":{}}
</code></pre>

<p>Viewing the volume from another node:</p>

<pre><code class="bash">$ docker volume ls
DRIVER              VOLUME NAME
convoy              test1
</code></pre>

<h2>Creating a Test Service:</h2>

<p>Create a test service to test the data persistence, our docker-compose.yml:</p>

<pre><code class="bash">version: '3.4'

volumes:
  test1:
    external: true

networks:
  appnet:
    external: true

services:
  test:
    image: alpine:edge
    command: sh -c "ping 127.0.0.1"
    volumes:
      - test1:/data
    networks:
      - appnet
</code></pre>

<p>Creating the Overlay Network and Deploying the Stack:</p>

<pre><code class="bash">$ docker network create -d overlay appnet
$ docker stack deploy -c docker-compose.yml apps
Creating service apps_test
</code></pre>

<p>Write data to the volume in the container:</p>

<pre><code>$ docker exec -it apps_test.1.iojo7fpw8jirqjs3iu8qr7qpe sh
/ # echo "ok" &gt; /data/file.txt
/ # cat /data/file.txt
ok
</code></pre>

<p>Scale the service:</p>

<pre><code class="bash">$ docker service scale apps_test=2
apps_test scaled to 2
</code></pre>

<p>Inspect to see if the new replica is on another node:</p>

<pre><code class="bash">$ docker service ps apps_test
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE               ERROR                         PORTS
myrq2pc3z26z        apps_test.1         alpine:edge         scw-docker-1        Running             Running 45 seconds ago
ny8t97l2q00c         \_ apps_test.1     alpine:edge         scw-docker-1        Shutdown            Failed 51 seconds ago       "task: non-zero exit (137)"
iojo7fpw8jir         \_ apps_test.1     alpine:edge         scw-docker-1        Shutdown            Failed about a minute ago   "task: non-zero exit (137)"
tt0nuusvgeki        apps_test.2         alpine:edge         scw-docker-2        Running             Running 15 seconds ago
</code></pre>

<p>Logon to the new container and test if the data is persisted:</p>

<pre><code class="bash">$ docker exec -it apps_test.2.tt0nuusvgekirw1c5myu720ga sh
/ # cat /data/file.txt
ok
</code></pre>

<p>Delete the Stack and Redeploy and have a look at the data we created earlier, and you will notice the data is persisted:</p>

<pre><code class="bash">$ docker stack rm apps
$ docker stack deploy -c docker-compose.yml apps
$ docker exec -it apps_test.1.la4w2sbuu8cmv6xamwxl7n0ip cat /data/file.txt
ok
$ docker stack rm apps
</code></pre>

<h2>Create Volume via Compose:</h2>

<p>You can also create the volume on service/stack creation level, so you dont need to create the volume before hand, the compose file:</p>

<pre><code class="yml">version: '3.4'

volumes:
  test2:
    driver: convoy
    driver_opts:
      size: 10

networks:
  appnet:
    external: true

services:
  test:
    image: alpine:edge
    command: sh -c "ping 127.0.0.1"
    volumes:
      - test2:/data
    networks:
      - appnet
</code></pre>

<p>Deploy the Stack:</p>

<pre><code class="bash">$ docker stack deploy -c docker-compose-new.yml apps
Creating service apps_test
</code></pre>

<p>List the volumes and you will notice that the volume was created:</p>

<pre><code class="bash">$ docker volume ls
DRIVER              VOLUME NAME
convoy              apps_test2
convoy              test1
</code></pre>

<p>Lets inspect the volume, to see more details about it:</p>

<pre><code class="bash">docker volume inspect apps_test2
[
    {
        "CreatedAt": "0001-01-01T00:00:00Z",
        "Driver": "convoy",
        "Labels": {
            "com.docker.stack.namespace": "apps"
        },
        "Mountpoint": "/mnt/docker/volumes/apps_test2",
        "Name": "apps_test2",
        "Options": {
            "size": "10"
        },
        "Scope": "local"
    }
]
</code></pre>

<p>As mentioned earlier, if you delete the volume, you need to delete the data directories as well</p>

<pre><code class="bash">$ docker volume rm test1
test1

$ ls /mnt/docker/volumes/
apps_test2  config  test1

$ rm -rf /mnt/docker/volumes/test1
</code></pre>

<p>More info about the project:
- <a href="https://github.com/rancher/convoy">https://github.com/rancher/convoy</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Hive for Small Datasets on My Mac Using Docker]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/01/23/using-hive-for-small-datasets-on-my-mac-using-docker/"/>
    <updated>2018-01-23T10:06:16-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/01/23/using-hive-for-small-datasets-on-my-mac-using-docker</id>
    <content type="html"><![CDATA[<p>I wanted to process a small subset of data, and not wanting to spin up a cluster, so I used <code>nagasuga/docker-hive</code> docker image to run Hive on my Mac.</p>

<h2>Running Hive</h2>

<pre><code class="bash">$ docker run -it -v /home/me/resource-data.csv:/resource-data.csv nagasuga/docker-hive /bin/bash -c 'cd /usr/local/hive &amp;&amp; ./bin/hive'
hive&gt;
</code></pre>

<p>Once I was entered into my hive shell, I created a table for my CSV data:</p>

<h2>Creating the Table</h2>

<pre><code class="sql">hive&gt; create table resources (ResourceType STRING, Owner STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ;

hive&gt; hive&gt; show tables;
OK
resources

hive&gt; describe resources;
OK
resourcetype            string
owner                   string
</code></pre>

<h2>Loading the Data</h2>

<p>My csv data is located at <code>/resource-data.csv</code> on the container, which I will load into my table:</p>

<pre><code class="sql">hive&gt; load data local inpath '/resource-data.csv' into table resources;
Loading data to table default.resources
</code></pre>

<h2>Query the Data</h2>

<p>Just two simple queries for demonstration:</p>

<pre><code class="sql">hive&gt; select * from resources limit 3;
OK
EC2  Engineering
EC2  Finance
EC2  Product

hive&gt; hive&gt; select count(resourcetype) as num, owner from resources group by owner order by num desc limit 3;
K
50   Engineering
20   Product
10   Finance
</code></pre>

<h2>Resource:</h2>

<p>Thanks to <a href="https://github.com/nagasuga/docker-hive">https://github.com/nagasuga/docker-hive</a></p>
]]></content>
  </entry>
  
</feed>
