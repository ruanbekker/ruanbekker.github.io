<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Docker | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/docker/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2018-05-28T03:10:47-04:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Manage Scaleway Instances via Their API Like a Boss With Their Command Line Tool Scw]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/05/09/manage-scaleway-instances-via-their-api-like-a-boss-with-their-command-line-tool-scw/"/>
    <updated>2018-05-09T12:31:11-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/05/09/manage-scaleway-instances-via-their-api-like-a-boss-with-their-command-line-tool-scw</id>
    <content type="html"><![CDATA[<p><img src="https://preview.ibb.co/bBRhn7/scw.png" alt="" /></p>

<p>Let&rsquo;s set things straight: I am a command line fan boy, If I can do the things I have to do with a command line interface, i&rsquo;m happy! And that means automation ftw! :D</p>

<h2>Scaleway Command Line Interface:</h2>

<p>I have been using Scaleway for about 2 years now, and absolutely loving their services! So I recently found their <a href="https://github.com/scaleway/scaleway-cli">command line interface utility: scw</a>, which is written in golang and has a very similar feel to docker.</p>

<h2>Install the SCW CLI Tool:</h2>

<p>A golang environment is needed and I will be using docker to drop myself into a golang environment and then install the scw utility:</p>

<pre><code class="bash">$ docker run -it golang:alpine sh
$ apk update
$ apk add openssl git openssh curl
$ go get -u github.com/scaleway/scaleway-cli/cmd/scw
</code></pre>

<p>Verify that it was installed:</p>

<pre><code class="bash">$ scw --version
scw version v1.16+dev, build
</code></pre>

<p>Awesome sauce!</p>

<h2>Authentication:</h2>

<p>When we authenticate to Scaleway, it will prompt you to upload your public ssh key, as I am doing this in a container I have no ssh keys, so therefore will generate one before I authenticate.</p>

<p>Generate the SSH Key:</p>

<pre><code class="bash">$ ssh-keygen -t rsa
Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa):
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /root/.ssh/id_rsa.
Your public key has been saved in /root/.ssh/id_rsa.pub.
</code></pre>

<p>Now loging to Scaleway using the cli tools:</p>

<pre><code class="bash">$ scw login
Login (cloud.scaleway.com): &lt;youremail@domain.com&gt;
Password:
Do you want to upload an SSH key ?
[0] I don't want to upload a key !
[1] id_rsa.pub
Which [id]: 1

You are now authenticated on Scaleway.com as Ruan.
You can list your existing servers using `scw ps` or create a new one using `scw run ubuntu-xenial`.
You can get a list of all available commands using `scw -h` and get more usage examples on github.com/scaleway/scaleway-cli.
Happy cloud riding.
</code></pre>

<p>Sweeet!</p>

<p><img src="https://pics.me.me/hacker-voice-im-in-24303160.png" alt="" /></p>

<h2>Getting Info from Scaleway</h2>

<p>List Instance Types:</p>

<pre><code class="bash">$ scw products servers
COMMERCIAL TYPE     ARCH     CPUs      RAM  BAREMETAL
ARM64-128GB        arm64       64   137 GB      false
ARM64-16GB         arm64       16    17 GB      false
ARM64-2GB          arm64        4   2.1 GB      false
ARM64-32GB         arm64       32    34 GB      false
ARM64-4GB          arm64        6   4.3 GB      false
ARM64-64GB         arm64       48    69 GB      false
ARM64-8GB          arm64        8   8.6 GB      false
C1                   arm        4   2.1 GB       true
C2L               x86_64        8    34 GB       true
C2M               x86_64        8    17 GB       true
C2S               x86_64        4   8.6 GB       true
START1-L          x86_64        8   8.6 GB      false
START1-M          x86_64        4   4.3 GB      false
START1-S          x86_64        2   2.1 GB      false
START1-XS         x86_64        1   1.1 GB      false
VC1L              x86_64        6   8.6 GB      false
VC1M              x86_64        4   4.3 GB      false
VC1S              x86_64        2   2.1 GB      false
X64-120GB         x86_64       12   129 GB      false
X64-15GB          x86_64        6    16 GB      false
X64-30GB          x86_64        8    32 GB      false
X64-60GB          x86_64       10    64 GB      false
</code></pre>

<p>Get a list of available Images, in my case I am just looking for Ubuntu:</p>

<pre><code class="bash">$ scw images | grep -i ubuntu
Ubuntu_Bionic               latest              a21bb700            11 days             [ams1 par1]         [x86_64]
Ubuntu_Mini_Xenial_25G      latest              bc75c00b            13 days             [ams1 par1]         [x86_64]
</code></pre>

<p>List Running Instances:</p>

<pre><code class="bash">$ scw ps
SERVER ID           IMAGE                       ZONE                CREATED             STATUS              PORTS               NAME                  COMMERCIAL TYPE
abc123de            Ubuntu_Xenial_16_04_lates   ams1                5 weeks             running             xx.xx.xx.xx         scw-elasticsearch-01  ARM64-4GB
abc456de            ruan-docker-swarm-17_03     par1                10 months           running             xx.xx.xxx.xxx       scw-swarm-manager-01  VC1M
...
</code></pre>

<p>List All Instances (Running, Stopped, Started, etc):</p>

<pre><code class="bash">$ scw ps -a
SERVER ID           IMAGE                       ZONE                CREATED             STATUS              PORTS               NAME                  COMMERCIAL TYPE
abc123df            Ubuntu_Xenial_16_04_lates   ams1                5 weeks             stopped             xx.xx.xx.xx         scw-elasticsearch-02  ARM64-4GB
...
</code></pre>

<p>List Instances with a filter based on its name:</p>

<pre><code class="bash">$ scw ps -f name=scw-swarm-worker-02
SERVER ID           IMAGE               ZONE                CREATED             STATUS              PORTS               NAME                COMMERCIAL TYPE
1234abcd            Ubuntu_Xenial       par1                8 minutes           running             xx.xx.xxx.xxx       scw-swarm-worker-2  START1-XS
</code></pre>

<p>List the Latest Instance that was created:</p>

<pre><code class="bash">$ scw ps -l
SERVER ID           IMAGE               ZONE                CREATED             STATUS              PORTS               NAME                COMMERCIAL TYPE
1234abce            Ubuntu_Xenial       par1                6 minutes           running             xx.xx.xxx.xxx       scw-swarm-worker-3  START1-XS
</code></pre>

<h2>Create Instances:</h2>

<p>In my scenario, I would like to create a instance named <code>docker-swarm-worker-4</code> with the instance type <code>START1-XS</code> in the Paris datacenter, and I will be using my key that I have uploaded, also the image id that I passed, was retrieved when listing for images:</p>

<pre><code class="bash">$ scw --region=par1 create --commercial-type=START1-XS --ip-address=dynamic --ipv6=false --name="docker-swarm-worker-4" --tmp-ssh-key=false  bc75c00b
&lt;response: random uuid string&gt;
</code></pre>

<p>Now that the instance is created, we can start it by calling either the name or the id:</p>

<pre><code class="bash">$ scw start docker-swarm-worker-4
</code></pre>

<p>To verify the status of the instance, we can do:</p>

<pre><code class="bash">$ scw ps -l
SERVER ID           IMAGE               ZONE                CREATED             STATUS              PORTS               NAME                   COMMERCIAL TYPE
102abc34            Ubuntu_Xenial                           28 seconds          starting                                docker-swarm-worker-4  START1-XS
</code></pre>

<p>At this moment it is still starting, after waiting a minute or so, run it again:</p>

<pre><code class="bash">$ scw ps -l
SERVER ID           IMAGE               ZONE                CREATED             STATUS              PORTS               NAME                   COMMERCIAL TYPE
102abc34            Ubuntu_Xenial       par1                About a minute      running             xx.xx.xx.xx         docker-swarm-worker-4  START1-XS
</code></pre>

<p>As we can see its in a running state, so we are good to access our instance. You have 2 options to access your server, via exec and ssh.</p>

<pre><code>$ scw exec docker-swarm-worker-4 /bin/bash
root@docker-swarm-worker-4:~
</code></pre>

<p>or via SSH:</p>

<pre><code>$ ssh root@xx.xx.xx.xx
root@docker-swarm-worker-4:~
</code></pre>

<p>If you would like to access your server without uploading your SSH key to your account, you can pass <code>--tmp-ssh-key=true</code> as in:</p>

<pre><code class="bash">$ scw --region=par1 create --commercial-type=START1-XS --ip-address=dynamic --ipv6=false --name="scw-temp-instance" --tmp-ssh-key=true  bc75c00b
</code></pre>

<h2>Terminating Resources:</h2>

<p>This wil stop, terminate the instance with the associated volumes and reserved ip</p>

<pre><code class="bash">$ scw stop --terminate=true scw-temp-instance 
scw-temp-instance
</code></pre>

<p>If you had to remove a volume that is not needed, or unused:</p>

<pre><code>$ scw rmi test-1-snapshot-&lt;long-string&gt;--2018-04-26_12:42
</code></pre>

<p>To logout:</p>

<pre><code class="bash">$ scw logout
</code></pre>

<h2>Resources:</h2>

<p>Have a look at <a href="https://github.com/scaleway/scaleway-cli">Scaleway-CLI Documentation</a> and their <a href="https://www.scaleway.com/">Website</a> for more info, and have a look at their new <code>START1-XS</code> instance types, that is only 1.99 Euro&rsquo;s, that is insane!</p>

<p>Personally love what they are doing, feel free to head over to their <a href="https://www.scaleway.com/pricing/">pricing page</a> to see some sweet deals!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup the Elasticsearch Log Driver on Docker Swarm]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/05/02/setup-the-elasticsearch-log-driver-on-docker-swarm/"/>
    <updated>2018-05-02T15:10:30-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/05/02/setup-the-elasticsearch-log-driver-on-docker-swarm</id>
    <content type="html"><![CDATA[<p>Today we will look at a Elasticsearch logging driver for Docker.</p>

<h2>Why a Log Driver?</h2>

<p>By default the log output can be retrieved when using the <code>docker service logs -f service_name</code>, where log output of that service is shown via stdout. When having a lot of services in your swarm, it becomes useful logging all of your log output to a database service.</p>

<p>This is not just for Swarm but Docker stand alone as well.</p>

<p>In this tutorial we will use the Elasticsearch Log Driver, to log our logs for all our docker swarm services to Elasticsearch.</p>

<h2>Installing to Elasticsearch Log Driver:</h2>

<p>If you are running Docker Swarm, run this on all the nodes:</p>

<pre><code class="bash">$ docker plugin install rchicoli/docker-log-elasticsearch:latest --alias elasticsearch_latest
</code></pre>

<p>Verify that the log driver has been installed:</p>

<pre><code class="bash">$ docker plugin ls
ID                  NAME                          DESCRIPTION                          ENABLED
eadf06ad3d2a        elasticsearch_latest:latest   Send log messages to elasticsearch   true
</code></pre>

<h2>Test the Log Driver:</h2>

<p>Run a container of Alpine and echo a string of text:</p>

<pre><code class="bash">$ docker run --rm -ti \
    --log-driver elasticsearch_latest \
    --log-opt elasticsearch-url=http://192.168.0.235:9200 \
    --log-opt elasticsearch-insecure=false \
    --log-opt elasticsearch-sniff=false \
    --log-opt elasticsearch-index=docker-%F \
    --log-opt elasticsearch-type=log \
    --log-opt elasticsearch-timeout=10 \
    --log-opt elasticsearch-version=5 \
    --log-opt elasticsearch-fields=containerID,containerName,containerImageID,containerImageName,containerCreated \
    --log-opt elasticsearch-bulk-workers=1 \
    --log-opt elasticsearch-bulk-actions=1000 \
    --log-opt elasticsearch-bulk-size=1024 \
    --log-opt elasticsearch-bulk-flush-interval=1s \
    --log-opt elasticsearch-bulk-stats=false \
        alpine echo -n "this is a test logging message"
</code></pre>

<p>Have a look at your Elasticsearch indexes, and you will find the index which was specified in the log-options:</p>

<pre><code class="bash">$ curl http://192.168.0.235:9200/_cat/indices?v
health status index             uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   docker-2018.05.01 8FTqWq6nQlSGpYjD9M5qSg   5   1          1            0      8.9kb          8.9kb
</code></pre>

<p>Lets have a look at the Elasticsearch Document which holds the data of the log entry:</p>

<pre><code class="bash">$ curl http://192.168.0.235:9200/docker-2018.05.01/_search?pretty
{
  "took" : 5,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "docker-2018.05.01",
        "_type" : "log",
        "_id" : "hMTUG2MBIFc8kAgSNkYo",
        "_score" : 1.0,
        "_source" : {
          "containerID" : "cee0dc758528",
          "containerName" : "jolly_goodall",
          "containerImageID" : "sha256:3fd9065eaf02feaf94d68376da52541925650b81698c53c6824d92ff63f98353",
          "containerImageName" : "alpine",
          "containerCreated" : "2018-05-01T13:11:20.819447101Z",
          "message" : "this is a test logging message",
          "source" : "stdout",
          "timestamp" : "2018-05-01T13:11:21.119861767Z",
          "partial" : true
        }
      }
    ]
  }
}
</code></pre>

<h2>Using Swarm and Docker Compose:</h2>

<p>We will deploy a stack with a whoami golang web app, which will use the elasticsearch log driver:</p>

<pre><code class="bash docker-compose.yml">version: '3.4'

services:
  whoami:
    image: rbekker87/golang-whoami:latest
    networks:
      - appnet
    deploy:
      labels:
        - "traefik.port=80"
        - "traefik.backend.loadbalancer.swarm=true"
        - "traefik.docker.network=appnet"
        - "traefik.frontend.rule=Host:whoami.homecloud.mydomain.com"
      mode: replicated
      replicas: 10
      restart_policy:
        condition: any
      update_config:
        parallelism: 1
        delay: 70s
        order: start-first
        failure_action: rollback
      placement:
        constraints:
          - 'node.role==worker'
      resources:
        limits:
          cpus: '0.01'
          memory: 128M
        reservations:
          cpus: '0.001'
          memory: 64M
    logging:
      driver: elasticsearch_latest
      options:
        elasticsearch-url: "http://192.168.0.235:9200"
        elasticsearch-sniff: "false"
        elasticsearch-index: "docker-whoami-%F"
        elasticsearch-type: "log"
        elasticsearch-timeout: "10"
        elasticsearch-version: "6"
        elasticsearch-fields: "containerID,containerName,containerImageID,containerImageName,containerCreated"
        elasticsearch-bulk-workers: "1"
        elasticsearch-bulk-actions: "1000"
        elasticsearch-bulk-size: "1024"
        elasticsearch-bulk-flush-interval: "1s"
        elasticsearch-bulk-stats: "false"
networks:
  appnet:
    external: true
</code></pre>

<p>Deploy the Stack:</p>

<pre><code class="bash">$ docker stack deploy -c docker-compose.yml web 
</code></pre>

<p>Give it some time to launch and have a look at your indexes, and you will find the index which it wrote to:</p>

<pre><code class="bash">$ curl http://192.168.0.235:9200/_cat/indices?v
health status index                     uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   docker-2018.05.01         8FTqWq6nQlSGpYjD9M5qSg   5   1          1            0      8.9kb          8.9kb
yellow open   docker-whoami-2018.05.01  YebUtKa1RnCy86iP5_ylgg   5   1         11            0     54.4kb         54.4kb
</code></pre>

<p>Having a look at the data:</p>

<pre><code>$ curl 'http://192.168.0.235:9200/docker-whoami-2018.05.01/_search?pretty&amp;size=1'
{
  "took" : 18,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 11,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "docker-whoami-2018.05.01",
        "_type" : "log",
        "_id" : "acbgG2MBIFc8kAgShQa7",
        "_score" : 1.0,
        "_source" : {
          "containerID" : "97c3b337735f",
          "containerName" : "web_whoami.6.t2prjiexkym14isbx3yfxa99w",
          "containerImageID" : "sha256:0f7762d2ce569fc2ccf95fbc4c7191dde727551a180253fac046daecc580c7e9",
          "containerImageName" : "rbekker87/golang-whoami:latest@sha256:5a55c5de9cc16fbdda376791c90efb7c704c81b8dba949dce21199945c14cc88",
          "containerCreated" : "2018-05-01T13:24:43.089365528Z",
          "message" : "Starting up on port 80",
          "source" : "stdout",
          "timestamp" : "2018-05-01T13:24:48.636773709Z",
          "partial" : false
        }
      }
    ]
  }
}
</code></pre>

<p>For more info about this, have a look at the referenced documentation below.</p>

<h2>Resources:</h2>

<ul>
<li><a href="https://github.com/rchicoli/docker-log-elasticsearch">https://github.com/rchicoli/docker-log-elasticsearch</a></li>
<li><a href="https://github.com/moby/moby/issues/25694">https://github.com/moby/moby/issues/25694</a></li>
<li><a href="https://docs.docker.com/v17.09/engine/admin/logging/view_container_logs/">https://docs.docker.com/v17.09/engine/admin/logging/view_container_logs/</a></li>
<li><a href="https://sysadmins.co.za/how-to-setup-a-2-node-elasticsearch-cluster-on-centos-7-with-some-example-usage/">https://sysadmins.co.za/how-to-setup-a-2-node-elasticsearch-cluster-on-centos-7-with-some-example-usage/</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Forwarding the Docker Socket via a SSH Tunnel to Execute Docker Commands Locally]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/04/30/forwarding-the-docker-socket-via-a-ssh-tunnel-to-execute-docker-commands-locally/"/>
    <updated>2018-04-30T08:30:23-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/04/30/forwarding-the-docker-socket-via-a-ssh-tunnel-to-execute-docker-commands-locally</id>
    <content type="html"><![CDATA[<p>With automation in mind, when you want to execute docker commands remotely, you want to do it in a secure manner, as you don&rsquo;t want to expose your Docker port to the whole world.</p>

<p>One way in doing that, is forwarding the remote docker socket via a local port over a SSH Tunnel. With this way, you can execute docker commands locally on your workstation, as if the swarm is running on your workstation/laptop/node/bastion host etc.</p>

<p>Without the tunnel, I have a swarm on my laptop with no running services:</p>

<pre><code class="bash">$ docker service ls
ID                  NAME                   MODE                REPLICAS            IMAGE                                                               PORTS
</code></pre>

<p>As you can see, we have no services running, but the remote swarm has a couple, so after forwarding the connection, we should see our remote services.</p>

<h2>Setting up the SSH Tunnel:</h2>

<p>Here we will forward the remote docker socket: <code>/var/run/docker.sock</code> to a local port bound to localhost: <code>localhost:2377</code>:</p>

<pre><code class="bash">$ screen -S docker
$ ssh -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -i ~/path/to/key.pem -NL localhost:2377:/var/run/docker.sock root@docker-managers.mydomain.com
</code></pre>

<p>Now the SSH Tunnel will be established, and you can detach your screen session, or open a new shell session. To detach your screen session: <code>'ctrl + a' then d</code></p>

<h2>Verifying that the tunnel is established:</h2>

<p>You can use netstat to verify that the port is listening:</p>

<pre><code class="bash">$ netstat -ant | grep 2377
tcp4       0      0  127.0.0.1.2377         *.*                    LISTEN
</code></pre>

<h2>Inform the Docker Client to use the Port:</h2>

<p>Now we need to inform the docker client, to use the new port to talk to the docker daemon. We do that by setting the <code>DOCKER_HOST</code> environment variable to point to <code>localhost:2377</code>:</p>

<pre><code class="bash">$ export DOCKER_HOST="localhost:2377"
</code></pre>

<p>This will remain for the lifetime of the shell session.</p>

<h2>Testing it Out:</h2>

<p>Now we can run our commands locally, and we should see the output of our remote swarm:</p>

<pre><code class="bash">$ docker service ls
ID                  NAME                   MODE                REPLICAS            IMAGE                                                               PORTS
xjta8e3ek2u2        apps_flask_reminders   replicated          3/3                 rbekker87/flask-reminders:debian
0l7ruktbqj99        apps_kibana            replicated          1/1                 kibana:latest
...
</code></pre>

<h2>Terminating our SSH Tunnel:</h2>

<p>To terminate our SSH Tunnel, reconnect to your shell session, and hit <code>ctrl + c</code>:</p>

<pre><code class="bash">$ screen -ls 
There is a screen on:
    50413.docker    (Detached)
$ screen -r 50413
</code></pre>

<p>Hit <code>ctrl + c</code> :</p>

<pre><code class="bash">CKilled by signal 2.
</code></pre>

<p>And exit the screen session:</p>

<pre><code class="bash">$ exit
</code></pre>

<p>With this way, you can do lots of automation with docker swarm, not limited to swarm, but one of them.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running a 3 Node Elasticsearch Cluster With Docker Compose on Your Laptop for Testing]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/04/29/running-a-3-node-elasticsearch-cluster-with-docker-compose-on-your-laptop-for-testing/"/>
    <updated>2018-04-29T13:43:35-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/04/29/running-a-3-node-elasticsearch-cluster-with-docker-compose-on-your-laptop-for-testing</id>
    <content type="html"><![CDATA[<p>Having a Elasticsearch cluster on your laptop with Docker for testing is great. And in this post I will show you how quick and easy it is, to have a 3 node elasticsearch cluster running on docker for testing.</p>

<h2>Pre-Requisites</h2>

<p>We need to set the <code>vm.max_map_count</code> kernel parameter:</p>

<pre><code class="bash">$ sudo sysctl -w vm.max_map_count=262144
</code></pre>

<p>To set this permanently, add it to <code>/etc/sysctl.conf</code> and reload with <code>sudo sysctl -p</code></p>

<h2>Docker Compose:</h2>

<p>The docker compose file that we will reference:</p>

<pre><code class="yaml">version: '2.2'
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.2.4
    container_name: elasticsearch
    environment:
      - cluster.name=docker-cluster
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - esdata1:/home/ruan/workspace/docker/elasticsearch/data
    ports:
      - 9200:9200
    networks:
      - esnet
  elasticsearch2:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.2.4
    container_name: elasticsearch2
    environment:
      - cluster.name=docker-cluster
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - "discovery.zen.ping.unicast.hosts=elasticsearch"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - esdata2:/home/ruan/workspace/docker/elasticsearch/data
    networks:
      - esnet
  elasticsearch3:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.2.4
    container_name: elasticsearch3
    environment:
      - cluster.name=docker-cluster
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - "discovery.zen.ping.unicast.hosts=elasticsearch"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - esdata3:/home/ruan/workspace/docker/elasticsearch/data
    networks:
      - esnet


volumes:
  esdata1:
    driver: local
  esdata2:
    driver: local
  esdata3:
    driver: local

networks:
  esnet:
</code></pre>

<p>Now make sure the paths exist that we referenced in the compose file, in my case <code>/home/ruan/workspace/docker/elasticsearch/data</code></p>

<h2>Deploy</h2>

<p>Deploy your elasticsearch cluster with docker compose:</p>

<pre><code class="bash">$ docker-compose up
</code></pre>

<p>This will run in the foreground, and you should see console output.</p>

<h2>Testing Elasticsearch</h2>

<p>Let&rsquo;s run a couple of queries, first up, check the cluster health api:</p>

<pre><code class="bash">$ curl http://127.0.0.1:9200/_cluster/health?pretty
{
  "cluster_name" : "docker-cluster",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 3,
  "number_of_data_nodes" : 3,
  "active_primary_shards" : 1,
  "active_shards" : 2,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 100.0
}
</code></pre>

<p>Create a index with replication count of 2:</p>

<pre><code class="bash">$ curl -H "Content-Type: application/json" -XPUT http://127.0.0.1:9200/test -d '{"number_of_replicas": 2}'
</code></pre>

<p>Ingest a document to elasticsearch:</p>

<pre><code class="bash">$ curl -H "Content-Type: application/json" -XPUT http://127.0.0.1:9200/test/docs/1 -d '{"name": "ruan"}'
{"_index":"test","_type":"docs","_id":"1","_version":1,"result":"created","_shards":{"total":3,"successful":3,"failed":0},"_seq_no":0,"_primary_term":1}
</code></pre>

<p>View the indices:</p>

<pre><code class="bash">$ curl http://127.0.0.1:9200/_cat/indices?v
health status index                       uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   test                        w4p2Q3fTR4uMSYBfpNVPqw   5   2          1            0      3.3kb          1.1kb
green  open   .monitoring-es-6-2018.04.29 W69lql-rSbORVfHZrj4vug   1   1       1601           38        4mb            2mb
</code></pre>

<h2>Deleting the Cluster:</h2>

<p>As its running in the foreground, you can just hit ctrl + c and as we persisted data in our compose, when you spin up the cluster again, the data will still be there.</p>

<h2>Resources:</h2>

<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Set Docker Environment Variables During Build Time]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/04/07/set-docker-environment-variables-during-build-time/"/>
    <updated>2018-04-07T09:51:35-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/04/07/set-docker-environment-variables-during-build-time</id>
    <content type="html"><![CDATA[<p>When using that <code>ARG</code> option in your Dockerfile, you can specify the <code>--build-args</code> option to define the value for the key that you specify in your Dockerfile to use for a environment variable as an example.</p>

<p>Today we will use the <code>arg</code> and <code>env</code> to set environment variables at build time.</p>

<h2>The Dockerfile:</h2>

<p>Our Dockerfile</p>

<pre><code class="dockerfile">FROM alpine:edge
ARG NAME
ENV OWNER=${NAME:-NOT_DEFINED}
CMD ["sh", "-c", "echo env var: ${OWNER}"]
</code></pre>

<p>Building our Image, we will pass the value to our NAME argument:</p>

<pre><code class="bash">$ docker build --build-arg NAME=james -t ruan:test .
</code></pre>

<p>Now when we run our container, we will notice that the build time argument has passed through to our environment variable from the running container:</p>

<pre><code class="bash">$ docker run -it ruan:test 
env var: james
</code></pre>

<p>When we build the image without specifying build arguments, and running the container:</p>

<pre><code class="bash">$ docker build -t ruan:test .
$ docker run -it ruan:test 
env var: NOT_DEFINED
</code></pre>
]]></content>
  </entry>
  
</feed>
