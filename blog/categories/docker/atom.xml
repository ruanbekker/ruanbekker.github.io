<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Docker | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/docker/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2019-04-17T08:55:33-04:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Build Small Golang Docker Containers]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/04/03/build-small-golang-docker-containers/"/>
    <updated>2019-04-03T08:24:08-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/04/03/build-small-golang-docker-containers</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/55478306-aabb0600-561b-11e9-9cc6-730fadb4beeb.png" alt="" /></p>

<p>In this tutorial I will show you how to build really small docker containers for golang applications. And I mean the difference between 310MB down to 2MB</p>

<h2>But Alpine..</h2>

<p>So we thinking lets go with alpine right? Yeah sure lets build a small, app running on go with alpine.</p>

<p>Our application code:</p>

<pre><code class="go app.go">package main

import (
  "fmt"
  "math/rand"
  "time"
)

func main() {
  lekkewords := []string{
    "dog", "cat", "fish", "giraffe",
    "moo", "spider", "lion", "apple",
    "tree", "moon", "snake", "mountain lion",
    "trooper", "burger", "nasa", "yes",
  }

  rand.Seed(time.Now().UnixNano())
  var zelength int = len(lekkewords)
  var indexnum int = rand.Intn(zelength-1)
  word := lekkewords[indexnum]

  fmt.Println("Number of words:", zelength)
  fmt.Println("Selected index number:", indexnum)
  fmt.Println("Selected word is:", word)
}
</code></pre>

<p>Our Dockerfile:</p>

<pre><code class="docker Dockerfile">FROM golang:alpine

WORKDIR $GOPATH/src/mylekkepackage/mylekkeapp/
COPY app.go .
RUN go build -o /go/app

CMD ["/go/app"]
</code></pre>

<p>Let&rsquo;s package our app to an image:</p>

<pre><code>❯ docker build -t mygolangapp:using-alpine .
</code></pre>

<p>Inspect the size of our image, as you can see it being <strong>310MB</strong></p>

<pre><code>❯ docker images "mygolangapp:*"
REPOSITORY          TAG                 IMAGE ID            CREATED              SIZE
mygolangapp         using-alpine        eea1d7bde218        About a minute ago   310MB
</code></pre>

<p>Just make sure it actually works:</p>

<pre><code>❯ docker run mygolangapp:using-alpine
Number of words: 16
Selected index number: 11
Selected word is: mountain lion
</code></pre>

<p>But for something just returning random selected text, 310MB is a bit crazy.</p>

<h2>Multi Stage Builds</h2>

<p>As Go binaries are self-contained, we can make use of docker&rsquo;s multi stage builds, where we can build our application on alpine and use the binary on a scratch image:</p>

<p>Our multi stage Dockerfile:</p>

<pre><code class="docker Dockerfile.mult">FROM golang:alpine AS builder

WORKDIR $GOPATH/src/mylekkepackage/mylekkeapp/
COPY app.go .
RUN go build -o /go/app

FROM scratch

COPY --from=builder /go/app /go/app

CMD ["/go/app"]
</code></pre>

<p>Build it:</p>

<pre><code>❯ docker build -t mygolangapp:using-multistage -f Dockerfile.multi .
</code></pre>

<p>Notice that the image is only <strong>2.01MB</strong>, say w000t!</p>

<pre><code>❯ docker images "mygolangapp:*"
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
mygolangapp         using-multistage    31474c61ba5b        15 seconds ago      2.01MB
mygolangapp         using-alpine        eea1d7bde218        2 minutes ago       310MB
</code></pre>

<p>Run the app:</p>

<pre><code>❯ docker run mygolangapp:using-multistage
Number of words: 16
Selected index number: 5
Selected word is: spider
</code></pre>

<h2>Resources</h2>

<p>Source code for this demonstration can be found at <a href="https://github.com/ruanbekker/golang-build-small-images">github.com/ruanbekker/golang-build-small-images</a></p>

<p><img src="https://user-images.githubusercontent.com/567298/55478904-236e9200-561d-11e9-9382-f31b25a9ae03.png" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Concourse Pipeline to Build a Docker Image Automatically on Git Commit]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/03/27/concourse-pipeline-to-build-a-docker-image-automatically-on-git-commit/"/>
    <updated>2019-03-27T17:50:54-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/03/27/concourse-pipeline-to-build-a-docker-image-automatically-on-git-commit</id>
    <content type="html"><![CDATA[<p><img src="https://i.snag.gy/gzkdu9.jpg?nocache=1511644783495" alt="" /></p>

<p>In this tutorial we will build a ci pipeline using concourse to build and push a image to dockerhub automatically, whenever a new git commit is made to the master branch.</p>

<h2>Our Project Setup</h2>

<p>Our Directory Tree:</p>

<pre><code class="bash">$ find .
./Dockerfile
./ci
./ci/pipeline.yml
./README.md
./docker-tunnel
</code></pre>

<p>The project used in this example is not important, but you can check it out at <a href="https://github.com/ruanbekker/docker-remote-tunnel">https://github.com/ruanbekker/docker-remote-tunnel</a></p>

<h2>Our Pipeline</h2>

<p>A visual to see how the pipeline will look like in concourse:</p>

<p><img src="https://user-images.githubusercontent.com/567298/55114996-1832d800-50ec-11e9-85ef-bc283711fbde.png" alt="" /></p>

<p>Our pipeline definition will consist of 3 resources, <code>github repo</code>, <code>dockerhub image</code> and a <code>slack resource</code> to inform use whether a build has completed.</p>

<p>Then we are specifying that the job should be triggered on a git commit for the master branch, build and push to our dockerhub repo.</p>

<p>Our pipeline definition <code>ci/pipeline.yml</code>:</p>

<pre><code class="yaml">resources:
- name: git-repo
  type: git
  source:
    uri: git@github.com:ruanbekker/docker-remote-tunnel.git
    branch: master
    private_key: ((github_private_key))

- name: docker-remote-tunnel-image
  type: docker-image
  source:
    repository: ruanbekker/docker-remote-tunnel
    tag: test
    username: ((dockerhub_user))
    password: ((dockerhub_password))

- name: slack-alert
  type: slack-notification
  source:
    url: ((slack_notification_url))

resource_types:
  - name: slack-notification
    type: docker-image
    source:
      repository: cfcommunity/slack-notification-resource
      tag: v1.3.0

jobs:
- name: build-cached-image
  plan:
  - get: git-repo
    trigger: true
  - task: build-cached-image-workspace
    config:
      platform: linux
      image_resource:
        type: docker-image
        source:
          repository: rbekker87/build-tools

      outputs:
      - name: workspace
      inputs:
      - name: git-repo

      run:
        path: /bin/sh
        args:
        - -c
        - |
          output_dir=workspace

          cat &lt;&lt; EOF &gt; "${output_dir}/Dockerfile"
          FROM alpine

          ADD git-repo /tmp/git-repo
          RUN mv /tmp/git-repo/docker-tunnel /usr/bin/docker-tunnel
          RUN apk --no-cache add screen docker openssl openssh-client apache2-utils
          RUN /usr/bin/docker-tunnel -h
          RUN rm -rf /tmp/git-repo
          EOF

          cp -R ./git-repo "${output_dir}/git-repo"

  - put: docker-remote-tunnel-image
    params:
      build: workspace

    on_failure:
      put: slack-alert
      params:
        channel: '#system_events'
        username: 'concourse'
        icon_emoji: ':concourse:'
        silent: true
        text: |
            *$BUILD_PIPELINE_NAME/$BUILD_JOB_NAME* ($BUILD_NAME) FAILED to build image
            https://ci.domain.com/teams/$BUILD_TEAM_NAME/pipelines/$BUILD_PIPELINE_NAME/jobs/$BUILD_JOB_NAME/builds/$BUILD_NAME
    on_success:
      put: slack-alert
      params:
        channel: '#system_events'
        username: 'concourse'
        icon_emoji: ':concourse:'
        silent: true
        text: |
            *$BUILD_PIPELINE_NAME/$BUILD_JOB_NAME* ($BUILD_NAME) SUCCESS - Image has been published
            https://ci.domain.com/teams/$BUILD_TEAM_NAME/pipelines/$BUILD_PIPELINE_NAME/jobs/$BUILD_JOB_NAME/builds/$BUILD_NAME

- name: test
  plan:
  - get: docker-remote-tunnel-image
    passed: [build-cached-image]
    trigger: true
  - get: git-repo
    passed: [build-cached-image]
  - task: run-tests
    image: docker-remote-tunnel-image
    config:
      platform: linux
      inputs:
      - name: git-repo
      run:
        dir: git-repo
        path: sh
        args:
        - /usr/bin/docker-tunnel
        - --help

    on_failure:
      put: slack-alert
      params:
        channel: '#system_events'
        username: 'concourse'
        icon_emoji: ':concourse:'
        silent: true
        text: |
            *$BUILD_PIPELINE_NAME/$BUILD_JOB_NAME* ($BUILD_NAME) FAILED - Testing image failure
            https://ci.domain.com/teams/$BUILD_TEAM_NAME/pipelines/$BUILD_PIPELINE_NAME/jobs/$BUILD_JOB_NAME/builds/$BUILD_NAME
    on_success:
      put: slack-alert
      params:
        channel: '#system_events'
        username: 'concourse'
        icon_emoji: ':concourse:'
        silent: true
        text: |
            *$BUILD_PIPELINE_NAME/$BUILD_JOB_NAME* ($BUILD_NAME) SUCCESS - Testing image Succeeded
            https://ci.domain.com/teams/$BUILD_TEAM_NAME/pipelines/$BUILD_PIPELINE_NAME/jobs/$BUILD_JOB_NAME/builds/$BUILD_NAME
</code></pre>

<p>Note that our secret information is templatized and saved in our local <code>credentials.yml</code> which should never be stored in version control:</p>

<pre><code class="yaml">slack_notification_url: https://api.slack.com/aaa/bbb/ccc
dockerhub_user: myuser
dockerhub_password: mypasswd
github_private_key: |-
        -----BEGIN RSA PRIVATE KEY-----
        some-secret-data
        -----END RSA PRIVATE KEY------
</code></pre>

<h2>Set the Pipeline:</h2>

<p>Now that we have our pipeline definition, credentials and application code (stored in version control), go ahead and set the pipeline, which will save the pipeline configuration in concourse:</p>

<pre><code class="bash"># pipeline name: my-docker-app-pipeline
$ fly -t scw sp -n main -c pipeline.yml -p my-docker-app-pipeline -l credentials.yml
</code></pre>

<p>Now the pipeline is saved on concourse but in a paused state, go ahead and unpause the pipeline:</p>

<pre><code class="bash">$ fly -t scw up -p my-docker-app-pipeline
</code></pre>

<h2>Test your Pipeline</h2>

<p>Make a commit to master and head over to concourse and look at it go:</p>

<p><img src="https://user-images.githubusercontent.com/567298/55116018-a5772c00-50ee-11e9-861e-a5ddc74550e2.png" alt="" /></p>

<p>Thanks for reading, make sure to check out my other posts on <a href="https://blog.ruanbekker.com/blog/categories/concourse">#concourse</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Deploy a Docker Swarm Cluster on Scaleway With Terraform]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/03/21/how-to-deploy-a-docker-swarm-cluster-on-scaleway-with-terraform/"/>
    <updated>2019-03-21T02:15:07-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/03/21/how-to-deploy-a-docker-swarm-cluster-on-scaleway-with-terraform</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/54737111-09fa2e80-4bb7-11e9-97f4-a94a31fc9a3a.png" alt="" /></p>

<p>We will deploy a 3 node docker swarm cluster with terraform on scaleway. I have used the base source code from <a href="https://github.com/stefanprodan/scaleway-swarm-terraform">this</a> repository but tweaked the configuration to my needs.</p>

<h2>Pre-Requisites</h2>

<p>Ensure terraform and jq is instaled:</p>

<pre><code class="bash">$ brew install terraform
$ brew install jq
</code></pre>

<h2>Terraform</h2>

<p>You can have a look at the linked source at the top for the source code, but below I will provide each file that will make up our terraform deployment.</p>

<p>Ource <code>main.tf</code></p>

<pre><code>provider "scaleway" {
  region = "${var.region}"
}

data "scaleway_bootscript" "debian" {
  architecture = "x86_64"
  name = "x86_64 mainline 4.15.11 rev1"
}

data "scaleway_image" "debian_stretch" {
  architecture = "x86_64"
  name         = "Debian Stretch"
}

data "template_file" "docker_conf" {
  template = "${file("conf/docker.tpl")}"

  vars {
    ip = "${var.docker_api_ip}"
  }
}
</code></pre>

<p>The <code>outputs.tf</code></p>

<pre><code>output "swarm_manager_public_ip" {
  value = "${scaleway_ip.swarm_manager_ip.0.ip}"
}

output "swarm_manager_private_ip" {
  value = "${scaleway_server.swarm_manager.0.private_ip}"
}

output "swarm_workers_public_ip" {
  value = "${concat(scaleway_server.swarm_worker.*.name, scaleway_server.swarm_worker.*.public_ip)}"
}

output "swarm_workers_private_ip" {
  value = "${concat(scaleway_server.swarm_worker.*.name, scaleway_server.swarm_worker.*.private_ip)}"
}

output "workspace" {
  value = "${terraform.workspace}"
}
</code></pre>

<p>Our <code>security-groups.tf</code></p>

<pre><code>resource "scaleway_security_group" "swarm_managers" {
  name        = "swarm_managers"
  description = "Allow HTTP/S and SSH traffic"
}

resource "scaleway_security_group_rule" "ssh_accept" {
  security_group = "${scaleway_security_group.swarm_managers.id}"

  action    = "accept"
  direction = "inbound"
  ip_range  = "0.0.0.0/0"
  protocol  = "TCP"
  port      = 22
}

resource "scaleway_security_group_rule" "http_accept" {
  security_group = "${scaleway_security_group.swarm_managers.id}"

  action    = "accept"
  direction = "inbound"
  ip_range  = "0.0.0.0/0"
  protocol  = "TCP"
  port      = 80
}

resource "scaleway_security_group_rule" "https_accept" {
  security_group = "${scaleway_security_group.swarm_managers.id}"

  action    = "accept"
  direction = "inbound"
  ip_range  = "0.0.0.0/0"
  protocol  = "TCP"
  port      = 443
}

resource "scaleway_security_group" "swarm_workers" {
  name        = "swarm_workers"
  description = "Allow SSH traffic"
}

resource "scaleway_security_group_rule" "ssh_accept_workers" {
  security_group = "${scaleway_security_group.swarm_workers.id}"

  action    = "accept"
  direction = "inbound"
  ip_range  = "0.0.0.0/0"
  protocol  = "TCP"
  port      = 22
}
</code></pre>

<p>Our <code>variables.tf</code></p>

<pre><code>variable "docker_version" {
  default = "18.06.3~ce~3-0~debian"
}

variable "region" {
  default = "ams1"
}

variable "manager_instance_type" {
  default = "START1-M"
}

variable "worker_instance_type" {
  default = "START1-M"
}

variable "worker_instance_count" {
  default = 2
}

variable "docker_api_ip" {
  default = "127.0.0.1"
}
</code></pre>

<p>Our <code>managers.tf</code></p>

<pre><code>resource "scaleway_ip" "swarm_manager_ip" {
  count = 1
}

resource "scaleway_server" "swarm_manager" {
  count          = 1
  name           = "${terraform.workspace}-manager-${count.index + 1}"
  image          = "${data.scaleway_image.debian_stretch.id}"
  type           = "${var.manager_instance_type}"
  bootscript     = "${data.scaleway_bootscript.debian.id}"
  security_group = "${scaleway_security_group.swarm_managers.id}"
  public_ip      = "${element(scaleway_ip.swarm_manager_ip.*.ip, count.index)}"

  volume {
    size_in_gb = 50
    type       = "l_ssd"
  }

  provisioner "remote-exec" {
    script = "scripts/mount-disk.sh"
  }

  connection {
    type = "ssh"
    user = "root"
    private_key = "${file("~/.ssh/id_rsa")}"
  }

  provisioner "remote-exec" {
    inline = [
      "mkdir -p /etc/systemd/system/docker.service.d",
    ]
  }

  provisioner "file" {
    content     = "${data.template_file.docker_conf.rendered}"
    destination = "/etc/systemd/system/docker.service.d/docker.conf"
  }

  provisioner "file" {
    source      = "scripts/install-docker-ce.sh"
    destination = "/tmp/install-docker-ce.sh"
  }

  provisioner "file" {
    source      = "scripts/local-persist-plugin.sh"
    destination = "/tmp/local-persist-plugin.sh"
  }

  provisioner "remote-exec" {
    inline = [
      "chmod +x /tmp/install-docker-ce.sh",
      "/tmp/install-docker-ce.sh ${var.docker_version}",
      "docker swarm init --advertise-addr ${self.private_ip}",
      "chmod +x /tmp/local-persist-plugin.sh",
      "/tmp/local-persist-plugin.sh"
    ]
  }
}
</code></pre>

<p>Our <code>workers.tf</code></p>

<pre><code>resource "scaleway_ip" "swarm_worker_ip" {
  count = "${var.worker_instance_count}"
}

resource "scaleway_server" "swarm_worker" {
  count          = "${var.worker_instance_count}"
  name           = "${terraform.workspace}-worker-${count.index + 1}"
  image          = "${data.scaleway_image.debian_stretch.id}"
  type           = "${var.worker_instance_type}"
  bootscript     = "${data.scaleway_bootscript.debian.id}"
  security_group = "${scaleway_security_group.swarm_workers.id}"
  public_ip      = "${element(scaleway_ip.swarm_worker_ip.*.ip, count.index)}"

  volume {
    size_in_gb = 50
    type       = "l_ssd"
  }

  provisioner "remote-exec" {
    script = "scripts/mount-disk.sh"
  }

  connection {
    type = "ssh"
    user = "root"
    private_key = "${file("~/.ssh/id_rsa")}"
  }

  provisioner "remote-exec" {
    inline = [
      "mkdir -p /etc/systemd/system/docker.service.d",
    ]
  }

  provisioner "file" {
    content     = "${data.template_file.docker_conf.rendered}"
    destination = "/etc/systemd/system/docker.service.d/docker.conf"
  }

  provisioner "file" {
    source      = "scripts/install-docker-ce.sh"
    destination = "/tmp/install-docker-ce.sh"
  }

  provisioner "file" {
    source      = "scripts/local-persist-plugin.sh"
    destination = "/tmp/local-persist-plugin.sh"
  }

  provisioner "remote-exec" {
    inline = [
      "chmod +x /tmp/install-docker-ce.sh",
      "/tmp/install-docker-ce.sh ${var.docker_version}",
      "docker swarm join --token ${data.external.swarm_tokens.result.worker} ${scaleway_server.swarm_manager.0.private_ip}:2377",
      "chmod +x /tmp/local-persist-plugin.sh",
      "/tmp/local-persist-plugin.sh",
    ]
  }

  provisioner "remote-exec" {
    when = "destroy"

    inline = [
      "docker node update --availability drain ${self.name}",
    ]

    on_failure = "continue"

    connection {
      type = "ssh"
      user = "root"
      host = "${scaleway_ip.swarm_manager_ip.0.ip}"
    }
  }

  provisioner "remote-exec" {
    when = "destroy"

    inline = [
      "docker swarm leave",
    ]

    on_failure = "continue"
  }

  provisioner "remote-exec" {
    when = "destroy"

    inline = [
      "docker node rm --force ${self.name}",
    ]

    on_failure = "continue"

    connection {
      type = "ssh"
      user = "root"
      host = "${scaleway_ip.swarm_manager_ip.0.ip}"
    }
  }
}

data "external" "swarm_tokens" {
  program = ["./scripts/fetch-tokens.sh"]

  query = {
    host = "${scaleway_ip.swarm_manager_ip.0.ip}"
  }

  depends_on = ["scaleway_server.swarm_manager"]
}
</code></pre>

<p>Our config for the docker daemon: <code>conf/docker.tpl</code></p>

<pre><code>[Service]
ExecStart=
ExecStart=/usr/bin/dockerd -H fd:// \
  -H tcp://${ip}:2375 \
  --storage-driver=overlay2 \
  --dns 8.8.4.4 --dns 8.8.8.8 \
  --log-driver json-file \
  --log-opt max-size=50m --log-opt max-file=10 \
  --experimental=true \
  --metrics-addr 172.17.0.1:9323
</code></pre>

<p>Our script to mount our additional disk: <code>scripts/mount-disk.sh</code></p>

<pre><code class="bash">#!/bin/bash
apt update
apt install xfsprogs attr -y
mkfs -t xfs /dev/vdb
echo "/dev/vdb /mnt xfs defaults 0 0" &gt;&gt; /etc/fstab
mount -a
</code></pre>

<p>Our script to install docker: <code>scripts/install-docker-ce.sh</code></p>

<pre><code class="bash">#!/usr/bin/env bash

DOCKER_VERSION=$1
DEBIAN_FRONTEND=noninteractive apt-get -qq update
apt-get -qq install apt-transport-https ca-certificates curl software-properties-common
curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -
add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable"

apt-get -q update -y
apt-get -q install -y docker-ce=$DOCKER_VERSION containerd.io
</code></pre>

<p>Our script that retrieves the swarm tokens: <code>scripts/fetch-tokens.sh</code></p>

<pre><code class="bash">#!/usr/bin/env bash

# Processing JSON in shell scripts
# https://www.terraform.io/docs/providers/external/data_source.html#processing-json-in-shell-scripts

set -e

# Extract "host" argument from the input into HOST shell variable
eval "$(jq -r '@sh "HOST=\(.host)"')"

MANAGER=$(ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null root@$HOST docker swarm join-token manager -q)
WORKER=$(ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null root@$HOST docker swarm join-token worker -q)

# produce a json object containing the tokens
jq -n --arg manager "$MANAGER" --arg worker "$WORKER" '{"manager":$manager,"worker":$worker}'
</code></pre>

<p>Our script to install the <a href="https://github.com/CWSpear/local-persist">local-persist docker volume</a> plugin: <code>scripts/local-persist-plugin.sh</code></p>

<pre><code>#!/usr/bin/env bash
set -e
curl -fsSL https://raw.githubusercontent.com/CWSpear/local-persist/master/scripts/install.sh | bash
</code></pre>

<h2>Deploy your Swarm</h2>

<p>Note that we will be deploying 3x SMART1-M servers with Debian Stretch. At this moment the image id is the one of debian stretch but may change in the future. If you want to change the distro, update the install script, and the terraform files.</p>

<p><a href="https://www.scaleway.com/docs/generate-an-api-token/">Generate API Token on Scaleway</a> then export it to your current shell:</p>

<pre><code class="bash">export SCALEWAY_ORGANIZATION="&lt;organization-id&gt;"
export SCALEWAY_TOKEN="&lt;secret&gt;"
</code></pre>

<p>Make sure that your ssh private key is the intended one as in the config, in my example: <code>~/.ssh/id_rsa</code> and that they are allowed in your servers <code>authorized_keys</code> file</p>

<p>Create a new workspace:</p>

<pre><code class="bash">$ terraform new workspace swarm
</code></pre>

<p>Pull down the providers and initialize:</p>

<pre><code class="bash">$ terraform init
</code></pre>

<p>Deploy!</p>

<pre><code class="bash">$ terraform apply
...
...
scaleway_server.swarm_worker[0]: Creation complete after 4m55s (ID: xx-xx-xx-xx-xx)

Apply complete! Resources: 14 added, 0 changed, 0 destroyed.
Outputs:

swarm_manager_private_ip = 10.21.x.x
swarm_manager_public_ip = 51.xx.xx.xx
swarm_workers_private_ip = [
    swarm-worker-1,
    swarm-worker-2,
    10.20.xx.xx,
    10.20.xx.xx,
]
swarm_workers_public_ip = [
    swarm-worker-1,
    swarm-worker-2,
    51.xx.xx.xx,
    51.xx.xx.xx,
]
workspace = swarm
</code></pre>

<p>Once your deployment is done you will be prompted with the public/private ip addresses of your nodes as seen above, you can also manually retrieve them:</p>

<pre><code>$ terraform terraform output
</code></pre>

<p>Or for a specific node, such as the manager:</p>

<pre><code>$ terraform terraform output swarm-manager
51.xx.xx.xx
</code></pre>

<p>Go ahead and ssh to your manager nodes and list the swarm nodes, boom, easy right.</p>

<pre><code>$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
2696o0vrt93x8qf2gblbfc8pf *   swarm-manager       Ready               Active              Leader              18.09.3
72ava7rrp2acnyadisg52n7ym     swarm-worker-1      Ready               Active                                  18.09.3
sy2otqn20qe9jc2v9io3a21jm     swarm-worker-2      Ready               Active                                  18.09.3
</code></pre>

<p>When you want to destroy the environment:</p>

<pre><code>$ terraform destroy -force
</code></pre>

<h2>References:</h2>

<p>Big thanks goes to <a href="https://github.com/stefanprodan">@stefanprodan</a></p>

<ul>
<li><a href="https://www.terraform.io/docs/index.html">https://www.terraform.io/docs/index.html</a></li>
<li><a href="https://docs.docker.com/engine/swarm/">https://docs.docker.com/engine/swarm/</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Container Persistent Storage for Docker Swarm Using a GlusterFS Volume Plugin]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/03/05/container-persistent-storage-for-docker-swarm-using-a-glusterfs-volume-plugin/"/>
    <updated>2019-03-05T13:18:30-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/03/05/container-persistent-storage-for-docker-swarm-using-a-glusterfs-volume-plugin</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/53351889-85572000-392a-11e9-9720-464e9318206e.jpg" alt="" /></p>

<p>From one of my previous posts I demonstrated how to provide persistent storage for your containers by using a <a href="https://blog.ruanbekker.com/blog/2018/02/16/guide-to-setup-docker-convoy-volume-driver-for-docker-swarm-with-nfs/">Convoy NFS Plugin</a>.</p>

<p>I&rsquo;ve stumbled upon one AWESOME GlusterFS Volume Plugin for Docker by <a href="https://github.com/trajano/docker-volume-plugins/tree/master/glusterfs-volume-plugin">@trajano</a>, please have a look at his repository. I&rsquo;ve been waiting for some time for one solid glusterfs volume plugin, and it works great.</p>

<h2>What we will be doing today</h2>

<p>We will setup a 3 node replicated glusterfs volume and show how easy it is to install the volume plugin and then demonstrate how storage from our swarms containers are persisted.</p>

<p>Our servers that we will be using will have the private ip&rsquo;s as shown below:</p>

<pre><code>10.22.125.101
10.22.125.102
10.22.125.103
</code></pre>

<h2>Setup GlusterFS</h2>

<p>Have a look at <a href="https://blog.ruanbekker.com/blog/2019/03/05/setup-a-3-node-replicated-storage-volume-with-glusterfs/?referral=github.com">this</a> post to setup the glusterfs volume.</p>

<h2>Install the GlusterFS Volume Plugin</h2>

<p>Below I&rsquo;m installing the plugin and setting the alias name as <code>glusterfs</code>, granting all permissions and keeping the plugin in a disabled state.</p>

<pre><code class="bash">$ docker plugin install --alias glusterfs trajano/glusterfs-volume-plugin --grant-all-permissions --disable
</code></pre>

<p>Set the glusterfs servers:</p>

<pre><code>$ docker plugin set glusterfs SERVERS=10.22.125.101,10.22.125.102,10.22.125.103
</code></pre>

<p>Enable the glusterfs plugin:</p>

<pre><code>$ docker plugin enable glusterfs
</code></pre>

<h2>Create a Service in Docker Swarm</h2>

<p>Deploy a sample service on docker swarm with a volume backed by glusterfs. Note that my glusterfs volume is called <code>gfs</code></p>

<pre><code class="yaml">version: "3.4"

services:
  foo:
    image: alpine
    command: ping localhost
    networks:
      - net
    volumes:
      - vol1:/tmp

networks:
  net:
    driver: overlay

volumes:
  vol1:
    driver: glusterfs
    name: "gfs/vol1"
</code></pre>

<p>Deploy the stack:</p>

<pre><code class="bash">$ docker stack deploy -c docker-compose.yml test
Creating service test_foo
</code></pre>

<p>Have a look on which node is your container running:</p>

<pre><code class="bash">$ docker service ps test_foo
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
jfwzb7yxnrxx        test_foo.1          alpine:latest       swarm-worker-1      Running             Running 37 seconds ago
</code></pre>

<p>Now jump to the <code>swarm-worker-1</code> node and verify that the container is running on that node:</p>

<pre><code class="bash">$ docker ps
CONTAINER ID        IMAGE                                          COMMAND                  CREATED             STATUS                  PORTS               NAMES
d469f341d836        alpine:latest                                  "ping localhost"           59 seconds ago      Up 57 seconds                               test_foo.1.jfwzb7yxnrxxnd0qxtcjex8lu
</code></pre>

<p>Now since the container is running on this node, we will also see that the volume defined in our task configuration will also be present:</p>

<pre><code class="bash">$ docker volume ls
DRIVER                       VOLUME NAME
glusterfs:latest             gfs/vol1
</code></pre>

<p>Exec into the container and look at the disk layout:</p>

<pre><code class="bash">$ docker exec -it d469f341d836 sh
/ # df -h
Filesystem                Size      Used Available Use% Mounted on
overlay                  45.6G      3.2G     40.0G   7% /
10.22.125.101:gfs/vol1   45.6G      3.3G     40.0G   8% /tmp
</code></pre>

<p>While you are in the container, write the hostname&rsquo;s value into a file which is mapped to the glusterfs volume:</p>

<pre><code class="bash">$ echo $HOSTNAME &gt; /tmp/data.txt
$ cat /tmp/data.txt
d469f341d836
</code></pre>

<h2>Testing Data Persistence</h2>

<p>Time to test the data persistence. Scale the service to 3 replicas, then hop onto a new node where a replica resides and check if the data was persisted.</p>

<pre><code class="bash">$ docker service scale test_foo=3
test_foo scaled to 3
overall progress: 3 out of 3 tasks
1/3: running   [==================================================&gt;]
2/3: running   [==================================================&gt;]
3/3: running   [==================================================&gt;]
verify: Service converged
</code></pre>

<p>Check where the containers are running:</p>

<pre><code class="bash">$ docker service ps test_foo
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
jfwzb7yxnrxx        test_foo.1          alpine:latest       swarm-worker-1      Running             Running 2 minutes ago
mdsg6c5b2nqb        test_foo.2          alpine:latest       swarm-worker-3      Running             Running 15 seconds ago
iybat57t4lha        test_foo.3          alpine:latest       swarm-worker-2      Running             Running 15 seconds ago
</code></pre>

<p>Hop onto the <code>swarm-worker-2</code> node and check if the data is persisted from our previous write:</p>

<pre><code class="bash">$ docker exec -it 4228529aba29 sh
$ cat /tmp/data.txt
d469f341d836
</code></pre>

<p>Now let&rsquo;s append data to that file, then delete the stack and recreate to test if the data is still persisted:</p>

<pre><code class="bash">$ echo $HOSTNAME &gt;&gt; /tmp/data.txt
$ cat /tmp/data.txt
d469f341d836
4228529aba29
</code></pre>

<p>On the manager delete the stack:</p>

<pre><code class="bash">$ docker stack rm test
Removing service test_foo
</code></pre>

<p>The deploy the stack again:</p>

<pre><code class="bash">$ docker stack deploy -c docker-compose.yml test
Creating service test_foo
</code></pre>

<p>Check where the container is running:</p>

<pre><code class="bash">$ docker service ps test_foo
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS
9d6z02m123jk        test_foo.1          alpine:latest       swarm-worker-1      Running             Running 2 seconds ago
</code></pre>

<p>Exec into the container and read the data:</p>

<pre><code class="bash">$ docker exec -it 3008b1e1bba1 cat /tmp/data.txt
d469f341d836
4228529aba29
</code></pre>

<p>And as you can see the data is persisted.</p>

<h2>Resources</h2>

<p>Please have a look and star <a href="https://github.com/trajano/docker-volume-plugins">@trajano&rsquo;s</a> repository:</p>

<ul>
<li><a href="https://github.com/trajano/docker-volume-plugins">https://github.com/trajano/docker-volume-plugins</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Use Swarm Managed Configs in Docker Swarm to Store Your Application Configs]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/02/28/use-swarm-managed-configs-in-docker-swarm-to-store-your-application-configs/"/>
    <updated>2019-02-28T09:48:28-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/02/28/use-swarm-managed-configs-in-docker-swarm-to-store-your-application-configs</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/53351889-85572000-392a-11e9-9720-464e9318206e.jpg" alt="" /></p>

<p>Docker version 17.06 introduced Swarm Service Configs, which allows you to store data like configuration files, note that this is for non-sensitive information.</p>

<p>In this tutorial we will store the data of our <code>index.html</code> in a service config, then attach the config to our service.</p>

<h2>Creating the Config</h2>

<p>Create the <code>index.html</code> file and store it as a config:</p>

<pre><code class="bash">$ cat &gt; index.html &lt;&lt; EOF
&lt;html&gt;
  &lt;body&gt;
    Hello, World!
  &lt;/body&gt;
&lt;/html&gt;
EOF
</code></pre>

<p>Store the config as <code>nginx_root_doc</code>:</p>

<pre><code class="bash">$ docker config create nginx_root_doc index.html
</code></pre>

<h2>Create the Service</h2>

<p>Create the swarm service and associate the config with the service and set the target path where the config will reside:</p>

<pre><code class="bash">$ docker service create --name web \
  --config source=nginx_root_doc,target=/usr/share/nginx/html/index.html \
  --publish 8080:80 nginx:alpine
</code></pre>

<p>Once the service is up, test it:</p>

<pre><code class="bash">$ curl -i http://localhost:8080/
&lt;html&gt;
HTTP/1.1 200 OK
Server: nginx/1.15.9
Date: Thu, 28 Feb 2019 12:00:19 GMT
Content-Type: text/html
Content-Length: 52
Last-Modified: Thu, 28 Feb 2019 11:59:37 GMT
Connection: keep-alive
ETag: "5c77cd29-34"
Accept-Ranges: bytes

&lt;html&gt;
  &lt;body&gt;
    Hello, World!
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>Delete the service:</p>

<pre><code class="bash">$ docker service rm web
</code></pre>

<p>Delete the config:</p>

<pre><code class="bash">$ docker config rm nginx_root_doc
</code></pre>

<h2>Create the Service using Compose:</h2>

<p>Doing the same with a docker-compose file, will look like the following. The first example will be where we will explicitly define our path of our secret, and will create on deploy time. Our compose file:</p>

<pre><code class="yaml">services:
  web:
    image: nginx:alpine
    ports:
      - 8080:80
    networks:
      - net
    configs:
      - source: nginx_root_doc
        target: /usr/share/nginx/html/index.html

configs:
  nginx_root_doc:
    file: ./index.html

networks:
  net:
    driver: overlay
</code></pre>

<p>Deploying our stack:</p>

<pre><code class="bash">$ docker stack deploy -c docker-compose.yml apps
Creating network apps_net
Creating config apps_nginx_root_doc
Creating service apps_web
</code></pre>

<p>Testing our our service:</p>

<pre><code class="bash">$ curl -i http://localhost:8080/
HTTP/1.1 200 OK
Server: nginx/1.15.9
Date: Thu, 28 Feb 2019 12:20:52 GMT
Content-Type: text/html
Content-Length: 56
Last-Modified: Thu, 28 Feb 2019 12:20:47 GMT
Connection: keep-alive
ETag: "5c77d21f-38"
Accept-Ranges: bytes

&lt;html&gt;
  &lt;body&gt;
    Hello, World!
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>Note, that configs cant be updated, if you want to rotate a config you will create a new config and update the target in your task definition to point to your new config.</p>

<p>Delete the stack:</p>

<pre><code class="bash">$ docker stack rm apps
Removing service apps_web
Removing config apps_nginx_root_doc
Removing network apps_net
</code></pre>

<p>Another example will be to point to a external config which already exists in swarm. The only change will be that we need to set the config as a external type.</p>

<p>Create the config:</p>

<pre><code class="bash">$ docker config create nginx_root_doc index.html
</code></pre>

<p>Now that the config exists, create this compose file:</p>

<pre><code class="yaml">version: "3.3"

services:
  web:
    image: nginx:alpine
    ports:
      - 8080:80
    networks:
      - net
    configs:
      - source: nginx_root_doc
        target: /usr/share/nginx/html/index.html

configs:
  nginx_root_doc:
    external: true

networks:
  net:
    driver: overlay
</code></pre>

<p>Then deploy the stack:</p>

<pre><code class="bash">$ docker stack deploy -c docker-compose.yml apps
Creating network apps_net
Creating service apps_web
</code></pre>

<p>Then testing:</p>

<pre><code class="bash">$ curl -i http://localhost:8080/
HTTP/1.1 200 OK
Server: nginx/1.15.9
Date: Thu, 28 Feb 2019 12:28:11 GMT
Content-Type: text/html
Content-Length: 56
Last-Modified: Thu, 28 Feb 2019 12:28:09 GMT
Connection: keep-alive
ETag: "5c77d3d9-38"
Accept-Ranges: bytes

&lt;html&gt;
  &lt;body&gt;
    Hello, World!
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>

<h2>Resources:</h2>

<p>For more information on docker swarm configs have a look at <a href="https://docs.docker.com/engine/swarm/configs/#example-rotate-a-config">docker&rsquo;s documentation</a></p>
]]></content>
  </entry>
  
</feed>
