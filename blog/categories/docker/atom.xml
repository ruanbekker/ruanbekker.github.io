<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Docker | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/docker/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2019-09-04T22:40:33+02:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Deploy Traefik Using Bekker Stacks]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/09/04/deploy-traefik-using-bekker-stacks/"/>
    <updated>2019-09-04T21:46:35+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/09/04/deploy-traefik-using-bekker-stacks</id>
    <content type="html"><![CDATA[<p><img src="![image](https://user-images.githubusercontent.com/50801771/64287218-67b0e600-cf5f-11e9-8fe7-f36cb8e71f6f.png" alt="" />)</p>

<p><a href="https://saythanks.io/to/ruanbekker"><img src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" alt="Say Thanks!" /></a> <a href="https://linux-hackers-slack.herokuapp.com/"><img src="https://linux-hackers-slack.herokuapp.com/badge.svg" alt="Slack Status" /></a> <a href="https://linux-hackers.slack.com/"><img src="https://img.shields.io/badge/chat-on_slack-orange.svg" alt="Chat on Slack" /></a> <a href="https://github.com/bekkerstacks/traefik"><img src="https://img.shields.io/github/followers/ruanbekker.svg?label=Follow&amp;style=social" alt="GitHub followers" /></a></p>

<p><a href="https://twitter.com/ruanbekker?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @ruanbekker</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>

<p>After a year or two spending quite a lot of time into docker and more specifically docker swarm, I found it quite tedious to write up docker-compose files for specific stacks that you are working on. I also felt the need for a docker swarm compose package manager.</p>

<p>Fair enough, you store them on a central repository and then you can reuse them as you go, and that is exactly what I did, but I felt that perhaps other people have the same problem.</p>

<h2>The Main Idea</h2>

<p>So the main idea is to have a central repository with docker swarm stacks, that you can pick and choose what you want, pull down the repository and use environment variables to override the default configuration and use the deploy script to deploy the stack that you want.</p>

<h2>Future Ideas</h2>

<p>In the future I would like to create a cli tool that you can use to list stacks, as example:</p>

<pre><code>$ bstacks list
traefik
monitoring-cpang (cAdvisor, Prometheus, Alertmanager, Node-Exporter, Grafana)
monitoring-tig   (Telegraf, InfluxDB, Grafana)
logging-efk      (Elasticsearch, Filebeat, Kibana)
...
</code></pre>

<p>Listing stacks by category:</p>

<pre><code>$ bstacks list --category logging
logging-efk
...
</code></pre>

<p>Deploying a stack:</p>

<pre><code>$ bstacks deploy --stack traefik --stack-name proxy --env-file ./stack.env
Username for Traefik UI: ruan
Password for Traefik UI: deploying traefik stack in http mode
Creating network public
Creating config proxy_traefik_htpasswd
Creating service proxy_traefik
Traefik UI is available at:
- http://traefik.localhost
</code></pre>

<p>At the time of writing the cli tool is not available yet, but the list of available templated docker stack repositories are availabe at <a href="https://github.com/bekkerstacks?tab=repositories">github.com/bekkerstacks</a></p>

<h2>What are we doing today</h2>

<p>In this tutorial we will deploy a <a href="https://github.com/bekkerstacks/traefik">Traefik</a> proxy on Docker Swarm. I will be demonstrating the deployment on my Mac, and currently I have only docker installed, without a swarm being initialized.</p>

<p>If you already have a swarm initialized and running this on servers, you can skip the local dev section.</p>

<h2>Local Dev</h2>

<p>We will be initializing a 3 node docker swarm on a mac using docker-in-docker. Get the repository:</p>

<pre><code>$ git clone https://github.com/bekkerstacks/docker-swarm
</code></pre>

<p>Switch to the directory and deploy the swarm:</p>

<pre><code>$ bash deploy.sh

ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
lkyjkvuc5uobzgps4m7e98l0u *   docker-desktop      Ready               Active              Leader              19.03.1
6djgz804emj89rs8icd53wfgn     worker-1            Ready               Active                                  18.06.3-ce
gcz6ou0s5p8kxve63ihnky7ai     worker-2            Ready               Active                                  18.06.3-ce
ll8zfvuaek8q4x9nlijib0dfa     worker-3            Ready               Active                                  18.06.3-ce
</code></pre>

<p>As you can see we have a 4 node docker swarm running on our local dev environment to continue.</p>

<h2>Deploy Traefik</h2>

<p>To deploy traefik in HTTPS mode, we need to set 3 environment variables: <code>EMAIL</code>, <code>DOMAIN</code>, <code>PROTOCOL</code>. We also need to setup our DNS to direct traefik to our swarm. In my case I will be using <code>1.2.3.4</code> as the IP of my Manager node and using the domain <code>mydomain.com</code></p>

<p>The DNS setup will look like this:</p>

<pre><code>A Record: mydomain.com -&gt; 1.1.1.1
A Record: *.mydomain.com -&gt; 1.1.1.1
</code></pre>

<p>And if you are using this locally, you can setup your <code>/etc/hosts</code> to <code>127.0.0.1 traefik.mydomain.com</code></p>

<p>Clone the repository:</p>

<pre><code>$ git clone https://github.com/bekkerstacks/traefik
</code></pre>

<p>Change the the repository and deploy the stack:</p>

<pre><code>$ EMAIL=me@mydomain.com DOMAIN=mydomain.com PROTOCOL=https bash deploy.sh
Username for Traefik UI: ruan
Password for Traefik UI: deploying traefik stack in https mode
Creating network public
Creating config proxy_traefik_htpasswd
Creating service proxy_traefik
Traefik UI is available at:
- https://traefik.mydomain.com
</code></pre>

<p>Verify that the Traefik service is running:</p>

<pre><code>$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
0wga71zbx1pe        proxy_traefik       replicated          1/1                 traefik:1.7.14      *:80-&gt;80/tcp
</code></pre>

<p>Navigating to the Traefik Dashboard, after providing your username and password, you should see the Traefik UI:</p>

<p><img src="https://user-images.githubusercontent.com/50801771/64284457-eefb5b00-cf59-11e9-90cb-eeb2b417c80c.png" alt="" /></p>

<p>Note: I don&rsquo;t own mydomain.com therefore I am using the traefik default cert, that will be why it&rsquo;s showing not secure.</p>

<h2>Deploy Traefik in HTTP Mode</h2>

<p>If you want to deploy Traefik in HTTP mode rather, you would use:</p>

<pre><code>$ DOMAIN=localhost PROTOCOL=http bash deploy.sh
Username for Traefik UI: ruan
Password for Traefik UI: deploying traefik stack in http mode
Creating network public
Creating config proxy_traefik_htpasswd
Creating service proxy_traefik
Traefik UI is available at:
- http://traefik.localhost
</code></pre>

<p>Navigating to the Traefik Dashboard, after providing your username and password, you should see the Traefik UI:</p>

<p><img src="https://user-images.githubusercontent.com/50801771/64283759-56b0a680-cf58-11e9-9f85-6721ab3b1500.png" alt="" /></p>

<h2>More Info</h2>

<p>In future posts, I will demonstrate how to deploy other stacks using bekkerstacks.</p>

<p>Have a look at the repositories on github for more info:</p>

<ul>
<li><a href="https://github.com/bekkerstacks">https://github.com/bekkerstacks</a></li>
<li><a href="https://github.com/bekkerstacks/docker-swarm">https://github.com/bekkerstacks/docker-swarm</a></li>
<li><a href="https://github.com/bekkerstacks/traefik">https://github.com/bekkerstacks/traefik</a></li>
</ul>


<h2>Thank You</h2>

<p>Let me know what you think. If you liked my content, feel free to checkout my content on <strong><a href="https://ruan.dev/">ruan.dev</a></strong> or follow me on twitter at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running a HA MySQL Galera Cluster on Docker Swarm]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/05/10/running-a-ha-mysql-galera-cluster-on-docker-swarm/"/>
    <updated>2019-05-10T13:02:39+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/05/10/running-a-ha-mysql-galera-cluster-on-docker-swarm</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/57523982-c904d780-7326-11e9-981a-7a9cb9552c2f.png" alt="image" /></p>

<p>In this post we will setup a highly available mysql galera cluster on docker swarm.</p>

<h2>About</h2>

<p>The service is based of <a href="https://github.com/toughIQ/docker-mariadb-cluster">docker-mariadb-cluster</a> repository and it&rsquo;s designed not to have any persistent data attached to the service, but rely on the &ldquo;nodes&rdquo; to replicate the data.</p>

<p>Note, that however this proof of concept works, I always recommend to use a remote mysql database outside your cluster, such as RDS etc.</p>

<p>Since we don&rsquo;t persist any data on the mysql cluster, I have associated a dbclient service that will run continious backups, which we will persist the path where the backups reside to disk.</p>

<h2>Deploy the MySQL Cluster</h2>

<p>The <a href="https://raw.githubusercontent.com/ruanbekker/dockerfiles/master/mysql-cluster/docker-compose.yml">docker-compose.yml</a> that we will use looks like this:</p>

<pre><code class="yaml">version: '3.5'
services:
  dbclient:
    image: alpine
    environment:
      - BACKUP_ENABLED=1
      - BACKUP_INTERVAL=3600
      - BACKUP_PATH=/data
      - BACKUP_FILENAME=db_backup
    networks:
      - dbnet
    entrypoint: |
      sh -c 'sh -s &lt;&lt; EOF
      apk add --no-cache mysql-client
      while true
        do
          if [ $$BACKUP_ENABLED == 1 ]
            then
              sleep $$BACKUP_INTERVAL
              mkdir -p $$BACKUP_PATH/$$(date +%F)
              echo "$$(date +%FT%H.%m) - Making Backup to : $$BACKUP_PATH/$$(date +%F)/$$BACKUP_FILENAME-$$(date +%FT%H.%m).sql.gz"
              mysqldump -u root -ppassword -h dblb --all-databases | gzip &gt; $$BACKUP_PATH/$$(date +%F)/$$BACKUP_FILENAME-$$(date +%FT%H.%m).sql.gz
              find $$BACKUP_PATH -mtime 7 -delete
          fi
        done
      EOF'
    volumes:
      - vol_dbclient:/data
    deploy:
      mode: replicated
      replicas: 1

  dbcluster:
    image: toughiq/mariadb-cluster
    networks:
      - dbnet
    environment:
      - DB_SERVICE_NAME=dbcluster
      - MYSQL_ROOT_PASSWORD=password
      - MYSQL_DATABASE=mydb
      - MYSQL_USER=mydbuser
      - MYSQL_PASSWORD=mydbpass
    deploy:
      mode: replicated
      replicas: 1

  dblb:
    image: toughiq/maxscale
    networks:
      - dbnet
    ports:
      - 3306:3306
    environment:
      - DB_SERVICE_NAME=dbcluster
      - ENABLE_ROOT_USER=1
    deploy:
      mode: replicated
      replicas: 1

volumes:
  vol_dbclient:
    driver: local

networks:
  dbnet:
    name: dbnet
    driver: overlay
</code></pre>

<p>The dbclient is configured to be in the same network as the cluster so it can reach the mysql service. The default behavior is that it will make a backup every hour (3600 seconds) to the <code>/data/{date}/</code> path.</p>

<p>Deploy the stack:</p>

<pre><code>$ docker stack deploy -c docker-compose.yml galera
Creating network dbnet
Creating service galera_dbcluster
Creating service galera_dblb
Creating service galera_dbclient
</code></pre>

<p>Have a look to see if all the services is running:</p>

<pre><code>$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE                            PORTS
jm7p70qre72u        galera_dbclient     replicated          1/1                 alpine:latest
p8kcr5y7szte        galera_dbcluster    replicated          1/1                 toughiq/mariadb-cluster:latest
1hu3oxhujgfm        galera_dblb         replicated          1/1                 toughiq/maxscale:latest          :3306-&gt;3306/tcp
</code></pre>

<h2>The Backup Client</h2>

<p>As mentioned the backup client backs up to the <code>/data/</code> path:</p>

<pre><code>$ docker exec -it $(docker ps -f name=galera_dbclient -q) find /data/
/data/
/data/2019-05-10
/data/2019-05-10/db_backup-2019-05-10T10.05.sql.gz
</code></pre>

<p>Let&rsquo;s go ahead and populate some data into our mysql database:</p>

<pre><code>$ docker exec -it $(docker ps -f name=galera_dbclient -q) mysql -uroot -ppassword -h dblb
MySQL [(none)]&gt; create table mydb.foo (name varchar(10));
MySQL [(none)]&gt; insert into mydb.foo values('ruan');
MySQL [(none)]&gt; exit
</code></pre>

<h2>Scale the Cluster</h2>

<p>At the moment we only have 1 replica for our mysql cluster, let&rsquo;s go ahead and scale the cluster to 3 replicas:</p>

<pre><code>$ docker service scale galera_dbcluster=3
galera_dbcluster scaled to 3
overall progress: 3 out of 3 tasks
1/3: running   [==================================================&gt;]
2/3: running   [==================================================&gt;]
3/3: running   [==================================================&gt;]
verify: Service converged
</code></pre>

<p>Verify that the service has been scaled:</p>

<pre><code>$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE                            PORTS
jm7p70qre72u        galera_dbclient     replicated          1/1                 alpine:latest
p8kcr5y7szte        galera_dbcluster    replicated          3/3                 toughiq/mariadb-cluster:latest
1hu3oxhujgfm        galera_dblb         replicated          1/1                 toughiq/maxscale:latest          :3306-&gt;3306/tcp
</code></pre>

<p>Test, by reading from the database:</p>

<pre><code>$ docker exec -it $(docker ps -f name=galera_dbclient -q) mysql -uroot -ppassword -h dblb -e'select * from mydb.foo;'
+------+
| name |
+------+
| ruan |
+------+
</code></pre>

<h2>Simulate a Node Failure:</h2>

<p>Simulate a node failure by killing one of the mysql containers:</p>

<pre><code>$ docker kill 9e336032ab52
</code></pre>

<p>Verify that one container is missing from our service:</p>

<pre><code>$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE                            PORTS
p8kcr5y7szte        galera_dbcluster    replicated          2/3                 toughiq/mariadb-cluster:latest
</code></pre>

<p>While the container is provisioning, as we have 2 out of 3 running containers, read the data 3 times so test that the round robin queries dont hit the affected container (the dblb wont route traffic to the affected container):</p>

<pre><code>$ docker exec -it $(docker ps -f name=galera_dbclient -q) mysql -uroot -ppassword -h dblb -e'select * from mydb.foo;'
+------+
| name |
+------+
| ruan |
+------+

$ docker exec -it $(docker ps -f name=galera_dbclient -q) mysql -uroot -ppassword -h dblb -e'select * from mydb.foo;'
+------+
| name |
+------+
| ruan |
+------+

$ docker exec -it $(docker ps -f name=galera_dbclient -q) mysql -uroot -ppassword -h dblb -e'select * from mydb.foo;'
+------+
| name |
+------+
| ruan |
+------+
</code></pre>

<p>Verify that the 3rd container has checked in:</p>

<pre><code>$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE                            PORTS
p8kcr5y7szte        galera_dbcluster    replicated          3/3                 toughiq/mariadb-cluster:latest
</code></pre>

<h2>How to Restore?</h2>

<p>I&rsquo;m deleting the database to simulate the scenario where we need to restore:</p>

<pre><code>$ docker exec -it $(docker ps -f name=galera_dbclient -q) sh
&gt; mysql -uroot -ppassword -h dblb -e'drop database mydb;'
</code></pre>

<p>Ensure the db is not present:</p>

<pre><code>&gt; mysql -uroot -ppassword -h dblb -e'select * from mydb.foo;'
ERROR 1146 (42S02) at line 1: Table 'mydb.foo' doesn't exist
</code></pre>

<p>Find the archive and extract:</p>

<pre><code>&gt; find /data/
/data/
/data/2019-05-10
/data/2019-05-10/db_backup-2019-05-10T10.05.sql.gz

&gt; gunzip /data/2019-05-10/db_backup-2019-05-10T10.05.sql.gz
</code></pre>

<p>Restore the backed up database to MySQL:</p>

<pre><code>&gt; mysql -uroot -ppassword -h dblb &lt; /data/2019-05-10/db_backup-2019-05-10T10.05.sql
</code></pre>

<p>Test that we can read our data:</p>

<pre><code>&gt; mysql -uroot -ppassword -h dblb -e'select * from mydb.foo;'
+------+
| name |
+------+
| ruan |
+------+
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup Hashicorp Vault Server on Docker and a Getting Started CLI Guide]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/05/06/setup-hashicorp-vault-server-on-docker-and-cli-guide/"/>
    <updated>2019-05-06T22:49:09+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/05/06/setup-hashicorp-vault-server-on-docker-and-cli-guide</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/57256060-f1a27e00-7055-11e9-9a05-77d3fdd6c76f.png" alt="" /></p>

<p>Vault is one of Hashicorp&rsquo;s awesome services, which enables you to centrally store, access and distribute dynamic secrets such as tokens, passwords, certificates and encryption keys.</p>

<h2>What will we be doing today</h2>

<p>We will setup a Vault Server on Docker and demonstrate a getting started guide with the Vault CLI to Initialize the Vault, Create / Use and Manage Secrets.</p>

<p>For related posts:</p>

<ul>
<li><a href="https://blog.ruanbekker.com/blog/2019/05/07/persist-vault-data-with-amazon-s3-as-a-storage-backend/">Use the S3 Storage Backend to Persist Data</a></li>
<li><a href="https://blog.ruanbekker.com/blog/2019/05/07/create-secrets-with-vaults-transits-secret-engine/">Create Secrets with Vaults Transit Secret Engine</a></li>
</ul>


<h2>Setting up the Vault Server</h2>

<p>Create the directory structure:</p>

<pre><code>$ touch docker-compose.yml
$ mkdir -p volumes/{config,file,logs}
</code></pre>

<p>Populate the vault config <code>vault.json</code>. (As you can see the config is local, in the next couple of posts, I will show how to persist this config to Amazon S3)</p>

<pre><code>$ cat &gt; volumes/config/vault.json &lt;&lt; EOF
{
  "backend": {
    "file": {
      "path": "/vault/file"
    }
  },
  "listener": {
    "tcp":{
      "address": "0.0.0.0:8200",
      "tls_disable": 1
    }
  },
  "ui": true
}
EOF
</code></pre>

<p>Populate the <code>docker-compose.yml</code>:</p>

<pre><code>$ cat &gt; docker-compose.yml &lt;&lt; EOF
version: '2'
services:
  vault:
    image: vault
    container_name: vault
    ports:
      - "8200:8200"
    restart: always
    volumes:
      - ./volumes/logs:/vault/logs
      - ./volumes/file:/vault/file
      - ./volumes/config:/vault/config
    cap_add:
      - IPC_LOCK
    entrypoint: vault server -config=/vault/config/vault.json
EOF
</code></pre>

<p>Start the Vault Server:</p>

<pre><code>$ docker-compose up
</code></pre>

<p>The UI is available at <a href="http://localhost:8200/ui">http://localhost:8200/ui</a> and the api at <a href="http://localhost:8200">http://localhost:8200</a></p>

<h2>Interacting with the Vault CLI</h2>

<p>I will demonstrate how to use the Vault CLI to interact with Vault. Let&rsquo;s start by installing the vault cli tools, I am using mac, so I will be using brew:</p>

<pre><code>$ brew install vault
</code></pre>

<p>Set environment variables:</p>

<pre><code>$ export VAULT_ADDR='http://127.0.0.1:8200'
</code></pre>

<h2>Initialize the Vault Cluster:</h2>

<p>Initialize new vault cluster with 6 key shares:</p>

<pre><code>$ vault operator init -key-shares=6 -key-threshold=3
Unseal Key 1: RntjR...DQv
Unseal Key 2: 7E1bG...0LL+
Unseal Key 3: AEuhl...A1NO
Unseal Key 4: bZU76...FMGl
Unseal Key 5: DmEjY...n7Hk
Unseal Key 6: pC4pK...XbKb

Initial Root Token: s.F0JGq..98s2U

Vault initialized with 10 key shares and a key threshold of 3. Please
securely distribute the key shares printed above. When the Vault is re-sealed,
restarted, or stopped, you must supply at least 3 of these keys to unseal it
before it can start servicing requests.

Vault does not store the generated master key. Without at least 3 key to
reconstruct the master key, Vault will remain permanently sealed!

It is possible to generate new unseal keys, provided you have a quorum of
existing unseal keys shares. See "vault operator rekey" for more information.
</code></pre>

<p>In order to unseal the vault cluster, we need to supply it with 3 key shares:</p>

<pre><code>$ vault operator unseal RntjR...DQv
$ vault operator unseal bZU76...FMGl
$ vault operator unseal pC4pK...XbKb
</code></pre>

<p>Ensure the vault is unsealed:</p>

<pre><code>$ vault status -format=json
{
  "type": "shamir",
  "initialized": true,
  "sealed": false,
  "t": 3,
  "n": 5,
  "progress": 0,
  "nonce": "",
  "version": "1.1.0",
  "migration": false,
  "cluster_name": "vault-cluster-dca2b572",
  "cluster_id": "469c2f1d-xx-xx-xx-03bfc497c883",
  "recovery_seal": false
}
</code></pre>

<p>Authenticate against the vault:</p>

<pre><code>$ vault login s.tdlEqsfzGbePVlke5hTpr9Um
Success! You are now authenticated. The token information displayed below
is already stored in the token helper. You do NOT need to run "vault login"
again. Future Vault requests will automatically use this token.
</code></pre>

<p>Using the cli your auth token will be saved locally at <code>~/.vault-token</code>.</p>

<p>Enable the secret kv engine:</p>

<pre><code>$ vault secrets enable -version=1 -path=secret kv
</code></pre>

<h2>Create and Read Secrets</h2>

<p>Write a secret to the path enabled above:</p>

<pre><code>$ vault kv put secret/my-app/password password=123
</code></pre>

<p>List your secrets:</p>

<pre><code>$ vault kv list secret/
Keys
----
my-app/
</code></pre>

<p>Read the secret (defaults in table format):</p>

<pre><code>$ vault kv get secret/my-app/password
Key                 Value
---                 -----
refresh_interval    768h
password            123
</code></pre>

<p>Read the secret in json format:</p>

<pre><code>$ vault kv get --format=json secret/my-app/password
{
  "request_id": "0249c878-7432-9555-835a-89b275fca32o",
  "lease_id": "",
  "lease_duration": 2764800,
  "renewable": false,
  "data": {
    "password": "123"
  },
  "warnings": null
}
</code></pre>

<p>Read only the password value in the secret:</p>

<pre><code>$ vault kv get -field=password secret/my-app/password
123
</code></pre>

<h2>Key with Multiple Secrets</h2>

<p>Create a key with multiple secrets:</p>

<pre><code>$ vault kv put secret/reminders/app db_username=db.ruanbekker.com username=root password=secret
</code></pre>

<p>Read all the secrets:</p>

<pre><code>$ vault kv get --format=json secret/reminders/app
{
  "request_id": "0144c878-7532-9555-835a-8cb275fca3dd",
  "lease_id": "",
  "lease_duration": 2764800,
  "renewable": false,
  "data": {
    "db_username": "db.ruanbekker.com",
    "password": "secret",
    "username": "root"
  },
  "warnings": null
}
</code></pre>

<p>Read only the username field in the key:</p>

<pre><code>$ vault kv get -field=username secret/reminders/app
root
</code></pre>

<p>Delete the secret:</p>

<pre><code>$ vault kv delete secret/reminders
</code></pre>

<h2>Versioning</h2>

<p>Create a key and set the metadata to max of 5 versions:</p>

<pre><code>$ vault kv metadata put -max-versions=5 secret/fooapp/appname
</code></pre>

<p>Get the metadata of the key:</p>

<pre><code>$ vault kv metadata get secret/fooapp/appname
======= Metadata =======
Key                Value
---                -----
cas_required       false
created_time       2019-04-07T12:35:54.355411Z
current_version    0
max_versions       5
oldest_version     0
updated_time       2019-04-07T12:35:54.355411Z
</code></pre>

<p>Write a secret <code>appname</code> to our key: <code>secret/fooapp/appname</code>:</p>

<pre><code>$ vault kv put secret/fooapp/appname appname=app1
Key              Value
---              -----
created_time     2019-04-07T12:36:41.7577102Z
deletion_time    n/a
destroyed        false
version          1
</code></pre>

<p>Overwrite the key with a couple of requests:</p>

<pre><code>$ vault kv put secret/fooapp/appname appname=app2
$ vault kv put secret/fooapp/appname appname=app3
</code></pre>

<p>Read the current value:</p>

<pre><code>$ vault kv get -field=appname secret/fooapp/appname
app3
</code></pre>

<p>Get the version=2 value of this file:</p>

<pre><code>$ vault kv get -field=appname -version=2 secret/fooapp/appname
app2
</code></pre>

<h2>Thanks</h2>

<p>Thanks for reading, hope this was informative. Have a look at <a href="https://www.vaultproject.io">Hashicorp&rsquo;s Vault Documentation</a> for more information on the project. I will post more posts on Vault under the <a href="https://blog.ruanbekker.com/blog/categories/vault">#vault</a> category.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Concourse CI to Deploy to Docker Swarm]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/05/04/using-concourse-ci-to-deploy-to-docker-swarm/"/>
    <updated>2019-05-04T23:11:17+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/05/04/using-concourse-ci-to-deploy-to-docker-swarm</id>
    <content type="html"><![CDATA[<p><img src="https://i.snag.gy/gzkdu9.jpg?nocache=1511644783495" alt="" /></p>

<p>In this tutorial we will use Concourse to Deploy our application to Docker Swarm.</p>

<h2>The Flow</h2>

<ul>
<li>Our application code resides on Github</li>
<li>The pipeline triggers when a commit is pushed to the master branch</li>
<li>The pipeline will automatically deploy to the staging environment</li>
<li>The pipeline requires a manual trigger to deploy to prod</li>
<li>Note: Staging and Prod on the same swarm for demonstration</li>
</ul>


<p>The code for this tutorial is available on my <strong><a href="https://github.com/ruanbekker/concourse-swarm-app-demo">github repository</a></strong></p>

<h2>Application Structure</h2>

<p>The application structure for our code looks like this:</p>

<p><img src="https://user-images.githubusercontent.com/567298/57184912-1d412f00-6ec3-11e9-85e9-6517d83e96e8.png" alt="" /></p>

<h2>Pipeline Walktrough</h2>

<p>Our <code>ci/pipeline.yml</code></p>

<pre><code class="yaml">resources:
  - name: main-repo
    type: git
    source:
      uri: git@github.com:ruanbekker/concourse-swarm-app-demo.git
      branch: master
      private_key: ((github_private_key))

  - name: main-repo-staging
    type: git
    source:
      uri: git@github.com:ruanbekker/concourse-swarm-app-demo.git
      branch: master
      private_key: ((github_private_key))
      paths:
        - config/staging/*

  - name: main-repo-prod
    type: git
    source:
      uri: git@github.com:ruanbekker/concourse-swarm-app-demo.git
      branch: master
      private_key: ((github_private_key))
      paths:
        - config/prod/*

  - name: slack-alert
    type: slack-notification
    source:
      url: ((slack_notification_url))

  - name: version-staging
    type: semver
    source:
      driver: git
      uri: git@github.com:ruanbekker/concourse-swarm-app-demo.git
      private_key: ((github_private_key))
      file: version-staging
      branch: version-staging

  - name: version-prod
    type: semver
    source:
      driver: git
      uri: git@github.com:ruanbekker/concourse-swarm-app-demo.git
      private_key: ((github_private_key))
      file: version-prod
      branch: version-prod

resource_types:
  - name: slack-notification
    type: docker-image
    source:
      repository: cfcommunity/slack-notification-resource
      tag: v1.3.0

jobs:
  - name: bump-staging-version
    plan:
    - get: main-repo-staging
      trigger: true
    - get: version-staging
    - put: version-staging
      params:
        bump: major

  - name: bump-prod-version
    plan:
    - get: main-repo-prod
      trigger: true
    - get: version-prod
    - put: version-prod
      params:
        bump: major

  - name: deploy-staging
    plan:
    - get: main-repo-staging
    - get: main-repo
    - get: version-staging
      passed:
      - bump-staging-version
      trigger: true
    - task: deploy-staging
      params:
        DOCKER_SWARM_HOSTNAME: ((docker_swarm_staging_host))
        DOCKER_SWARM_KEY: ((docker_swarm_key))
        DOCKER_HUB_USER: ((docker_hub_user))
        DOCKER_HUB_PASSWORD: ((docker_hub_password))
        SERVICE_NAME: app-staging
        SWARM: staging
        ENVIRONMENT: staging
        AWS_ACCESS_KEY_ID: ((aws_access_key_id))
        AWS_SECRET_ACCESS_KEY: ((aws_secret_access_key))
        AWS_DEFAULT_REGION: ((aws_region))
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: rbekker87/build-tools
            tag: latest
            username: ((docker_hub_user))
            password: ((docker_hub_password))
        inputs:
        - name: main-repo-staging
        - name: main-repo
        - name: version-staging
        run:
          path: /bin/sh
          args:
            - -c
            - |
              ./main-repo/ci/scripts/deploy.sh
      on_failure:
        put: slack-alert
        params:
          channel: '#system_events'
          username: 'concourse'
          icon_emoji: ':concourse:'
          silent: true
          text: |
            *$BUILD_PIPELINE_NAME/$BUILD_JOB_NAME* ($BUILD_NAME) FAILED :rage: - TestApp Deploy to staging-swarm failed
            http://ci.example.local/teams/$BUILD_TEAM_NAME/pipelines/$BUILD_PIPELINE_NAME/jobs/$BUILD_JOB_NAME/builds/$BUILD_NAME
      on_success:
        put: slack-alert
        params:
          channel: '#system_events'
          username: 'concourse'
          icon_emoji: ':concourse:'
          silent: true
          text: |
            *$BUILD_PIPELINE_NAME/$BUILD_JOB_NAME* ($BUILD_NAME) SUCCESS :aww_yeah: - TestApp Deploy to staging-swarm succeeded
            http://ci.example.local/teams/$BUILD_TEAM_NAME/pipelines/$BUILD_PIPELINE_NAME/jobs/$BUILD_JOB_NAME/builds/$BUILD_NAME

  - name: deploy-prod
    plan:
    - get: main-repo-prod
    - get: main-repo
    - get: version-prod
      passed:
      - bump-prod-version
    - task: deploy-prod
      params:
        DOCKER_SWARM_HOSTNAME: ((docker_swarm_prod_host))
        DOCKER_SWARM_KEY: ((docker_swarm_key))
        DOCKER_HUB_USER: ((docker_hub_user))
        DOCKER_HUB_PASSWORD: ((docker_hub_password))
        SERVICE_NAME: app-prod
        SWARM: prod
        ENVIRONMENT: production
        AWS_ACCESS_KEY_ID: ((aws_access_key_id))
        AWS_SECRET_ACCESS_KEY: ((aws_secret_access_key))
        AWS_DEFAULT_REGION: ((aws_region))
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: rbekker87/build-tools
            tag: latest
            username: ((docker_hub_user))
            password: ((docker_hub_password))
        inputs:
        - name: main-repo-prod
        - name: main-repo
        - name: version-prod
        run:
          path: /bin/sh
          args:
            - -c
            - |
              ./main-repo/ci/scripts/deploy.sh
      on_failure:
        put: slack-alert
        params:
          channel: '#system_events'
          username: 'concourse'
          icon_emoji: ':concourse:'
          silent: true
          text: |
            *$BUILD_PIPELINE_NAME/$BUILD_JOB_NAME* ($BUILD_NAME) FAILED :rage: - TestApp Deploy to prod-swarm failed
            http://ci.example.local/teams/$BUILD_TEAM_NAME/pipelines/$BUILD_PIPELINE_NAME/jobs/$BUILD_JOB_NAME/builds/$BUILD_NAME
      on_success:
        put: slack-alert
        params:
          channel: '#system_events'
          username: 'concourse'
          icon_emoji: ':concourse:'
          silent: true
          text: |
            *$BUILD_PIPELINE_NAME/$BUILD_JOB_NAME* ($BUILD_NAME) SUCCESS :aww_yeah: - TestApp Deploy to prod-swarm succeeded
            http://ci.example.local/teams/$BUILD_TEAM_NAME/pipelines/$BUILD_PIPELINE_NAME/jobs/$BUILD_JOB_NAME/builds/$BUILD_NAME
</code></pre>

<p>Our <code>ci/credentials.yml</code> which will hold all our secret info, which will remain local:</p>

<pre><code>username: yourdockerusername
password: yourdockerpassword
docker_swarm_prod_host: 10.20.30.40
...
</code></pre>

<p>The first step of our deploy will invoke a shell script that will establish a ssh tunnel to the docker host, mounting the docker socket to a tcp local port, then exporting the docker host port to the tunneled port, <code>ci/scripts/deploy.sh</code>:</p>

<pre><code>#!/usr/bin/env sh

export DOCKER_HOST="localhost:2376"

echo "${DOCKER_SWARM_KEY}" | sed -e 's/\(KEY-----\)\s/\1\n/g; s/\s\(-----END\)/\n\1/g' | sed -e '2s/\s\+/\n/g' &gt; key.pem
chmod 600 key.pem

screen -S \
  sshtunnel -m -d sh -c \
  "ssh -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -i ./key.pem -NL localhost:2376:/var/run/docker.sock root@$DOCKER_SWARM_HOSTNAME"

sleep 5
docker login -u "${DOCKER_HUB_USER}" -p "${DOCKER_HUB_PASSWORD}"
docker stack deploy --prune -c ./main-repo/ci/docker/docker-compose.${ENVIRONMENT}.yml $SERVICE_NAME --with-registry-auth

if [ $? != "0" ]
  then
    echo "deploy failure for: $SERVICE_NAME"
    screen -S sshtunnel -X quit
    exit 1
  else
    set -x
    echo "deploy success for: $SERVICE_NAME"
    screen -S sshtunnel -X quit
fi
</code></pre>

<p>The deploy script references the docker-compose files, first our <code>ci/docker/docker-compose.staging.yml</code>:</p>

<pre><code>version: "3.4"

services:
  web:
    image: ruanbekker/web-center-name
    environment:
      - APP_ENVIRONMENT=Staging
    ports:
      - 81:5000
    networks:
      - web_net
    deploy:
      mode: replicated
      replicas: 2

networks:
  web_net: {}
</code></pre>

<p>Also, our docker-compose for production, <code>ci/docker/docker-compose.production.yml</code>:</p>

<pre><code>version: "3.4"

services:
  web:
    image: ruanbekker/web-center-name
    environment:
      - APP_ENVIRONMENT=Production
    ports:
      - 80:5000
    networks:
      - web_net
    deploy:
      mode: replicated
      replicas: 10

networks:
  web_net: {}
</code></pre>

<h2>Set the Pipeline in Concourse</h2>

<p>Create 2 branches in your github repository for versioning: <code>version-staging</code> and <code>version-prod</code>, then logon to concourse and save the target:</p>

<pre><code>$ fly -t ci login -n main -c http://&lt;concourse-ip&gt;
</code></pre>

<p>Set the pipeline, point the config, local variables definition and name the pipeline:</p>

<pre><code>$ fly -t ci sp -n main -c ci/pipeline.yml -p &lt;pipeline-name&gt; -l ci/&lt;variables&gt;.yml
</code></pre>

<p>You will find that the pipeline will look like below and that it will be in a paused state:</p>

<p><img src="https://user-images.githubusercontent.com/567298/54060759-96dfd800-4206-11e9-9236-e3b86783417c.png" alt="" /></p>

<p>Unpause the pipeline:</p>

<pre><code>$ fly -t ci up -p swarm-demo
</code></pre>

<p>The pipeline should kick-off automatically due to the trigger that is set to true:</p>

<p><img src="https://user-images.githubusercontent.com/567298/54060811-cbec2a80-4206-11e9-8de7-a0b308f20cef.png" alt="" /></p>

<p>Deployed automatically to staging, prod is a manual trigger:</p>

<p><img src="https://user-images.githubusercontent.com/567298/54060991-8e3bd180-4207-11e9-9726-2c01ca10d24a.png" alt="" /></p>

<h2>Testing our Application</h2>

<p>For demonstration purposes we have deployed staging on port 81 and production on port 80.</p>

<p>Testing Staging on <a href="http://">http://</a><swarm-ip>:81/</p>

<p><img src="https://user-images.githubusercontent.com/567298/57185377-73fe3700-6eca-11e9-91d3-953e754cbde9.png" alt="" /></p>

<p>Testing Production on <a href="http://">http://</a><swarm-ip>:80/</p>

<p><img src="https://user-images.githubusercontent.com/567298/57185383-8d06e800-6eca-11e9-8cff-c3a665f9f377.png" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Drone CI to Build a Jekyll Site and Deploy to Docker Swarm]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/04/23/using-drone-ci-to-build-a-jekyll-site-and-deploy-to-docker-swarm/"/>
    <updated>2019-04-23T23:57:02+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/04/23/using-drone-ci-to-build-a-jekyll-site-and-deploy-to-docker-swarm</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/56618556-3de7ca00-6623-11e9-995f-c22792f0ab21.png" alt="image" /></p>

<p>CICD Pipelines! &lt;3</p>

<p>In this post I will show you how to setup a cicd pipeline using drone to build a jekyll site and deploy to docker swarm.</p>

<h2>Environment Overview</h2>

<p><strong>Jekyll&rsquo;s Codebase</strong>: Our code will be hosted on Github (I will demonstrate how to set it up from scratch)</p>

<p><strong>Secret Store</strong>: Our secrets such as ssh key, swarm host address etc will be stored in drones secrets manager</p>

<p><strong>Docker Swarm</strong>: Docker Swarm has Traefik as a HTTP Loadbalancer</p>

<p><strong>Drone Server and Agent</strong>: If you dont have drone, you can setup <a href="https://blog.ruanbekker.com/blog/2019/04/18/setup-a-drone-cicd-environment-on-docker-with-letsencrypt/">drone server and agent on docker</a> or have a look at <a href="https://cloud.drone.io">cloud.drone.io</a></p>

<p><strong>Workflow:</strong></p>

<pre><code>* Whenever a push to master is receive on github, the pipeline will be triggered
* The content from our github repository will be cloned to the agent on a container
* Jekyll will build and the output will be transferred to docker swarm using rsync
* The docker-compose.yml will be transferred to the docker swarm host using scp
* A docker stack deploy is ran via ssh
</code></pre>

<h2>Install Jekyll Locally</h2>

<p>Install Jekyll locally, as we will use it to create the initial site. I am using a mac, so I will be using <code>brew</code>. For other operating systems, have a look at <a href="https://jekyllrb.com/docs/installation/">this post</a>.</p>

<p>I will be demonstrating with a weightloss blog as an example.</p>

<p>Install jekyll:</p>

<pre><code>$ brew install jekyll
</code></pre>

<p>Go ahead and create a new site which will host the data for your jekyll site:</p>

<pre><code>$ jekyll new blog-weightloss
</code></pre>

<h2>Create a Github Repository</h2>

<p>First we need to create an empty github repository, in my example it was <code>github.com/ruanbekker/blog-weightloss.git</code>. Once you create the repo change into the directory created by the <code>jekyll new</code> command:</p>

<pre><code>$ cd blog-weightloss
</code></pre>

<p>Now initialize git, set the remote, add the jekyll data and push to github:</p>

<pre><code>$ git init
$ git remote add origin git@github.com:ruanbekker/blog-weightloss.git # &lt;== change to your repository
$ git add .
$ git commit -m "first commit"
$ git push origin master
</code></pre>

<p>You should see your data on your github repository.</p>

<h2>Create Secrets on Drone</h2>

<p>Logon to the Drone UI, sync repositories, activate the new repository and head over to settings where you will find the secrets section.</p>

<p>Add the following secrets:</p>

<pre><code>Secret Name: swarm_host
Secret Value: ip address of your swarm

Secret Name: swarm_key
Secret Value: contents of your private ssh key

Secret Name: swarm_user
Secret Value: the user that is allowed to ssh
</code></pre>

<p>You should see the following:</p>

<p><img src="https://user-images.githubusercontent.com/567298/56619871-5c4fc480-6627-11e9-8820-c9d4ddff698c.png" alt="image" /></p>

<h2>Add the Drone Config</h2>

<p>Drone looks from a <code>.drone.yml</code> file in the root directory for instructions on how to do its tasks. Lets go ahead and declare our pipeline:</p>

<pre><code>$ vim .drone.yml
</code></pre>

<p>And populate the drone config:</p>

<pre><code>pipeline:
  jekyll-build:
    image: jekyll/jekyll:latest
    commands:
      - touch Gemfile.lock
      - chmod a+w Gemfile.lock
      - chown -R jekyll:jekyll /drone
      - gem update --system
      - gem install bundler
      - bundle install
      - bundle exec jekyll build

  transfer-build:
    image: drillster/drone-rsync
    hosts:
      from_secret: swarm_host
    key:
      from_secret: swarm_key
    user:
      from_secret: swarm_user
    source: ./*
    target: ~/my-weightloss-blog.com
    recursive: true
    delete: true
    when:
      branch: [master]
      event: [push]

  transfer-compose:
    image: appleboy/drone-scp
    host:
      from_secret: swarm_host
    username:
      from_secret: swarm_user
    key:
      from_secret: swarm_key
    target: /root/my-weightloss-blog.com
    source:
      - docker-compose.yml
    when:
      branch: [master]
      event: [push]

  deploy-jekyll-to-swarm:
    image: appleboy/drone-ssh
    host:
      from_secret: swarm_host
    username:
      from_secret: swarm_user
    key:
      from_secret: swarm_key
    port: 22
    script:
      - docker stack deploy --prune -c /root/my-weightloss-blog.com/docker-compose.yml apps
    when:
      branch: [master]
      event: [push]
</code></pre>

<h2>Notifications?</h2>

<p>If you want to be notified about your builds, you can add a slack notification step as the last step.</p>

<p>To do that, create a new webhook integration, you can <a href="https://blog.ruanbekker.com/blog/2019/04/18/setup-a-slack-webhook-for-sending-messages-from-applications/">follow this post for a step by step guide</a>. After you have the webhook, go to secrets and create a <code>slack_webhook</code> secret.</p>

<p>Then apply the notification step as shown below:</p>

<pre><code>  notify-via-slack:
    image: plugins/slack
    webhook:
      from_secret: slack_webhook
    channel: system_events
    template: &gt;
      
        [DRONE CI]: ** : /
        ( -  | )

      
        [DRONE CI]: ** : /
        ( -  | )
      
</code></pre>

<p>Based on the status, you should get a notification similar like this:</p>

<p><img src="https://user-images.githubusercontent.com/567298/56622206-6e356580-662f-11e9-8d93-286c9c126d24.png" alt="image" /></p>

<h2>Add the Docker Compose</h2>

<p>Next we need to declare our docker compose file which is needed to deploy our jekyll service to the swarm:</p>

<pre><code>$ vim docker-compose.yml
</code></pre>

<p>And populate this info (just change the values for your own environment/settings):</p>

<pre><code class="yaml">version: '3.5'

services:
  myweightlossblog:
    image: ruanbekker/jekyll:contrast
    command: jekyll serve --watch --force_polling --verbose
    networks:
      - appnet
    volumes:
      - /root/my-weightloss-blog.com:/srv/jekyll
    deploy:
      mode: replicated
      replicas: 1
      labels:
        - "traefik.backend.loadbalancer.sticky=false"
        - "traefik.backend.loadbalancer.swarm=true"
        - "traefik.backend=myweightlossblog"
        - "traefik.docker.network=appnet"
        - "traefik.entrypoints=https"
        - "traefik.frontend.passHostHeader=true"
        - "traefik.frontend.rule=Host:www.my-weightloss-blog.com,my-weightloss-blog.com"
        - "traefik.port=4000"
      update_config:
        parallelism: 2
        delay: 10s
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.role == manager
networks:
  appnet:
    external: true
</code></pre>

<h2>Push to Github</h2>

<p>Now we need to push our <code>.drone.yml</code> and <code>docker-compose.yml</code> to github. Since the repository is activated on drone, any push to master will trigger the pipeline, so after this push we should go to drone to look at our pipeline running.</p>

<p>Add the untracked files and push to github:</p>

<pre><code>$ git add .drone.yml
$ git add docker-compose.yml
$ git commit -m "add drone and docker config"
$ git push origin master
</code></pre>

<p>As you head over to your drone ui, you should see your pipeline output which will look more or less like this (just look how pretty it is! :D )</p>

<p><img src="https://user-images.githubusercontent.com/567298/56620236-91a8e200-6628-11e9-9278-38e3305fdcd7.png" alt="image" /></p>

<h2>Test Jekyll</h2>

<p>If your deployment has completed you should be able to access your application on the configured domain. A screenshot of my response when accessing Jekyll:</p>

<p><img src="https://user-images.githubusercontent.com/567298/56620280-af764700-6628-11e9-9d4f-c2592e6cf561.png" alt="image" /></p>

<p>Absolutely Amazingness! I really love drone!</p>
]]></content>
  </entry>
  
</feed>
