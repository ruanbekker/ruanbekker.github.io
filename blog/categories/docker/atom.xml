<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Docker | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/docker/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2018-04-30T09:29:16-04:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Forwarding the Docker Socket via a SSH Tunnel to Execute Docker Commands Locally]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/04/30/forwarding-the-docker-socket-via-a-ssh-tunnel-to-execute-docker-commands-locally/"/>
    <updated>2018-04-30T08:30:23-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/04/30/forwarding-the-docker-socket-via-a-ssh-tunnel-to-execute-docker-commands-locally</id>
    <content type="html"><![CDATA[<p>With automation in mind, when you want to execute docker commands remotely, you want to do it in a secure manner, as you don&rsquo;t want to expose your Docker port to the whole world.</p>

<p>One way in doing that, is forwarding the remote docker socket via a local port over a SSH Tunnel. With this way, you can execute docker commands locally on your workstation, as if the swarm is running on your workstation/laptop/node/bastion host etc.</p>

<p>Without the tunnel, I have a swarm on my laptop with no running services:</p>

<pre><code class="bash">$ docker service ls
ID                  NAME                   MODE                REPLICAS            IMAGE                                                               PORTS
</code></pre>

<p>As you can see, we have no services running, but the remote swarm has a couple, so after forwarding the connection, we should see our remote services.</p>

<h2>Setting up the SSH Tunnel:</h2>

<p>Here we will forward the remote docker socket: <code>/var/run/docker.sock</code> to a local port bound to localhost: <code>localhost:2377</code>:</p>

<pre><code class="bash">$ screen -S docker
$ ssh -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -i ~/path/to/key.pem -NL localhost:2377:/var/run/docker.sock root@docker-managers.mydomain.com
</code></pre>

<p>Now the SSH Tunnel will be established, and you can detach your screen session, or open a new shell session. To detach your screen session: <code>'ctrl + a' then d</code></p>

<h2>Verifying that the tunnel is established:</h2>

<p>You can use netstat to verify that the port is listening:</p>

<pre><code class="bash">$ netstat -ant | grep 2377
tcp4       0      0  127.0.0.1.2377         *.*                    LISTEN
</code></pre>

<h2>Inform the Docker Client to use the Port:</h2>

<p>Now we need to inform the docker client, to use the new port to talk to the docker daemon. We do that by setting the <code>DOCKER_HOST</code> environment variable to point to <code>localhost:2377</code>:</p>

<pre><code class="bash">$ export DOCKER_HOST="localhost:2377"
</code></pre>

<p>This will remain for the lifetime of the shell session.</p>

<h2>Testing it Out:</h2>

<p>Now we can run our commands locally, and we should see the output of our remote swarm:</p>

<pre><code class="bash">$ docker service ls
ID                  NAME                   MODE                REPLICAS            IMAGE                                                               PORTS
xjta8e3ek2u2        apps_flask_reminders   replicated          3/3                 rbekker87/flask-reminders:debian
0l7ruktbqj99        apps_kibana            replicated          1/1                 kibana:latest
...
</code></pre>

<h2>Terminating our SSH Tunnel:</h2>

<p>To terminate our SSH Tunnel, reconnect to your shell session, and hit <code>ctrl + c</code>:</p>

<pre><code class="bash">$ screen -ls 
There is a screen on:
    50413.docker    (Detached)
$ screen -r 50413
</code></pre>

<p>Hit <code>ctrl + c</code> :</p>

<pre><code class="bash">CKilled by signal 2.
</code></pre>

<p>And exit the screen session:</p>

<pre><code class="bash">$ exit
</code></pre>

<p>With this way, you can do lots of automation with docker swarm, not limited to swarm, but one of them.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running a 3 Node Elasticsearch Cluster With Docker Compose on Your Laptop for Testing]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/04/29/running-a-3-node-elasticsearch-cluster-with-docker-compose-on-your-laptop-for-testing/"/>
    <updated>2018-04-29T13:43:35-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/04/29/running-a-3-node-elasticsearch-cluster-with-docker-compose-on-your-laptop-for-testing</id>
    <content type="html"><![CDATA[<p>Having a Elasticsearch cluster on your laptop with Docker for testing is great. And in this post I will show you how quick and easy it is, to have a 3 node elasticsearch cluster running on docker for testing.</p>

<h2>Pre-Requisites</h2>

<p>We need to set the <code>vm.max_map_count</code> kernel parameter:</p>

<pre><code class="bash">$ sudo sysctl -w vm.max_map_count=262144
</code></pre>

<p>To set this permanently, add it to <code>/etc/sysctl.conf</code> and reload with <code>sudo sysctl -p</code></p>

<h2>Docker Compose:</h2>

<p>The docker compose file that we will reference:</p>

<pre><code class="yaml">version: '2.2'
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.2.4
    container_name: elasticsearch
    environment:
      - cluster.name=docker-cluster
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - esdata1:/home/ruan/workspace/docker/elasticsearch/data
    ports:
      - 9200:9200
    networks:
      - esnet
  elasticsearch2:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.2.4
    container_name: elasticsearch2
    environment:
      - cluster.name=docker-cluster
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - "discovery.zen.ping.unicast.hosts=elasticsearch"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - esdata2:/home/ruan/workspace/docker/elasticsearch/data
    networks:
      - esnet
  elasticsearch3:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.2.4
    container_name: elasticsearch3
    environment:
      - cluster.name=docker-cluster
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - "discovery.zen.ping.unicast.hosts=elasticsearch"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - esdata3:/home/ruan/workspace/docker/elasticsearch/data
    networks:
      - esnet


volumes:
  esdata1:
    driver: local
  esdata2:
    driver: local
  esdata3:
    driver: local

networks:
  esnet:
</code></pre>

<p>Now make sure the paths exist that we referenced in the compose file, in my case <code>/home/ruan/workspace/docker/elasticsearch/data</code></p>

<h2>Deploy</h2>

<p>Deploy your elasticsearch cluster with docker compose:</p>

<pre><code class="bash">$ docker-compose up
</code></pre>

<p>This will run in the foreground, and you should see console output.</p>

<h2>Testing Elasticsearch</h2>

<p>Let&rsquo;s run a couple of queries, first up, check the cluster health api:</p>

<pre><code class="bash">$ curl http://127.0.0.1:9200/_cluster/health?pretty
{
  "cluster_name" : "docker-cluster",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 3,
  "number_of_data_nodes" : 3,
  "active_primary_shards" : 1,
  "active_shards" : 2,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 100.0
}
</code></pre>

<p>Create a index with replication count of 2:</p>

<pre><code class="bash">$ curl -H "Content-Type: application/json" -XPUT http://127.0.0.1:9200/test -d '{"number_of_replicas": 2}'
</code></pre>

<p>Ingest a document to elasticsearch:</p>

<pre><code class="bash">$ curl -H "Content-Type: application/json" -XPUT http://127.0.0.1:9200/test/docs/1 -d '{"name": "ruan"}'
{"_index":"test","_type":"docs","_id":"1","_version":1,"result":"created","_shards":{"total":3,"successful":3,"failed":0},"_seq_no":0,"_primary_term":1}
</code></pre>

<p>View the indices:</p>

<pre><code class="bash">$ curl http://127.0.0.1:9200/_cat/indices?v
health status index                       uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   test                        w4p2Q3fTR4uMSYBfpNVPqw   5   2          1            0      3.3kb          1.1kb
green  open   .monitoring-es-6-2018.04.29 W69lql-rSbORVfHZrj4vug   1   1       1601           38        4mb            2mb
</code></pre>

<h2>Deleting the Cluster:</h2>

<p>As its running in the foreground, you can just hit ctrl + c and as we persisted data in our compose, when you spin up the cluster again, the data will still be there.</p>

<h2>Resources:</h2>

<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Set Docker Environment Variables During Build Time]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/04/07/set-docker-environment-variables-during-build-time/"/>
    <updated>2018-04-07T09:51:35-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/04/07/set-docker-environment-variables-during-build-time</id>
    <content type="html"><![CDATA[<p>When using that <code>ARG</code> option in your Dockerfile, you can specify the <code>--build-args</code> option to define the value for the key that you specify in your Dockerfile to use for a environment variable as an example.</p>

<p>Today we will use the <code>arg</code> and <code>env</code> to set environment variables at build time.</p>

<h2>The Dockerfile:</h2>

<p>Our Dockerfile</p>

<pre><code class="dockerfile">FROM alpine:edge
ARG NAME
ENV OWNER=${NAME:-NOT_DEFINED}
CMD ["sh", "-c", "echo env var: ${OWNER}"]
</code></pre>

<p>Building our Image, we will pass the value to our NAME argument:</p>

<pre><code class="bash">$ docker build --build-arg NAME=james -t ruan:test .
</code></pre>

<p>Now when we run our container, we will notice that the build time argument has passed through to our environment variable from the running container:</p>

<pre><code class="bash">$ docker run -it ruan:test 
env var: james
</code></pre>

<p>When we build the image without specifying build arguments, and running the container:</p>

<pre><code class="bash">$ docker build -t ruan:test .
$ docker run -it ruan:test 
env var: NOT_DEFINED
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker Environment Substitution With Dockerfile]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/04/07/docker-environment-substitution-with-dockerfile/"/>
    <updated>2018-04-07T09:18:20-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/04/07/docker-environment-substitution-with-dockerfile</id>
    <content type="html"><![CDATA[<p>The <a href="https://12factor.net/">12 Factor</a> way, is a general guideline that provides best practices when building applications. One of them is using environment variables to store application configuration.</p>

<h2>What will we be doing:</h2>

<p>In this post we will build a simple docker application that returns the environment variable&rsquo;s value to standard out. We are using environment substitution, so if the environment variable is not provided, we will set a default value of <code>NOT_DEFINED</code>.</p>

<p>We will have the environment variable <code>OWNER</code> and when no value is set for that Environment Variable, the <code>NOT_DEFINED</code> value will be returned.</p>

<h2>The Dockerfile</h2>

<p>Our Dockerfile:</p>

<pre><code class="dockerfile">FROM alpine:edge
ENV OWNER=${OWNER:-NOT_DEFINED}
CMD ["sh", "-c", "echo env var: ${OWNER}"]
</code></pre>

<p>Building the image:</p>

<pre><code class="bash">$ docker build -t test:envs .
</code></pre>

<h2>Putting it to action:</h2>

<p>Now we will run a container and pass the <code>OWNER</code> environment variable as an option:</p>

<pre><code class="bash">$ docker run -it -e OWNER=ruan test:envs . 
env var: ruan
</code></pre>

<p>When we run a container without specifying the environment variable:</p>

<pre><code class="bash">$ docker run -it ruan:test 
env var: NOT_DEFINED
</code></pre>

<h2>Resources:</h2>

<ul>
<li><a href="https://stackify.com/config-values-docker-containers-environment-variables/">https://stackify.com/config-values-docker-containers-environment-variables/</a></li>
<li><a href="https://tryolabs.com/blog/2015/03/26/configurable-docker-containers-for-multiple-environments/">https://tryolabs.com/blog/2015/03/26/configurable-docker-containers-for-multiple-environments/</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Populate Environment Variables From Docker Secrets With a Flask Demo App]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/03/12/populate-environment-variables-from-docker-secrets-with-a-flask-demo-app/"/>
    <updated>2018-03-12T18:16:42-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/03/12/populate-environment-variables-from-docker-secrets-with-a-flask-demo-app</id>
    <content type="html"><![CDATA[<p><img src="http://obj-cache.cloud.ruanbekker.com/docker-logo.png" alt="" /></p>

<p>In this post we will create a basic Python Flask WebApp on Docker Swarm, but we will read our Flask Host, and Flask Port from Environment Variables, which will be populated from Docker Secrets, which we will read in from a python script.</p>

<h2>Our Directory Setup:</h2>

<p>This can be retrieved from <a href="https://github.com/ruanbekker/docker-swarm-apps/tree/master/tools-secrets-env-exporter">github.com/ruanbekker/docker-swarm-apps/tool-secrets-env-exporter</a>, but I will place the code in here as well.</p>

<pre><code class="bash Dockerfile:">FROM alpine:edge
RUN apk add --no-cache python2 py2-pip &amp;&amp; pip install flask
ADD exporter.py /exporter.py
ADD boot.sh /boot.sh
ADD app.py /app.py
CMD ["/bin/sh", "/boot.sh"]
</code></pre>

<pre><code class="python exporter.py">import os
from glob import glob

for var in glob('/run/secrets/*'):
    k=var.split('/')[-1]
    v=open(var).read().rstrip('\n')
    os.environ[k] = v
    print("export {key}={value}".format(key=k,value=v))
</code></pre>

<pre><code class="python app.py">import os
from flask import Flask

flask_host = str(os.environ['flask_host'])
flask_port = int(os.environ['flask_port'])

app = Flask(__name__)

@app.route('/')
def index():
    return 'ok\n'

if __name__ == '__main__':
    app.run(host=flask_host, port=flask_port)
</code></pre>

<pre><code class="bash boot.sh">#!/bin/sh
set -e
eval $(python /exporter.py)
python /app.py
</code></pre>

<h2>Flow Information:</h2>

<p>The exporter script checks all the secrets that is mounted to the container, then formats the secrets to a key/value pair, which then exports the environment variables to the current shell, which thereafter gets read by the flask application.</p>

<h2>Usage:</h2>

<p>Create Docker Secrets:</p>

<pre><code>$ echo 5001 | docker secret create flask_port -
$ echo 0.0.0.0 | docker secret create flask_host -
</code></pre>

<p>Build and Push the Image:</p>

<pre><code>$ docker build -t registry.gitlab.com/&lt;user&gt;/&lt;repo&gt;/&lt;image&gt;:&lt;tag&gt;
$ docker push registry.gitlab.com/&lt;user&gt;/&lt;repo&gt;/&lt;image&gt;:&lt;tag&gt;
</code></pre>

<p>Create the Service, and specify the secrets that we created earlier:</p>

<pre><code>$ docker service create --name webapp \
--secret source=flask_host,target=flask_host \
--secret source=flask_port,target=flask_port \
registry.gitlab.com/&lt;user&gt;/&lt;repo&gt;/&lt;image&gt;:&lt;tag&gt;
</code></pre>

<p>Exec into the container, list to see where the secrets got populated:</p>

<pre><code>$ ls /run/secrets/
flask_host  flask_port
</code></pre>

<p>Do a netstat, to see that the value from the created secret is listening:</p>

<pre><code>$ netstat -tulpn
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:5001            0.0.0.0:*               LISTEN      7/python
</code></pre>

<p>Do a GET request on the Flask Application:</p>

<pre><code>$ curl http://0.0.0.0:5001/
ok
</code></pre>
]]></content>
  </entry>
  
</feed>
