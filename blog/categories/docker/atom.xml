<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Docker | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/docker/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2018-06-14T10:18:59-04:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Deploy Docker Swarm Using Ansible]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/06/14/deploy-docker-swarm-using-ansible/"/>
    <updated>2018-06-14T06:05:46-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/06/14/deploy-docker-swarm-using-ansible</id>
    <content type="html"><![CDATA[<p><img src="http://obj-cache.cloud.ruanbekker.com/docker-logo.png" alt="" /></p>

<p>In this setup we will use Ansible to Deploy Docker Swarm.</p>

<p>With this setup, I have a client node, which will be my jump box, as it will be used to ssh with the docker user to my swarm nodes with passwordless ssh access.</p>

<p>The repository for the source code can be found on my <a href="https://github.com/ruanbekker/ansible-docker-swarm">Github Repository</a></p>

<h2>Pre-Check</h2>

<p>Hosts file:</p>

<pre><code>$ cat /etc/hosts
10.0.8.2 client
192.168.1.10 swarm-manager
192.168.1.11 swarm-worker-1
192.168.1.12 swarm-worker-2
</code></pre>

<p>SSH Config:</p>

<pre><code>$ cat ~/.ssh/config 
Host client
  Hostname client
  User root
  IdentityFile /tmp/key.pem
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null

Host swarm-manager
  Hostname swarm-manager
  User root
  IdentityFile /tmp/key.pem
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null

Host swarm-worker-1
  Hostname swarm-worker-1
  User root
  IdentityFile /tmp/key.pem
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null

Host swarm-worker-2
  Hostname swarm-worker-2
  User root
  IdentityFile /tmp/key.pem
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null
</code></pre>

<p>Install Ansible:</p>

<pre><code>$ apt install python-setuptools -y
$ easy_install pip
$ pip install ansible
</code></pre>

<p>Ensure passwordless ssh is working:</p>

<pre><code>$ ansible -i inventory.ini -u root -m ping all
client | SUCCESS =&gt; {
    "changed": false, 
    "ping": "pong"
}
swarm-manager | SUCCESS =&gt; {
    "changed": false, 
    "ping": "pong"
}
swarm-worker-2 | SUCCESS =&gt; {
    "changed": false, 
    "ping": "pong"
}
swarm-worker-1 | SUCCESS =&gt; {
    "changed": false, 
    "ping": "pong"
}
</code></pre>

<h2>Deploy Docker Swarm</h2>

<pre><code>$ ansible-playbook -i inventory.ini -u root deploy-swarm.yml 
PLAY RECAP 

client                     : ok=11   changed=3    unreachable=0    failed=0   
swarm-manager              : ok=18   changed=4    unreachable=0    failed=0   
swarm-worker-1             : ok=15   changed=1    unreachable=0    failed=0   
swarm-worker-2             : ok=15   changed=1    unreachable=0    failed=0   
</code></pre>

<p>SSH to the Swarm Manager and List the Nodes:</p>

<pre><code>$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
0ead0jshzkpyrw7livudrzq9o *   swarm-manager       Ready               Active              Leader              18.03.1-ce
iwyp6t3wcjdww0r797kwwkvvy     swarm-worker-1      Ready               Active                                  18.03.1-ce
ytcc86ixi0kuuw5mq5xxqamt1     swarm-worker-2      Ready               Active                                  18.03.1-ce
</code></pre>

<h2>Test Application on Swarm</h2>

<p>Create a Nginx Demo Service:</p>

<pre><code>$ docker network create --driver overlay appnet
$ docker service create --name nginx --publish 80:80 --network appnet --replicas 6 nginx
$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
k3vwvhmiqbfk        nginx               replicated          6/6                 nginx:latest        *:80-&gt;80/tcp

$ docker service ps nginx
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
tspsypgis3qe        nginx.1             nginx:latest        swarm-manager       Running             Running 34 seconds ago                       
g2f0ytwb2jjg        nginx.2             nginx:latest        swarm-worker-1      Running             Running 34 seconds ago                       
clcmew8bcvom        nginx.3             nginx:latest        swarm-manager       Running             Running 34 seconds ago                       
q293r8zwu692        nginx.4             nginx:latest        swarm-worker-2      Running             Running 34 seconds ago                       
sv7bqa5e08zw        nginx.5             nginx:latest        swarm-worker-1      Running             Running 34 seconds ago                       
r7qg9nk0a9o2        nginx.6             nginx:latest        swarm-worker-2      Running             Running 34 seconds ago   
</code></pre>

<p>Test the Application:</p>

<pre><code>$ curl -i http://192.168.1.10
HTTP/1.1 200 OK
Server: nginx/1.15.0
Date: Thu, 14 Jun 2018 10:01:34 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Tue, 05 Jun 2018 12:00:18 GMT
Connection: keep-alive
ETag: "5b167b52-264"
Accept-Ranges: bytes
</code></pre>

<p>Delete the Service:</p>

<pre><code>
$ docker service rm nginx
nginx
</code></pre>

<h2>Delete the Swarm:</h2>

<pre><code>$ ansible-playbook -i inventory.ini -u root delete-swarm.yml 

PLAY RECAP 
swarm-manager              : ok=2    changed=1    unreachable=0    failed=0   
swarm-worker-1             : ok=2    changed=1    unreachable=0    failed=0   
swarm-worker-2             : ok=2    changed=1    unreachable=0    failed=0   
</code></pre>

<p>Ensure the Nodes is removed from the Swarm, SSH to your Swarm Manager:</p>

<pre><code>$ docker node ls
Error response from daemon: This node is not a swarm manager. Use "docker swarm init" or "docker swarm join" to connect this node to swarm and try again.
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Clearing Up Disk Space on Docker Swarm by Removing Unused Data With Prune]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/06/01/clearing-up-disk-space-on-docker-swarm-by-removing-unused-data-with-prune/"/>
    <updated>2018-06-01T02:19:21-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/06/01/clearing-up-disk-space-on-docker-swarm-by-removing-unused-data-with-prune</id>
    <content type="html"><![CDATA[<p><img src="http://obj-cache.cloud.ruanbekker.com/docker-logo.png" alt="" /></p>

<p>After some time, your system can run out of disk space when running a lot of containers / volumes etc. You will find that at times, you will have a lot of unused containers, stopped containers, unused images, unused networks that is just sitting there, which consumes data on your nodes.</p>

<p>One way to clean them is by using <code>docker system prune</code>.</p>

<h2>Check Docker Disk Space</h2>

<p>The command below will show the amount of disk space consumed, and how much is reclaimable:</p>

<pre><code class="bash">$ docker system df
TYPE                TOTAL               ACTIVE              SIZE                RECLAIMABLE
Images              229                 125                 23.94GB             14.65GB (61%)
Containers          322                 16                  8.229GB             8.222GB (99%)
Local Volumes       77                  41                  698MB               19.13MB (2%)
Build Cache                                                 0B                  0B
</code></pre>

<h2>Removing Unsued Data:</h2>

<p>By using Prune, we can remove the unused resources that is consuming data:</p>

<pre><code class="bash">$ docker system prune

WARNING! This will remove:
        - all stopped containers
        - all networks not used by at least one container
        - all dangling images
        - all build cache
Are you sure you want to continue? [y/N] y

Deleted Containers:
a3d7db158e065d0c86160fd5d688875f8b7435848ea91db57ed007
47890dcfea4a105f43e790dd8ad3c6d7c4ad7e738186c034d7a46b

Deleted Networks:
traefik-net
app_appnet

Deleted Images:
deleted: sha256:5b9909c10e93afec
deleted: sha256:d81eesdfihweo3rk

Total reclaimed space: 14.18GB
</code></pre>

<p>For related <a href="https://goo.gl/L2NYxU">Docker</a> posts.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Wildcard SSL Certificate With Letsencrypt on Docker Swarm Using Traefik]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/05/28/wildcard-ssl-certificate-with-letsencrypt-on-docker-swarm-using-traefik/"/>
    <updated>2018-05-28T17:36:17-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/05/28/wildcard-ssl-certificate-with-letsencrypt-on-docker-swarm-using-traefik</id>
    <content type="html"><![CDATA[<p><img src="http://obj-cache.cloud.ruanbekker.com/traefik.png" alt="" /></p>

<p>With Letsencrypt supporting Wildcard certificates is really awesome. Now, we can setup traefik to listen on 443, acting as a reverse proxy and is doing HTTPS Termination to our Applications thats running in our Swarm.</p>

<h2>Architectural Design:</h2>

<p>At the moment we have 3 Manager Nodes, and 5 Worker Nodes:</p>

<ul>
<li>Using a Dummy Domain example.com which is set to the 3 Public IP&rsquo;s of our Manager Nodes</li>
<li>DNS is set for: <code>example.com</code> A Record to: <code>52.10.1.10</code>, <code>52.10.1.11</code>, <code>52.10.1.12</code></li>
<li>DNS is set for: <code>*.example.com</code> CNAME to <code>example.com</code></li>
<li>Any application that is spawned into our Swarm, will be labeled with a <code>traefik.frontend.rule</code> which will be routed to the service and redirected from HTTP to HTTPS</li>
</ul>


<h2>Create the Overlay Network:</h2>

<p>Create the overlay network that will be used for our stack:</p>

<pre><code class="bash">$ docker network create --driver overlay appnet
</code></pre>

<h2>Create the Compose Files for our Stacks:</h2>

<p>Create the Traefik Service Compose file, we will deploy it in Global Mode, constraint to our Manager Nodes, so that every manager node has a copy of traefik running.</p>

<pre><code class="bash">$ cat &gt; traefik-compose.yml &lt;&lt; EOF

version: "3.4"
services:
  proxy:
    image: traefik:latest
    command:
      - "--api"
      - "--entrypoints=Name:http Address::80 Redirect.EntryPoint:https"
      - "--entrypoints=Name:https Address::443 TLS"
      - "--defaultentrypoints=http,https"
      - "--acme"
      - "--acme.storage=/etc/traefik/acme/acme.json"
      - "--acme.entryPoint=https"
      - "--acme.httpChallenge.entryPoint=http"
      - "--acme.onHostRule=true"
      - "--acme.onDemand=false"
      - "--acme.email=me@example.com"
      - "--docker"
      - "--docker.swarmMode"
      - "--docker.domain=example.com"
      - "--docker.watch"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /mnt/traefik/acme.json:/etc/traefik/acme/acme.json
    networks:
      - appnet
    ports:
      - target: 80
        published: 80
        mode: host
      - target: 443
        published: 443
        mode: host
      - target: 8080
        published: 8080
        mode: host
    deploy:
      mode: global
      placement:
        constraints:
          - node.role == manager
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
networks:
  appnet:
    external: true

EOF
</code></pre>

<p>Create the Application Compose file, in this example we will be deploying a Ghost Blog:</p>

<pre><code class="bash">$ cat &gt; ghost-compose.yml &lt;&lt; EOF

version: '3.4'

services:
  blog:
    image: ghost:1.22.7-alpine
    networks:
      - appnet
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints: 
          - node.role == worker
      labels:
        - "traefik.backend.loadbalancer.sticky=false"
        - "traefik.backend.loadbalancer.swarm=true"
        - "traefik.backend=blog-1"
        - "traefik.docker.network=appnet"
        - "traefik.entrypoints=https"
        - "traefik.frontend.passHostHeader=true"
        - "traefik.frontend.rule=Host:blog.example.com"
        - "traefik.port=2368"

networks:
  appnet:
    external: true

EOF
</code></pre>

<h2>Prepare the Path for Traefik:</h2>

<p>We have a <a href="https://sysadmins.co.za/tag/glusterfs/">replicated volume</a> under our <code>/mnt</code> partition, so that all our managers can read from that path, create the file and provide the sufficient permissions:</p>

<pre><code class="bash">$ mkdir -p /mnt/traefik
$ touch /mnt/traefik/acme.json
$ chmod 600 /mnt/traefik/acme.json
</code></pre>

<h2>Deploy the Stacks:</h2>

<p>Deploy the Traefik Stack:</p>

<pre><code class="bash">$ docker stack deploy -c traefik-compose.yml traefik
</code></pre>

<p>Wait until the services are deployed:</p>

<pre><code class="bash">$ docker stack services traefik
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
f8ru5gbcgd2v        traefik_proxy       global              3/3                 traefik:latest
</code></pre>

<p>Deploy the Application Stack:</p>

<pre><code class="bash">$ docker stack deploy -c ghost-compose.yml apps
</code></pre>

<p>Verify that the Application Stack has been deployed:</p>

<pre><code class="bash">$ docker stack services apps
ID                  NAME                MODE                REPLICAS            IMAGE                          PORTS
516zlfs2cfdv        apps_blog           replicated          1/1                 ghost:1.22.7-alpine
</code></pre>

<p>At the moment we will have 2 stacks in our Swarm:</p>

<pre><code class="bash">$ docker stack ls
NAME                SERVICES
apps                1
traefik             1
</code></pre>

<h2>Test the Application:</h2>

<p>Let&rsquo;s test our blog to see if we get redirected to <a href="HTTPS:">HTTPS:</a></p>

<pre><code class="bash">$ curl -iL http://blog.example.com
HTTP/1.1 302 Found
Location: https://blog.example.com:443/
Date: Mon, 28 May 2018 22:02:41 GMT
Content-Length: 5
Content-Type: text/plain; charset=utf-8

HTTP/1.1 200 OK
Cache-Control: public, max-age=0
Content-Type: text/html; charset=utf-8
Date: Mon, 28 May 2018 22:02:42 GMT
Etag: W/"4166-J2ooSIa8gtTkYjbnr7vnPUFlRJI"
Vary: Accept-Encoding
X-Powered-By: Express
Transfer-Encoding: chunked
</code></pre>

<p>Works like a charm! Traefik FTW!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Manage Scaleway Instances via Their API Like a Boss With Their Command Line Tool Scw]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/05/09/manage-scaleway-instances-via-their-api-like-a-boss-with-their-command-line-tool-scw/"/>
    <updated>2018-05-09T12:31:11-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/05/09/manage-scaleway-instances-via-their-api-like-a-boss-with-their-command-line-tool-scw</id>
    <content type="html"><![CDATA[<p><img src="https://preview.ibb.co/bBRhn7/scw.png" alt="" /></p>

<p>Let&rsquo;s set things straight: I am a command line fan boy, If I can do the things I have to do with a command line interface, i&rsquo;m happy! And that means automation ftw! :D</p>

<h2>Scaleway Command Line Interface:</h2>

<p>I have been using Scaleway for about 2 years now, and absolutely loving their services! So I recently found their <a href="https://github.com/scaleway/scaleway-cli">command line interface utility: scw</a>, which is written in golang and has a very similar feel to docker.</p>

<h2>Install the SCW CLI Tool:</h2>

<p>A golang environment is needed and I will be using docker to drop myself into a golang environment and then install the scw utility:</p>

<pre><code class="bash">$ docker run -it golang:alpine sh
$ apk update
$ apk add openssl git openssh curl
$ go get -u github.com/scaleway/scaleway-cli/cmd/scw
</code></pre>

<p>Verify that it was installed:</p>

<pre><code class="bash">$ scw --version
scw version v1.16+dev, build
</code></pre>

<p>Awesome sauce!</p>

<h2>Authentication:</h2>

<p>When we authenticate to Scaleway, it will prompt you to upload your public ssh key, as I am doing this in a container I have no ssh keys, so therefore will generate one before I authenticate.</p>

<p>Generate the SSH Key:</p>

<pre><code class="bash">$ ssh-keygen -t rsa
Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa):
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /root/.ssh/id_rsa.
Your public key has been saved in /root/.ssh/id_rsa.pub.
</code></pre>

<p>Now loging to Scaleway using the cli tools:</p>

<pre><code class="bash">$ scw login
Login (cloud.scaleway.com): &lt;youremail@domain.com&gt;
Password:
Do you want to upload an SSH key ?
[0] I don't want to upload a key !
[1] id_rsa.pub
Which [id]: 1

You are now authenticated on Scaleway.com as Ruan.
You can list your existing servers using `scw ps` or create a new one using `scw run ubuntu-xenial`.
You can get a list of all available commands using `scw -h` and get more usage examples on github.com/scaleway/scaleway-cli.
Happy cloud riding.
</code></pre>

<p>Sweeet!</p>

<p><img src="https://pics.me.me/hacker-voice-im-in-24303160.png" alt="" /></p>

<h2>Getting Info from Scaleway</h2>

<p>List Instance Types:</p>

<pre><code class="bash">$ scw products servers
COMMERCIAL TYPE     ARCH     CPUs      RAM  BAREMETAL
ARM64-128GB        arm64       64   137 GB      false
ARM64-16GB         arm64       16    17 GB      false
ARM64-2GB          arm64        4   2.1 GB      false
ARM64-32GB         arm64       32    34 GB      false
ARM64-4GB          arm64        6   4.3 GB      false
ARM64-64GB         arm64       48    69 GB      false
ARM64-8GB          arm64        8   8.6 GB      false
C1                   arm        4   2.1 GB       true
C2L               x86_64        8    34 GB       true
C2M               x86_64        8    17 GB       true
C2S               x86_64        4   8.6 GB       true
START1-L          x86_64        8   8.6 GB      false
START1-M          x86_64        4   4.3 GB      false
START1-S          x86_64        2   2.1 GB      false
START1-XS         x86_64        1   1.1 GB      false
VC1L              x86_64        6   8.6 GB      false
VC1M              x86_64        4   4.3 GB      false
VC1S              x86_64        2   2.1 GB      false
X64-120GB         x86_64       12   129 GB      false
X64-15GB          x86_64        6    16 GB      false
X64-30GB          x86_64        8    32 GB      false
X64-60GB          x86_64       10    64 GB      false
</code></pre>

<p>Get a list of available Images, in my case I am just looking for Ubuntu:</p>

<pre><code class="bash">$ scw images | grep -i ubuntu
Ubuntu_Bionic               latest              a21bb700            11 days             [ams1 par1]         [x86_64]
Ubuntu_Mini_Xenial_25G      latest              bc75c00b            13 days             [ams1 par1]         [x86_64]
</code></pre>

<p>List Running Instances:</p>

<pre><code class="bash">$ scw ps
SERVER ID           IMAGE                       ZONE                CREATED             STATUS              PORTS               NAME                  COMMERCIAL TYPE
abc123de            Ubuntu_Xenial_16_04_lates   ams1                5 weeks             running             xx.xx.xx.xx         scw-elasticsearch-01  ARM64-4GB
abc456de            ruan-docker-swarm-17_03     par1                10 months           running             xx.xx.xxx.xxx       scw-swarm-manager-01  VC1M
...
</code></pre>

<p>List All Instances (Running, Stopped, Started, etc):</p>

<pre><code class="bash">$ scw ps -a
SERVER ID           IMAGE                       ZONE                CREATED             STATUS              PORTS               NAME                  COMMERCIAL TYPE
abc123df            Ubuntu_Xenial_16_04_lates   ams1                5 weeks             stopped             xx.xx.xx.xx         scw-elasticsearch-02  ARM64-4GB
...
</code></pre>

<p>List Instances with a filter based on its name:</p>

<pre><code class="bash">$ scw ps -f name=scw-swarm-worker-02
SERVER ID           IMAGE               ZONE                CREATED             STATUS              PORTS               NAME                COMMERCIAL TYPE
1234abcd            Ubuntu_Xenial       par1                8 minutes           running             xx.xx.xxx.xxx       scw-swarm-worker-2  START1-XS
</code></pre>

<p>List the Latest Instance that was created:</p>

<pre><code class="bash">$ scw ps -l
SERVER ID           IMAGE               ZONE                CREATED             STATUS              PORTS               NAME                COMMERCIAL TYPE
1234abce            Ubuntu_Xenial       par1                6 minutes           running             xx.xx.xxx.xxx       scw-swarm-worker-3  START1-XS
</code></pre>

<h2>Create Instances:</h2>

<p>In my scenario, I would like to create a instance named <code>docker-swarm-worker-4</code> with the instance type <code>START1-XS</code> in the Paris datacenter, and I will be using my key that I have uploaded, also the image id that I passed, was retrieved when listing for images:</p>

<pre><code class="bash">$ scw --region=par1 create --commercial-type=START1-XS --ip-address=dynamic --ipv6=false --name="docker-swarm-worker-4" --tmp-ssh-key=false  bc75c00b
&lt;response: random uuid string&gt;
</code></pre>

<p>Now that the instance is created, we can start it by calling either the name or the id:</p>

<pre><code class="bash">$ scw start docker-swarm-worker-4
</code></pre>

<p>To verify the status of the instance, we can do:</p>

<pre><code class="bash">$ scw ps -l
SERVER ID           IMAGE               ZONE                CREATED             STATUS              PORTS               NAME                   COMMERCIAL TYPE
102abc34            Ubuntu_Xenial                           28 seconds          starting                                docker-swarm-worker-4  START1-XS
</code></pre>

<p>At this moment it is still starting, after waiting a minute or so, run it again:</p>

<pre><code class="bash">$ scw ps -l
SERVER ID           IMAGE               ZONE                CREATED             STATUS              PORTS               NAME                   COMMERCIAL TYPE
102abc34            Ubuntu_Xenial       par1                About a minute      running             xx.xx.xx.xx         docker-swarm-worker-4  START1-XS
</code></pre>

<p>As we can see its in a running state, so we are good to access our instance. You have 2 options to access your server, via exec and ssh.</p>

<pre><code>$ scw exec docker-swarm-worker-4 /bin/bash
root@docker-swarm-worker-4:~
</code></pre>

<p>or via SSH:</p>

<pre><code>$ ssh root@xx.xx.xx.xx
root@docker-swarm-worker-4:~
</code></pre>

<p>If you would like to access your server without uploading your SSH key to your account, you can pass <code>--tmp-ssh-key=true</code> as in:</p>

<pre><code class="bash">$ scw --region=par1 create --commercial-type=START1-XS --ip-address=dynamic --ipv6=false --name="scw-temp-instance" --tmp-ssh-key=true  bc75c00b
</code></pre>

<h2>Terminating Resources:</h2>

<p>This wil stop, terminate the instance with the associated volumes and reserved ip</p>

<pre><code class="bash">$ scw stop --terminate=true scw-temp-instance 
scw-temp-instance
</code></pre>

<p>If you had to remove a volume that is not needed, or unused:</p>

<pre><code>$ scw rmi test-1-snapshot-&lt;long-string&gt;--2018-04-26_12:42
</code></pre>

<p>To logout:</p>

<pre><code class="bash">$ scw logout
</code></pre>

<h2>Resources:</h2>

<p>Have a look at <a href="https://github.com/scaleway/scaleway-cli">Scaleway-CLI Documentation</a> and their <a href="https://www.scaleway.com/">Website</a> for more info, and have a look at their new <code>START1-XS</code> instance types, that is only 1.99 Euro&rsquo;s, that is insane!</p>

<p>Personally love what they are doing, feel free to head over to their <a href="https://www.scaleway.com/pricing/">pricing page</a> to see some sweet deals!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup the Elasticsearch Log Driver on Docker Swarm]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/05/02/setup-the-elasticsearch-log-driver-on-docker-swarm/"/>
    <updated>2018-05-02T15:10:30-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/05/02/setup-the-elasticsearch-log-driver-on-docker-swarm</id>
    <content type="html"><![CDATA[<p><img src="http://obj-cache.cloud.ruanbekker.com/docker-logo.png" alt="" /></p>

<p>Today we will look at a Elasticsearch logging driver for Docker.</p>

<h2>Why a Log Driver?</h2>

<p>By default the log output can be retrieved when using the <code>docker service logs -f service_name</code>, where log output of that service is shown via stdout. When having a lot of services in your swarm, it becomes useful logging all of your log output to a database service.</p>

<p>This is not just for Swarm but Docker stand alone as well.</p>

<p>In this tutorial we will use the Elasticsearch Log Driver, to log our logs for all our docker swarm services to Elasticsearch.</p>

<h2>Installing to Elasticsearch Log Driver:</h2>

<p>If you are running Docker Swarm, run this on all the nodes:</p>

<pre><code class="bash">$ docker plugin install rchicoli/docker-log-elasticsearch:latest --alias elasticsearch_latest
</code></pre>

<p>Verify that the log driver has been installed:</p>

<pre><code class="bash">$ docker plugin ls
ID                  NAME                          DESCRIPTION                          ENABLED
eadf06ad3d2a        elasticsearch_latest:latest   Send log messages to elasticsearch   true
</code></pre>

<h2>Test the Log Driver:</h2>

<p>Run a container of Alpine and echo a string of text:</p>

<pre><code class="bash">$ docker run --rm -ti \
    --log-driver elasticsearch_latest \
    --log-opt elasticsearch-url=http://192.168.0.235:9200 \
    --log-opt elasticsearch-insecure=false \
    --log-opt elasticsearch-sniff=false \
    --log-opt elasticsearch-index=docker-%F \
    --log-opt elasticsearch-type=log \
    --log-opt elasticsearch-timeout=10 \
    --log-opt elasticsearch-version=5 \
    --log-opt elasticsearch-fields=containerID,containerName,containerImageID,containerImageName,containerCreated \
    --log-opt elasticsearch-bulk-workers=1 \
    --log-opt elasticsearch-bulk-actions=1000 \
    --log-opt elasticsearch-bulk-size=1024 \
    --log-opt elasticsearch-bulk-flush-interval=1s \
    --log-opt elasticsearch-bulk-stats=false \
        alpine echo -n "this is a test logging message"
</code></pre>

<p>Have a look at your Elasticsearch indexes, and you will find the index which was specified in the log-options:</p>

<pre><code class="bash">$ curl http://192.168.0.235:9200/_cat/indices?v
health status index             uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   docker-2018.05.01 8FTqWq6nQlSGpYjD9M5qSg   5   1          1            0      8.9kb          8.9kb
</code></pre>

<p>Lets have a look at the Elasticsearch Document which holds the data of the log entry:</p>

<pre><code class="bash">$ curl http://192.168.0.235:9200/docker-2018.05.01/_search?pretty
{
  "took" : 5,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "docker-2018.05.01",
        "_type" : "log",
        "_id" : "hMTUG2MBIFc8kAgSNkYo",
        "_score" : 1.0,
        "_source" : {
          "containerID" : "cee0dc758528",
          "containerName" : "jolly_goodall",
          "containerImageID" : "sha256:3fd9065eaf02feaf94d68376da52541925650b81698c53c6824d92ff63f98353",
          "containerImageName" : "alpine",
          "containerCreated" : "2018-05-01T13:11:20.819447101Z",
          "message" : "this is a test logging message",
          "source" : "stdout",
          "timestamp" : "2018-05-01T13:11:21.119861767Z",
          "partial" : true
        }
      }
    ]
  }
}
</code></pre>

<h2>Using Swarm and Docker Compose:</h2>

<p>We will deploy a stack with a whoami golang web app, which will use the elasticsearch log driver:</p>

<pre><code class="bash docker-compose.yml">version: '3.4'

services:
  whoami:
    image: rbekker87/golang-whoami:latest
    networks:
      - appnet
    deploy:
      labels:
        - "traefik.port=80"
        - "traefik.backend.loadbalancer.swarm=true"
        - "traefik.docker.network=appnet"
        - "traefik.frontend.rule=Host:whoami.homecloud.mydomain.com"
      mode: replicated
      replicas: 10
      restart_policy:
        condition: any
      update_config:
        parallelism: 1
        delay: 70s
        order: start-first
        failure_action: rollback
      placement:
        constraints:
          - 'node.role==worker'
      resources:
        limits:
          cpus: '0.01'
          memory: 128M
        reservations:
          cpus: '0.001'
          memory: 64M
    logging:
      driver: elasticsearch_latest
      options:
        elasticsearch-url: "http://192.168.0.235:9200"
        elasticsearch-sniff: "false"
        elasticsearch-index: "docker-whoami-%F"
        elasticsearch-type: "log"
        elasticsearch-timeout: "10"
        elasticsearch-version: "6"
        elasticsearch-fields: "containerID,containerName,containerImageID,containerImageName,containerCreated"
        elasticsearch-bulk-workers: "1"
        elasticsearch-bulk-actions: "1000"
        elasticsearch-bulk-size: "1024"
        elasticsearch-bulk-flush-interval: "1s"
        elasticsearch-bulk-stats: "false"
networks:
  appnet:
    external: true
</code></pre>

<p>Deploy the Stack:</p>

<pre><code class="bash">$ docker stack deploy -c docker-compose.yml web 
</code></pre>

<p>Give it some time to launch and have a look at your indexes, and you will find the index which it wrote to:</p>

<pre><code class="bash">$ curl http://192.168.0.235:9200/_cat/indices?v
health status index                     uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   docker-2018.05.01         8FTqWq6nQlSGpYjD9M5qSg   5   1          1            0      8.9kb          8.9kb
yellow open   docker-whoami-2018.05.01  YebUtKa1RnCy86iP5_ylgg   5   1         11            0     54.4kb         54.4kb
</code></pre>

<p>Having a look at the data:</p>

<pre><code>$ curl 'http://192.168.0.235:9200/docker-whoami-2018.05.01/_search?pretty&amp;size=1'
{
  "took" : 18,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 11,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "docker-whoami-2018.05.01",
        "_type" : "log",
        "_id" : "acbgG2MBIFc8kAgShQa7",
        "_score" : 1.0,
        "_source" : {
          "containerID" : "97c3b337735f",
          "containerName" : "web_whoami.6.t2prjiexkym14isbx3yfxa99w",
          "containerImageID" : "sha256:0f7762d2ce569fc2ccf95fbc4c7191dde727551a180253fac046daecc580c7e9",
          "containerImageName" : "rbekker87/golang-whoami:latest@sha256:5a55c5de9cc16fbdda376791c90efb7c704c81b8dba949dce21199945c14cc88",
          "containerCreated" : "2018-05-01T13:24:43.089365528Z",
          "message" : "Starting up on port 80",
          "source" : "stdout",
          "timestamp" : "2018-05-01T13:24:48.636773709Z",
          "partial" : false
        }
      }
    ]
  }
}
</code></pre>

<p>For more info about this, have a look at the referenced documentation below.</p>

<h2>Resources:</h2>

<ul>
<li><a href="https://github.com/rchicoli/docker-log-elasticsearch">https://github.com/rchicoli/docker-log-elasticsearch</a></li>
<li><a href="https://github.com/moby/moby/issues/25694">https://github.com/moby/moby/issues/25694</a></li>
<li><a href="https://docs.docker.com/v17.09/engine/admin/logging/view_container_logs/">https://docs.docker.com/v17.09/engine/admin/logging/view_container_logs/</a></li>
<li><a href="https://sysadmins.co.za/how-to-setup-a-2-node-elasticsearch-cluster-on-centos-7-with-some-example-usage/">https://sysadmins.co.za/how-to-setup-a-2-node-elasticsearch-cluster-on-centos-7-with-some-example-usage/</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
