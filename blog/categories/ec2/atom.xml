<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Ec2 | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/ec2/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2023-07-15T18:00:35-04:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[AWS EC2 Linux - Warning: Setlocale: LC_CTYPE: Cannot Change Locale UTF-8]]></title>
    <link href="https://blog.ruanbekker.com/blog/2021/08/02/aws-ec2-linux-warning-setlocale-lc-ctype-cannot-change-locale-utf-8/"/>
    <updated>2021-08-02T02:40:53-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2021/08/02/aws-ec2-linux-warning-setlocale-lc-ctype-cannot-change-locale-utf-8</id>
    <content type="html"><![CDATA[<p>On Amazon Linux EC2 Instances, I noticed the following error when SSH onto them:</p>

<pre><code>-bash: warning: setlocale: LC_CTYPE: cannot change locale (UTF-8): No such file or directory
</code></pre>

<p>To resolve, add the following to the <code>/etc/environment</code> file:</p>

<pre><code>$ cat /etc/environment
LANG=en_US.utf-8
LC_ALL=en_US.utf-8
</code></pre>

<p>Logout and log back in and it should be resolved.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running SSH Commands on AWS EC2 Instances With Python]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/11/02/running-ssh-commands-on-aws-ec2-instances-with-python/"/>
    <updated>2020-11-02T09:55:43+00:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/11/02/running-ssh-commands-on-aws-ec2-instances-with-python</id>
    <content type="html"><![CDATA[<p>In this quick post I will demonstrate how to discover a EC2 Instance&rsquo;s Private IP Address using the AWS API by using Tags then use Paramiko in Python to SSH to the EC2 instance and run SSH commands on the target instance.</p>

<p>Install the required dependencies:</p>

<pre><code>$ virtualenv -p python3 .venv
$ source .venve/bin/activate
$ pip install boto3 paramiko
</code></pre>

<p>I have my development profile for aws configured under <code>dev</code> as can seen below:</p>

<pre><code>$ aws --profile dev configure list
      Name                    Value             Type    Location
      ----                    -----             ----    --------
   profile                      dev           manual    --profile
access_key     ****************xxxx      assume-role
secret_key     ****************xxxx      assume-role
    region                eu-west-1      config-file    ~/.aws/config
</code></pre>

<p>First we need to discover the private ip address from the api by referencing tags, and in this example we will use the <code>Name</code> tag:</p>

<pre><code>import boto3
ec2 = boto3.Session(profile_name='dev', region_name='eu-west-1').client('ec2')

target_instances = ec2.describe_instances(
    Filters=[{'Name':'tag:Name','Values':['my-demo-ec2-instance']}]
)

ec2_instances = []
for each_instance in target_instances['Reservations']:
    for found_instance in each_instance['Instances']:
        ec2_instances.append(found_instance['PrivateIpAddress'])

# ec2_instances
# ['172.31.2.89']
</code></pre>

<p>So we are instantiating a ec2 instance with our configured dev profile, then we describe all our instances using the tag key <code>Name</code> and value <code>my-demo-ec2-instance</code> and then access the private ip address and append it to our <code>ec2_instances</code> list.</p>

<p>Next we want to define the commands that we want to run on the target ec2 instance:</p>

<pre><code>commands = [
    "echo hi",
    "whoami",
    "hostname"
]
</code></pre>

<p>In my case I only have 1 ec2 instance with the name <code>my-demo-ec2-instance</code>, but if you have more you can just loop through the list and perform the actions.</p>

<p>Next we want to establish the SSH connection:</p>

<pre><code>k = paramiko.RSAKey.from_private_key_file("/Users/ruan/.ssh/id_rsa")
c = paramiko.SSHClient()
c.set_missing_host_key_policy(paramiko.AutoAddPolicy())
c.connect(hostname=ec2_instances[0], username="ruan", pkey=k, allow_agent=False, look_for_keys=False)
</code></pre>

<p>Once our SSH connection has established, we can loop through our commands and execute them:</p>

<pre><code>for command in commands:
    print("running command: {}".format(command))
    stdin , stdout, stderr = c.exec_command(command)
    print(stdout.read())
    print(stderr.read())
</code></pre>

<p>Which will output the folling:</p>

<pre><code>running command: echo hi
b'hi\n'
b''
running command: whoami
b'ruan\n'
b''
running command: hostname
b'ip-172-31-2-89\n'
b''
</code></pre>

<p>And then close the SSH connection:</p>

<pre><code>c.close()
</code></pre>

<p>And the full script will look like this:</p>

<pre><code class="python">import boto3
ssh_username = "ruan"
ssh_key_file = "/Users/ruan/.ssh/id_rsa"

ec2 = boto3.Session(profile_name='dev', region_name='eu-west-1').client('ec2')

target_instances = ec2.describe_instances(
    Filters=[{'Name':'tag:Name','Values':['my-demo-ec2-instance']}]
)

ec2_instances = []
for each_instance in target_instances['Reservations']:
    for found_instance in each_instance['Instances']:
        ec2_instances.append(found_instance['PrivateIpAddress'])

commands = [
    "echo hi",
    "whoami",
    "hostname"
]

k = paramiko.RSAKey.from_private_key_file(ssh_key_file)
c = paramiko.SSHClient()
c.set_missing_host_key_policy(paramiko.AutoAddPolicy())
c.connect(hostname=ec2_instances[0], username=ssh_username, pkey=k, allow_agent=False, look_for_keys=False)

for command in commands:
    print("running command: {}".format(command))
    stdin , stdout, stderr = c.exec_command(command)
    print(stdout.read())
    print(stderr.read())

c.close()
</code></pre>


]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Temporary IAM Credentials From EC2 Instance Metadata Using Python]]></title>
    <link href="https://blog.ruanbekker.com/blog/2018/05/09/temporary-iam-credentials-from-ec2-instance-metadata-using-python/"/>
    <updated>2018-05-09T12:14:11-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2018/05/09/temporary-iam-credentials-from-ec2-instance-metadata-using-python</id>
    <content type="html"><![CDATA[<p>From a Best Practice Perspective its good not having to pass sensitive information around, and especially not hard coding them.</p>

<h2>Best Practice: Security</h2>

<p>One good way is to use SSM with KMS to Encrypt/Decrypt them, but since EC2 has a <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html">Metadata Service</a> available, we can make use of that to retrieve <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html">temporary credentials</a>. One requirement though, is that the instance will require an IAM Role where the code will be executed on. The IAM Role also needs to have sufficient privileges to be able to execute, whatever you need to do.</p>

<p>The <a href="https://12factor.net/">12 Factor</a> Methodology however states to use config in your environment variables, but from the application logic, its easy to save it in our environment.</p>

<h2>Scenario: Applications on AWS EC2</h2>

<p>When you run applications on Amazon EC2 the nodes has access to the EC2 Metadata Service, so in this case our IAM Role has a Policy that authorizes GetItem on our DynamoDB table, therefore we can define our code with no sensitive information, as the code will do all the work to get the credentials and use the credentials to access DynamoDB.</p>

<h2>Use Temporary Credentials to Read from DynamoDB using botocore</h2>

<p>In this example we will get the temporary credentials from the metadata service, then define the temporary credentials in our session to authorize our request against dynamodb to read from our table:</p>

<pre><code class="python">&gt;&gt;&gt; import boto3
&gt;&gt;&gt; from botocore.utils import InstanceMetadataFetcher
&gt;&gt;&gt; from botocore.credentials import InstanceMetadataProvider
&gt;&gt;&gt; provider = InstanceMetadataProvider(iam_role_fetcher=InstanceMetadataFetcher(timeout=1000, num_attempts=2))
&gt;&gt;&gt; creds = provider.load()

&gt;&gt;&gt; session = boto3.Session(
    aws_access_key_id=creds.access_key,
    aws_secret_access_key=creds.secret_key,
    aws_session_token=creds.token
)

&gt;&gt;&gt; ddb = session.client('dynamodb')

&gt;&gt;&gt; response = ddb.get_item(
    TableName='my-dynamodb-table',
    Key={
        'node_type': {
            'S': 'primary_manager'
        }
    }
)

&gt;&gt;&gt; print(response['Item']['ip']['S'])
'10.0.0.32
</code></pre>

<p>Also, when you are logged onto the EC2 instance, you can use curl to see the temporary credentials information:</p>

<pre><code class="bash">$ iam_role_name=$(curl -s http://169.254.169.254/latest/meta-data/iam/security-credentials/)
$ curl -s http://169.254.169.254/latest/meta-data/iam/security-credentials/${iam_role_name}
{
  "Code" : "Success",
  "LastUpdated" : "2018-05-09T14:25:48Z",
  "Type" : "AWS-HMAC",
  "AccessKeyId" : "",
  "SecretAccessKey" : "",
  "Token" : "",
  "Expiration" : "2018-05-09T20:46:55Z"
}
</code></pre>

<h2>Another method is boto3 Session:</h2>

<p>You can also use boto3.Session to achieve this:</p>

<pre><code class="bash">&gt;&gt;&gt; session = boto3.Session(region_name='eu-west-1')
&gt;&gt;&gt; credentials = session.get_credentials()
&gt;&gt;&gt; credentials = credentials.get_frozen_credentials()
&gt;&gt;&gt; credentials.access_key
u'ABC...'
&gt;&gt;&gt; credentials.secret_key
u'DEF...'
&gt;&gt;&gt; credentials.token
u'ZXC...'
&gt;&gt;&gt; access_key = credentials.access_key
&gt;&gt;&gt; secret_key = credentials.secret_key
&gt;&gt;&gt; ddb = session.client('dynamodb')
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Expanding the Size of Your EBS Volume on AWS EC2 for Linux]]></title>
    <link href="https://blog.ruanbekker.com/blog/2018/03/28/expanding-the-size-of-your-ebs-volume-on-aws-ec2-for-linux/"/>
    <updated>2018-03-28T01:45:07-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2018/03/28/expanding-the-size-of-your-ebs-volume-on-aws-ec2-for-linux</id>
    <content type="html"><![CDATA[<p><img src="https://i.snag.gy/BJLbwQ.jpg" alt="" /></p>

<p>Resizing your EBS Volume on the fly, that is attached to your EC2 Linux instance, on Amazon Web Services.</p>

<p>We want to resize our EBS Volume from 100GB to 1000GB and at the moment my EBS Volume is 100GB, as you can see:</p>

<pre><code class="bash">$ df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        7.9G   60K  7.9G   1% /dev
tmpfs           7.9G     0  7.9G   0% /dev/shm
/dev/xvda1       99G   32G   67G  32% /
</code></pre>

<p>Now we want to resize the volume to 1000GB, without shutting down our EC2 instance.</p>

<p>Go to your EC2 Management Console, Select your EC2 Instance, scroll down to the EBS Volume, click on it and click the EBS Volume ID, from there select Actions, modify and resize the disk to the needed size. As you can see the disk is now 1000GB:</p>

<pre><code class="bash">$ lsblk
NAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
xvda    202:0    0 1000G  0 disk
xvda1 202:1    0 1000G  0 part /
</code></pre>

<p>But our partition is still 100GB:</p>

<pre><code class="bash">$ df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        7.9G   60K  7.9G   1% /dev
tmpfs           7.9G     0  7.9G   0% /dev/shm
/dev/xvda1       99G   32G   67G  32% /
</code></pre>

<p>We need to use <code>growpart</code> and <code>resize2fs</code> to resize our partition:</p>

<pre><code class="bash">$ sudo growpart /dev/xvda 1
CHANGED: disk=/dev/xvda partition=1: start=4096 old: size=209711070,end=209715166 new: size=2097147870,end=2097151966
</code></pre>



<pre><code class="bash">$ sudo resize2fs /dev/xvda1
resize2fs 1.42.12 (29-Aug-2014)
Filesystem at /dev/xvda1 is mounted on /; on-line resizing required
old_desc_blocks = 7, new_desc_blocks = 63
The filesystem on /dev/xvda1 is now 262143483 (4k) blocks long.
</code></pre>

<p><strong>Note:</strong> If you are using XFS as your filesystem type, you will need to use <code>xfs_growfs</code> instead of <code>resize2fs</code>. (Thanks Donovan).</p>

<p>Example using XFS shown below:</p>

<pre><code class="bash">$ sudo xfs_growfs /dev/xvda1
</code></pre>

<p><strong>Note:</strong> If you are using nvme, it will look like this:</p>

<pre><code>$ sudo lsblk
NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
nvme1n1     259:0    0  160G  0 disk
-nvme1n1p1  259:1    0   80G  0 part /data

$ sudo growpart /dev/nvme1n1 1
CHANGED: partition=1 start=2048 old: size=167770112 end=167772160 new: size=335542239 end=335544287

$ resize2fs /dev/nvme1n1p1
resize2fs 1.45.5 (07-Jan-2020)

$ sudo lsblk
NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
nvme1n1     259:0    0  160G  0 disk
-nvme1n1p1  259:1    0  160G  0 part /data
</code></pre>

<p>Now we will have a resized partition to 100GB:</p>

<pre><code class="bash">$ df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        7.9G   60K  7.9G   1% /dev
tmpfs           7.9G     0  7.9G   0% /dev/shm
/dev/xvda1      985G   33G  952G   4% /
</code></pre>

<p>Resources:</p>

<ul>
<li><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/recognize-expanded-volume-linux.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/recognize-expanded-volume-linux.html</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
