<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Elasticsearch | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/elasticsearch/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2019-06-12T01:51:06-04:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Setup a Logstash Server for Amazon Elasticsearch Service and Auth With IAM]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/06/04/setup-a-logstash-server-for-amazon-elasticsearch-service-and-auth-with-iam/"/>
    <updated>2019-06-04T17:46:27-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/06/04/setup-a-logstash-server-for-amazon-elasticsearch-service-and-auth-with-iam</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/59209960-ca872100-8bac-11e9-8672-8c6af502afe0.png" alt="logstash" /></p>

<p>As many of you might know, when you deploy a ELK stack on Amazon Web Services, you only get E and K in the ELK stack, which is Elasticsearch and Kibana. Here we will be dealing with Logstash on EC2.</p>

<h2>What will we be doing</h2>

<p>In this tutorial we will setup a Logstash Server on EC2, setup a IAM Role and Autenticate Requests to Elasticsearch with an IAM Role, setup Nginx so that logstash can ship logs to Elasticsearch.</p>

<p>I am not fond of working with access key&rsquo;s and secret keys, and if I can stay away from handling secret information the better. So instead of creating a access key and secret key for logstash, we will instead create a IAM Policy that will allow the actions to Elasticsearch, associate that policy to an IAM Role, set EC2 as a trusted entity and strap that IAM Role to the EC2 Instance.</p>

<p>Then we will allow the IAM Role ARN to the Elasticsearch Policy, then when Logstash makes requests against Elasticsearch, it will use the IAM Role to assume temporary credentials to authenticate. That way we don&rsquo;t have to deal with keys. But I mean you can create access keys if that is your preferred method, I&rsquo;m just not a big fan of keeping secret keys.</p>

<p>The benefit of authenticating with IAM, allows you to remove a reverse proxy that is another hop to the path of your target.</p>

<h2>Create the IAM Policy:</h2>

<p>Create a IAM Policy that will allow actions to Elasticsearch:</p>

<pre><code>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "es:ESHttpHead",
                "es:ESHttpPost",
                "es:ESHttpGet",
                "es:ESHttpPut"
            ],
            "Resource": "arn:aws:es:eu-west-1:0123456789012:domain/my-es-domain"
        }
    ]
}
</code></pre>

<p>Create Role logstash-system-es with &ldquo;ec2.amazonaws.com&rdquo; as trusted entity in trust the relationship and associate the above policy to the role.</p>

<h2>Authorize your Role in Elasticsearch Policy</h2>

<p>Head over to your Elasticsearch Domain and configure your Elasticsearch Policy to include your IAM Role to grant requests to your Domain:</p>

<pre><code>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "arn:aws:iam::0123456789012:role/logstash-system-es"
        ]
      },
      "Action": "es:*",
      "Resource": "arn:aws:es:eu-west-1:0123456789012:domain/my-es-domain/*"
    }
  ]
}
</code></pre>

<h2>Install Logstash on EC2</h2>

<p>I will be using Ubuntu Server 18. Update the repositories and install dependencies:</p>

<pre><code>$ apt update &amp;&amp; apt upgrade -y
$ apt install build-essential apt-transport-https -y
$ wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
$ echo "deb https://artifacts.elastic.co/packages/6.x/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-6.x.list
$ apt update
</code></pre>

<p>As logstash requires Java, install the the Java OpenJDK Runtime Environment:</p>

<pre><code>$ apt install default-jre -y
</code></pre>

<p>Verify that Java is installed:</p>

<pre><code>$ java -version
openjdk version "11.0.3" 2019-04-16
OpenJDK Runtime Environment (build 11.0.3+7-Ubuntu-1ubuntu218.04.1)
OpenJDK 64-Bit Server VM (build 11.0.3+7-Ubuntu-1ubuntu218.04.1, mixed mode, sharing)
</code></pre>

<p>Now, install logstash and enable the service on boot:</p>

<pre><code>$ apt install logstash -y
$ systemctl enable logstash.service
$ service logstash stop
</code></pre>

<h2>Install the Amazon ES Logstash Output Plugin</h2>

<p>For us to be able to authenticate using IAM, we should use the Amazon-ES Logstash Output Plugin. Update and install the plugin:</p>

<pre><code>$ /usr/share/logstash/bin/logstash-plugin update
$ /usr/share/logstash/bin/logstash-plugin install logstash-output-amazon_es
</code></pre>

<h2>Configure Logstash</h2>

<p>I like to split up my configuration in 3 parts, (input, filter, output).</p>

<p>Let&rsquo;s create the input configuration: <code>/etc/logstash/conf.d/10-input.conf</code></p>

<pre><code>input {
  file {
    path =&gt; "/var/log/nginx/access.log"
    start_position =&gt; "beginning"
  }
}
</code></pre>

<p>Our filter configuration: <code>/etc/logstash/conf.d/20-filter.conf</code></p>

<pre><code>filter {
  grok {
    match =&gt; { "message" =&gt; "%{HTTPD_COMMONLOG}" }
  }
  mutate {
    add_field =&gt; {
      "custom_field1" =&gt; "hello from: %{host}"
    }
  }
}
</code></pre>

<p>And lastly, our output configuration: <code>/etc/logstash/conf.d/30-outputs.conf</code>:</p>

<pre><code>output {
  amazon_es {
      hosts =&gt; ["my-es-domain.abcdef.eu-west-1.es.amazonaws.com"]
      index =&gt; "new-logstash-%{+YYYY.MM.dd}"
      region =&gt; "eu-west-1"
      aws_access_key_id =&gt; ''
      aws_secret_access_key =&gt; ''
  }
}
</code></pre>

<p>Note that the <code>aws_</code> directives has been left empty as that seems to be the way it needs to be set when using roles. Authentication will be assumed via the Role which is associated to the EC2 Instance.</p>

<p>If you are using access keys, you can populate them there.</p>

<h2>Start Logstash</h2>

<p>Start logstash:</p>

<pre><code>$ service logstash start
</code></pre>

<p>Tail the logs to see if logstash starts up correctly, it should look more or less like this:</p>

<pre><code>$ tail -f /var/log/logstash/logstash-plain.log

[2019-06-04T16:38:12,087][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=&gt;"6.8.0"}
[2019-06-04T16:38:14,480][INFO ][logstash.pipeline        ] Starting pipeline {:pipeline_id=&gt;"main", "pipeline.workers"=&gt;2, "pipeline.batch.size"=&gt;125, "pipeline.batch.delay"=&gt;50}
[2019-06-04T16:38:15,226][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=&gt;{:removed=&gt;[], :added=&gt;[https://search-my-es-domain-xx.eu-west-1.es.amazonaws.com:443/]}}
[2019-06-04T16:38:15,234][INFO ][logstash.outputs.elasticsearch] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=&gt;https://search-my-es-domain-xx.eu-west-1.es.amazonaws.com:443/, :path=&gt;"/"}
</code></pre>

<h2>Install Nginx</h2>

<p>As you noticed, I have specified <code>/var/log/nginx/access.log</code> as my input file for logstash, as we will test logstash by shipping nginx access logs to Elasticsearch Service.</p>

<p>Install Nginx:</p>

<pre><code>$ apt install nginx -y
</code></pre>

<p>Start the service:</p>

<pre><code>$ systemctl restart nginx 
$ systemctl enable nginx
</code></pre>

<p>Make a GET request on your Nginx Web Server and inspect the log on Kibana, where it should look like this:</p>

<p><img src="https://user-images.githubusercontent.com/567298/58917559-4dc8f280-8727-11e9-9e9d-7950217abe34.png" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Install Elasticsearch With Ansible Tutorial]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/04/06/install-elasticsearch-with-ansible-tutorial/"/>
    <updated>2019-04-06T15:45:09-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/04/06/install-elasticsearch-with-ansible-tutorial</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/55700285-f3cdda00-59ce-11e9-9c00-a05b9d469e23.png" alt="" /></p>

<p>In this tutorial we will install a elasticsearch cluster with ansible (well rather a node)</p>

<p>Our inventory:</p>

<pre><code>$ cat inventory.ini
[newes]
esnewnode

[newes:vars]
ansible_python_interpreter=/usr/bin/python3
</code></pre>

<p>Our playbook to bootstrap our nodes with Python:</p>

<pre><code>$ cat bootstrap-python.yml
---
- hosts: newes
  gather_facts: False

  tasks:
  - name: install python
    raw: test -e /usr/bin/python || ( apt update &amp;&amp; apt install python -y )
</code></pre>

<p>Our playbook to provision users:</p>

<pre><code>$ cat provision-users.yml
---
# Provisions User on Nodes
# Setup Passwordless SSH from Jumpbox
# Install Packages using APT
- name: bootstrap python
  hosts: newes
  roles:
    - bootstrap-python

- name: setup pre-requisites
  hosts: newes
  roles:
    - create-sudo-user
    - install-modules
    - configure-hosts-file

#- name: setup ruan user on the nodes
#  become: yes
#  become_user: ruan
#  hosts: admin
#  roles:
#    - configure-admin

- name: copy public key to nodes
  become: yes
  become_user: ruan
  hosts: newes
  roles:
    - copy-keys

- name: install elasticsearch
  hosts: newes
  roles:
    - elasticsearch
</code></pre>

<p>Our roles that will be included in our playbooks from above:</p>

<pre><code>$ cat roles/create-sudo-user/tasks/main.yml
---
- name: Create Sudo User
  user: name=ruan
        groups=sudo
        shell=/bin/bash
        generate_ssh_key=no
        state=present

- name: Set Passwordless SSH Access for ruan user
  copy: src=sudoers
        dest=/etc/sudoers.d
        mode=0440
</code></pre>

<p>Sudoers file for the create sudo role:</p>

<pre><code>$ cat roles/create-sudo-user/files/sudoers
ruan ALL=(ALL) NOPASSWD:ALL
</code></pre>

<p>The role to install all the apt packages:</p>

<pre><code>$ cat roles/install-modules/tasks/main.yml
---
- name: Install Packages
  apt: name={{ item }} state=latest update_cache=yes
  with_items:
    - apt-transport-https
    - ntp
    - python
    - tcpdump
    - wget
    - openssl
    - curl
</code></pre>

<p>Role to configure hosts file:</p>

<pre><code>$ cat roles/configure-hosts-file/tasks/main.yml
---
- name: Configure Hosts File
  lineinfile: path=/etc/hosts regexp='.*{{ item }}$' line="{{ hostvars[item].ansible_default_ipv4.address }} {{item}}" state=present
  when: hostvars[item].ansible_default_ipv4.address is defined
  with_items: "{{ groups['newes'] }}"
</code></pre>

<p>The role to copy the ssh keys:</p>

<pre><code>$ cat roles/copy-keys/tasks/main.yml
---
- name: Copy Public Key to Other Hosts
  become: true
  become_user: ruan
  copy:
    src: /tmp/id_rsa.pub
    dest: /tmp/id_rsa.pub
    mode: 0644
- name: Append Public key in authorized_keys file
  authorized_key:
    user: ruan
    state: present
    key: "{{ lookup('file', '/tmp/id_rsa.pub') }}"
</code></pre>

<p>The role to install elasticsearch:</p>

<pre><code>$ cat roles/elasticsearch/tasks/main.yml
---
- name: get apt repo key
  apt_key:
    url: https://artifacts.elastic.co/GPG-KEY-elasticsearch
    state: present

- name: install apt repo
  apt_repository:
    repo: deb https://artifacts.elastic.co/packages/6.x/apt stable main
    state: present
    filename: elastic-6.x.list
    update_cache: yes

- name: install java
  apt:
    name: openjdk-8-jre
    state: present
    update_cache: yes

- name: install elasticsearch
  apt:
    name: elasticsearch
    state: present
    update_cache: yes

- name: reload systemd config
  systemd: daemon_reload=yes

- name: enable service elasticsearch and ensure it is not masked
  systemd:
    name: elasticsearch
    enabled: yes
    masked: no

- name: ensure elasticsearch is running
  systemd: state=started name=elasticsearch
</code></pre>

<h2>Deploy Elasticsearch</h2>

<p>Bootstrap python then deploy elasticsearch:</p>

<pre><code>$ ansible-playbook -i inventory.ini -u root bootstrap-python.yml
$ ansible-playbook -i inventory.ini -u root provision-users.yml
</code></pre>

<p>Test out elasticsearch:</p>

<pre><code>$ curl http://127.0.0.1:9200/
{
  "name" : "Z52AEZ7",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "fUiYVjsSQpCbo9QKEiuvaA",
  "version" : {
    "number" : "6.3.0",
    "build_flavor" : "default",
    "build_type" : "deb",
    "build_hash" : "424e937",
    "build_date" : "2018-06-11T23:38:03.357887Z",
    "build_snapshot" : false,
    "lucene_version" : "7.3.1",
    "minimum_wire_compatibility_version" : "5.6.0",
    "minimum_index_compatibility_version" : "5.0.0"
  },
  "tagline" : "You Know, for Search"
}
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Elasticsearch Templates Tutorial]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/04/06/elasticsearch-templates-tutorial/"/>
    <updated>2019-04-06T15:41:53-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/04/06/elasticsearch-templates-tutorial</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/53352581-b3892f80-392b-11e9-9532-5db5cbfc8f1c.jpg" alt="elasticsearch" /></p>

<p>Elasticsearch Index templates allow you to define templates that will automatically be applied on index creation time. The templates can include both settings and mappings..</p>

<h2>What are we doing?</h2>

<p>We want to create a template on how we would a target index to look like. It should consist of 1 primary shard and 2 replica shards and we want to update the mapping that we can make use of text and keyword string fields.</p>

<p>So then whenever we create an index which matches our template, the template will be applied on index creation.</p>

<h2>String Fields</h2>

<p>We will make use of the following string fields in our mappings which will be included in our templates:</p>

<p><strong>Text</strong>:</p>

<p>A field to index full-text values, such as the body of an email or the description of a product. These fields are analyzed, that is they are passed through an analyzer to convert the string into a list of individual terms before being indexed. The analysis process allows Elasticsearch to search for individual words within each full text field</p>

<p><strong>Keyword"</strong>:</p>

<p>A field to index structured content such as email addresses, hostnames, status codes, zip codes or tags.</p>

<p>They are typically used for filtering (Find me all blog posts where status is published), for sorting, and for aggregations. Keyword fields are only searchable by their exact value</p>

<h2>Note about templates:</h2>

<p>Couple of things to keep in mind:</p>

<pre><code>1. Templates gets referenced on index creation and does not affect existing indexes
2. When you update a template, you need to specify the exact template, the payload overwrites the whole template
</code></pre>

<p>View your current templates in your cluster:</p>

<pre><code>$ curl -XGET http://localhost:9200/_cat/templates?v
name                          index_patterns             order      version
.monitoring-kibana            [.monitoring-kibana-6-*]   0          6020099
filebeat-6.3.1                [filebeat-6.3.1-*]         1
</code></pre>

<p>Create the template <code>foobar_docs</code> which will match any indexes matching <code>foo-*</code> and <code>bar-*</code> which will inherit index settings of 1 primary shards and 2 replica shards and also apply a mapping template shown below:</p>

<pre><code>$ curl -H 'Content-Type: application/json' -XPUT http://localhost:9200/_template/foobar_docs -d '
{
  "index_patterns": [
    "foo-*", "bar-*"
  ], 
  "settings": {
    "number_of_shards": 1, 
    "number_of_replicas": 2
  }, 
  "mappings": {
    "type1": {
      "_source": {"enabled": true}, 
      "properties": {"created_at": {"type": "date"}, 
      "title": {"type": "text"}, 
      "status": {"type": "keyword"}, 
      "content": {"type":"text"}, 
      "first_name": {"type": "keyword"}, 
      "last_name": {"type": "keyword"}, 
      "age": {"type":"integer"}, 
      "registered": {"type": "boolean"}
      }
    }
  }
}'
{"acknowledged":true}
</code></pre>

<p>View the template from the api:</p>

<pre><code>$ curl -XGET http://localhost:9200/_cat/templates/foobar_docs?v
name        index_patterns order version
foobar_docs [foo-*, bar-*] 0
</code></pre>

<p>Create a index that will match the templates definition:</p>

<pre><code>$ curl -H 'Content-Type: application/json' -XPUT http://localhost:9200/test-2018.07.20
{"acknowledged":true,"shards_acknowledged":true,"index":"test-2018.07.20"}
</code></pre>

<p>Verify that the index has been created:</p>

<pre><code>$ curl -XGET http://localhost:9200/_cat/indices/test-2018.07.20?v
health status index           uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   test-2018.07.20 -5XOfl0GTEGeHycTwL51vQ   5   1          0            0        2kb          1.1kb
</code></pre>

<p>We can also inspect the template like shown below:</p>

<pre><code>$ curl -XGET http://localhost:9200/_template/foobar_docs?pretty
{
  "foobar_docs" : {
    "order" : 0,
    "index_patterns" : [
      "foo-*",
      "bar-*"
    ],
    "settings" : {
      "index" : {
        "number_of_shards" : "1",
        "number_of_replicas" : "2"
      }
    },
    "mappings" : {
      "type1" : {
        "_source" : {
          "enabled" : true
        },
        "properties" : {
          "created_at" : {
            "type" : "date"
          },
          "title" : {
            "type" : "text"
          },
          "status" : {
            "type" : "keyword"
          },
          "content" : {
            "type" : "text"
          },
          "first_name" : {
            "type" : "keyword"
          },
          "last_name" : {
            "type" : "keyword"
          },
          "age" : {
            "type" : "integer"
          },
          "registered" : {
            "type" : "boolean"
          }
        }
      }
    },
    "aliases" : { }
  }
}
</code></pre>

<p>Ingest a document to your index:</p>

<pre><code>$ curl -H 'Content-Type: application/json' -XPOST http://localhost:9200/foo-2018.07.20/type1/ -d '
{
  "title": "this is a post", 
  "status": "active", 
  "content": "introduction post", 
  "first_name": "ruan", 
  "last_name": "bekker", 
  "age": "31", 
  "registered": "true"
}'
</code></pre>

<p>Run a search against your elasticsearch index to view the data:</p>

<pre><code>$ curl -XGET http://localhost:9200/foo-2018.07.20/_search?pretty
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "foo-2018.07.20",
        "_type" : "type1",
        "_id" : "ZYfotmQB9mQGWzJT7W2y",
        "_score" : 1.0,
        "_source" : {
          "title" : "this is a post",
          "status" : "active",
          "content" : "introduction post",
          "first_name" : "ruan",
          "last_name" : "bekker",
          "age" : "31",
          "registered" : "true"
        }
      }
    ]
  }
}
</code></pre>

<p>Create another document:</p>

<pre><code>$ curl -H 'Content-Type: application/json' -XPOST http://localhost:9200/foo-2018.07.20/type1/ -d '
{
  "created_at": 1532077144, 
  "title": "this is a another post", 
  "status": "ae", 
  "content": "introduction post", 
  "first_name": "stefan", 
  "last_name": "bester", 
  "age": 34, 
  "registered": "true"
}'
</code></pre>

<p>As you guessed, executing another search against elasticsearch shows us both documents:</p>

<pre><code>$ curl -XGET http://localhost:9200/foo-2018.07.20/_search?pretty
{
  "took" : 0,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "foo-2018.07.20",
        "_type" : "type1",
        "_id" : "ZYfotmQB9mQGWzJT7W2y",
        "_score" : 1.0,
        "_source" : {
          "title" : "this is a post",
          "status" : "active",
          "content" : "introduction post",
          "first_name" : "ruan",
          "last_name" : "bekker",
          "age" : "31",
          "registered" : "true"
        }
      },
      {
        "_index" : "foo-2018.07.20",
        "_type" : "type1",
        "_id" : "rofrtmQB9mQGWzJTxnvp",
        "_score" : 1.0,
        "_source" : {
          "created_at" : 1532077144,
          "title" : "this is a another post",
          "status" : "active",
          "content" : "introduction post",
          "first_name" : "stefan",
          "last_name" : "bester",
          "age" : 34,
          "registered" : "true"
        }
      }
    ]
  }
}
</code></pre>

<p>Let&rsquo;s run a search query for any documents matching people with the age between <strong>30</strong> and <strong>40</strong>:</p>

<pre><code>$ curl -H 'Content-Type: application/json' -XGET http://localhost:9200/foo-2018.07.20/_search?pretty -d '{"query": {"range": {"age": {"gte": 30, "lte": 40}}}}'
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "foo-2018.07.20",
        "_type" : "type1",
        "_id" : "ZYfotmQB9mQGWzJT7W2y",
        "_score" : 1.0,
        "_source" : {
          "title" : "this is a post",
          "status" : "active",
          "content" : "introduction post",
          "first_name" : "ruan",
          "last_name" : "bekker",
          "age" : "31",
          "registered" : "true"
        }
      },
      {
        "_index" : "foo-2018.07.20",
        "_type" : "type1",
        "_id" : "rofrtmQB9mQGWzJTxnvp",
        "_score" : 1.0,
        "_source" : {
          "created_at" : 1532077144,
          "title" : "this is a another post",
          "status" : "active",
          "content" : "introduction post",
          "first_name" : "stefan",
          "last_name" : "bester",
          "age" : 34,
          "registered" : "true"
        }
      }
    ]
  }
}
</code></pre>

<p>Search for people with the age between <strong>32</strong> and <strong>40</strong>:</p>

<pre><code>$ curl -H 'Content-Type: application/json' -XGET http://localhost:9200/foo-2018.07.20/_search?pretty -d '{"query": {"range": {"age": {"gte": 32, "lte": 40}}}}'
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "foo-2018.07.20",
        "_type" : "type1",
        "_id" : "rofrtmQB9mQGWzJTxnvp",
        "_score" : 1.0,
        "_source" : {
          "created_at" : 1532077144,
          "title" : "this is a another post",
          "status" : "active",
          "content" : "introduction post",
          "first_name" : "stefan",
          "last_name" : "bester",
          "age" : 34,
          "registered" : "true"
        }
      }
    ]
  }
}
</code></pre>

<p>Let&rsquo;s say we want to update our template with <code>refresh_interval</code>, primary shards of 2 and replicas of 1 settings:</p>

<pre><code>$ curl -H 'Content-Type: application/json' -XPUT http://localhost:9200/_template/foobar_docs -d '
{
  "index_patterns": ["foo-*", "bar-*"], 
  "settings": {"number_of_shards": 2, "number_of_replicas": 1, "refresh_interval": "15s"}
}'
</code></pre>

<p>View the template, as you can see the target template will look exactly like the data body that we are posting to the template api:</p>

<pre><code>$ curl -XGET http://localhost:9200/_template/foobar_docs?pretty
{
  "foobar_docs" : {
    "order" : 0,
    "index_patterns" : [
      "foo-*",
      "bar-*"
    ],
    "settings" : {
      "index" : {
        "number_of_shards" : "2",
        "number_of_replicas" : "1",
        "refresh_interval" : "15s"
      }
    },
    "mappings" : { },
    "aliases" : { }
  }
}
</code></pre>

<p>View our current index, as you can see the index is unaffected of the template change as only new indexes will retrieve the update of the template:</p>

<pre><code>$ curl -XGET http://localhost:9200/_cat/indices/foo-2018.07.20?v
health status index          uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   foo-2018.07.20 ol1pGugrQCKd0xES4R6oFg   1   2          2            0     20.4kb         10.2kb
</code></pre>

<p>Create a new index to verify that the template&rsquo;s config is pulled into the new index:</p>

<pre><code>$ curl -H 'Content-Type: application/json' -XPUT http://localhost:9200/foo-2018.07.20-new
</code></pre>

<p>View the elasticsearch indexes to verify the behavior:</p>

<pre><code>$ curl -XGET http://localhost:9200/_cat/indices/foo-2018.07.*?v
health status index              uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   foo-2018.07.20     ol1pGugrQCKd0xES4R6oFg   1   2          2            0     20.4kb         10.2kb
green  open   foo-2018.07.20-new g6Ii8jtKRFa1zDVB2IsDBQ   2   1          0            0       920b           460b
</code></pre>

<p>Delete the indexes:</p>

<pre><code>$ curl -XDELETE http://localhost:9200/foo-*
{"acknowledged":true}
</code></pre>

<p>Delete the templates:</p>

<pre><code>$ curl -XDELETE 'http://localhost:9200/_template/foobar_docs'
{"acknowledged":true}
</code></pre>

<p>Verify that the templates are gone:</p>

<pre><code>$ curl -XGET http://localhost:9200/_cat/templates/foobar_docs?v
name index_patterns order version
</code></pre>

<h2>Resources:</h2>

<p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-templates.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-templates.html</a>
<a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.3/mapping-types.html">https://www.elastic.co/guide/en/elasticsearch/reference/6.3/mapping-types.html</a>
<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-range-query.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-range-query.html</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Reindex Your Elasticsearch Indexes Tutorial]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/04/06/reindex-your-elasticsearch-indexes-tutorial/"/>
    <updated>2019-04-06T15:37:18-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/04/06/reindex-your-elasticsearch-indexes-tutorial</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/53352581-b3892f80-392b-11e9-9532-5db5cbfc8f1c.jpg" alt="" /></p>

<p>At times you may find that the indexes in your cluster are not queried that often but you still want them around. But you also want to reduce the resource footprint by reducing the number of shards, and perhaps increase the refresh interval.</p>

<p>For refresh interval, if new data comes in and we dont care to have it available near real time, we can set the refresh interval for example to 60 seconds, so the index will only have the data available every 60 seconds. (default: 1s)</p>

<h2>Reindexing Elasticsearch Indexes</h2>

<p>In this example we will use the scenario where we have daily indexes with 5 primary shards and 1 set of replicas and we would like to create a weekly index with 1 primary shard, 1 replica and the refresh interval of 60 seconds, and reindex the previous weeks data into our weekly index.</p>

<p>Create the target weekly index with the mentioned configuration:</p>

<pre><code>$ curl -H "Content-Type: application/json" -XPUT 'http://127.0.0.1:9200/my-index-2019.01.01-07' -d '
{
    "settings": {
        "number_of_shards": "1",
        "number_of_replicas": "1",
        "refresh_interval" : "60s"
    }
}
'
</code></pre>

<p>Ensure the index exist:</p>

<pre><code>$ curl -s -XGET 'http://127.0.0.1:9200/_cat/indices/my-index-2019.01.01*?v'
health status index                    uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   my-index-2019.01.01      wbFEJCApSpSlbOXzb1Tjxw   5   1      22007            0      6.6mb          3.2mb
green  open   my-index-2019.01.02      cbDmJR7pbpRT3O2x46fj20   5   1      28031            0      7.2mb          3.4mb
..
green  open   my-index-2019.01.01-07   mJR7pJ9O4T3O9jzyI943ca   1   1          0            0       466b           233b
</code></pre>

<p>Create the reindex job, specify the source indexes and the destination index where the data must be reindexed to:</p>

<pre><code>$ curl -s -H 'Content-Type: application/json' -XPOST 'http://127.0.0.1:9200/_reindex' -d '
{
    "source": {
        "index": [
            "my-index-2019.01.01",
            "my-index-2019.01.02",
            "my-index-2019.01.03",
            "my-index-2019.01.04",
            "my-index-2019.01.05",
            "my-index-2019.01.06",
            "my-index-2019.01.07"
        ]
    },
    "dest": {
        "index": "my-index-2019.01.01-07"
    }
}
'
</code></pre>

<p>You can use the tasks api to monitor the progress:</p>

<pre><code>$ curl -s -XGET 'http://127.0.0.1:9200/_cat/tasks?'
indices:data/write/bulk        -3MIFskURPKxd1tg8P2j0w:912621270 -                                transport 1538459598188 22:53:18 3.1ms       x.x.x.x -3MIFsk
indices:data/write/bulk[s]     -3MIFskURPKxd1tg8P2j0w:912621271 -3MIFskURPKxd1tg8P2j0w:816648230 transport 1538459598188 22:53:18 3.1ms       x.x.x.x -3MIFsk
</code></pre>

<p>You manipulate the output of the tasks api by either fetching specific actions:</p>

<pre><code>$ curl -s -XGET 'http://127.0.0.1:9200/_tasks?actions=*data/write/reindex&amp;detailed&amp;pretty'
</code></pre>

<p>Or viewing detailed output:</p>

<pre><code>$ curl -s -XGET 'http://127.0.0.1:9200/_cat/tasks?detailed' | grep 'indices:data/write/reindex'
indices:data/write/reindex     IvoqWoUqSgGCQ0ELG21nhg:740560815 -                                transport 1538462294714 23:38:14 1.7m        x.x.x.x IvoqWoU reindex from [my-index-2019.01.01] to [my-index-2019.01.01-07]
</code></pre>

<p>Or you could get the json response:</p>

<pre><code>$ curl -s -XGET 'http://127.0.0.1:9200/_tasks?actions=*data/write/reindex&amp;detailed&amp;pretty'
{
  "nodes" : {
    "xx" : {
      "name" : "xx",
      "roles" : [ "data", "ingest" ],
      "tasks" : {
        "xx:876452606" : {
          "node" : "xx",
          "id" : 776452606,
          "type" : "transport",
          "action" : "indices:data/write/reindex",
          "status" : {
            "total" : 4785475,
            "updated" : 0,
            "created" : 234000,
            "deleted" : 0,
            "batches" : 235,
            "version_conflicts" : 0,
            "noops" : 0,
            "retries" : {
              "bulk" : 0,
              "search" : 0
            },
            "throttled_millis" : 0,
            "requests_per_second" : -1.0,
            "throttled_until_millis" : 0
          },
          "description" : "reindex from [my-index-2019.01.07] to [my-index-2019.01.01-07]",
          "start_time_in_millis" : 1538462901120,
          "running_time_in_nanos" : 64654161339,
          "cancellable" : true
        }
      }
    }
  }
}
</code></pre>

<p>Anyways, moving along. Reindex jobs will always be listed as a <code>data/write/reindex</code> action, so we can count the output:</p>

<pre><code>$ curl -s -XGET 'http://127.0.0.1:9200/_cat/tasks?'  | grep 'data/write/reindex' | wc -l
</code></pre>

<p>If the response is 0 then all the tasks completed and we can have a look at our index again:</p>

<pre><code>$ curl -s -XGET 'http://127.0.0.1:9200/_cat/indices/my-index-2019.01.0*?v'
health status index                    uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   my-index-2019.01.01      wbFEJCApSpSlbOXzb1Tjxw   5   1      22007            0      6.6mb          3.2mb
green  open   my-index-2019.01.02      cbDmJR7pbpRT3O2x46fj20   5   1      28031            0      7.2mb          3.4mb
..
green  open   my-index-2019.01.01-07   mJR7pJ9O4T3O9jzyI943ca   1   1     322007            0     45.9mb         22.9mb
</code></pre>

<p>Now that we can verify that the reindex tasks finished and we can see the aggregated result in our target index, we can delete our source indexes:</p>

<pre><code>$ curl -XDELETE 'http://127.0.0.1:9200/my-index-2019.01.01,my-index-2019.01.02,my-index-2019.01.03,my-index-2019.01.04,my-index-2019.01.05,my-index-2019.01.06,my-index-2019.01.07'
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Shrink Your Elasticsearch Index by Reducing the Shard Count With the Shards API]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/04/06/shrink-your-elasticsearch-index-by-reducing-the-shard-count-with-the-shards-api/"/>
    <updated>2019-04-06T15:33:48-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/04/06/shrink-your-elasticsearch-index-by-reducing-the-shard-count-with-the-shards-api</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/53352581-b3892f80-392b-11e9-9532-5db5cbfc8f1c.jpg" alt="elasticsearch" /></p>

<p>Resize your Elasticsearch Index with fewer Primary Shards by using the Shrink API.</p>

<p>In Elasticsearch, every index consists of multiple shards and every shard in your elasticsearch cluster contributes to the usage of your cpu, memory, file descriptors etc. This definitely helps for performance in parallel processing. As for an example with time series data, you would write and read a lot to an index with ie the current date.</p>

<p>If that index drops in requests and only read from the index every now and then, we dont need that many shards anymore and if we have multiple indexes, they may build up and take up unessacary compute power.</p>

<p>For a scenario where we want to reduce the size of our indexes, we can use the Shrink API to reduce the number of primary shards.</p>

<h2>The Shrink API</h2>

<p>The shrink index API allows you to shrink an existing index into a new index with fewer primary shards. The requested number of primary shards in the target index must be a factor of the number of shards in the source index. For example an index with 8 primary shards can be shrunk into 4, 2 or 1 primary shards or an index with 15 primary shards can be shrunk into 5, 3 or 1. If the number of shards in the index is a prime number it can only be shrunk into a single primary shard. Before shrinking, a (primary or replica) copy of every shard in the index must be present on the same node.</p>

<p>Steps on Shrinking:</p>

<p>Create the target index with the same definition as the source index, but with a smaller number of primary shards.
Then it hard-links segments from the source index into the target index.
Finally, it recovers the target index as though it were a closed index which had just been re-opened.</p>

<h2>Reduce the Primary Shards of an Index.</h2>

<p>As you may know, you can only set the Primary Shards on Index Creation time and Replica Shards you can set on the fly.</p>

<p>In this example we have a source index: <code>my-index-2019.01.10</code> with 5 primary shards and 1 replica shard, which gives us 10 shards for that index, that we would like to shrink to an index named: <code>archive_my-index-2019.01.10</code> with 1 primary shard and 1 replica shard, which will give us 2 shards for that index.</p>

<p>Have a look at your index:</p>

<pre><code>$ curl -XGET "http://127.0.0.1:9200/_cat/indices/my-index-2019.01.*?v"
health status index                                     uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   my-index-2019.01.10                       xAijUTSevXirdyTZTN3cuA   5   1   80795533            0      5.9gb          2.9gb
green  open   my-index-2019.01.11                       yb8Cjy9eQwqde8mJhR_vlw   5   5   80590481            0      5.7gb          2.8gb
...
</code></pre>

<p>And have a look at the nodes, as we will relocate the shards to a specific node:</p>

<pre><code>$ curl http://127.0.0.1:9200/_cat/nodes?v
ip            heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
x.x.x.x             8          98   0    0.04    0.03     0.01 m         -      3E9yp60
x.x.x.x            65          99   4    0.43    0.23     0.36 di        -      znFrs18
</code></pre>

<p>In this demonstration we only have 2 nodes with a replication factor of 1, which means a index&rsquo;s shards will always reside on both nodes. In a case with more nodes, we need to ensure that we choose a node where a primary index reside on.</p>

<p>Look at the shards api, by passing the index name to get the index to shard allocation:</p>

<pre><code>$ curl http://127.0.0.1:9200/_cat/shards/my-index-2019.01.10?v'
index               shard prirep state   docs  store ip       node
my-index-2019.01.10 2     p      STARTED  193  101mb x.x.x.x  Lq9P7eP
my-index-2019.01.10 2     r      STARTED  193  101mb x.x.x.x  F5edOwK
my-index-2019.01.10 4     p      STARTED  197  101mb x.x.x.x  Lq9P7eP
my-index-2019.01.10 4     r      STARTED  197  101mb x.x.x.x  F5edOwK
my-index-2019.01.10 3     r      STARTED  184  101mb x.x.x.x  Lq9P7eP
my-index-2019.01.10 3     p      STARTED  184  101mb x.x.x.x  F5edOwK
my-index-2019.01.10 1     r      STARTED  180  101mb x.x.x.x  Lq9P7eP
my-index-2019.01.10 1     p      STARTED  180  101mb x.x.x.x  F5edOwK
my-index-2019.01.10 0     p      STARTED  187  101mb x.x.x.x  Lq9P7eP
my-index-2019.01.10 0     r      STARTED  187  101mb x.x.x.x  F5edOwK
</code></pre>

<p>Create the target index:</p>

<pre><code>$ curl -XPUT -H 'Content-Type: application/json' http://127.0.0.1:9200/archive_my-index-2019.01.10 -d '
{
    "settings": {
        "number_of_shards": "1",
        "number_of_replicas": "1"
    }
}
'
</code></pre>

<p>Set the index as read only and relocate every copy of shard to node we indentified in a previous step:</p>

<pre><code>$ curl -XPUT -H 'Content-Type: application/json' http://127.0.0.1:9200/my-index-2019.01.10/_settings -d '
{
    "settings": {
        "index.routing.allocation.require._name": "Lq9P7eP",
        "index.blocks.write": true
    }
}
'
</code></pre>

<p>Now shrink the source index (my-index-2019.01.10) to the target index (archive_my-index-2019.01.10):</p>

<pre><code>$ curl -XPOST -H 'Content-Type: application/json' http://127.0.0.1:9200/my-index-2019.01.10/_shrink/archive_my-index-2019.01.10
</code></pre>

<p>You can monitor the progress by using the Recovery API:</p>

<pre><code>$ curl -s -XGET "http://127.0.0.1:9200/_cat/recovery/my-index-2019.01.10?human&amp;detailed=true"
my-index-2019.01.10 0 23.3s peer done x.x.x.x  F5edOwK x.x.x.x Lq9P7eP n/a n/a 15 15 100.0% 15 635836677 635836677 100.0% 635836677 0 0 100.0%
my-index-2019.01.10 1 22s   peer done x.x.x.x  Lq9P7eP x.x.x.x Lq9P7eP n/a n/a 15 15 100.0% 15 636392649 636392649 100.0% 636392649 0 0 100.0%
my-index-2019.01.10 2 19.6s peer done x.x.x.x  F5edOwK x.x.x.x Lq9P7eP n/a n/a 15 15 100.0% 15 636809671 636809671 100.0% 636809671 0 0 100.0%
my-index-2019.01.10 3 21.5s peer done x.x.x.x  Lq9P7eP x.x.x.x Lq9P7eP n/a n/a 15 15 100.0% 15 636378870 636378870 100.0% 636378870 0 0 100.0%
my-index-2019.01.10 4 23.3s peer done x.x.x.x F5edOwK- x.x.x.x Lq9P7eP n/a n/a 15 15 100.0% 15 636545756 636545756 100.0% 636545756 0 0 100.0%
</code></pre>

<p>You can also pass aliases as your table columns for output:</p>

<pre><code>$ curl -s -XGET "http://127.0.0.1:9200/_cat/recovery/my-index-2019.01.10?v&amp;detailed=true&amp;h=index,shard,time,ty,st,shost,thost,f,fp,b,bp"
index                            shard time  ty   st   shost         thost        f  fp     b         bp
my-index-2019.01.10              0     23.3s peer done x.x.x.x x.x.x.x 15 100.0% 635836677 100.0%
...
</code></pre>

<p>When the job is done, have a look at your indexes:</p>

<pre><code>$ curl -XGET "http://127.0.0.1:9200/_cat/indices/*my-index-2019.01.10?v"
health status index                                     uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   archive_my-index-2019.01.10               PAijUTSeRvirdyTZTN3cuA   1   1   80795533            0      5.9gb          2.9gb
green  open   my-index-2019.01.10                       Cb8Cjy9CQwqde8mJhR_vlw   5   1   80795533            0      2.9gb          2.9gb
</code></pre>

<p>Remove the block on your old index in order to make it writable:</p>

<pre><code>$ curl -XPUT -H 'Content-Type: application/json' http://127.0.0.1:9200/my-index-2019.01.10/_settings" -d '
{
    "settings": {
        "index.routing.allocation.require._name": null,
        "index.blocks.write": null
    }
}
'
</code></pre>

<p>Delete the old index:</p>

<pre><code>$ curl -XDELETE -H 'Content-Type: application/json' http://127.0.0.1:9200/my-index-2019.01.10
</code></pre>

<p>Note:, On AWS Elasticsearch Service, if you dont remove the block and you trigger a redeployment, you will end up with something like this. Shard may still be constraint to a node.</p>

<pre><code>$ curl -s -XGET ${ES_HOST/_cat/allocation?v
shards disk.indices disk.used disk.avail disk.total disk.percent host          ip  node
     0           0b    51.2gb    956.5gb   1007.8gb            5 x.x.x.x  x.x.x.x  ap9Mx1R
     1        3.6gb    54.9gb    952.8gb   1007.8gb            5 x.x.x.x  x.x.x.x  PqmoQpN   &lt;-----------
     0           0b    51.2gb    956.5gb   1007.8gb            5 x.x.x.x  x.x.x.x  5p7x4Lc
     0           0b    51.2gb    956.5gb   1007.8gb            5 x.x.x.x  x.x.x.x  c8kniP3
     0           0b    51.2gb    956.5gb   1007.8gb            5 x.x.x.x  x.x.x.x  jPwlwsD
     0           0b    51.2gb    956.5gb   1007.8gb            5 x.x.x.x  x.x.x.x  ljos4mu
   481      904.1gb   990.3gb    521.3gb      1.4tb           65 x.x.x.x  x.x.x.x  qAF-gIU
   481      820.2gb   903.6gb    608.1gb      1.4tb           59 x.x.x.x  x.x.x.x  dR3sNwA
   481      824.6gb   909.1gb    602.6gb      1.4tb           60 x.x.x.x  x.x.x.x  fvL4A9X
   481      792.7gb   876.5gb    635.2gb      1.4tb           57 x.x.x.x  x.x.x.x  lk4svht
   481      779.2gb   864.4gb    647.3gb      1.4tb           57 x.x.x.x  x.x.x.x  uLsej9m
     0           0b    51.2gb    956.5gb   1007.8gb            5 x.x.x.x  x.x.x.x  yM4Ka9l
</code></pre>

<h2>Resources:</h2>

<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-shrink-index.html">https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-shrink-index.html</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
