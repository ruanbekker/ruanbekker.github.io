<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Elasticsearch | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/elasticsearch/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2019-04-02T13:39:50-04:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Setup Kibana Dashboards for Nginx Log Data to Understand the Behavior]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/04/02/setup-kibana-dashboards-for-nginx-log-data-to-understand-the-behavior/"/>
    <updated>2019-04-02T12:34:18-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/04/02/setup-kibana-dashboards-for-nginx-log-data-to-understand-the-behavior</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/55419050-44c77380-5574-11e9-988c-dae1e001f7bd.png" alt="kibana" /></p>

<p>In this tutorial we will setup a Basic Kibana Dashboard for a Web Server that is running a Blog on Nginx.</p>

<h2>What do we want to achieve?</h2>

<p>We will setup common visualizations to give us an idea on how our blog/website is doing.</p>

<p>In some situations we need to create visualizations to understand the behavior of our log data in order to answer these type of questions:</p>

<table>
<thead>
<tr>
<th> <strong>Number</strong> </th>
<th> <strong>Scenario</strong>                                                         </th>
</tr>
</thead>
<tbody>
<tr>
<td> 1          </td>
<td> Geographical map to see where people are connecting from             </td>
</tr>
<tr>
<td> 2          </td>
<td> Piechart that represents the percentage of cities accessing my blog  </td>
</tr>
<tr>
<td> 3          </td>
<td> Top 10 Most Accessed Pages                                           </td>
</tr>
<tr>
<td> 4          </td>
<td> Top 5 HTTP Status Codes                                              </td>
</tr>
<tr>
<td> 5          </td>
<td> Top 10 Pages that returned 404 Responses                             </td>
</tr>
<tr>
<td> 6          </td>
<td> The Top 10 User Agents                                               </td>
</tr>
<tr>
<td> 7          </td>
<td> Timeseries: Status Codes Over Time                                   </td>
</tr>
<tr>
<td> 8          </td>
<td> Timeseries: Successfull Website Hits over time                       </td>
</tr>
<tr>
<td> 9          </td>
<td> Counter with Website Hits                                            </td>
</tr>
<tr>
<td> 10         </td>
<td> Average Bytes Returned                                               </td>
</tr>
<tr>
<td> 11         </td>
<td> Tag Cloud with the City Names that Accessed my Blog                  </td>
</tr>
</tbody>
</table>


<h2>Pre-Requirements</h2>

<p>I am consuming my nginx access logs with filebeat and shipping them to elasticsearch. You can check out <a href="https://blog.ruanbekker.com/blog/2019/03/27/ship-your-logs-to-elasticsearch-with-filebeat/">this blogpost</a> to set that up.</p>

<p>The GeoIP Processor plugin is installed on elasticsearch to enrich our data with geographical information. You can check out <a href="https://blog.ruanbekker.com/blog/2018/09/12/using-the-geoip-processor-plugin-with-elasticsearch-to-enrich-your-location-based-data/">this blogpost</a> to setup geoip.</p>

<p>You can setup <a href="https://blog.ruanbekker.com/blog/2018/04/29/running-a-3-node-elasticsearch-cluster-with-docker-compose-on-your-laptop-for-testing/">Kibana and Elasticsearch on Docker</a> or setup a <a href="https://blog.ruanbekker.com/blog/2019/04/02/setup-a-5-node-highly-available-elasticsearch-cluster/">5 Node Elasticsearch Cluster</a></p>

<h2>Setup Kibana Visulizations</h2>

<p>Head over to Kibana, make sure that you have added the <code>filebeat-*</code> index patterns. If not, head over to Management -> Index Patterns -> Create Index -> Enter filebeat-* as you Index Pattern, select Next, select your @timestamp as your timestamp field, select create.</p>

<p>Now from the visualization section we will add 11 Visualizations. Everytime that you create a visualization, make sure that you select filebeat as your pattern (thats if you are using filebeat).</p>

<p>When configuring your visualization it will look like the configuration box from below:</p>

<p><img width="337" alt="image" src="https://user-images.githubusercontent.com/567298/55053831-1aeaea00-5066-11e9-93cf-edfd2ac98e44.png"></p>

<h2>Geomap: Map to see where people are connecting from</h2>

<p><img width="728" alt="kibana geomap" src="https://user-images.githubusercontent.com/567298/55258476-9cf83000-526b-11e9-8560-014098c435ee.png"></p>

<p>Select New Visualization: Coordinate Map</p>

<pre><code>  -&gt; Metrics, Value: Count. 
     Buckets, Geo Coordinates, Aggregation: Geohash, 
     Field: nginx.access.geoip.location. 
</code></pre>

<p>Save the visualization, in my case Nginx:GeoMap:Filebeat</p>

<h2>Piechart: Cities</h2>

<p>This can give us a quick overview on the percentage of people interested in our website grouped per city.</p>

<p><img width="688" alt="image" src="https://user-images.githubusercontent.com/567298/55258529-c44efd00-526b-11e9-900a-e93ed7eed11e.png"></p>

<p>Select New Visualization, Pie</p>

<pre><code> -&gt; Metrics: Slice Size, Aggregation: Count
 -&gt; Buckets: Split Slices, 
    Aggregation: Terms, Field: nginx.access.geoip.city_name, 
    Order by: metric: count, 
    Order: Descending, Size: 20
</code></pre>

<p>Save Visualization.</p>

<h2>Top 10 Accessed Pages</h2>

<p>Great for seeing which page is popular, and Kibana makes it easy to see which page is doing good over a specific time.</p>

<p><img width="750" alt="image" src="https://user-images.githubusercontent.com/567298/55258583-f2ccd800-526b-11e9-98b8-4f1da917cb52.png"></p>

<p>New Visualization: Vertical</p>

<pre><code>  -&gt; Metrics: Y-Axis, Aggregation: Count
  -&gt; Buckets: X-Axis, Aggregation: Terms, 
     Field: nginx.access.url, 
     Ordery by: Metric count, 
     Order: Descending, Size 10
</code></pre>

<p>I would like to remove <code>/rss</code> and <code>/</code> from my results, so in the search box:</p>

<pre><code>NOT (nginx.access.url:"/" OR nginx.access.url:"/rss/" OR nginx.access.url:"/subscribe/" OR nginx.access.url:*.txt)
</code></pre>

<p>Save Visualization.</p>

<h2>Top 5 HTTP Status Codes</h2>

<p>A Grouping of Status Codes (You should see more 200&rsquo;s) but its quick to identify when 404&rsquo;s spike etc.</p>

<p><img width="692" alt="image" src="https://user-images.githubusercontent.com/567298/55258695-44756280-526c-11e9-887c-bd58e9b25750.png"></p>

<p>Select new visualization: Vertical Bar</p>

<pre><code>  -&gt; Metrics: Y-Axis, Aggregation: Count
  -&gt; Buckets: X-Axis, Aggregation: Terms, 
     Field: nginx.access.response_code, 
     Ordery by: Metric count, 
     Order: Descending, Size 5
</code></pre>

<p>Save Visualization</p>

<h2>Top 404 Pages</h2>

<p>So when people are requesting pages that does not exist, it could most probably be bots trying to attack your site, or trying to gain access etc. This is a great view to see which ones are they trying and then you can handle it from there.</p>

<p><img width="699" alt="image" src="https://user-images.githubusercontent.com/567298/55259089-5b688480-526d-11e9-963d-936a61285507.png"></p>

<pre><code>  -&gt; Metrics: Y-Axis, Aggregation: Count
  -&gt; Buckets: X-Axis, Aggregation: Terms, 
     Terms, Field: nginx.access.url, 
     Order by: Metric count, 
     Order: Descending, Size 20
</code></pre>

<h2>Top 10 User Agents</h2>

<p>Some insights to see the top 10 browsers.</p>

<p><img width="611" alt="image" src="https://user-images.githubusercontent.com/567298/55258763-74246a80-526c-11e9-9873-da2766e4a518.png"></p>

<p>New Visualization: Data Table</p>

<pre><code>  -&gt; Metrics: Y-Axis, Aggregation: Count
  -&gt; Buckets: Split Rows, 
     Aggregation: Terms, 
     Field: nginx.access.user_agent.name, 
     Ordery by: Metric count, 
     Order: Descending, Size 10
</code></pre>

<p>Save Visualization</p>

<h2>Timeseries: Status Codes over Time</h2>

<p>With timeseries data its great to see when there was a spike in status codes, when you identify the time, you can further investigate why that happened.</p>

<p>New Visualization: Timelion</p>

<pre><code>.es(index=filebeat*, timefield='@timestamp', q=nginx.access.response_code:200).label('OK'), .es(index=filebeat*, timefield='@timestamp', q=nginx.access.response_code:404).label('Page Not Found')
</code></pre>

<h2>Timeseries: Successfull Website Hits over Time</h2>

<p>This is a good view to see how your website is serving traffic over time.</p>

<p><img width="734" alt="image" src="https://user-images.githubusercontent.com/567298/55258920-eac16800-526c-11e9-85ca-5ebff8978b2b.png"></p>

<p>New Visualization: Timelion</p>

<pre><code>.es(index=filebeat*, timefield='@timestamp', q=nginx.access.response_code:200).label('200')
</code></pre>

<h2>Count Metric: Website Hits</h2>

<p>A counter to see the number of website hits over time.</p>

<p><img width="606" alt="image" src="https://user-images.githubusercontent.com/567298/55258980-17757f80-526d-11e9-8664-91afda3db62c.png"></p>

<p>New Visualization: Metric</p>

<pre><code>  -&gt; Search Query: fields.blog_name:sysadmins AND nginx.access.response_code:200
  -&gt; Metrics: Y-Axis, Aggregation: Count
</code></pre>

<h2>Average Bytes Transferred</h2>

<p>Line chart with the amount of bandwidth being transferred.</p>

<p><img width="677" alt="image" src="https://user-images.githubusercontent.com/567298/55259036-3c69f280-526d-11e9-90fe-0853d2c0313d.png"></p>

<pre><code>New Visualization: Line

-&gt; Metrics: Y-Axis, Aggregation: Average, Field: nginx.access.body_sent.bytes
-&gt; Buckets: X-Axis, Aggregation: Date Histogram, Field: @timestamp
</code></pre>

<h2>Tag Cloud with Most Popular Cities</h2>

<p>I&rsquo;ve used cities here, but its a nice looking visualization to group the most accessed fields. With server logs you can use this for the usernames failed in ssh attempts for example.</p>

<p><img width="646" alt="image" src="https://user-images.githubusercontent.com/567298/55259343-14c75a00-526e-11e9-88dd-7c3a3f396280.png"></p>

<pre><code>  -&gt; Metrics: Tag size, Aggregation: Count
  -&gt; Buckets: Tags, 
     Aggregation: Terms, 
     Field: nginx.access.geoip.city_name, 
     Ordery by: Metric count, 
     Order: Descending, Size 10
</code></pre>

<h2>Create the Dashboard</h2>

<p>Now that we have all our visualizations, lets build the dashboard that hosts all our visualizations.</p>

<p>Select Dashboard -> Create New Dashboard -> Add -> Select your visualizations -> Reorder and Save</p>

<p>The visualizations in my dashboard looks like this:</p>

<p><img width="1266" alt="image" src="https://user-images.githubusercontent.com/567298/55418811-b8b54c00-5573-11e9-810d-d244d27c4fb3.png"></p>

<p>This is a basic dashboard but its just enough so that you can get your hands dirty and build some awesome visualizations.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a 5 Node Highly Available Elasticsearch Cluster]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/04/02/setup-a-5-node-highly-available-elasticsearch-cluster/"/>
    <updated>2019-04-02T05:05:19-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/04/02/setup-a-5-node-highly-available-elasticsearch-cluster</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/53352581-b3892f80-392b-11e9-9532-5db5cbfc8f1c.jpg" alt="elasticsearch" /></p>

<p>This is post 1 of my big collection of <strong><a href="https://blog.ruanbekker.com/blog/categories/elasticsearch-tutorials">elasticsearch-tutorials</a></strong> which includes, setup, index, management, searching, etc. More details at the bottom.</p>

<p>In this tutorial we will setup a <strong>5 node highly available elasticsearch cluster</strong> that will consist of 3 Elasticsearch Master Nodes and 2 Elasticsearch Data Nodes.</p>

<blockquote><p>&ldquo;Three master nodes is the way to start, but only if you&rsquo;re building a full cluster, which at minimum is 3 master nodes plus at least 2 data nodes.&rdquo;
 - <a href="https://discuss.elastic.co/t/should-dedicated-master-nodes-and-data-nodes-be-considered-separately/75093/14">https://discuss.elastic.co/t/should-dedicated-master-nodes-and-data-nodes-be-considered-separately/75093/14</a></p></blockquote>

<h2>The Overview:</h2>

<p>In short the responsibilites of the node types:</p>

<p><strong>Master Nodes</strong>: Master nodes are responsible for Cluster related tasks, creating / deleting indexes, tracking of nodes, allocate shards to nodes, etc.</p>

<p><strong>Data Nodes</strong>: Data nodes are responsible for hosting the actual shards that has the indexed data also handles data related operations like CRUD, search, and aggregations.</p>

<p>For more concepts of Elasticsearch, have a look at their <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started-concepts.html">basic-concepts</a> documentation.</p>

<p>Our Inventory will consist of:</p>

<p><strong>Master Nodes:</strong></p>

<pre><code>Hostname: es-master-1, Private IP: 172.31.0.77
Hostname: es-master-2, Private IP: 172.31.0.45
Hostname: es-master-3, Private IP: 172.31.1.31
</code></pre>

<p><strong>Data Nodes:</strong></p>

<pre><code>Hostname: es-data-1, Private IP:172.31.2.30
Hostname: es-data-2, Private IP:172.31.0.83
</code></pre>

<p><strong>Reserved Volumes</strong> for Data Nodes:</p>

<pre><code>es-data-1: 10GB assigned to /dev/vdb
es-data-2: 10GB assigned to /dev/vdb
</code></pre>

<p><strong>Authentication:</strong></p>

<p>Note that I have configured the bind address for elasticsearch to <code>0.0.0.0</code> using <code>network.host: 0.0.0.0</code> for this demonstration, but this means that if your server has a public ip address with no firewall rules or no auth, that anyone will be able to interact with your cluster.</p>

<p>This address will also be reachable for all nodes to see each other.</p>

<p>It&rsquo;s advisable do protect your endpoint, either with <a href="https://blog.ruanbekker.com/blog/2017/08/31/secure-your-access-to-kibana-5-and-elasticsearch-5-with-nginx-for-aws/">basic auth using nginx</a> which can be found in the embedded link, or using firewall rules to protect communication from the outside (depending on your setup)</p>

<h2>Setup the Elasticsearch Master Nodes</h2>

<p>The setup below how to provision a elasticsearch master node. Repeat this on node: <code>es-master-1</code>, <code>es-master-2</code>, <code>es-master-3</code></p>

<p>Set your hosts file for name resolution (if you don&rsquo;t have private dns in place):</p>

<pre><code>$ cat &gt; /etc/hosts &lt;&lt; EOF
127.0.0.1 localhost
172.31.0.77 es-master-1
172.31.0.45 es-master-2
172.31.1.31 es-master-3
172.31.2.30 es-data-1
172.31.0.83 es-data-2
EOF
</code></pre>

<p>Get the elasticsearch repositories, install the java development kit dependency and install elasticsearch:</p>

<pre><code>$ apt update &amp;&amp; apt upgrade -y
$ apt install software-properties-common python-software-properties apt-transport-https -y
$ wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
$ echo "deb https://artifacts.elastic.co/packages/6.x/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-6.x.list
$ apt update
$ apt install default-jdk -y
$ apt install elasticsearch -y
</code></pre>

<p>The elasticsearch config, before we get to the full example config, I just want to show a snippet of how you could split up logs and data.</p>

<p>Note that you can seperate your logging between data/logs like this:</p>

<pre><code># example of log splitting:
...
path:
  logs: /var/log/elasticsearch
  data: /var/data/elasticsearch
...
</code></pre>

<p>Also, your data can be divided between paths:</p>

<pre><code># example of data paths:
...
path:
  data:
    - /mnt/elasticsearch_1
    - /mnt/elasticsearch_2
    - /mnt/elasticsearch_3
...
</code></pre>

<p>Bootstrap the elasticsearch config with a cluster name (all the nodes should have the same cluster name), set the nodes as master <code>node.master: true</code> disable the <code>node.data</code> and specify that the cluster should at least have a minimum of 2 master nodes before it stops. This is used to prevent split brain.</p>

<p>To avoid a split brain, this setting should be set to a quorum of master-eligible nodes:
<code>(master_eligible_nodes / 2) + 1</code></p>

<p>The full example config:</p>

<pre><code>$ cat &gt; /etc/elasticsearch/elasticsearch.yml &lt;&lt; EOF
cluster.name: es-cluster
node.name: \${HOSTNAME}
node.master: true
node.data: false
path.logs: /var/log/elasticsearch
bootstrap.memory_lock: true
network.host: 0.0.0.0
discovery.zen.minimum_master_nodes: 2
discovery.zen.ping.unicast.hosts: ["es-master-1", "es-master-2", "es-master-3"]
EOF
</code></pre>

<p>Important settings for your elasticsearch cluster is described on their <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/system-config.html">docs</a>:</p>

<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration-memory.html">Disable swapping</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/file-descriptors.html">Increase file descriptors</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html">Ensure sufficient virtual memory</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/max-number-of-threads.html">Ensure sufficient threads</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/networkaddress-cache-ttl.html">JVM DNS cache settings</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/executable-jna-tmpdir.html">Temporary directory not mounted with noexec</a></li>
</ul>


<pre><code>$ cat &gt; /etc/default/elasticsearch &lt;&lt; EOF
ES_STARTUP_SLEEP_TIME=5
MAX_OPEN_FILES=65536
MAX_LOCKED_MEMORY=unlimited
EOF
</code></pre>

<p>Ensure that pages are not swapped out to disk by requesting the JVM to lock the heap in memory by setting <code>LimitMEMLOCK=infinity</code>. Set the maxiumim file descriptor number for this process: <code>LimitNOFILE</code> and increase the number of threads using <code>LimitNPROC</code>:</p>

<pre><code>$ vim /usr/lib/systemd/system/elasticsearch.service

[Service]
LimitMEMLOCK=infinity
LimitNOFILE=65535
LimitNPROC=4096
...
</code></pre>

<p>Increase the limit on the number of open files descriptors to user elasticsearch of 65536 or higher</p>

<pre><code>$ cat &gt; /etc/security/limits.conf &lt;&lt; EOF
elasticsearch soft memlock unlimited
elasticsearch hard memlock unlimited
EOF
</code></pre>

<p>Increase the value of the mmap counts as elasticsearch uses mmapfs directory to store its indices:</p>

<pre><code>$ sysctl -w vm.max_map_count=262144
</code></pre>

<p>For a permanent setting, update <code>vm.max_map_count</code> in <code>/etc/sysctl.conf</code> and run</p>

<pre><code>$ sysctl -p /etc/sysctl.conf 
</code></pre>

<p>Prepare the directories and set the ownership to elasticsearch:</p>

<pre><code>$ mkdir /usr/share/elasticsearch/data
$ chown -R elasticsearch:elasticsearch /usr/share/elasticsearch/data
</code></pre>

<p>Reload the systemd daemon, enable and start elasticsearch</p>

<pre><code>$ systemctl daemon-reload
$ systemctl enable elasticsearch
$ systemctl restart elasticsearch
</code></pre>

<p>Once all 3 elasticsearch masters has been started, verify that they are listening: <code>netstat -tulpn | grep 9200</code> then look at the cluster health:</p>

<pre><code>$ curl http://127.0.0.1:9200/_cluster/health?pretty
{
  "cluster_name" : "es-cluster",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 3,
  "number_of_data_nodes" : 0,
  "active_primary_shards" : 0,
  "active_shards" : 0,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 100.0
}
</code></pre>

<p>Have a look at the nodes, you will see that the node.role for now shows <code>mi</code>:</p>

<pre><code>$ curl http://127.0.0.1:9200/_cat/nodes?v
ip          heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
10.163.68.8           11          80  18    0.28    0.14     0.09 mi        -      es-master-2
10.163.68.5           14          80  14    0.27    0.18     0.11 mi        *      es-master-1
10.163.68.4           15          79   6    0.62    0.47     0.18 mi        -      es-master-3
</code></pre>

<h2>Setup the Elasticsearch Data Nodes</h2>

<p>Now that we have our 3 elasticsearch master nodes running, its time to provision the 2 elasticsearch data nodes. This setup needs to be repeated on both <code>es-data-1</code> and <code>es-data-2</code>.</p>

<p>Configure the hosts file for name resolution:</p>

<pre><code>$ cat &gt; /etc/hosts &lt;&lt; EOF
127.0.0.1 localhost
172.31.0.77 es-master-1
172.31.0.45 es-master-2
172.31.1.31 es-master-3
172.31.2.30 es-data-1
172.31.0.83 es-data-2
EOF
</code></pre>

<p>Get the elasticsearch repositories, install the java development kit dependency and install elasticsearch:</p>

<pre><code>$ apt update &amp;&amp; apt upgrade -y
$ apt install software-properties-common python-software-properties apt-transport-https -y
$ wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
$ echo "deb https://artifacts.elastic.co/packages/6.x/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-6.x.list
$ apt update
$ apt install default-jdk -y
$ apt install elasticsearch -y
</code></pre>

<p>Since we attached an extra disk to our data nodes, verify that you can see the disk:</p>

<pre><code>$ lsblk
NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
vda    253:0    0  25G  0 disk
└─vda1 253:1    0  25G  0 part /
vdb    253:16   0  10G  0 disk             &lt;----
</code></pre>

<p>Provision the block device with xfs or anything else that you prefer, create the directory where elasticsearch data will reside, change the ownership that elasticsearch has permission to write/read, set the device on startup and mount the disk:</p>

<pre><code>$ mkfs.xfs /dev/vdb
$ mkdir /data
$ mkdir /data/nodes
$ chown -R elasticsearch:elasticsearch /data
$ chown -R elasticsearch:elasticsearch /data/nodes
$ echo '/dev/vdb /data xfs defaults 0 0' &gt;&gt; /etc/fstab
$ mount -a
</code></pre>

<p>Verify that the disk is mounted:</p>

<pre><code>$ df -h
Filesystem      Size  Used Avail Use% Mounted on
udev            994M     0  994M   0% /dev
tmpfs           201M  3.1M  197M   2% /run
/dev/vda1        25G  1.8G   23G   8% /
/dev/vdb         10G   33M   10G   1% /data
</code></pre>

<p>Bootstrap the elasticsearch config with a cluster name, set the <code>node.name</code> to an identifier, in this case I will use the servers hostname, set the <code>node.master</code> to false as this will be data nodes, also enable these nodes as data nodes: <code>node.data: true</code>, configure the <code>path.data: /data</code> to the path that we configured, etc:</p>

<pre><code>$ cat &gt; /etc/elasticsearch/elasticsearch.yml &lt;&lt; EOF
cluster.name: es-cluster
node.name: \${HOSTNAME}
node.master: false
node.data: true
path.data: /data
path.logs: /var/log/elasticsearch
bootstrap.memory_lock: true
network.host: 0.0.0.0
discovery.zen.minimum_master_nodes: 2
discovery.zen.ping.unicast.hosts: ["es-master-1", "es-master-2", "es-master-3"]
EOF
</code></pre>

<p>Set a couple of important settings for your elasticsearch cluster is described on their <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/system-config.html">docs</a>:</p>

<pre><code>$ cat &gt; /etc/default/elasticsearch &lt;&lt; EOF
ES_STARTUP_SLEEP_TIME=5
MAX_OPEN_FILES=65536
MAX_LOCKED_MEMORY=unlimited
EOF
</code></pre>

<p>Disable swapping, increase the file descriptors and increase the maximum number of threads:</p>

<pre><code>$ vim /usr/lib/systemd/system/elasticsearch.service
[Service]
LimitMEMLOCK=infinity
LimitNOFILE=65535
LimitNPROC=4096
</code></pre>

<p>Also update them via limits.conf:</p>

<pre><code>$ cat &gt; /etc/security/limits.conf &lt;&lt; EOF
elasticsearch soft memlock unlimited
elasticsearch hard memlock unlimited
EOF
</code></pre>

<p>Reload the systemd daemon, enable and start elasticsearch. Allow it to start and check if the ports are listening with <code>netstat -tulpn | grep 9200</code>, then:</p>

<pre><code>$ systemctl daemon-reload
$ systemctl enable elasticsearch
$ systemctl restart elasticsearch
</code></pre>

<p>Verify that everything works as expected, look at the cluster health and look at the status and number of nodes:</p>

<pre><code>$ curl http://127.0.0.1:9200/_cluster/health?pretty
{
  "cluster_name" : "es-cluster",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 5,
  "number_of_data_nodes" : 2,
  "active_primary_shards" : 0,
  "active_shards" : 0,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 100.0
}
</code></pre>

<p>Look at the nodes api and you will see that we now have the extra 2 nodes showing up on <code>node.role</code> as <code>di</code>:</p>

<pre><code>$ curl http://127.0.0.1:9200/_cat/nodes?v
ip           heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
10.163.68.7             9          96   6    0.12    0.11     0.03 di        -      es-data-2
10.163.68.5            10          80   2    0.20    0.09     0.08 mi        *      es-master-1
10.163.68.11           12          96   9    0.12    0.09     0.03 di        -      es-data-1
10.163.68.4            10          79   0    0.00    0.12     0.11 mi        -      es-master-3
10.163.68.8            12          79   1    0.05    0.06     0.07 mi        -      es-master-2
</code></pre>

<h2>Interact with Elasticsearch</h2>

<p>Let&rsquo;s interact with elasticsearch, the overview:</p>

<pre><code>$ curl http://127.0.0.1:9200
{
  "name" : "es-data-1",
  "cluster_name" : "es-cluster",
  "cluster_uuid" : "5BLs4sxsSEK-4OxlGnmlmw",
  "version" : {
    "number" : "6.7.0",
    "build_flavor" : "default",
    "build_type" : "deb",
    "build_hash" : "8453f77",
    "build_date" : "2019-03-21T15:32:29.844721Z",
    "build_snapshot" : false,
    "lucene_version" : "7.7.0",
    "minimum_wire_compatibility_version" : "5.6.0",
    "minimum_index_compatibility_version" : "5.0.0"
  },
  "tagline" : "You Know, for Search"
}
</code></pre>

<p>Let&rsquo;s look at the Health API:</p>

<pre><code>$ curl http://127.0.0.1:9200/_cat/health?v
epoch      timestamp cluster    status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent
1554154652 21:37:32  es-cluster green           5         2     10   5    0    0        0             0                  -                100.0%
</code></pre>

<p>Let&rsquo;s ingest some data into elasticsearch, we will create an index named <code>first-index</code> with some dummy data about people, username, name, surname, location and hobbies:</p>

<pre><code>$ curl -H 'Content-Type: application/json' -XPOST http://127.0.0.1:9200/first-index/docs/ -d '{"username": "mikes", "name": "mike", "surname": "steyn", "location": {"country": "south africa", "city": "cape town"}, "hobbies": ["sport", "coffee"]}'

$ curl -H 'Content-Type: application/json' -XPOST http://127.0.0.1:9200/first-index/docs/ -d '{"username": "clarissas", "name": "clarissa", "surname": "smith", "location": {"country": "ireland", "city": "dublin"}, "hobbies": ["shopping", "reading", "chess"]}'

$ curl -H 'Content-Type: application/json' -XPOST http://127.0.0.1:9200/first-index/docs/ -d '{"username": "franka", "name": "frank", "surname": "adams", "location": {"country": "new zealand", "city": "auckland"}, "hobbies": ["programming", "swimming", "rugby"]}'
</code></pre>

<p>Now that we ingested our data into elasticsearch, lets have a look at the Indices API, where the number of documents, size etc should reflect:</p>

<pre><code>$ curl http://127.0.0.1:9200/_cat/indices?v
health status index       uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   first-index 1o6yM7tCSqagqoeihKM7_g   5   1          3            0     40.6kb         20.3kb
</code></pre>

<p>Now lets request a search, which will give you by default 10 returned documents:</p>

<pre><code>$ curl http://127.0.0.1:9200/first-index/_search?pretty
{
  "took" : 116,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 3,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "first-index",
        "_type" : "docs",
        "_id" : "-NTO2mkB8pugP4aC2jtZ",
        "_score" : 1.0,
        "_source" : {
          "username" : "mikes",
          "name" : "mike",
          "surname" : "steyn",
          "location" : {
            "country" : "south africa",
            "city" : "cape town"
          },
          "hobbies" : [
            "sport",
            "coffee"
          ]
        }
      },
      {
        "_index" : "first-index",
        "_type" : "docs",
        "_id" : "-tTR2mkB8pugP4aCAzvG",
        "_score" : 1.0,
        "_source" : {
          "username" : "franka",
          "name" : "frank",
          "surname" : "adams",
          "location" : {
            "country" : "new zealand",
            "city" : "auckland"
          },
          "hobbies" : [
            "programming",
            "swimming",
            "rugby"
          ]
        }
      },
      {
        "_index" : "first-index",
        "_type" : "docs",
        "_id" : "-dTP2mkB8pugP4aC1ztI",
        "_score" : 1.0,
        "_source" : {
          "username" : "clarissas",
          "name" : "clarissa",
          "surname" : "smith",
          "location" : {
            "country" : "ireland",
            "city" : "dublin"
          },
          "hobbies" : [
            "shopping",
            "reading",
            "chess"
          ]
        }
      }
    ]
  }
}
</code></pre>

<p>Let&rsquo;s have a look at our shards using the Shards API, you will also see where each document is assigned to a specific shard, and also if its a primary or replica shard:</p>

<pre><code>$ curl http://127.0.0.1:9200/_cat/shards?v
index       shard prirep state   docs store ip           node
first-index 4     p      STARTED    0  230b 10.163.68.7  es-data-2
first-index 4     r      STARTED    0  230b 10.163.68.11 es-data-1
first-index 2     p      STARTED    0  230b 10.163.68.7  es-data-2
first-index 2     r      STARTED    0  230b 10.163.68.11 es-data-1
first-index 3     r      STARTED    1 6.6kb 10.163.68.7  es-data-2
first-index 3     p      STARTED    1 6.6kb 10.163.68.11 es-data-1
first-index 1     r      STARTED    2  13kb 10.163.68.7  es-data-2
first-index 1     p      STARTED    2  13kb 10.163.68.11 es-data-1
first-index 0     p      STARTED    0  230b 10.163.68.7  es-data-2
first-index 0     r      STARTED    0  230b 10.163.68.11 es-data-1
</code></pre>

<p>Then we can also use the Allocation API to see the size of our indices, disk space per node:</p>

<pre><code>$ curl http://127.0.0.1:9200/_cat/allocation?v
shards disk.indices disk.used disk.avail disk.total disk.percent host         ip           node
     5       20.3kb    32.4mb      9.9gb      9.9gb            0 10.163.68.11 10.163.68.11 es-data-1
     5       20.3kb    32.4mb      9.9gb      9.9gb            0 10.163.68.7  10.163.68.7  es-data-2
</code></pre>

<p>Let&rsquo;s search for anyone with the surname <code>smith</code>:</p>

<pre><code>$ curl -s http://127.0.0.1:9200/first-index/_search?q=surname=smith | jq .
{
  "took": 22,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 0.2876821,
    "hits": [
      {
        "_index": "first-index",
        "_type": "docs",
        "_id": "-dTP2mkB8pugP4aC1ztI",
        "_score": 0.2876821,
        "_source": {
          "username": "clarissas",
          "name": "clarissa",
          "surname": "smith",
          "location": {
            "country": "ireland",
            "city": "dublin"
          },
          "hobbies": [
            "shopping",
            "reading",
            "chess"
          ]
        }
      }
    ]
  }
}
</code></pre>

<p>Let&rsquo;s search for anyone with <code>rugby</code> as one of their hobbies:</p>

<pre><code>$ curl -s http://127.0.0.1:9200/first-index/_search?q=hobbies=rugby | jq .
{
  "took": 23,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 0.64072424,
    "hits": [
      {
        "_index": "first-index",
        "_type": "docs",
        "_id": "-tTR2mkB8pugP4aCAzvG",
        "_score": 0.64072424,
        "_source": {
          "username": "franka",
          "name": "frank",
          "surname": "adams",
          "location": {
            "country": "new zealand",
            "city": "auckland"
          },
          "hobbies": [
            "programming",
            "swimming",
            "rugby"
          ]
        }
      }
    ]
  }
}
</code></pre>

<h2>More on Elasticsearch</h2>

<p>I am planning to write up <strong>elasticsearch</strong> articles on the following topics:</p>

<ul>
<li><a href="">Setting up a 5 Node HA Elasticsearch Cluster</a></li>
<li>Indexes / Replicas</li>
<li>Search Queries</li>
<li>Delete Queries</li>
<li>Elasticsearch Snapshots and Restores on S3</li>
<li>Mapping Templates</li>
<li>Resizing Index Shards</li>
<li>Dealing with Old Timeseries Data</li>
<li>Elasticsearch Percentiles</li>
<li>Managing Yellow and Red Status Clusters</li>
<li>Managing High JVM Memory Pressure</li>
<li>and more</li>
</ul>


<p>As I finish up the writing of these posts they will be published under the <a href="https://blog.ruanbekker.com/blog/categories/elasticsearch-tutorials">#elasticsearch-tutorials</a> category on my blog and for any other elasticsearch tutorials, you can find them under the <a href="https://blog.ruanbekker.com/blog/categories/elasticsearch">#elasticsearch</a> category.</p>

<p>Oke byyyyyye :D</p>

<h2>Resources</h2>

<ul>
<li><a href="https://www.elastic.co/products/elasticsearch">Elasticsearch</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html">Elasticsearch Docs</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ship Your Logs to Elasticsearch With Filebeat]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/03/27/ship-your-logs-to-elasticsearch-with-filebeat/"/>
    <updated>2019-03-27T10:52:21-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/03/27/ship-your-logs-to-elasticsearch-with-filebeat</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/55086561-4b0baa80-50b1-11e9-8062-a9e6de5ab56a.png" alt="" /></p>

<p><strong><a href="https://www.elastic.co/guide/en/beats/filebeat/6.7/filebeat-overview.html">Filebeat</a></strong> by Elastic is a lightweight log shipper, that ships your logs to Elastic products such as Elasticsearch and Logstash. Filbeat monitors the logfiles from the given configuration and ships the to the locations that is specified.</p>

<h2>Filebeat Overview</h2>

<p>Filebeat runs as agents, monitors your logs and ships them in response of events, or whenever the logfile receives data.</p>

<p>Below is a overview (credit: elastic.co) how Filebeat works</p>

<p><img src="https://user-images.githubusercontent.com/567298/55086346-e18b9c00-50b0-11e9-8eac-ea4880cb1aff.png" alt="" /></p>

<h2>Installing Filebeat</h2>

<p>Let&rsquo;s go ahead and install Filebeat. I will be using version 6.7 as that will be the same version that I am running on my Elasticsearch. To check the version of your elasticsearch cluster:</p>

<pre><code class="bash">$ curl http://127.0.0.1:9200/_cluster/health?pretty # i have es running locally
</code></pre>

<p>Install the dependencies:</p>

<pre><code class="bash">$ apt install wget apt-transport-https -y
</code></pre>

<p>Get the public signing key:</p>

<pre><code class="bash">$ wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
</code></pre>

<p>Get the repository definition:</p>

<pre><code class="bash">$ echo "deb https://artifacts.elastic.co/packages/6.x/apt stable main" | tee -a /etc/apt/sources.list.d/elastic-6.x.list
</code></pre>

<p>Update the repositories:</p>

<pre><code class="bash">$ apt update &amp;&amp; apt upgrade -y
</code></pre>

<p>Install Filebeat and enable the service on boot:</p>

<pre><code class="bash">$ apt install filebeat -y
$ systemctl enable filebeat
</code></pre>

<h2>Configure Filebeat</h2>

<p>Let&rsquo;s configure our main configuration in filebeat, to specify our location where the data should be shipped to (in this case elasticsearch) and I will also like to set some extra fields that will apply to this specific server.</p>

<p>Open up <code>/etc/filebeat/filebeat.yml</code> and edit the following:</p>

<pre><code class="yaml">filebeat.inputs:

- type: log
  enabled: false
  paths:
    - /var/log/nginx/*.log 

filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
  reload.enabled: false

setup.template.settings:
  index.number_of_shards: 3

fields:
  blog_name: sysadmins
  service_type: webserver
  cloud_provider: aws

setup.kibana:
  host: "http://localhost:5601"
  username: "elastic"
  password: "changeme"

output.elasticsearch:
  hosts: ["localhost:9200"]
  protocol: "http"
  username: "elastic"
  password: "changeme"
</code></pre>

<p>Above, just setting my path to nginx access logs, some extra fields, including that it shoulds seed kibana with example visualizations and the output configuration of elasticsearch.</p>

<h2>Filebeat Modules</h2>

<p>Filebeat comes with modules that has context on specific applications like nginx, mysql etc. Lets enable system (syslog, auth, etc) and nginx for our web server:</p>

<pre><code class="bash">$ filebeat modules enable system
$ filebeat modules enable nginx
</code></pre>

<p>Example of my <code>/etc/filebeat/modules.d/system.yml</code> configuration:</p>

<pre><code class="yaml">- module: system
  syslog:
    enabled: true
    var.paths: ["/var/log/syslog"]

  auth:
    enabled: true
    var.paths: ["/var/log/auth.log"]
</code></pre>

<p>Example of my <code>/etc/filebeat/modules.d/nginx.yml</code> configuration:</p>

<pre><code class="yaml">- module: nginx
  access:
    enabled: true
    var.paths: ["/var/log/nginx/access.log"]

  error:
    enabled: true
    var.paths: ["/var/log/nginx/error.log"]
</code></pre>

<p>Now setup the <a href="https://www.elastic.co/guide/en/beats/filebeat/6.7/filebeat-template.html">templates</a></p>

<pre><code class="bash">$ filebeat setup
</code></pre>

<p>Then restart filebeat:</p>

<pre><code class="bash">$ /etc/init.d/filebeat restart
</code></pre>

<p>You can have a look at the logs, should you need to debug:</p>

<pre><code class="bash">tail -f /var/log/filebeat/filebeat
</code></pre>

<p>Your data should now be shipped to elasticsearch, by default under the <code>filebeat-YYYY.mm.dd</code> index pattern.</p>

<pre><code class="bash">$ curl 'http://127.0.0.1:9200/_cat/indices/filebeat*?v'
health status index                     uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   filebeat-6.7.1-2019.03.27 CBdV7adjRKypN1wguwuHDA   3   1     453220            0    230.2mb        115.9mb
</code></pre>

<h2>Kibana</h2>

<p>You can head over to Kibana at <a href="http://localhost:5601">http://localhost:5601</a> (in this case) to visualize the data that is ingested into your filebeat index. I will write a tutorial on how to graph up most common dashboards later this week.</p>

<p>Thats it for now :D</p>

<h2>Resources:</h2>

<ul>
<li><a href="https://www.elastic.co/guide/en/beats/filebeat/6.7/index.html">https://www.elastic.co/guide/en/beats/filebeat/6.7/index.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Get Application Performance Metrics on Python Flask With Elastic APM on Kibana and Elasticsearch]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/11/11/get-application-performance-metrics-on-python-flask-with-elastic-apm-on-kibana-and-elasticsearch/"/>
    <updated>2018-11-11T13:09:18-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/11/11/get-application-performance-metrics-on-python-flask-with-elastic-apm-on-kibana-and-elasticsearch</id>
    <content type="html"><![CDATA[<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-banner.png" alt="" /></p>

<p>In this post we will setup a Python Flask Application which includes the APM Agent which will collect metrics, that gets pushed to the APM Server. If you have not setup the Elastic Stack with / or APM Server, you <a href="https://blog.ruanbekker.com/blog/2018/11/11/setup-apm-server-on-ubuntu-for-your-elastic-stack-to-get-insights-in-your-application-performance-metrics/">can follow this post</a> to setup the needed.</p>

<p>Then we will make a bunch of HTTP Requests to our Application and will go through the metrics per request type.</p>

<h2>Application Metrics</h2>

<p>Our Application will have the following Request Paths:</p>

<ul>
<li><code>/</code> - Returns static text</li>
<li><code>/delay</code> - random delays to simulate increased response latencies</li>
<li><code>/upstream</code> - get data from a upstream provider, if statements to provide dummy 200, 404 and 502 reponses to visualize</li>
<li><code>/5xx</code> - request path that will raise an exception so that we can see the error via apm</li>
<li><code>/sql-write</code> - inserts 5 rows into a sqlite database</li>
<li><code>/sql-read</code> - executes a select all from the database</li>
<li><code>/sql-group</code> - sql query to group all the cities and count them</li>
</ul>


<p>This is just simple request paths to demonstrate the metrics via APM (Application Performance Monitoring) on Kibana.</p>

<h2>Install Flask and APM Agent</h2>

<p>Create a virtual environment and install the dependencies:</p>

<pre><code class="bash">$ apt install python python-setuptools -y
$ easy_install pip
$ pip install virtualenv
$ pip install elastic-apm[flask]
$ pip install flask
</code></pre>

<p>For more info on <a href="https://www.elastic.co/guide/en/apm/agent/python/current/getting-started.html">APM Configuration</a>.</p>

<h2>Instrument a Bare Bones Python Flask app with APM:</h2>

<p>A Barebones app with APM Configured will look like this:</p>

<pre><code class="python">from flask import Flask, jsonify
from elasticapm.contrib.flask import ElasticAPM
from elasticapm.handlers.logging import LoggingHandler

app = Flask(__name__)
apm = ElasticAPM(app, server_url='http://localhost:8200', service_name='flask-app-1', logging=True)

@app.route('/')
def index():
    return jsonify({"message": "response ok"}), 200

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=80)
</code></pre>

<p>This will provide metrics on the <code>/</code> request path. In order to trace transaction ids from the metrics, we need to configure the index on Kibana. To do this, head over to Kibana, Management, Index Patterns, Add Index Pattern, <code>apm*</code>, select <code>@timestamp</code> as the time filter field name.</p>

<p>This will allow you to see the data when tracing the transaction id&rsquo;s via the Discover UI.</p>

<h2>Create the Python Flask App</h2>

<p>Create the Flask App with the request paths as mentioned in the beginning:</p>

<pre><code class="python">import sqlite3, requests, time, logging, random
from flask import Flask, jsonify
from elasticapm.contrib.flask import ElasticAPM
from elasticapm.handlers.logging import LoggingHandler

names = ['ruan', 'stefan', 'philip', 'norman', 'frank', 'pete', 'johnny', 'peter', 'adam']
cities = ['cape town', 'johannesburg', 'pretoria', 'dublin', 'kroonstad', 'bloemfontein', 'port elizabeth', 'auckland', 'sydney']
lastnames = ['smith', 'bekker', 'admams', 'phillips', 'james', 'adamson']

conn = sqlite3.connect('database.db')
conn.execute('CREATE TABLE IF NOT EXISTS people (name STRING, age INTEGER, surname STRING, city STRING)')
#sqlquery_write = conn.execute('INSERT INTO people VALUES("{}", "{}", "{}", "{}")'.format(random.choice(names), random.randint(18,40), random.choice(lastnames), random.choice(cities)))
seconds = [0.002, 0.003, 0.004, 0.01, 0.3, 0.2, 0.009, 0.015, 0.02, 0.225, 0.009, 0.001, 0.25, 0.030, 0.018]

app = Flask(__name__)
apm = ElasticAPM(app, server_url='http://localhost:8200', service_name='my-app-01', logging=False)

@app.route('/')
def index():
    return jsonify({"message": "response ok"})

@app.route('/delay')
def delay():
    time.sleep(random.choice(seconds))
    return jsonify({"message": "response delay"})

@app.route('/upstream')
def upstream():
    r = requests.get('https://api.ruanbekker.com/people').json()
    r.get('country')
    if r.get('country') == 'italy':
        return 'Italalia!', 200
    elif r.get('country') == 'canada':
        return 'Canada!', 502
    else:
        return 'Not Found', 404

@app.route('/5xx')
def fail_with_5xx():
    value = 'a' + 1
    return jsonify({"message": value})

@app.route('/sql-write')
def sqlw():
    conn = sqlite3.connect('database.db')
    conn.execute('INSERT INTO people VALUES("{}", "{}", "{}", "{}")'.format(random.choice(names), random.randint(18,40), random.choice(lastnames), random.choice(cities)))
    conn.execute('INSERT INTO people VALUES("{}", "{}", "{}", "{}")'.format(random.choice(names), random.randint(18,40), random.choice(lastnames), random.choice(cities)))
    conn.execute('INSERT INTO people VALUES("{}", "{}", "{}", "{}")'.format(random.choice(names), random.randint(18,40), random.choice(lastnames), random.choice(cities)))
    conn.execute('INSERT INTO people VALUES("{}", "{}", "{}", "{}")'.format(random.choice(names), random.randint(18,40), random.choice(lastnames), random.choice(cities)))
    conn.execute('INSERT INTO people VALUES("{}", "{}", "{}", "{}")'.format(random.choice(names), random.randint(18,40), random.choice(lastnames), random.choice(cities)))
    conn.commit()
    conn.close()
    return 'ok', 200

@app.route('/sql-read')
def sqlr():
    conn = sqlite3.connect('database.db')
    conn.row_factory = sqlite3.Row
    cur = conn.cursor()
    cur.execute('select * from people')
    rows = cur.fetchall()
    conn.close()
    return 'ok', 200

@app.route('/sql-group')
def slqg():
    conn = sqlite3.connect('database.db')
    conn.row_factory = sqlite3.Row
    cur = conn.cursor()
    cur.execute('select count(*) as num, city from people group by city')
    rows = cur.fetchall()
    conn.close()
    return 'ok', 200

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=80)
</code></pre>

<p>Run the app:</p>

<pre><code class="bash">$ python app.py
</code></pre>

<p>At this point, we wont have any data on APM as we need to make requests to our application. Let&rsquo;s make 10 HTTP GET Requests on the <code>/</code> Request Path:</p>

<pre><code class="bash">$ count=0 &amp;&amp; while [ $count -lt 10 ]; do curl http://application-routable-address:80/; sleep 1; count=$((count+1)); done
</code></pre>

<h2>Visualize the Root Request Path</h2>

<p>Head over to Kibana, Select APM and you will see something similar like below when selecting the timepicker to 15 minutes at the right top corner. This page will give you the overview of all your configured applications and the average response times over the selected time, transactions per minute, errors per minute etc:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-home-root.png" alt="" /></p>

<p>When you select your application, you will find the graphs on you response times and requests per minute, also a breakdown per request path:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-app-view-1.png" alt="" /></p>

<p>When selecting the request path, in this case <code>GET /</code>, you will find a breakdown of metrics only for that request and also the response time distribution for that request path, if you select frame from the response time distribution, it will filter the focus to that specific transaction.</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-req-view-1.png" alt="" /></p>

<p>When you scroll a bit down to the Transaction Sample section, you will find data about the request, response, system etc:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-app-transaction-1.png" alt="" /></p>

<p>From the Transaction Sample, you can select the View Transaction in Discover button, which will trace that transaction id on the Discover UI:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-transaction-disc-1.png" alt="" /></p>

<p>Increasing the http curl clients running simultaneously from different servers and increasing the time for 15 minutes to have more metrics will result in the screenshot below, notice the 6ms response time can easily be traced selecting it in the response time distribution, then discovering it in the UI, which will give you the raw data from that request:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-req-view-2.png" alt="" /></p>

<h2>Viewing Application Errors in APM</h2>

<p>Make a couple of requests to <code>/5xx</code>:</p>

<pre><code class="bash">$ curl http://application-routable-endpoint:80/5xx
</code></pre>

<p>Navigate to the app, select Errors, then you will see the exception details that was returned. Here we can see that in our code we tried to concatenate integers with strings:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-error-1.png" alt="" /></p>

<p>Furthermore we can select that error and it will provide us a direct view on where in our code the error gets generated:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-error-details-1.png" alt="" /></p>

<p>Pretty cool right?! You can also further select the library frames, which will take you to the lower level code that raised the exception. If this errors can be drilled down via the discover ui, to group by source address etc.</p>

<h2>Simulate Response Latencies:</h2>

<p>Make a couple of requests to the <code>/delay</code> request path, and you should see the increased response times from earlier:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-app-view-delay.png" alt="" /></p>

<h2>Requests where Database Calls are Executed</h2>

<p>The while loop to call random request paths:</p>

<pre><code class="bash">count=0 &amp;&amp; while [ $count -lt 1000 ]; 
do 
  curl -H "Host: my-za-server" -i http://x.x.x.x/sql-write; 
  curl -H "Host: my-za-server" -i http://x.x.x.x/sql-read; 
  curl -H "Host: my-za-server" -i http://x.x.x.x/sql-group; 
  curl -H "Host: my-eu-server" -i http://x.x.x.x/sql-write; 
  curl -H "Host: my-us-server" -i http://x.x.x.x/sql-write; 
  curl -H "Host: my-za-server" -i http://x.x.x.x/sql-write; 
  curl -H "Host: my-za-server" -i http://x.x.x.x/sql-write; 
  curl -H "Host: my-za-server" -i http://x.x.x.x/sql-read; 
  curl -H "Host: my-eu-server" -i http://x.x.x.x/sql-group; 
  curl -H "Host: my-us-server" -i http://x.x.x.x/sql-group; 
  curl -H "Host: my-za-server" -i http://x.x.x.x/sql-write; 
  curl -H "Host: my-za-server" -i http://x.x.x.x/sql-write; 
  curl -H "Host: my-eu-server" -i http://x.x.x.x/sql-group; 
  curl -H "Host: my-za-server" -i http://x.x.x.x/sql-group; 
  count=$((count+1)); 
done
</code></pre>

<p>When we look at our applications performance monitoring overview, we can see the writes provide more latencies as the group by&rsquo;s:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-db-view-1.png" alt="" /></p>

<p>The <code>/sql-write</code> request overview:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-db-sqlwrite-1.png" alt="" /></p>

<p>When selecting a transaction sample, we can see the timeline of each database call:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-db-sqlwrite-1-details.png" alt="" /></p>

<p>When looking at the <code>/sql-group</code> request overview, we can see that the response times increasing overtime, as more data is written to the database, it takes longer to read and group all the data from the database:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-db-sqlgroup-1.png" alt="" /></p>

<p>The transaction details shows the timeline of the database query from that request:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-db-sqlgroup-1-details.png" alt="" /></p>

<p>When you select the database select query on the timeline view, it will take you to the exact database query that was executed:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-db-sqlgroup-1-span.png" alt="" /></p>

<p>When we include a database call with a external request to a remote http endpoint, we will see something like:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-db-sqlread-ext.png" alt="" /></p>

<h2>Viewing 4xx and 5xx Response Codes</h2>

<p>From the application code we are returning 2xx, 4xx, and 5xx response codes for this demonstration to visualize them:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-app-response-codes.png" alt="" /></p>

<h2>Configuring more Applications</h2>

<p>Once more apps are configured, and they start serving traffic, they will start appearing on the APM UI as below:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-apps.png" alt="" /></p>

<p>APM is available for other languages as well and provides a getting started snippets from the APM UI. For more information on APM, have a look at their <a href="https://www.elastic.co/solutions/apm">Documentation</a></p>

<p>Hope this was useful.</p>

<script type="text/javascript">
  ( function() {
    if (window.CHITIKA === undefined) { window.CHITIKA = { 'units' : [] }; };
    var unit = {"calltype":"async[2]","publisher":"rbekker87","width":728,"height":90,"sid":"Chitika Default"};
    var placement_id = window.CHITIKA.units.length;
    window.CHITIKA.units.push(unit);
    document.write('<div id="chitikaAdBlock-' + placement_id + '"></div>');
}());
</script>


<script type="text/javascript" src="//cdn.chitika.net/getads.js" async></script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup APM Server on Ubuntu for Your Elastic Stack to Get Insights in Your Application Performance Metrics]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/11/11/setup-apm-server-on-ubuntu-for-your-elastic-stack-to-get-insights-in-your-application-performance-metrics/"/>
    <updated>2018-11-11T12:31:43-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/11/11/setup-apm-server-on-ubuntu-for-your-elastic-stack-to-get-insights-in-your-application-performance-metrics</id>
    <content type="html"><![CDATA[<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-overview.png" alt="" /></p>

<p>In this post we will setup the Elastic Stack with Elasticsearc, Kibana and APM . The APM Server (Application Performance Metrics) which will receive the metric data from the application side, and is then pushed to apm indices on Elasticsearch.</p>

<p>This will be a 2 post blog on APM:</p>

<ul>
<li>1) <a href="">Setup the Elastic Stack with Elasticsearch, Kibana and APM Server</a> - this post</li>
<li>2) <a href="https://blog.ruanbekker.com/blog/2018/11/11/get-application-performance-metrics-on-python-flask-with-elastic-apm-on-kibana-and-elasticsearch/">Setup a Python Flask application with the APM Agent</a></li>
</ul>


<h2>What is APM</h2>

<p>From their website APM is described as: &ldquo;Elastic APM is an application performance monitoring system built on the Elastic Stack. It allows you to monitor software services and applications in real time, collecting detailed performance information on response time for incoming requests, database queries, calls to caches, external HTTP requests, etc.&rdquo;</p>

<p>You get metrics like average, p99 response times etc, and also have insights when errors occur, it even allows you to look at the stacktrace, poinpointing on which line of your code it ocurred etc.</p>

<ul>
<li><a href="https://www.elastic.co/solutions/apm">More Info</a></li>
</ul>


<h2>APM Agents:</h2>

<p>The APM Agents will be loaded inside your application, application metrics will then be pushed to the APM Server (which we will setup in this post), which then gets pushed to Elasticsearch and is then consumed by Kibana.</p>

<p>At the time of writing, the APM Agents are supported in the following languages:</p>

<ul>
<li>Node.js</li>
<li>Django</li>
<li>Flask</li>
<li>Ruby on Rails</li>
<li>Rack</li>
<li>RUM</li>
<li>Golang</li>
<li>Java</li>
</ul>


<h2>Setup the Elastic Stack</h2>

<p>One thing to note, every service in your Elastic Stack needs to be running on the same version. In this post we will setup Elasticsearch, APM and Kibana all running on version <code>6.4.3</code></p>

<h2>Setup the Pre-Requirements:</h2>

<p>Elasticsearch depends on Java, se we will go ahead and setup the repositories:</p>

<pre><code class="bash">$ wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
$ apt-get install apt-transport-https -y
$ echo "deb https://artifacts.elastic.co/packages/6.x/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-6.x.list
$ apt update &amp;&amp; apt upgrade -y 
$ apt install openjdk-8-jdk -y
</code></pre>

<p>Verify that Java is installed:</p>

<pre><code class="bash">$ java -version
openjdk version "1.8.0_181"
OpenJDK Runtime Environment (build 1.8.0_181-8u181-b13-1ubuntu0.16.04.1-b13)
OpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)
</code></pre>

<p>Setup Kernel parameters for Elasticsearch:</p>

<pre><code class="bash">$ sysctl -w vm.max_map_count=262144
$ echo 'vm.max_map_count=262144' &gt;&gt; /etc/sysctl.conf
</code></pre>

<h2>Setup Elasticsearch:</h2>

<p>Search for the latest versions (when already having elasticsearch, either upgrade or install apm on the same version as elasticsearch/kibana):</p>

<pre><code class="bash">$ apt-cache madison elasticsearch
elasticsearch |      6.4.3 | https://artifacts.elastic.co/packages/6.x/apt stable/main amd64 Packages
elasticsearch |      6.4.2 | https://artifacts.elastic.co/packages/6.x/apt stable/main amd64 Packages
</code></pre>

<p>Install Elasticsearch:</p>

<pre><code class="bash">$ apt-get install elasticsearch=6.4.3 -y
</code></pre>

<p>Configure Elasticsearch to lock the memory on startup:</p>

<pre><code class="bash">$ sed -i 's/#bootstrap.memory_lock: true/bootstrap.memory_lock: true/g' /etc/elasticsearch/elasticsearch.yml
</code></pre>

<p>Enable Elasticsearch on startup and start the service:</p>

<pre><code>$ systemctl daemon-reload
$ systemctl enable elasticsearch.service
$ systemctl start elasticsearch.service
</code></pre>

<h2>Install Kibana:</h2>

<p>Install Kibana version <code>6.4.3</code>:</p>

<pre><code class="bash">$ apt install kibana=6.4.3 -y
</code></pre>

<p>For demonstration, I will configure Kibana to listen on all interfaces on port <code>5601</code>, but note this will enable access for everyone, you can [follow this blogpost] to setup a Nginx Reverse Proxy to upstream to localhost on port 5601.</p>

<p>Since this demonstration we are using Elasticsearch locally, so if you have a remote cluster, configuration can be applied where needed.</p>

<pre><code class="bash">$ sed -i 's/#server.host: "localhost"/server.host: "0.0.0.0"/'g /etc/kibana/kibana.yml
$ sed -i 's/#elasticsearch.url: "http:\/\/localhost:9200"/elasticsearch.url: "http:\/\/localhost:9200"/'g /etc/kibana/kibana.yml
</code></pre>

<p>Enable Kibana on startup and start the service:</p>

<pre><code class="bash">$ systemctl enable kibana.service
$ systemctl restart kibana.service
</code></pre>

<h2>Install the APM Server</h2>

<p>Install APM Server version <code>6.4.3</code>:</p>

<pre><code class="bash">$ apt install apm-server=6.4.3 -y
</code></pre>

<p>Since we have everything locally, the configuration can be kept as is, but if you need to configure the elasticsearch or kibana hosts, it can be done via <code>/etc/apm-server/apm-server.yml</code></p>

<p>Then once Kibana and Elasticsearch is started, load the mapping templates, enable and start the service:</p>

<pre><code class="bash">$ apm-server setup
$ systemctl enable apm-server.service
$ systemctl restart apm-server.service
</code></pre>

<p>Ensure all the services are running with <code>netstat -tulpn</code> and port <code>9200</code>, <code>9300</code>, <code>5601</code> and <code>8300</code> should be listening</p>

<h2>Access Your Elastic Stack</h2>

<p>Access Kibana on your routable endpoint on port <code>5601</code> and you should see something like this:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-startup.png" alt="" /></p>

<h2>Configuring a Application to push metrics to APM</h2>

<p>In the <a href="https://blog.ruanbekker.com/blog/2018/11/11/get-application-performance-metrics-on-python-flask-with-elastic-apm-on-kibana-and-elasticsearch/">next post</a> I will setup a Python Flask Application on APM</p>

<script type="text/javascript">
  ( function() {
    if (window.CHITIKA === undefined) { window.CHITIKA = { 'units' : [] }; };
    var unit = {"calltype":"async[2]","publisher":"rbekker87","width":728,"height":90,"sid":"Chitika Default"};
    var placement_id = window.CHITIKA.units.length;
    window.CHITIKA.units.push(unit);
    document.write('<div id="chitikaAdBlock-' + placement_id + '"></div>');
}());
</script>


<script type="text/javascript" src="//cdn.chitika.net/getads.js" async></script>

]]></content>
  </entry>
  
</feed>
