<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Elasticsearch | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/elasticsearch/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2017-10-22T06:01:56-04:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Create a 3 Node Elasticsearch Stack With HAProxy on Docker Swarm]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/09/24/create-a-3-node-elasticsearch-stack-with-haproxy-on-docker-swarm/"/>
    <updated>2017-09-24T15:40:19-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/09/24/create-a-3-node-elasticsearch-stack-with-haproxy-on-docker-swarm</id>
    <content type="html"><![CDATA[<p>Tried out creating a 3 node elasticsearch stack on docker swarm using docker-compose, that sits behind a haproxy service.</p>

<h2>Environment:</h2>

<p>Images:</p>

<ul>
<li><a href="https://hub.docker.com/r/dockercloud/haproxy/">dockercloud/haproxy</a></li>
<li><a href="https://github.com/ruanbekker/docker-elasticsearch">rbekker87/elasticsearch:master-5.6-alpine</a></li>
</ul>


<p>Stack:</p>

<ul>
<li>1 x haproxy</li>
<li>1 x elasticsearch master (haproxy wont send requests to this one)</li>
<li>2 x elasticsearch master/data</li>
<li>1 x esnet overlay network</li>
</ul>


<h2>Defining our Stack</h2>

<p>First we will create our compose file, which we will call <code>es-compose.yml</code>:</p>

<pre><code class="yaml">version: '3'

services:
  es-master:
    image: rbekker87/elasticsearch:master-5.6-alpine
    networks:
      - esnet
    deploy:
      replicas: 1

  es-data-1:
    image: rbekker87/elasticsearch:master-5.6-alpine
    environment:
     - SERVICE_PORTS=9200
    networks:
      - esnet
    deploy:
      replicas: 2

  es-data-2:
    image: rbekker87/elasticsearch:master-5.6-alpine
    environment:
     - SERVICE_PORTS=9200
    networks:
      - esnet
    deploy:
      replicas: 2

  loadbalancer:
    image: dockercloud/haproxy:latest
    depends_on:
      - es-data-1
      - es-data-2
    environment:
      - BALANCE=leastconn
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - 9200:80
    networks:
      - esnet
    deploy:
      placement:
        constraints: [node.role == manager]

networks:
  esnet:
    driver: overlay
</code></pre>

<p>The above compose file defines that we want a overlay network, which we will associate with all our services, 3 elasticsearch services, haproxy service which will expose port 9200, then from haproxy it has a container port of 80, which sends to the backend <code>SERVICE_PORTS</code> of each elasticsearch service.</p>

<p>We have only defined <code>SERVICE_PORTS=9200</code> on our es-data services, as I just want to proxy client connections to them.</p>

<h2>Creating our Elasticsearch Stack</h2>

<p>Now that we have our compose file ready, let&rsquo;s create our stack using <code>docker stack deploy</code>:</p>

<pre><code class="bash Create the Stack">$ docker stack deploy -c es-compose.yml analytics

Creating network analytics_esnet
Creating service analytics_loadbalancer
Creating service analytics_es-master
Creating service analytics_es-data-1
Creating service analytics_es-data-2
</code></pre>

<p>Let&rsquo;s have a look at our stack:</p>

<pre><code class="bash Docker Stack Status ">$ docker stack ps analytics
ID                  NAME                       IMAGE                                       NODE                  DESIRED STATE       CURRENT STATE            ERROR               PORTS
4t3ukxl2kch3        analytics_loadbalancer.1   dockercloud/haproxy:latest                  scw-swarm-master-01   Running             Running 27 seconds ago
jgbxtgqkg9jp        analytics_es-data-2.1      rbekker87/elasticsearch:master-5.6-alpine   scw-swarm-master-01   Running             Running 33 seconds ago
x5cq6pm7u7mn        analytics_es-data-1.1      rbekker87/elasticsearch:master-5.6-alpine   scw-swarm-master-01   Running             Running 36 seconds ago
5v22w1hvtdvm        analytics_es-master.1      rbekker87/elasticsearch:master-5.6-alpine   scw-swarm-master-01   Running             Running 38 seconds ago
</code></pre>

<p>View the logs of our haproxy service:</p>

<pre><code class="bash HAProxy Service Logs">$ docker service logs -f analytics_loadbalancer
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | INFO:haproxy:dockercloud/haproxy 1.6.7 is running outside Docker Cloud
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | INFO:haproxy:Haproxy is running in SwarmMode, loading HAProxy definition through docker api
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | INFO:haproxy:dockercloud/haproxy PID: 7
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | INFO:haproxy:=&gt; Add task: Initial start - Swarm Mode
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | INFO:haproxy:=&gt; Executing task: Initial start - Swarm Mode
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | INFO:haproxy:==========BEGIN==========
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | INFO:haproxy:Linked service: analytics_es-data-1, analytics_es-data-2, analytics_es-master
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | INFO:haproxy:Linked container: analytics_es-data-1.1.u641c5bq5vkjklk8sb1scnnlc, analytics_es-data-2.1.ic9an6bzj6aejs0lx0vzfpia6, analytics_es-master.1.h4erlgwzit509p0zehzmozy3u
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | INFO:haproxy:HAProxy configuration:
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | global
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   log 127.0.0.1 local0
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   log 127.0.0.1 local1 notice
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   log-send-hostname
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   maxconn 4096
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   pidfile /var/run/haproxy.pid
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   user haproxy
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   group haproxy
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   daemon
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   stats socket /var/run/haproxy.stats level admin
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   ssl-default-bind-options no-sslv3
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   ssl-default-bind-ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:AES128-GCM-SHA256:AES128-SHA256:AES128-SHA:AES256-GCM-SHA384:AES256-SHA256:AES256-SHA:DHE-DSS-AES128-SHA:DES-CBC3-SHA
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | defaults
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   balance leastconn
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   log global
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   mode http
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   option redispatch
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   option httplog
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   option dontlognull
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   option forwardfor
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   timeout connect 5000
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   timeout client 50000
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   timeout server 50000
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | listen stats
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   bind :1936
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   mode http
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   stats enable
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   timeout connect 10s
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   timeout client 1m
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   timeout server 1m
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   stats hide-version
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   stats realm Haproxy\ Statistics
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   stats uri /
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   stats auth stats:stats
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | frontend default_port_80
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   bind :80
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   reqadd X-Forwarded-Proto:\ http
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   maxconn 4096
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   default_backend default_service
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | backend default_service
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   server analytics_es-data-1.1.u641c5bq5vkjklk8sb1scnnlc 10.0.7.5:9200 check inter 2000 rise 2 fall 3
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    |   server analytics_es-data-2.1.ic9an6bzj6aejs0lx0vzfpia6 10.0.7.7:9200 check inter 2000 rise 2 fall 3
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | INFO:haproxy:Launching HAProxy
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | INFO:haproxy:HAProxy has been launched(PID: 10)
analytics_loadbalancer.1.lcpgiz0ooeas@scw-swarm-master-01    | INFO:haproxy:===========END===========
</code></pre>

<h2>Testing Elasticsearch:</h2>

<p>Do a GET request on our HAProxy&rsquo;s Expose port: 9200</p>

<pre><code class="bash Test Elasticsearch on port 9200">$ curl -XGET http://127.0.0.1:9200
{
  "name" : "5306a0c2ee24",
  "cluster_name" : "es-cluster",
  "cluster_uuid" : "FUJmMekFQVq6zXofPCin2A",
  "version" : {
    "number" : "5.6.0",
    "build_hash" : "781a835",
    "build_date" : "2017-09-07T03:09:58.087Z",
    "build_snapshot" : false,
    "lucene_version" : "6.6.0"
  },
  "tagline" : "You Know, for Search"
}
</code></pre>

<p>Have a look at the <code>/_cat/nodes</code> API:</p>

<pre><code class="bash Get the Node Info">$  curl -XGET http://127.0.0.1:9200/_cat/nodes?v
ip       heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
10.0.7.6           28          84  14    3.09    2.28     1.49 mdi       -      56c1b0aebc5f
10.0.7.2           27          84  15    3.09    2.28     1.49 mdi       *      572c68bca904
10.0.7.4           29          84  15    3.09    2.28     1.49 mdi       -      5306a0c2ee24
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using the AWS CLI Tools to Grab CloudWatch Metrics for Elasticsearch]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/09/22/using-the-aws-cli-tools-to-grab-cloudwatch-metrics-for-elasticsearch/"/>
    <updated>2017-09-22T18:06:23-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/09/22/using-the-aws-cli-tools-to-grab-cloudwatch-metrics-for-elasticsearch</id>
    <content type="html"><![CDATA[<p>Using the AWS CLI Tools to get CloudWatch Metrics for Elasticsearch.</p>

<h2>Elasticsearch:</h2>

<p>List the JVM Memory Pressure Metric:</p>

<pre><code class="bash">$ aws cloudwatch list-metrics --namespace AWS/ES --metric-name JVMMemoryPressure
{
    "Metrics": [
        {
            "Namespace": "AWS/ES",
            "Dimensions": [
                {
                    "Name": "DomainName",
                    "Value": "elasticsearch-cluster"
                },
                {
                    "Name": "ClientId",
                    "Value": "123456789012"
                }
            ],
            "MetricName": "JVMMemoryPressure"
        }
    ]
}
</code></pre>

<h2>Metric: JVMMemoryPressure</h2>

<p>Getting Metrics for JVMMemoryPressure, every 10 Minutes for Max Statistic:</p>

<pre><code class="bash">$ aws cloudwatch get-metric-statistics --namespace AWS/ES --dimensions Name=DomainName,Value=elasticsearch-cluster Name=ClientId,Value=123456789012 --metric-name JVMMemoryPressure --start-time 2017-09-08T04:00:00 --end-time 2017-09-08T05:00:00 --period 600 --statistics Maximum
{
    "Datapoints": [
        {
            "Timestamp": "2017-09-08T04:40:00Z",
            "Maximum": 58.7,
            "Unit": "Percent"
        },
        {
            "Timestamp": "2017-09-08T04:00:00Z",
            "Maximum": 58.5,
            "Unit": "Percent"
        },
        {
            "Timestamp": "2017-09-08T04:30:00Z",
            "Maximum": 58.7,
            "Unit": "Percent"
        },
        {
            "Timestamp": "2017-09-08T04:20:00Z",
            "Maximum": 58.5,
            "Unit": "Percent"
        },
        {
            "Timestamp": "2017-09-08T04:50:00Z",
            "Maximum": 58.7,
            "Unit": "Percent"
        },
        {
            "Timestamp": "2017-09-08T04:10:00Z",
            "Maximum": 58.5,
            "Unit": "Percent"
        }
    ],
    "Label": "JVMMemoryPressure"
}
</code></pre>

<h2>Metric: WriteIOPS</h2>

<p>Getting Metrics for WriteIOPS, Every 10 Minutes for Max Statistic:</p>

<pre><code class="bash">$ aws cloudwatch get-metric-statistics --namespace AWS/ES --dimensions Name=DomainName,Value=elasticsearch-cluster Name=ClientId,Value=123456789012 --metric-name WriteIOPS --start-time 2017-09-08T04:00:00 --end-time 2017-09-08T05:00:00 --period 600 --statistics Maximum
{
    "Datapoints": [
        {
            "Timestamp": "2017-09-08T04:30:00Z",
            "Maximum": 0.5266666666666666,
            "Unit": "Count/Second"
        },
        {
            "Timestamp": "2017-09-08T04:00:00Z",
            "Maximum": 0.0,
            "Unit": "Count/Second"
        },
        {
            "Timestamp": "2017-09-08T04:40:00Z",
            "Maximum": 0.09666666666666666,
            "Unit": "Count/Second"
        },
        {
            "Timestamp": "2017-09-08T04:10:00Z",
            "Maximum": 0.0,
            "Unit": "Count/Second"
        },
        {
            "Timestamp": "2017-09-08T04:50:00Z",
            "Maximum": 0.07,
            "Unit": "Count/Second"
        },
        {
            "Timestamp": "2017-09-08T04:20:00Z",
            "Maximum": 0.0,
            "Unit": "Count/Second"
        }
    ],
    "Label": "WriteIOPS"
}
</code></pre>

<h2>Metric: FreeStorageSpace</h2>

<p>Getting Metrics for FreeStorageSpace in Megabytes:</p>

<pre><code class="bash">$ aws cloudwatch get-metric-statistics --namespace AWS/ES --dimensions Name=DomainName,Value=elasticsearch-cluster Name=ClientId,Value=123456789012 --metric-name FreeStorageSpace --start-time 2017-09-11T05:00:00 --end-time 2017-09-11T06:00:00 --period 600 --statistics Minimum --unit Megabytes
{
    "Datapoints": [
        {
            "Timestamp": "2017-09-11T05:50:00Z",
            "Minimum": 25510.438,
            "Unit": "Megabytes"
        },
        {
            "Timestamp": "2017-09-11T05:10:00Z",
            "Minimum": 25573.032,
            "Unit": "Megabytes"
        },
        {
            "Timestamp": "2017-09-11T05:20:00Z",
            "Minimum": 25554.051,
            "Unit": "Megabytes"
        },
        {
            "Timestamp": "2017-09-11T05:30:00Z",
            "Minimum": 25540.957,
            "Unit": "Megabytes"
        },
        {
            "Timestamp": "2017-09-11T05:40:00Z",
            "Minimum": 25525.473,
            "Unit": "Megabytes"
        },
        {
            "Timestamp": "2017-09-11T05:00:00Z",
            "Minimum": 25584.383,
            "Unit": "Megabytes"
        }
    ],
    "Label": "FreeStorageSpace"
}
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nginx Reverse Proxy for Elasticsearch and Kibana 5 on AWS]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/09/16/nginx-reverse-proxy-for-elasticsearch-and-kibana-5-on-aws/"/>
    <updated>2017-09-16T17:24:32-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/09/16/nginx-reverse-proxy-for-elasticsearch-and-kibana-5-on-aws</id>
    <content type="html"><![CDATA[<p>As up untill today, there&rsquo;s currently no VPC Support for Amazon&rsquo;s Elasticsearch Service.</p>

<p>So for scenarios where you would like to allow private network traffic to Elasticsearch is impossible straight out of the box as Amazon&rsquo;s Elasticsearch Services, only sees Public Internet Traffic.</p>

<p>We will setup 2 configs, one for Kibana and one for Elasticsearch, each one having its own FQDN:</p>

<ul>
<li>Kibana: <code>http://kibana.domain.com</code></li>
<li>Elasticsearch: <code>http://elasticsearch.domain.com</code></li>
</ul>


<h2>Workaround:</h2>

<p>There&rsquo;s a couple of workarounds, which includes:</p>

<ul>
<li>Nginx Reverse Proxy</li>
<li>NAT Gateway</li>
<li>Allow IAM Users/Roles</li>
</ul>


<p>Today we will tackle the Nginx Reverse Proxy Route.</p>

<p>The benefit of this, would be to associate an EIP to the Nginx EC2 Instnace, then whitelist your EIP with Elasticsearch, so the only traffic that will be accepted will be the traffic that is coming from the Nginx Instance. We will also apply an additional layer of security, in this case we will use HTTP Basic Authentication, then also authorize network sources on a Security Group level.</p>

<h2>Installing Nginx:</h2>

<p>In this case I am using Ubuntu 16.04, so we will need to install <code>nginx</code> and <code>apache2-utils</code> for creating the Basic HTTP Auth accounts.</p>

<pre><code class="bash">$ apt update &amp;&amp; apt upgrade -y
$ apt install nginx apache2-utils -y
</code></pre>

<h2>Configure Nginx:</h2>

<p>Our main config: <code>/etc/nginx/nginx.conf</code>:</p>

<pre><code class="bash /etc/nginx/nginx.conf">user www-data;
worker_processes auto;
pid /run/nginx.pid;
error_log /var/log/nginx/error.log;

events {
    worker_connections 1024;
}

http {

    # Basic Settings
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    server_names_hash_bucket_size 128;

    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # Logging Settings
        log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

    access_log /var/log/nginx/access.log main;

    # Gzip Settings
    gzip on;
    gzip_disable "msie6";

    # Elasticsearch and Kibana Configs
    include /etc/nginx/conf.d/elasticsearch.conf;
    include /etc/nginx/conf.d/kibana.conf;
}
</code></pre>

<p>Our <code>/etc/nginx/conf.d/elasticsearch.conf</code> configuration:</p>

<pre><code class="bash /etc/nginx/conf.d/elasticsearch.conf">server {

  listen 80;
  server_name elasticsearch.domain.com;

  # error logging
  error_log /var/log/nginx/elasticsearch_error.log;

  # authentication: elasticsearch
  auth_basic "Elasticsearch Auth";
  auth_basic_user_file /etc/nginx/.secrets_elasticsearch;

  location / {

    proxy_http_version 1.1;
    proxy_set_header Host https://search-elasticsearch-name.eu-west-1.es.amazonaws.com;
    proxy_set_header X-Real-IP &lt;ELASTIC-IP&gt;;
    proxy_set_header Connection "Keep-Alive";
    proxy_set_header Proxy-Connection "Keep-Alive";
    proxy_set_header Authorization "";

    proxy_pass https://search-elasticsearch-name.eu-west-1.es.amazonaws.com/;
    proxy_redirect https://search-elasticsearch-name.eu-west-1.es.amazonaws.com/ http://&lt;ELASTIC-IP&gt;/;

  }

  # ELB Health Checks
  location /status {
    root /usr/share/nginx/html/;
  }

}
</code></pre>

<p>Our <code>/etc/nginx/conf.d/kibana.conf</code> configuration:</p>

<pre><code class="bash /etc/nginx/conf.d/kibana.conf">server {

  listen 80;
  server_name kibana.domain.com;

  # error logging
  error_log /var/log/nginx/kibana_error.log;

  # authentication: kibana
  auth_basic "Kibana Auth";
  auth_basic_user_file /etc/nginx/.secrets_kibana;

  location / {

    proxy_http_version 1.1;
    proxy_set_header Host https://search.elasticsearch-name.eu-west-1.es.amazonaws.com;
    proxy_set_header X-Real-IP &lt;ELASTIC-IP&gt;;
    proxy_set_header Connection "Keep-Alive";
    proxy_set_header Proxy-Connection "Keep-Alive";
    proxy_set_header Authorization "";

    proxy_pass https://search.elasticsearch-name.eu-west-1.es.amazonaws.com/_plugin/kibana/;
    proxy_redirect https://search.elasticsearch-name.eu-west-1.es.amazonaws.com/_plugin/kibana/ http://&lt;ELASTIC-IP&gt;/kibana/;

  }

      location ~ (/app/kibana|/app/timelion|/bundles|/es_admin|/plugins|/api|/ui|/elasticsearch) {
         proxy_pass              https://search.elasticsearch-name.eu-west-1.es.amazonaws.com;
         proxy_set_header        Host $host;
         proxy_set_header        X-Real-IP $remote_addr;
         proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;
         proxy_set_header        X-Forwarded-Proto $scheme;
         proxy_set_header        X-Forwarded-Host $http_host;
         proxy_set_header    Authorization  "";
    }
}
</code></pre>

<p>Once you have replaced the elasticsearch endpoint and your EPI values, we can go ahead and create the auth accounts.</p>

<h2>Create User Accounts for HTTP Basic Auth</h2>

<p>Create the 2 accounts for authentication on kibana and elasticsearch:</p>

<pre><code class="bash">$ htpasswd -c /etc/nginx/.secrets_elasticsearch elasticsearch-admin
$ htpasswd -c /etc/nginx/.secrets_kibana kibana-admin
</code></pre>

<h2>Restart Nginx:</h2>

<p>Restart and enable Nginx on boot:</p>

<pre><code class="bash">$ systemctl enable nginx
$ systemctl restart nginx
</code></pre>

<p>Once your Nginx Service is running, you should be able to access Kibana and Elasticsearch using the credentials that you created.</p>

<h2>Resources:</h2>

<ul>
<li><a href="https://www.nginx.com/blog/tcp-load-balancing-udp-load-balancing-nginx-tips-tricks/">https://www.nginx.com/blog/tcp-load-balancing-udp-load-balancing-nginx-tips-tricks/</a></li>
<li><a href="https://www.elastic.co/blog/playing-http-tricks-nginx">https://www.elastic.co/blog/playing-http-tricks-nginx</a></li>
<li><a href="https://sysadmins.co.za/aws-access-kibana-5-behind-elb-via-nginx-reverse-proxy-on-custom-dns/">https://sysadmins.co.za/aws-access-kibana-5-behind-elb-via-nginx-reverse-proxy-on-custom-dns/</a></li>
</ul>


<center>
<script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Buy Me a Coffee', '#46b798', 'A6423ZIQ');kofiwidget2.draw();</script> 
</center>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Secure Your Access to Kibana 5 and Elasticsearch 5 With Nginx for AWS]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/08/31/secure-your-access-to-kibana-5-and-elasticsearch-5-with-nginx-for-aws/"/>
    <updated>2017-08-31T10:40:09-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/08/31/secure-your-access-to-kibana-5-and-elasticsearch-5-with-nginx-for-aws</id>
    <content type="html"><![CDATA[<p>As until now, AWS does not offer VPC Support for Elasticsearch, so this make things a bit difficult authorizing Private IP Ranges.</p>

<p>One workaround would be to setup a Nginx Reverse Proxy on AWS within the your Private VPC, associate a EIP on your Nginx EC2 Instance, then authorize your EIP on your Elasticsearch IP Access Policy.</p>

<h2>Our Setup:</h2>

<p>In this setup, we will have an Internal ELB (Elastic Load Balancer), which we will associate 1 or more EC2 Nginx Instances behind the ELB, then setup our Nginx to Revere Proxy our connections through to our Elasticsearch Endpoint.</p>

<p>We will also setup Basic HTTP Authentication for our <code>/</code> elasticsearch endpoint, and our <code>/kibana</code> endpoint. But we will keep the authentication seperate from each other, so that credentials for ES and Kibana is not the same, but depending on your use case, you can allow both endpoints to reference the same credential file.</p>

<h2>Install Nginx</h2>

<p>Depending on your Linux Distribution, the package manager may differ, I am using Amazon Linux:</p>

<pre><code class="bash Install Nginx">$ sudo yum update -y
$ sudo yum install nginx httpd-tools -y
</code></pre>

<h2>Configure Nginx:</h2>

<p>Remove the default configuration and replace the <code>nginx.conf</code> with the following:</p>

<pre><code class="bash Remove Default Nginx Config">$ sudo rm -r /etc/nginx/nginx.conf
</code></pre>

<p>Main Nginx Configuration:</p>

<pre><code class="bash /etc/nginx/nginx.conf">user nginx;
worker_processes auto;
pid /run/nginx.pid;
error_log /var/log/nginx/error.log;

events {
    worker_connections 1024;
}

http {

    # Basic Settings
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    server_names_hash_bucket_size 128;

    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # Logging Settings
        log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

    access_log /var/log/nginx/access.log main;

    # Gzip Settings
    gzip on;
    gzip_disable "msie6";

    # Elasticsearch Config
    include /etc/nginx/conf.d/elasticsearch.conf;
}
</code></pre>

<p>The Reverse Proxy Configuration:</p>

<pre><code class="bash /etc/nginx/conf.d/elasticsearch.conf">server {

  listen 80;
  server_name elk.mydomain.com;

  # error logging
  error_log /var/log/nginx/elasticsearch_error.log;

  # authentication: server wide
  #auth_basic "Auth";
  #auth_basic_user_file /etc/nginx/.secrets;

  location / {

    # authentication: elasticsearch
    auth_basic "Elasticsearch Auth";
    auth_basic_user_file /etc/nginx/.secrets_elasticsearch;

    proxy_http_version 1.1;
    proxy_set_header Host https://search.eu-west-1.es.amazonaws.com;
    proxy_set_header X-Real-IP {NGINX-EIP};
    proxy_set_header Connection "Keep-Alive";
    proxy_set_header Proxy-Connection "Keep-Alive";
    proxy_set_header Authorization "";

    proxy_pass https://search.eu-west-1.es.amazonaws.com/;
    proxy_redirect https://search.eu-west-1.es.amazonaws.com/ http://{NGINX-EIP}/;

  }

  location /kibana {

    # authentication: kibana
    auth_basic "Kibana Auth";
    auth_basic_user_file /etc/nginx/.secrets_kibana;

    proxy_http_version 1.1;
    proxy_set_header Host https://search.eu-west-1.es.amazonaws.com;
    proxy_set_header X-Real-IP {NGINX-EIP};
    proxy_set_header Connection "Keep-Alive";
    proxy_set_header Proxy-Connection "Keep-Alive";
    proxy_set_header Authorization "";

    proxy_pass https://search.eu-west-1.es.amazonaws.com/_plugin/kibana/;
    proxy_redirect https://search.eu-west-1.es.amazonaws.com/_plugin/kibana/ http://{NGINX_EIP}/kibana/;

  }

  # elb checks
  location /status {
    root /usr/share/nginx/html/;
  }

}
</code></pre>

<h2>Setup Authentication:</h2>

<p>Setup the authentication for elasticsearch and kibana:</p>

<pre><code class="bash Create Auth for Kibana and Elasticsearch">$ sudo htpasswd -c /etc/nginx/.secrets_elasticsearch admin
$ sudo htpasswd -c /etc/nginx/.secrets_kibana admin
</code></pre>

<h2>Restart Nginx and Enable on Startup</h2>

<p>Restart the nginx process and enable the process on boot:</p>

<pre><code class="bash Restart Nginx">$ sudo /etc/init.d/nginx restart
$ sudo chkconfig nginx on
</code></pre>

<h2>Configure ELB:</h2>

<p>Create a New Internal ELB, set the Backend Instances on Port 80, and the healthcheck should point to <code>/status/index.html</code> as this location block does not require authentication and our ELB will be able to get a 200 reponse if all is good.
Next you can configure your Route 53 Hosted Zone, <code>elk.mydomain.com</code> to map to your ELB.</p>

<h2>End Result</h2>

<p>Now you should be able to access Elasticsearch on <code>http://elk.mydomain.com/</code> and Kibana on <code>http://elk.mydomain.com/kibana</code> after authenticating.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Structured Search With Elasticsearch]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/08/25/structured-search-with-elasticsearch/"/>
    <updated>2017-08-25T23:42:19-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/08/25/structured-search-with-elasticsearch</id>
    <content type="html"><![CDATA[<p><a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/structured-search.html">Structured Search</a> with Elasticsearch:</p>

<p><img src="https://dl.dropboxusercontent.com/u/31991539/images/sysadmins/elasticsearch-logo.png" alt="" /></p>

<p>In this post we will ingest some dummy data into elasticsearch, then we will perform some queries to get the following info:</p>

<ul>
<li>Student Names</li>
<li>Student Ages</li>
<li>Include / Exclude</li>
<li>Marks greater than</li>
<li>Finding Students with Specific marks, etc.</li>
</ul>


<h2>Create the Mapping for our Index:</h2>

<p>for our Data as we wont use the <a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/dynamic-mapping.html">Dynamic Mapping</a> that comes with by default:</p>

<pre><code>$ curl -XPUT http://127.0.0.1:9200/school -d '
{
  "mappings": {
    "students": {
      "properties": {
        "name": {"type": "string"},
        "marks": {"type": "short"},
        "gender": {"type": "string"},
        "age": {"type": "short"}
      }
    }
  }
}'
</code></pre>

<h2>Ingesting the Data into Elasticsearch:</h2>

<p>You can either using the following:</p>

<pre><code>curl -XPOST http://127.0.0.1:9200/school/students/ -d ' {"name": "james", "marks": 60, "gender": "male", "age": 14} '
curl -XPOST http://127.0.0.1:9200/school/students/ -d ' {"name": "simon", "marks": 70, "gender": "male", "age": 15} '
curl -XPOST http://127.0.0.1:9200/school/students/ -d ' {"name": "samantha", "marks": 70, "gender": "female", "age": 14} '
curl -XPOST http://127.0.0.1:9200/school/students/ -d ' {"name": "john", "marks": 60, "gender": "male", "age": 14} '
curl -XPOST http://127.0.0.1:9200/school/students/ -d ' {"name": "michelle", "marks": 30, "gender": "female", "age": 14} '
curl -XPOST http://127.0.0.1:9200/school/students/ -d ' {"name": "max", "marks": 75, "gender": "female", "age": 15} '
curl -XPOST http://127.0.0.1:9200/school/students/ -d ' {"name": "frank", "marks": 79, "gender": "male", "age": 15} '
</code></pre>

<p>or using the Bulk API:</p>

<p>Save the following as <code>bulk.json</code>:</p>

<pre><code>{"index" : {"_index" : "school", "_type" : "students", "_id" : "AV4cYob6VgXGdBeaBa1c" } }
{"name": "james", "marks": 60, "gender": "male", "age": 14}
{"index" : {"_index" : "school", "_type" : "students", "_id" : "AV4cYqU3VgXGdBeaBa1d" } }
{"name": "simon", "marks": 70, "gender": "male", "age": 15}
{"index" : {"_index" : "school", "_type" : "students", "_id" : "AV4cYrzFVgXGdBeaBa1v" } }
{"name": "samantha", "marks": 70, "gender": "female", "age": 14}
{"index" : {"_index" : "school", "_type" : "students", "_id" : "AV4cYtuUVgXGdBeaBa2I" } }
{"name": "john", "marks": 60, "gender": "male", "age": 14}
{"index" : {"_index" : "school", "_type" : "students", "_id" : "AV4cYvMOVgXGdBeaBa2K" } }
{"name": "michelle", "marks": 30, "gender": "female", "age": 14}
{"index" : {"_index" : "school", "_type" : "students", "_id" : "AV4cYwwnVgXGdBeaBa2j" } }
{"name": "max", "marks": 75, "gender": "female", "age": 15}
{"index" : {"_index" : "school", "_type" : "students", "_id" : "AV4cYyXYVgXGdBeaBa29" } }
{"name": "frank", "marks": 79, "gender": "male", "age": 15}
</code></pre>

<p>Then use the Bulk API to Ingest into Elasticsearch:</p>

<pre><code>$ curl -s -XPOST http://127.0.0.1:9200/_bulk --data-binary @bulk.json
</code></pre>

<p>Then you should have 7 documents ingested into Elasticsearch:</p>

<pre><code>$ curl -XGET http://10.4.156.13:9200/_cat/indices/school?v
health status index  pri rep docs.count docs.deleted store.size pri.store.size
yellow open   school   5   1          7            0     19.4kb         19.4kb
</code></pre>

<p>You should notice that I have a yellow state, as this is a single instance where I am running elasticsearch on, so there will be unassigned shards.</p>

<h2>Query Student Names:</h2>

<p>Let&rsquo;s search for the Student with the Name <code>Max</code>:</p>

<pre><code>$ curl -XGET http://10.4.156.13:9200/school/students/_search?pretty -d '{"query": {"term" : {"name" : "max"}}}'
</code></pre>

<pre><code>{
  "took" : 5,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "school",
      "_type" : "students",
      "_id" : "AV4cRqaLVgXGdBeaBaWD",
      "_score" : 1.0,
      "_source" : {
        "name" : "max",
        "marks" : 75,
        "gender" : "female",
        "age" : 15
      }
    } ]
  }
}
</code></pre>

<h2>Search Student Ages:</h2>

<p>Search for all students with the age of <code>15</code>:</p>

<pre><code>$ curl -XGET http://10.4.156.13:9200/school/students/_search?pretty -d '{"query": {"term" : {"age" : 15}}}'
</code></pre>

<pre><code>{
  "took" : 14,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 3,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "school",
      "_type" : "students",
      "_id" : "AV4cRWPPVgXGdBeaBaVF",
      "_score" : 1.0,
      "_source" : {
        "name" : "simon",
        "marks" : 70,
        "gender" : "male",
        "age" : 15
      }
    }, {
      "_index" : "school",
      "_type" : "students",
      "_id" : "AV4cRqaLVgXGdBeaBaWD",
      "_score" : 1.0,
      "_source" : {
        "name" : "max",
        "marks" : 75,
        "gender" : "female",
        "age" : 15
      }
    }, {
      "_index" : "school",
      "_type" : "students",
      "_id" : "AV4cRxayVgXGdBeaBaWg",
      "_score" : 0.30685282,
      "_source" : {
        "name" : "frank",
        "marks" : 79,
        "gender" : "male",
        "age" : 15
      }
    } ]
  }
}
</code></pre>

<h2>Query, Include but also Exclude:</h2>

<p>Query everyone that is 14, and which is males, except the Student called John:</p>

<pre><code>$ curl -XGET http://10.4.156.13:9200/school/students/_search?pretty -d '
{
   "query" : {
      "constant_score" : {
         "filter" : {
            "bool" : {
              "should" : [
                 { "term" : {"age" : 14}},
                 { "term" : {"gender" : "male"}}
              ],
              "must_not" : {
                 "term" : {"name" : "john"}
              }
           }
         }
      }
   }
}'
</code></pre>

<pre><code>"hits" : {
  "total" : 5,
  "max_score" : 1.0,
  "hits" : [ {
    "_index" : "school",
    "_type" : "students",
    "_id" : "AV4cYob6VgXGdBeaBa1c",
    "_score" : 1.0,
    "_source" : {
      "name" : "james",
      "marks" : 60,
      "gender" : "male",
      "age" : 14
    }
  }, {
    "_index" : "school",
    "_type" : "students",
    "_id" : "AV4cYvMOVgXGdBeaBa2K",
    "_score" : 1.0,
    "_source" : {
      "name" : "michelle",
      "marks" : 30,
      "gender" : "female",
      "age" : 14
    }
  }, {
    "_index" : "school",
    "_type" : "students",
    "_id" : "AV4cYyXYVgXGdBeaBa29",
    "_score" : 1.0,
    "_source" : {
      "name" : "frank",
      "marks" : 79,
      "gender" : "male",
      "age" : 15
    }
  }, {
    "_index" : "school",
    "_type" : "students",
    "_id" : "AV4cYqU3VgXGdBeaBa1d",
    "_score" : 1.0,
    "_source" : {
      "name" : "simon",
      "marks" : 70,
      "gender" : "male",
      "age" : 15
    }
  }, {
    "_index" : "school",
    "_type" : "students",
    "_id" : "AV4cYrzFVgXGdBeaBa1v",
    "_score" : 1.0,
    "_source" : {
      "name" : "samantha",
      "marks" : 70,
      "gender" : "female",
      "age" : 14
    }
  } ]
}
}
</code></pre>

<h2>Query for Age, Gender with High Grades:</h2>

<p>Show me everyone thats 14 years old, including males, but only with scores better than 70 and up</p>

<pre><code>$ curl -XGET http://10.4.156.13:9200/school/students/_search?pretty -d '
{
   "query" : {
      "constant_score" : {
         "filter" : {
            "bool" : {
              "should" : [
                 { "term" : {"age" : 14}},
                 { "term" : {"gender" : "male"}}
              ],
              "must_not" : {
                 "range" : {"marks": {"lt": 70, "gte": 0}}
              }
           }
         }
      }
   }
}'
</code></pre>

<h2>Everyone that got 70 and more:</h2>

<p>Show me all the students that has marks of 70 and above:</p>

<pre><code>$ curl -XGET http://10.4.156.13:9200/school/students/_search?pretty -d '
{
   "query" : {
      "constant_score" : {
         "filter" : {
            "bool" : {
            "must" : [
              { "range" : {"marks" : {"lt": 100, "gte": 70}}}
            ]
          }
        }              
     }
   }
}'
</code></pre>

<pre><code>{
  "took" : 6,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 4,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "school",
      "_type" : "students",
      "_id" : "AV4cYwwnVgXGdBeaBa2j",
      "_score" : 1.0,
      "_source" : {
        "name" : "max",
        "marks" : 75,
        "gender" : "female",
        "age" : 15
      }
    }, {
      "_index" : "school",
      "_type" : "students",
      "_id" : "AV4cYyXYVgXGdBeaBa29",
      "_score" : 1.0,
      "_source" : {
        "name" : "frank",
        "marks" : 79,
        "gender" : "male",
        "age" : 15
      }
    }, {
      "_index" : "school",
      "_type" : "students",
      "_id" : "AV4cYqU3VgXGdBeaBa1d",
      "_score" : 1.0,
      "_source" : {
        "name" : "simon",
        "marks" : 70,
        "gender" : "male",
        "age" : 15
      }
    }, {
      "_index" : "school",
      "_type" : "students",
      "_id" : "AV4cYrzFVgXGdBeaBa1v",
      "_score" : 1.0,
      "_source" : {
        "name" : "samantha",
        "marks" : 70,
        "gender" : "female",
        "age" : 14
      }
    } ]
  }
}
</code></pre>

<p>Or you can do it like this:</p>

<h2>Query Range only with <code>gt</code>:</h2>

<p>Show me everyone that got more than 70:</p>

<pre><code>$ curl -XGET http://10.4.156.13:9200/school/students/_search?pretty -d '
{
   "query" : {
      "constant_score" : {
         "filter" : {
            "bool" : {
              "must" : [
                { "range" : {"marks" : {"gt": 70}}}
            ]
          }
        }              
     }
   }
}'
</code></pre>

<pre><code>{
  "took" : 7,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "school",
      "_type" : "students",
      "_id" : "AV4cYwwnVgXGdBeaBa2j",
      "_score" : 1.0,
      "_source" : {
        "name" : "max",
        "marks" : 75,
        "gender" : "female",
        "age" : 15
      }
    }, {
      "_index" : "school",
      "_type" : "students",
      "_id" : "AV4cYyXYVgXGdBeaBa29",
      "_score" : 1.0,
      "_source" : {
        "name" : "frank",
        "marks" : 79,
        "gender" : "male",
        "age" : 15
      }
    } ]
  }
}
</code></pre>

<h2>Gender Specific, with Grades more than 70:</h2>

<p>Show me females that has more than 70:</p>

<pre><code>$ curl -XGET http://10.4.156.13:9200/school/students/_search?pretty -d '
{
   "query" : {
      "constant_score" : {
         "filter" : {
            "bool" : {
            "must" : [
              { "range" : {"marks" : {"lt": 100, "gte": 70}}}
            ],
            "must_not": [
              {"term": {"gender": "male"}}
            ]
          }
        }              
     }
   }
}'
</code></pre>

<pre><code>{
  "took" : 13,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "school",
      "_type" : "students",
      "_id" : "AV4cYwwnVgXGdBeaBa2j",
      "_score" : 1.0,
      "_source" : {
        "name" : "max",
        "marks" : 75,
        "gender" : "female",
        "age" : 15
      }
    }, {
      "_index" : "school",
      "_type" : "students",
      "_id" : "AV4cYrzFVgXGdBeaBa1v",
      "_score" : 1.0,
      "_source" : {
        "name" : "samantha",
        "marks" : 70,
        "gender" : "female",
        "age" : 14
      }
    } ]
  }
}
</code></pre>

<h2>Grade Specific:</h2>

<p>Show me the ones that got 30 and 75:</p>

<pre><code>$ curl -XGET http://10.4.156.13:9200/school/students/_search?pretty -d '
{
   "query" : {
      "constant_score" : {
         "filter" : {
            "terms" : {
              "marks": [30, 75]
            }
         }
      }
   }
}'
</code></pre>

<pre><code>{
  "took" : 6,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "school",
      "_type" : "students",
      "_id" : "AV4cYwwnVgXGdBeaBa2j",
      "_score" : 1.0,
      "_source" : {
        "name" : "max",
        "marks" : 75,
        "gender" : "female",
        "age" : 15
      }
    }, {
      "_index" : "school",
      "_type" : "students",
      "_id" : "AV4cYvMOVgXGdBeaBa2K",
      "_score" : 1.0,
      "_source" : {
        "name" : "michelle",
        "marks" : 30,
        "gender" : "female",
        "age" : 14
      }
    } ]
  }
}
</code></pre>

<p>For more information on this, have a look at <a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/structured-search.html">Elasticsearch: Structured Search</a></p>
]]></content>
  </entry>
  
</feed>
