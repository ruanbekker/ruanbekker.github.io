<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Glusterfs | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/glusterfs/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2020-06-06T01:29:17+02:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Setup a 3 Node Replicated Storage Volume With GlusterFS]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/03/05/setup-a-3-node-replicated-storage-volume-with-glusterfs/"/>
    <updated>2019-03-05T21:01:37+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/03/05/setup-a-3-node-replicated-storage-volume-with-glusterfs</id>
    <content type="html"><![CDATA[<p><img src="https://access.redhat.com/documentation/en-US/Red_Hat_Storage/2.1/html/Administration_Guide/images/Replicated_Volume.png" alt="" /></p>

<p>In one of my earlier posts on <a href="https://sysadmins.co.za/tag/glusterfs">GlusterFS</a>, we went through the steps on how to setup a <a href="https://sysadmins.co.za/setup-a-distributed-storage-volume-with-glusterfs/">Distributed Storage Volume</a>, where the end result was to have scalable storage, where size was the requirement.</p>

<p><a href="https://bekkerclothing.com/collections/developer?utm_source=blog.ruanbekker.com&utm_medium=blog&utm_campaign=leaderboard_ad" target="_blank"><img alt="bekker-clothing-developer-tshirts" src="https://user-images.githubusercontent.com/567298/70170981-7c278a80-16d6-11ea-9759-6621d02c1423.png"></a></p>

<h2>What will we be doing today with GlusterFS?</h2>

<p>Today, we will be going through the steps on how to setup a Replicated Storage Volume with GlusterFS, where we will have 3 GlusterFS Nodes, and using the replication factor of 3.</p>

<p><strong>Replication Factor of 3:</strong></p>

<p>In other words, having 3 copies of our data and in our case, since we will have 3 nodes in our cluster, a copy of our data will reside on each node.</p>

<p><strong>What about Split-Brain:</strong></p>

<p>In Clustering, we get the term Split-Brain, where a node dies or leaves the cluster, the cluster reforms itself with the available nodes and then during this reformation, instead of the remaining nodes staying with the same cluster, 2 subset of cluster are created, and they are not aware of each other, which causes data corruption, here&rsquo;s a great resource on <a href="http://techthoughts.typepad.com/managing_computers/2007/10/split-brain-quo.html">Split-Brain</a></p>

<p>To prevent Split-Brain in GlusterFS, we can setup a <a href="https://gluster.readthedocs.io/en/latest/Administrator%20Guide/arbiter-volumes-and-quorum/">Arbiter Volume</a>. In a Replica Count of 3 and Arbiter count of 1: 2 Nodes will hold the replicated data, and the 1 Node which will be the Arbiter node, will only host the file/directory names and metadata but not any data. I will write up an <a href="">article</a> on this in the future.</p>

<h2>Getting Started:</h2>

<p>Let&rsquo;s get started on setting up a 3 Node Replicated GlusterFS. Each node will have an additional drive that is 50GB in size, which will be part of our GlusterFS Replicated Volume. I will also be using Ubuntu 16.04 as my linux distro.</p>

<p><strong>Preparing DNS Resolution:</strong></p>

<p>I will install GlusterFS on each node, and in my setup I have the following DNS entries:</p>

<ul>
<li>gfs01 (10.0.0.2)</li>
<li>gfs02 (10.0.0.3)</li>
<li>gfs03 (10.0.0.4)</li>
</ul>


<p><strong>Preparing our Secondary Drives:</strong></p>

<p>I will be formatting my drives with <code>XFS</code>. Listing our block volumes:</p>

<pre><code class="bash">$ lsblk
NAME MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vdb  253:16   0 46.6G  0 disk
vda  253:0    0 18.6G  0 disk /
</code></pre>

<p>Creating the FileSystem with XFS, which we will be running on each node:</p>

<pre><code class="bash">$ mkfs.xfs /dev/vdb
</code></pre>

<p>Then creating the directories where our bricks will reside, and also add an entry to our <code>/etc/fstab</code> so that our disk gets mounted when the operating system boots:</p>

<pre><code class="bash"># node: gfs01
$ mkdir /gluster/bricks/1 -p
$ echo '/dev/vdb /gluster/bricks/1 xfs defaults 0 0' &gt;&gt; /etc/fstab
$ mount -a
$ mkdir /gluster/bricks/1/brick

# node: gfs02
$ mkdir /gluster/bricks/2 -p
$ echo '/dev/vdb /gluster/bricks/2 xfs defaults 0 0' &gt;&gt; /etc/fstab
$ mount -a
$ mkdir /gluster/bricks/2/brick

# node: gfs03
$ mkdir /gluster/bricks/3 -p
$ echo '/dev/vdb /gluster/bricks/3 xfs defaults 0 0' &gt;&gt; /etc/fstab
$ mount -a
$ mkdir /gluster/bricks/3/brick
</code></pre>

<p>After this has been done, we should see that the disks are mounted, for example on node: <code>gfs01</code>:</p>

<pre><code>$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/vda         18G  909M   17G   3% /
/dev/vdb         47G   80M   47G   1% /gluster/bricks/1
</code></pre>

<h2>Installing GlusterFS on Each Node:</h2>

<p>Installing GlusterFS, repeat this on all 3 Nodes:</p>

<pre><code class="bash">$ apt update &amp;&amp; sudo apt upgrade -y
$ apt install xfsprogs attr glusterfs-server glusterfs-common glusterfs-client -y
$ systemctl enable glusterfs-server
</code></pre>

<p>In order to add the nodes to the trusted storage pool, we will have to add them by using <code>gluster peer probe</code>. Make sure that you can resolve the hostnames to the designated IP Addresses, and that traffic is allowed.</p>

<pre><code class="bash">$ gluster peer probe gfs01
$ gluster peer probe gfs02
$ gluster peer probe gfs03
</code></pre>

<p>Now that we have added our nodes to our trusted storage pool, lets verify that by listing our pool:</p>

<pre><code class="bash">$ gluster pool list
UUID                                    Hostname                State
f63d0e77-9602-4024-8945-5a7f7332bf89    gfs02                   Connected
2d4ac6c1-0611-4e2e-b4af-9e4aa8c1556d    gfs03                   Connected
6a604cd9-9a9c-406d-b1b7-69caf166a20e    localhost               Connected
</code></pre>

<p>Great! All looks good.</p>

<h2>Create the Replicated GlusterFS Volume:</h2>

<p>Let&rsquo;s create our Replicated GlusterFS Volume, named <code>gfs</code>:</p>

<pre><code class="bash">$ gluster volume create gfs \
  replica 3 \
  gfs01:/gluster/bricks/1/brick \
  gfs02:/gluster/bricks/2/brick \
  gfs03:/gluster/bricks/2/brick 

volume create: gfs: success: please start the volume to access data
</code></pre>

<p>Now that our volume is created, lets list it to verify that it is created:</p>

<pre><code class="bash">$ gluster volume list
gfs
</code></pre>

<p>Now, start the volume:</p>

<pre><code class="bash">$ gluster volume start gfs
volume start: gfs: success
</code></pre>

<p>View the status of our volume:</p>

<pre><code class="bash">$ gluster volume status gfs
Status of volume: gfs
Gluster process                             TCP Port  RDMA Port  Online  Pid
------------------------------------------------------------------------------
Brick gfs01:/gluster/bricks/1/brick         49152     0          Y       6450
Brick gfs02:/gluster/bricks/2/brick         49152     0          Y       3460
Brick gfs03:/gluster/bricks/3/brick         49152     0          Y       3309
</code></pre>

<p>Next, view the volume inforation:</p>

<pre><code class="bash">$ gluster volume info gfs

Volume Name: gfs
Type: Replicate
Volume ID: 6f827df4-6df5-4c25-99ee-8d1a055d30f0
Status: Started
Number of Bricks: 1 x 3 = 3
Transport-type: tcp
Bricks:
Brick1: gfs01:/gluster/bricks/1/brick
Brick2: gfs02:/gluster/bricks/2/brick
Brick3: gfs03:/gluster/bricks/3/brick
</code></pre>

<h2>Security:</h2>

<p>From a GlusterFS level, it will allow clients to connect by default. To authorize these 3 nodes to connect to the GlusterFS Volume:</p>

<pre><code class="bash">$ gluster volume set gfs auth.allow 10.0.0.2,10.0.0.3,10.0.0.4
</code></pre>

<p>Then if you would like to remove this rule:</p>

<pre><code class="bash">$ gluster volume set gfs auth.allow *
</code></pre>

<h2>Mount the GlusterFS Volume to the Host:</h2>

<p>Mount the GlusterFS Volume to each node, so we will have to mount it to each node, and also append it to our <code>/etc/fstab</code> file so that it mounts on boot:</p>

<pre><code class="bash">$ echo 'localhost:/gfs /mnt glusterfs defaults,_netdev,backupvolfile-server=localhost 0 0' &gt;&gt; /etc/fstab
$ mount.glusterfs localhost:/gfs /mnt
</code></pre>

<p><strong>Verify the Mounted Volume:</strong></p>

<p>Check the mounted disks, and you will find that the Replicated GlusterFS Volume is mounted on our <code>/mnt</code> partition.</p>

<pre><code class="bash">$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/vda         18G  909M   17G   3% /
/dev/vdb         47G   80M   47G   1% /gluster/bricks/1
localhost:/gfs   47G   80M   47G   1% /mnt
</code></pre>

<p>You will note that GlusterFS Volume has a total size of 47GB usable space, which is the same size as one of our disks, but that is because we have a replicated volume with a replication factor of 3:  <code>(47 * 3 / 3)</code></p>

<p>Now we have a Storage Volume which has 3 Replicas, one copy on each node, which allows us Data Durability on our Storage.</p>

<p><p></p>

<p><center><script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init(&lsquo;Buy Me a Coffee&rsquo;, &lsquo;#46b798&rsquo;, &lsquo;A6423ZIQ&rsquo;);kofiwidget2.draw();</script></center></p>

<p><p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Container Persistent Storage for Docker Swarm Using a GlusterFS Volume Plugin]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/03/05/container-persistent-storage-for-docker-swarm-using-a-glusterfs-volume-plugin/"/>
    <updated>2019-03-05T20:18:30+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/03/05/container-persistent-storage-for-docker-swarm-using-a-glusterfs-volume-plugin</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/53351889-85572000-392a-11e9-9720-464e9318206e.jpg" alt="" /></p>

<p>From one of my previous posts I demonstrated how to provide persistent storage for your containers by using a <a href="https://blog.ruanbekker.com/blog/2018/02/16/guide-to-setup-docker-convoy-volume-driver-for-docker-swarm-with-nfs/">Convoy NFS Plugin</a>.</p>

<p>I&rsquo;ve stumbled upon one AWESOME GlusterFS Volume Plugin for Docker by <a href="https://github.com/trajano/docker-volume-plugins/tree/master/glusterfs-volume-plugin">@trajano</a>, please have a look at his repository. I&rsquo;ve been waiting for some time for one solid glusterfs volume plugin, and it works great.</p>

<p><a href="https://bekkerclothing.com/collections/developer?utm_source=blog.ruanbekker.com&utm_medium=blog&utm_campaign=leaderboard_ad" target="_blank"><img alt="bekker-clothing-developer-tshirts" src="https://user-images.githubusercontent.com/567298/70170981-7c278a80-16d6-11ea-9759-6621d02c1423.png"></a></p>

<h2>What we will be doing today</h2>

<p>We will setup a 3 node replicated glusterfs volume and show how easy it is to install the volume plugin and then demonstrate how storage from our swarms containers are persisted.</p>

<p>Our servers that we will be using will have the private ip&rsquo;s as shown below:</p>

<pre><code>10.22.125.101
10.22.125.102
10.22.125.103
</code></pre>

<h2>Setup GlusterFS</h2>

<p>Have a look at <a href="https://blog.ruanbekker.com/blog/2019/03/05/setup-a-3-node-replicated-storage-volume-with-glusterfs/?referral=github.com">this</a> post to setup the glusterfs volume.</p>

<h2>Install the GlusterFS Volume Plugin</h2>

<p>Below I&rsquo;m installing the plugin and setting the alias name as <code>glusterfs</code>, granting all permissions and keeping the plugin in a disabled state.</p>

<pre><code class="bash">$ docker plugin install --alias glusterfs trajano/glusterfs-volume-plugin --grant-all-permissions --disable
</code></pre>

<p>Set the glusterfs servers:</p>

<pre><code>$ docker plugin set glusterfs SERVERS=10.22.125.101,10.22.125.102,10.22.125.103
</code></pre>

<p>Enable the glusterfs plugin:</p>

<pre><code>$ docker plugin enable glusterfs
</code></pre>

<h2>Create a Service in Docker Swarm</h2>

<p>Deploy a sample service on docker swarm with a volume backed by glusterfs. Note that my glusterfs volume is called <code>gfs</code></p>

<pre><code class="yaml">version: "3.4"

services:
  foo:
    image: alpine
    command: ping localhost
    networks:
      - net
    volumes:
      - vol1:/tmp

networks:
  net:
    driver: overlay

volumes:
  vol1:
    driver: glusterfs
    name: "gfs/vol1"
</code></pre>

<p>Deploy the stack:</p>

<pre><code class="bash">$ docker stack deploy -c docker-compose.yml test
Creating service test_foo
</code></pre>

<p>Have a look on which node is your container running:</p>

<pre><code class="bash">$ docker service ps test_foo
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
jfwzb7yxnrxx        test_foo.1          alpine:latest       swarm-worker-1      Running             Running 37 seconds ago
</code></pre>

<p>Now jump to the <code>swarm-worker-1</code> node and verify that the container is running on that node:</p>

<pre><code class="bash">$ docker ps
CONTAINER ID        IMAGE                                          COMMAND                  CREATED             STATUS                  PORTS               NAMES
d469f341d836        alpine:latest                                  "ping localhost"           59 seconds ago      Up 57 seconds                               test_foo.1.jfwzb7yxnrxxnd0qxtcjex8lu
</code></pre>

<p>Now since the container is running on this node, we will also see that the volume defined in our task configuration will also be present:</p>

<pre><code class="bash">$ docker volume ls
DRIVER                       VOLUME NAME
glusterfs:latest             gfs/vol1
</code></pre>

<p>Exec into the container and look at the disk layout:</p>

<pre><code class="bash">$ docker exec -it d469f341d836 sh
/ # df -h
Filesystem                Size      Used Available Use% Mounted on
overlay                  45.6G      3.2G     40.0G   7% /
10.22.125.101:gfs/vol1   45.6G      3.3G     40.0G   8% /tmp
</code></pre>

<p>While you are in the container, write the hostname&rsquo;s value into a file which is mapped to the glusterfs volume:</p>

<pre><code class="bash">$ echo $HOSTNAME &gt; /tmp/data.txt
$ cat /tmp/data.txt
d469f341d836
</code></pre>

<h2>Testing Data Persistence</h2>

<p>Time to test the data persistence. Scale the service to 3 replicas, then hop onto a new node where a replica resides and check if the data was persisted.</p>

<pre><code class="bash">$ docker service scale test_foo=3
test_foo scaled to 3
overall progress: 3 out of 3 tasks
1/3: running   [==================================================&gt;]
2/3: running   [==================================================&gt;]
3/3: running   [==================================================&gt;]
verify: Service converged
</code></pre>

<p>Check where the containers are running:</p>

<pre><code class="bash">$ docker service ps test_foo
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
jfwzb7yxnrxx        test_foo.1          alpine:latest       swarm-worker-1      Running             Running 2 minutes ago
mdsg6c5b2nqb        test_foo.2          alpine:latest       swarm-worker-3      Running             Running 15 seconds ago
iybat57t4lha        test_foo.3          alpine:latest       swarm-worker-2      Running             Running 15 seconds ago
</code></pre>

<p>Hop onto the <code>swarm-worker-2</code> node and check if the data is persisted from our previous write:</p>

<pre><code class="bash">$ docker exec -it 4228529aba29 sh
$ cat /tmp/data.txt
d469f341d836
</code></pre>

<p>Now let&rsquo;s append data to that file, then delete the stack and recreate to test if the data is still persisted:</p>

<pre><code class="bash">$ echo $HOSTNAME &gt;&gt; /tmp/data.txt
$ cat /tmp/data.txt
d469f341d836
4228529aba29
</code></pre>

<p>On the manager delete the stack:</p>

<pre><code class="bash">$ docker stack rm test
Removing service test_foo
</code></pre>

<p>The deploy the stack again:</p>

<pre><code class="bash">$ docker stack deploy -c docker-compose.yml test
Creating service test_foo
</code></pre>

<p>Check where the container is running:</p>

<pre><code class="bash">$ docker service ps test_foo
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS
9d6z02m123jk        test_foo.1          alpine:latest       swarm-worker-1      Running             Running 2 seconds ago
</code></pre>

<p>Exec into the container and read the data:</p>

<pre><code class="bash">$ docker exec -it 3008b1e1bba1 cat /tmp/data.txt
d469f341d836
4228529aba29
</code></pre>

<p>And as you can see the data is persisted.</p>

<h2>Resources</h2>

<p>Please have a look and star <a href="https://github.com/trajano/docker-volume-plugins">@trajano&rsquo;s</a> repository:</p>

<ul>
<li><a href="https://github.com/trajano/docker-volume-plugins">https://github.com/trajano/docker-volume-plugins</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a Distributed Storage Volume With GlusterFS]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/03/04/setup-a-distributed-storage-volume-with-glusterfs/"/>
    <updated>2019-03-04T22:32:53+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/03/04/setup-a-distributed-storage-volume-with-glusterfs</id>
    <content type="html"><![CDATA[<p><img src="https://glusterdocs-beta.readthedocs.io/en/latest/_images/dist-volume.png" alt="" /></p>

<p>GlusterFS is a Awesome Scalable Networked Filesystem, which makes it Easy to Create Large and Scalable Storage Solutions on Commodity Hardware.</p>

<p><a href="https://bekkerclothing.com/collections/developer?utm_source=blog.ruanbekker.com&utm_medium=blog&utm_campaign=leaderboard_ad" target="_blank"><img alt="bekker-clothing-developer-tshirts" src="https://user-images.githubusercontent.com/567298/70170981-7c278a80-16d6-11ea-9759-6621d02c1423.png"></a></p>

<p><strong>Basic Concepts of GlusterFS:</strong></p>

<p>Brick:
* In GlusterFS, a brick is the basic unit of storage, represented by a directory on the server in the trusted storage pool.</p>

<p>Gluster Volume:
* A Gluster volume is a Logical Collection of Bricks.</p>

<p>Distributed Filesystem:
* The concept is to enable multiple clients to concurrently access data which is spread across multple servers in a trusted storage pool. This is also a great solution to prevent data corruption, enable highly available storage systems, etc.</p>

<p><a href="http://gluster.readthedocs.io/en/latest/Administrator%20Guide/glossary/">More concepts</a> can be retrieved from their documentation.</p>

<h2>Different GlusterFS Volume Types:</h2>

<p>With GlusterFS you can create the following types of Gluster Volumes:</p>

<ul>
<li>Distributed Volumes: (Ideal for Scalable Storage, No Data Redundancy)</li>
<li>Replicated Volumes: (Better reliability and data redundancy)</li>
<li>Distributed-Replicated Volumes: (HA of Data due to Redundancy and Scaling Storage)</li>
<li><a href="http://gluster.readthedocs.io/en/latest/Quick-Start-Guide/Architecture/">More detail</a> on GlusterFS Architecture</li>
</ul>


<h2>Setup a Distributed Gluster Volume:</h2>

<p>In this guide we will setup a 3 Node Distributed GlusterFS Volume on Ubuntu 16.04.</p>

<p>For this use case we would like to achieve a storage solution to scale the size of our storage, and not really worried about redundancy as, with a Distributed Setup we can increase the size of our volume, the more bricks we add to our GlusterFS Volume.</p>

<h2>Setup: Our Environment</h2>

<p>Each node has 2 disks, <code>/dev/xvda</code> for the Operating System wich is 20GB and <code>/dev/xvdb</code> which has 100GB. After we have created our GlusterFS Volume, we will have a Gluster Volume of 300GB.</p>

<p>Having a look at our disks:</p>

<pre><code class="bash">$ lsblk
NAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
xvda    202:0    0   20G  0 disk
└─xvda1 202:1    0   20G  0 part /
xvdb    202:16   0  100G  0 disk 
</code></pre>

<p>If you don&rsquo;t have DNS setup for your nodes, you can use your /etc/hosts file for all 3 nodes, which I will be using in this demonstration:</p>

<pre><code class="bash">$ cat /etc/hosts
172.31.13.226   gluster-node-1
172.31.9.7      gluster-node-2
172.31.15.34    gluster-node-3
127.0.0.1       localhost
</code></pre>

<h2>Install GlusterFS from the Package Manager:</h2>

<p>Note that all the steps below needs to be performed on all 3 nodes, unless specified otherwise:</p>

<pre><code class="bash">$ apt update &amp;&amp; apt upgrade -y
$ apt install xfsprogs attr glusterfs-server glusterfs-client glusterfs-common -y
</code></pre>

<h2>Format and Prepare the Gluster Disks:</h2>

<p>We will create a XFS Filesystem for our 100GB disk, create the directory path where we will mount our disk onto, and also load it into <code>/etc/fstab</code>:</p>

<pre><code class="bash">$ mkfs.xfs /dev/xvdb
$ mkdir /gluster
$ echo '/dev/xvdb /gluster xfs defaults 0 0' &gt;&gt; /etc/fstab
$ mount -a
</code></pre>

<p>After we mounted the disk, we should see that our disk is mounted to <code>/gluster</code>:</p>

<pre><code class="bash">$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/xvda1       20G  1.2G   19G   7% /
/dev/xvdb       100G   33M  100G   1% /gluster
</code></pre>

<p>After our disk is mounted, we can proceed by creating the brick directory on our disk that we mounted, from the step above:</p>

<pre><code class="bash">$ mkdir /gluster/brick
</code></pre>

<h2>Start GlusterFS Service:</h2>

<p>Enable GlusterFS on startup, start the service and make sure that the service is running:</p>

<pre><code class="bash">$ systemctl enable glusterfs-server
$ systemctl restart glusterfs-server
$ systemctl is-active glusterfs-server
active
</code></pre>

<h2>Discover All the Nodes for our Cluster:</h2>

<p>The following will only be done on one of the nodes. First we need to discover our other nodes.</p>

<p>The node that you are currently on, will be discovered by default and only needs the other 2 nodes to be discovered:</p>

<pre><code class="bash">$ gluster peer probe gluster-node-2
$ gluster peer probe gluster-node-3
</code></pre>

<p>Let&rsquo;s verify this by listing all the nodes in our cluster:</p>

<pre><code class="bash">$ gluster pool list
UUID                                    Hostname        State
6e02731c-6472-4ea4-bd48-d5dd87150e8b    gluster-node-2  Connected
9d4c2605-57ba-49e2-b5da-a970448dc886    gluster-node-3  Connected
608f027e-e953-413b-b370-ce84050a83c9    localhost       Connected
</code></pre>

<h2>Create the Distributed GlusterFS Volume:</h2>

<p>We will create a Distributed GlusterFS Volume across 3 nodes, and we will name the volume <code>gfs</code>:</p>

<pre><code class="bash">$ gluster volume create gfs \
  gluster-node-1:/gluster/brick \
  gluster-node-2:/gluster/brick \
  gluster-node-3:/gluster/brick

volume create: gfs: success: please start the volume to access data
</code></pre>

<h2>Start the GlusterFS Volume:</h2>

<p>Now start the <code>gfs</code> GlusterFS Volume:</p>

<pre><code class="bash">$ gluster volume start gfs
volume start: gfs: success
</code></pre>

<p>To get information about the volume:</p>

<pre><code class="bash">$ gluster volume info gfs

Volume Name: gfs
Type: Distribute
Volume ID: c08bc2e8-59b3-49e7-bc17-d4bc8d99a92f
Status: Started
Number of Bricks: 3
Transport-type: tcp
Bricks:
Brick1: gluster-node-1:/gluster/brick
Brick2: gluster-node-2:/gluster/brick
Brick3: gluster-node-3:/gluster/brick
Options Reconfigured:
performance.readdir-ahead: on
</code></pre>

<p>Status information about our Volume:</p>

<pre><code class="bash">$ gluster volume status

Status of volume: gfs
Gluster process                             TCP Port  RDMA Port  Online  Pid
------------------------------------------------------------------------------
Brick gluster-node-1:/gluster/brick         49152     0          Y       7139
Brick gluster-node-2:/gluster/brick         49152     0          Y       7027
Brick gluster-node-3:/gluster/brick         49152     0          Y       7099
NFS Server on localhost                     2049      0          Y       7158
NFS Server on gluster-node-2                2049      0          Y       7046
NFS Server on gluster-node-3                2049      0          Y       7118

Task Status of Volume gfs
------------------------------------------------------------------------------
There are no active volume tasks
</code></pre>

<h2>Mounting our GlusterFS Volume:</h2>

<p>On all the clients, in this case our 3 nodes, load the mount information into <code>/etc/fstab</code> and then mount the GlusterFS Volume:</p>

<pre><code class="bash">$ echo 'localhost:/gfs /mnt glusterfs defaults,_netdev,backupvolfile-server=gluster-node-1 0 0' &gt;&gt; /etc/fstab
$ mount -a
</code></pre>

<p>Now that the volume is mounted, have a look at your disk info, and you will find that you have a <code>300GB</code> GlusterFS Volume mounted:</p>

<pre><code class="bash">$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/xvda1       20G  1.3G   19G   7% /
/dev/xvdb       100G   33M  100G   1% /gluster
localhost:/gfs  300G   98M  300G   1% /mnt
</code></pre>

<p>As mentioned before, this is most probably for a scenario where you would like to achieve a high storage size and not really concerned about data availability.</p>

<p>In the next couple of weeks I will also go through the Replicated, Distributed-Replicated and <a href="https://gluster.readthedocs.io/en/latest/Administrator%20Guide/Gluster%20On%20ZFS/">GlusterFS with ZFS</a> setups.</p>

<h2>Resources:</h2>

<ul>
<li><a href="http://gluster.readthedocs.io/en/latest/Quick-Start-Guide/Terminologies/">GlusterFS Terminologies</a></li>
<li><a href="http://gluster.readthedocs.io/en/latest/Quick-Start-Guide/Architecture/">GlusterFS Architecture</a></li>
<li><a href="http://gluster.readthedocs.io/en/latest/Administrator%20Guide/Gluster%20On%20ZFS/">GlusterFS with ZFS</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setting Up a Docker Swarm Cluster on 3 RaspberryPi Nodes]]></title>
    <link href="https://blog.ruanbekker.com/blog/2018/10/23/setting-up-a-docker-swarm-cluster-on-3-raspberrypi-nodes/"/>
    <updated>2018-10-23T22:24:00+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2018/10/23/setting-up-a-docker-swarm-cluster-on-3-raspberrypi-nodes</id>
    <content type="html"><![CDATA[<p><img src="https://objects.ruanbekker.com/assets/images/rpi-docker-swarm.png" alt="" /></p>

<p>As the curious person that I am, I like to play around with new stuff that I stumble upon, and one of them was having a docker swarm cluster running on 3 Raspberry Pi&rsquo;s on my LAN.</p>

<p>The idea is to have 3 Raspberry Pi&rsquo;s (Model 3 B), a Manager Node, and 2 Worker Nodes, each with a 32 GB SanDisk SD Card, which I will also be part of a 3x Replicated GlusterFS Volume that will come in handy later for some data that needs persistent data.</p>

<p>More Inforamtion on: <a href="https://docs.docker.com/engine/swarm/">Docker Swarm</a></p>

<script id="mNCC" language="javascript">
    medianet_width = "728";
    medianet_height = "90";
    medianet_crid = "218284798";
    medianet_versionId = "3111299"; 
  </script>


<script src="//contextual.media.net/nmedianet.js?cid=8CUD78FSV"></script>


<h2>Provision Raspbian on each RaspberryPi</h2>

<p>Grab the <a href="https://downloads.raspberrypi.org/raspbian_lite_latest">Latest Raspbian Lite ISO</a> and the following <a href="https://www.raspberrypi.org/documentation/installation/installing-images/">source</a> will help provisioning your RaspberryPi with Raspbian.</p>

<h2>Installing Docker on Raspberry PI</h2>

<p>On each node, run the following to install docker, and also add your user to the docker group, so that you can run docker commands with a normal user:</p>

<pre><code class="bash">$ apt-get update &amp;&amp; sudo apt-get upgrade -y
$ sudo apt-get remove docker.io
$ curl https://get.docker.com | sudo bash
$ sudo usermod -aG docker pi
</code></pre>

<p>If you have an internal DNS Server, set an A Record for each node, or for simplicity, set your hosts file on each node so that your hostname for each node responds to it&rsquo;s provisioned IP Address:</p>

<pre><code class="bash">$ cat /etc/hosts
192.168.0.2   rpi-01
192.168.0.3   rpi-02
192.168.0.4   rpi-03
</code></pre>

<p>Also, to have passwordless SSH, from each node:</p>

<pre><code class="bash">$ ssh-keygen -t rsa
$ ssh-copy-id rpi-01
$ ssh-copy-id rpi-02
$ ssh-copy-id rpi-03
</code></pre>

<h2>Initialize the Swarm</h2>

<p>Time to set up our swarm. As we have more than one network interface, we will need to setup our swarm by specifying the IP Address of our network interface that is accessible from our LAN:</p>

<pre><code class="bash">$ ifconfig eth0
eth0      Link encap:Ethernet  HWaddr a1:12:bc:d3:cd:4d
          inet addr:192.168.0.2  Bcast:192.168.0.255  Mask:255.255.255.0
</code></pre>

<p>Now that we have our IP Address, initialize the swarm on the manager node:</p>

<pre><code class="bash">pi@rpi-01:~ $ docker swarm init --advertise-addr 192.168.0.2
Swarm initialized: current node (siqyf3yricsvjkzvej00a9b8h) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join \
    --token SWMTKN-1-0eith07xkcg93lzftuhjmxaxwfa6mbkjsmjzb3d3sx9cobc2zp-97s6xzdt27y2gk3kpm0cgo6y2 \
    192.168.0.2:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.  
</code></pre>

<p>Then from <code>rpi-02</code> join the manager node of the swarm:</p>

<pre><code class="bash">pi@rpi-02:~ $ docker swarm join --token SWMTKN-1-0eith07xkcg93lzftuhjmxaxwfa6mbkjsmjzb3d3sx9cobc2zp-97s6xzdt27y2gk3kpm0cgo6y2 192.168.0.2:2377
This node joined a swarm as a worker.  
</code></pre>

<p>Then from <code>rpi-03</code> join the manager node of the swarm:</p>

<pre><code class="bash">pi@rpi-03:~ $ docker swarm join --token SWMTKN-1-0eith07xkcg93lzftuhjmxaxwfa6mbkjsmjzb3d3sx9cobc2zp-97s6xzdt27y2gk3kpm0cgo6y2 192.168.0.2:2377
This node joined a swarm as a worker.  
</code></pre>

<p>Then from the manager node: <code>rpi-01</code>, ensure that the nodes are checked in:</p>

<pre><code class="bash">pi@rpi-01:~ $ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS
62s7gx1xdm2e3gp5qoca2ru0d     rpi-03              Ready               Active
6fhyfy9yt761ar9pl84dkxck3 *   rpi-01              Ready               Active              Leader
pg0nyy9l27mtfc13qnv9kywe7     rpi-02              Ready               Active
</code></pre>

<h2>Setting Up a Replicated GlusterFS Volume</h2>

<p>I have decided to setup a replicated glusterfs volume to have data replicated throughout the cluster if I would like to have some persistent data. From each node, install the GlusterFS Client and Server:</p>

<pre><code class="bash">$ sudo apt install glusterfs-server glusterfs-client -y &amp;&amp; sudo systemctl enable glusterfs-server
</code></pre>

<p>Probe the other nodes from the manager node:</p>

<pre><code class="bash">pi@rpi-01:~ $ sudo gluster peer probe rpi-02
peer probe: success.

pi@rpi-01:~ $ sudo gluster peer probe rpi-03
peer probe: success.
</code></pre>

<p>Ensure that we can see all 3 nodes in our GlusterFS Pool:</p>

<pre><code class="bash">pi@rpi-01:~ $ sudo gluster pool list
UUID                                    Hostname        State
778c7463-ba48-43de-9f97-83a960bba99e    rpi-02          Connected
00a20a3c-5902-477e-a8fe-da35aa955b5e    rpi-03          Connected
d82fb688-c50b-405d-a26f-9cb2922cce75    localhost       Connected
</code></pre>

<p>From each node, create the directory where GlusterFS will store the data for the bricks that we will specify when creating the volume:</p>

<pre><code class="bash">pi@rpi-01:~ $ sudo mkdir -p /gluster/brick 
pi@rpi-02:~ $ sudo mkdir -p /gluster/brick
pi@rpi-03:~ $ sudo mkdir -p /gluster/brick
</code></pre>

<p>Next, create a 3 Way Replicated GlusterFS Volume:</p>

<pre><code class="bash">pi@rpi-01:~ $ sudo gluster volume create rpi-gfs replica 3 \
rpi-01:/gluster/brick \
rpi-02:/gluster/brick \
rpi-03:/gluster/brick \
force

volume create: rpi-gfs: success: please start the volume to access data
</code></pre>

<p>Start the GlusterFS Volume:</p>

<pre><code class="bash">pi@rpi-01:~ $ sudo gluster volume start rpi-gfs
volume start: rpi-gfs: success
</code></pre>

<p>Verify the GlusterFS Volume Info, and from the below output you will see that the volume is replicated 3 ways from the 3 bricks that we specified</p>

<pre><code class="bash">pi@rpi-01:~ $ sudo gluster volume info

Volume Name: rpi-gfs
Type: Replicate
Volume ID: b879db15-63e9-44ca-ad76-eeaa3e247623
Status: Started
Number of Bricks: 1 x 3 = 3
Transport-type: tcp
Bricks:
Brick1: rpi-01:/gluster/brick
Brick2: rpi-02:/gluster/brick
Brick3: rpi-03:/gluster/brick
</code></pre>

<p>Mount the GlusterFS Volume on each Node, first on <code>rpi-01</code>:</p>

<pre><code class="bash">pi@rpi-01:~ $ sudo umount /mnt
pi@rpi-01:~ $ sudo echo 'localhost:/rpi-gfs /mnt glusterfs defaults,_netdev,backupvolfile-server=localhost 0 0' &gt;&gt; /etc/fstab
pi@rpi-01:~ $ sudo mount.glusterfs localhost:/rpi-gfs /mnt
pi@rpi-01:~ $ sudo chown -R pi:docker /mnt
</code></pre>

<p>Then on <code>rpi-02</code>:</p>

<pre><code class="bash">pi@rpi-02:~ $ sudo umount /mnt
pi@rpi-02:~ $ sudo echo 'localhost:/rpi-gfs /mnt glusterfs defaults,_netdev,backupvolfile-server=localhost 0 0' &gt;&gt; /etc/fstab
pi@rpi-02:~ $ sudo mount.glusterfs localhost:/rpi-gfs /mnt
pi@rpi-02:~ $ sudo chown -R pi:docker /mnt
</code></pre>

<p>And lastly on <code>rpi-03</code>:</p>

<pre><code class="bash">pi@rpi-03:~ $ sudo umount /mnt
pi@rpi-03:~ $ sudo echo 'localhost:/rpi-gfs /mnt glusterfs defaults,_netdev,backupvolfile-server=localhost 0 0' &gt;&gt; /etc/fstab
pi@rpi-03:~ $ sudo mount.glusterfs localhost:/rpi-gfs /mnt
pi@rpi-03:~ $ sudo chown -R pi:docker /mnt
</code></pre>

<p>Then your GlusterFS Volume will be mounted on all the nodes, and when a file is written to the <code>/mnt/</code> partition, data will be replicated to all the nodes in the Cluster:</p>

<pre><code class="bash">pi@rpi-01:~ $ df -h
Filesystem          Size  Used Avail Use% Mounted on
/dev/root            30G  4.5G   24G  16% /
localhost:/rpi-gfs   30G  4.5G   24G  16% /mnt
</code></pre>

<h2>Create a Web Service on Docker Swarm:</h2>

<p>Let&rsquo;s create a Web Service in our Swarm, called <code>web</code> and by specifying <code>1</code> replica and publishing the exposed port <code>80</code> to our containers port <code>80</code>:</p>

<pre><code class="bash">pi@rpi-01:~ $ docker service create --name web --replicas 1 --publish 80:80 hypriot/rpi-busybox-httpd
vsvyanuw6q6yf4jr52m5z7vr1
</code></pre>

<p>Verifying that our Service is Started and equals to the desired replica count:</p>

<pre><code class="bash">pi@rpi-01:~ $ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE                                                    PORTS
vsvyanuw6q6y        web                 replicated          1/1                 hypriot/rpi-busybox-httpd:latest                         *:891-&gt;80/tcp
</code></pre>

<p>Inspecting the Service:</p>

<pre><code class="bash">pi@rpi-01:~ $ docker service inspect web
[
    {
        "ID": "vsvyanuw6q6yf4jr52m5z7vr1",
        "Version": {
            "Index": 2493
        },
        "CreatedAt": "2017-07-16T21:20:00.017836646Z",
        "UpdatedAt": "2017-07-16T21:20:00.026359794Z",
        "Spec": {
            "Name": "web",
            "Labels": {},
            "TaskTemplate": {
                "ContainerSpec": {
                    "Image": "hypriot/rpi-busybox-httpd:latest@sha256:c00342f952d97628bf5dda457d3b409c37df687c859df82b9424f61264f54cd1",
                    "StopGracePeriod": 10000000000,
                    "DNSConfig": {}
                },
                "Resources": {
                    "Limits": {},
                    "Reservations": {}
                },
                "RestartPolicy": {
                    "Condition": "any",
                    "Delay": 5000000000,
                    "MaxAttempts": 0
                },
                "Placement": {},
                "ForceUpdate": 0
            },
            "Mode": {
                "Replicated": {
                    "Replicas": 1
                }
            },
            "UpdateConfig": {
                "Parallelism": 1,
                "FailureAction": "pause",
                "Monitor": 5000000000,
                "MaxFailureRatio": 0,
                "Order": "stop-first"
            },
            "RollbackConfig": {
                "Parallelism": 1,
                "FailureAction": "pause",
                "Monitor": 5000000000,
                "MaxFailureRatio": 0,
                "Order": "stop-first"
            },
            "EndpointSpec": {
                "Mode": "vip",
                "Ports": [
                    {
                        "Protocol": "tcp",
                        "TargetPort": 80,
                        "PublishedPort": 80,
                        "PublishMode": "ingress"
                    }
                ]
            }
        },
        "Endpoint": {
            "Spec": {
                "Mode": "vip",
                "Ports": [
                    {
                        "Protocol": "tcp",
                        "TargetPort": 80,
                        "PublishedPort": 80,
                        "PublishMode": "ingress"
                    }
                ]
            },
            "Ports": [
                {
                    "Protocol": "tcp",
                    "TargetPort": 80,
                    "PublishedPort": 80,
                    "PublishMode": "ingress"
                }
            ],
            "VirtualIPs": [
                {
                    "NetworkID": "zjerz0xsw39icnh24enja4cgk",
                    "Addr": "10.255.0.13/16"
                }
            ]
        }
    }
]
</code></pre>

<p>Docker Swarm&rsquo;s Routing mesh takes care of the internal routing, so requests will respond even if the container is not running on the node that you are making the request against.</p>

<p>With that said, verifying on which node our service is running:</p>

<pre><code class="bash">pi@rpi-01:~ $ docker service ps web
ID                  NAME                IMAGE                              NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS
sd67cd18s5m0        web.1               hypriot/rpi-busybox-httpd:latest   rpi-02              Running             Running 2 minutes ago
</code></pre>

<p>When we make a HTTP Request to one of these Nodes IP Addresses, our request will be responded with this awesome static page:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/armed-with-hypriot.jpg" alt="" /></p>

<p>We can see we only have one container in our swarm, let&rsquo;s scale that up to <code>3</code> containers:</p>

<pre><code class="bash">pi@rpi-01:~ $ docker service scale web01=3
web01 scaled to 3
</code></pre>

<p>Now that the service is scaled to 3 containers, requests will be handled using the round-robin algorithm. To ensured that the service scaled, we will see that we will have 3 replicas:</p>

<pre><code class="bash">pi@rpi-01:~ $ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE                                                    PORTS
vsvyanuw6q6y        web                 replicated          3/3                 hypriot/rpi-busybox-httpd:latest                         *:891-&gt;80/tcp
</code></pre>

<p>Verifying where these containers are running on:</p>

<pre><code class="bash">pi@rpi-01:~ $ docker service ps web01
ID                  NAME                IMAGE                              NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
sd67cd18s5m0        web.1               hypriot/rpi-busybox-httpd:latest   rpi-02              Running             Running 2 minutes ago
ope3ya7hh9j4        web.2               hypriot/rpi-busybox-httpd:latest   rpi-03              Running             Running 30 seconds ago
07m1ww7ptxro        web.3               hypriot/rpi-busybox-httpd:latest   rpi-01              Running             Running 28 seconds ago
</code></pre>

<p>Lastly, removing the service from our swarm:</p>

<pre><code class="bash">pi@rpi-01:~ $ docker service rm web01
web01
</code></pre>

<h2>Massive Thanks:</h2>

<p>a Massive thanks to <a href="https://twitter.com/alexellisuk">Alex Ellis</a> for mentioning me on one of his blogposts:</p>

<ul>
<li><a href="https://blog.alexellis.io/blog-community-inspiration/">https://blog.alexellis.io/blog-community-inspiration/</a></li>
</ul>


<p><img src="https://objects.ruanbekker.com/assets/images/tweet-alexellis-21072017.png" alt="" /></p>
]]></content>
  </entry>
  
</feed>
