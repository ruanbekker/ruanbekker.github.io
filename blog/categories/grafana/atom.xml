<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Grafana | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/grafana/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2020-10-26T05:57:29+00:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Getting Started on Logging With Loki Using Docker]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/08/13/getting-started-on-logging-with-loki-using-docker/"/>
    <updated>2020-08-13T13:39:28+00:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/08/13/getting-started-on-logging-with-loki-using-docker</id>
    <content type="html"><![CDATA[<p>Logging with Loki is AMAZING!</p>

<p>In the past couple of months i&rsquo;ve been working a lot with logging, but more specifically logging with loki. As most of my metrics reside in prometheus, I use grafana quite extensively and logging was always the one that stood out a bit as I pushed my logs to elasticsearch and consumed them from grafana. Which worked pretty well, but the maintenance and resource costs was a bit too much for what I was looking for.</p>

<p>And then grafana released Loki, which is like prometheus, but for logs. And that was just super, exactly what I was looking for. For my use case, I was looking for something that can be consumed by grafana as a presentation layer, central based so I can push all sorts of logs, and want a easy way to grep for logs and a bonus would be to have a cli tool.</p>

<p>And Loki checked all those boxes!</p>

<div class="tenor-gif-embed" data-postid="7644619" data-share-method="host" data-width="100%" data-aspect-ratio="1.1971153846153846"><a href="https://tenor.com/view/oh-yeah-gif-7644619">Oh Yeah Parks And Recreation GIF</a> from <a href="https://tenor.com/search/ohyeah-gifs">Ohyeah GIFs</a></div>


<script type="text/javascript" async src="https://tenor.com/embed.js"></script>


<h2>What can you expect from this blog</h2>

<p>In this post will be a getting started guide to Loki, we will provision Loki, Grafana and Nginx using Docker to get our environment up and running, so that we can push our nginx container logs to the loki datasource, and access the logs via grafana.</p>

<p>We will then generate some logs so that we can show a couple of query examples using the log query language (LogQL) and use the LogCLI to access our logs via cli.</p>

<p>In a <a href="">future post</a>, I will demonstrate how to setup Loki for a non-docker deployment.</p>

<h2>Some useful information about Loki</h2>

<p>Let&rsquo;s first talk about Loki compared with Elasticsearch, as they are not the same:</p>

<ol>
<li>Loki does not index the text of the logs, instead grouping entries into streams and index those with labels</li>
<li>Things like full text search engines tokenizes your text into k/v pairs and gets written to an inverted index, which over time in my opinion gets complex to maintain, expensive to scale, storage retention, etc.</li>
<li>Loki is advertised as easy to scale, affordable to operate as it uses DynamoDB for Indexing and S3 for Storage</li>
<li>When using Loki, you may need to forget what you know and look to see how the problem can be solved differently with parallelization. Loki’s superpower is breaking up queries into small pieces and dispatching them in parallel so that you can query huge amounts of log data in small amounts of time.</li>
</ol>


<p>If we look at the <strong>Loki Log Model</strong>, we can see that the timestamp and the labels are indexed and the content of the logs are not indexed:</p>

<p><img src="https://img.sysadmins.co.za/cpr6n7.png" alt="loki" /></p>

<p>A <strong>log stream</strong> is a stream of log entries with the same exact label set:</p>

<p><img src="https://img.sysadmins.co.za/el6djk.png" alt="loki" /></p>

<p>For the <strong>storage</strong> side, inside each chunk, log entries are sorted by timestamp. Loki only indexes minimum and maximum timestamps of a chunk. Storage options support local storage, AWS S3, Google Cloud Storage and Azure</p>

<p><img src="https://img.sysadmins.co.za/959pjw.png" alt="loki" /></p>

<p>For <strong>chunks and querying</strong>, chunks are filled per stream and they are flushed of a few criterias such as age and size:</p>

<p><img src="https://img.sysadmins.co.za/ekm8cy.png" alt="loki" /></p>

<p>And one of the most important parts are the <strong>labels</strong>, labels define the stream and therefore its very important.</p>

<p>High cardinality is bad for labels, as something like a IP address can reduce your performance a lot, as it will create a stream for every unique IP label.</p>

<p>Static defined labels such as environment, hostnames are good, you can read more up about it <a href="https://grafana.com/blog/2020/04/21/how-labels-in-loki-can-make-log-queries-faster-and-easier/">here</a></p>

<p>Here is a info graphic on how one log line can be split up into 36 streams:</p>

<p><img src="https://img.sysadmins.co.za/g119oq.png" alt="" /></p>

<p>So with that being said, <strong>good labels</strong> will be considered as cluster, job, namespace, environment, etc where as <strong>bad labels</strong> will be things such as userid, ip address, url path, etc</p>

<h2>Selecting logstreams with Loki</h2>

<p>Selecting logstreams, is done by using <strong>label matchers</strong> and <strong>filter expressions</strong>, such as this example:</p>

<pre><code>{job="dockerlogs", environment="development"} |= "POST" |~ "196.35.64.+"
</code></pre>

<p>Label Matchers and Filter Expressions support:</p>

<ul>
<li><code>=</code> Contains string</li>
<li><code>!=</code> Does not contain string</li>
<li><code>=~</code> Matches regular expression</li>
<li><code>!~</code> Does not match regular expression</li>
</ul>


<h2>Supported Clients</h2>

<p>At the moment of writing, loki supports the following log clients:</p>

<ul>
<li>Promtail (tails logs and ships to Loki)</li>
<li>Docker Driver</li>
<li>Fluentd</li>
<li>Fluent Bit</li>
<li>Logstash</li>
</ul>


<p>We will be going into more detail on using promtail in a <a href="">future post</a>, but you can read more up about it <a href="https://github.com/grafana/loki/tree/master/cmd">here</a></p>

<h2>Loki in Action</h2>

<p>Time to get to the fun part, clone my <a href="https://github.com/ruanbekker/loki-docker-nginx-example">github repo</a>:</p>

<pre><code>$ git clone https://github.com/ruanbekker/loki-docker-nginx-example
$ cd loki-docker-nginx-example
</code></pre>

<p>You can inspect the docker-compose.yml:</p>

<pre><code>$ cat docker-compose.yml
version: "3.4"

services:
  my-nginx-service:
    image: nginx
    container_name: my-nginx-service
    ports:
      - 8000:80
    environment:
      - FOO=bar
    logging:
      driver: loki
      options:
        loki-url: http://localhost:3100/loki/api/v1/push
        loki-external-labels: job=dockerlogs,owner=ruan,environment=development

  grafana:
    image: grafana/grafana:7.1.1
    volumes:
    - ./config/datasource.yml:/etc/grafana/provisioning/datasources/datasource.yml
    ports:
    - "3000:3000"

  loki:
   image: grafana/loki:v1.3.0
   volumes:
     - ./config/loki.yaml:/etc/config/loki.yaml
   entrypoint:
     - /usr/bin/loki
     - -config.file=/etc/config/loki.yaml
   ports:
     - "3100:3100"
</code></pre>

<p>As you can see loki will be the datasource where we will be pushing our logs to from our nginx container and we are defining our logging section where it should find loki and we are also setting labels to that log stream using <code>loki-external-labels</code>. Then we are using grafana to auto configure the loki datasource from the <code>./config/datasource.yml</code> section so that we can visualize our logs.</p>

<p>If you don&rsquo;t want to define the logging section per container, you can always set the defaults using the <code>/etc/docker/daemon.json</code> by following <a href="https://grafana.com/docs/loki/latest/clients/docker-driver/configuration/#change-the-default-logging-driver">this guide</a></p>

<p>Let&rsquo;s boot up our stack:</p>

<pre><code>$ docker-compose up
</code></pre>

<p>After everything is up, you should be able to access nginx by visiting: <code>http://nginx.localdns.xyz:8000/</code>, after you received a response, visit Grafana on <code>http://grafana.localdns.xyz:3000</code> using the username and password: <code>admin/admin</code>.</p>

<p>If you head over to datasources, you should see the loki datasource which was provisioned for you:</p>

<p><img src="https://img.sysadmins.co.za/tyn0ny.png" alt="loki-grafana" /></p>

<p>When you head to the left on explore and you select the loki datasource on <code>http://grafana.localdns.xyz:3000/explore</code> you should see the following:</p>

<p><img src="https://img.sysadmins.co.za/5kp07m.png" alt="loki-grafana" /></p>

<p>You will see that grafana discovers logstreams with the label <code>job</code> as you can see that our <code>job="dockerlogs"</code> can be seen there. We can either click on it, select the log labels from the left and browse the label we want to select or manually enter the query.</p>

<p>I will be using the query manually:</p>

<pre><code>{job="dockerlogs"}
</code></pre>

<p>So now we will get all the logs that has that label associated and as you can see, we see our request that we made:</p>

<p><img src="https://img.sysadmins.co.za/gra0oe.png" alt="nginx-grafana-loki" /></p>

<p>We can see one error due to the favicon.ico that it could not find, but let&rsquo;s first inspect our first log line:</p>

<p><img src="https://img.sysadmins.co.za/6dbuqn.png" alt="loki" /></p>

<p>Here we can see the labels assigned to that log event, which we can include in our query, like if we had multiple services and different environments, we can use a query like the following to only see logs for a specific service and environment:</p>

<pre><code>{job="dockerlogs", environment="development", compose_service="my-nginx-service"}
</code></pre>

<p>In the example above we used the selectors to select the logs we want to see, now we can use our filter expressions, to &ldquo;grep&rdquo; our logs.</p>

<p>Let&rsquo;s say we want to focus only on one service, and we want to filter for any logs with GET requests, so first we select to service then apply the filter expression:</p>

<pre><code>{compose_service="my-nginx-service"} |= "GET"
</code></pre>

<p><img src="https://img.sysadmins.co.za/vv609g.png" alt="loki-logs" /></p>

<p>As you can see we can see the ones we were looking for, we can also chain them, so we want to se GET&rsquo;s and errors:</p>

<pre><code>{compose_service="my-nginx-service"} |= "GET" |= "error"
</code></pre>

<p>And lets say for some reason we only want to see the logs that comes from a 192.168.32 subnet:</p>

<pre><code>{compose_service="my-nginx-service"} |= "GET" |= "error" |~ "192.168.32."
</code></pre>

<p>But we dont want to see requests from &ldquo;nginx.localdns.xyz&rdquo;:</p>

<pre><code>{compose_service="my-nginx-service"} |= "GET" |= "error" |~ "192.168.32." != "nginx.localdns.xyz"
</code></pre>

<p>Make two extra get requests to &ldquo;foo.localdns.xyz:8000&rdquo; and &ldquo;bar.localdns.xyz:8000&rdquo; and then we change the query to say that we only want to see errors and hostnames coming from the 2 requests that we made:</p>

<pre><code>{compose_service="my-nginx-service"} |= "error" |~ "(foo|bar).localdns.xyz"
</code></pre>

<p>If we expand one of the log lines, we can do a ad-hoc analysis to see the percentage of logs by source for example:</p>

<p><img src="https://img.sysadmins.co.za/9ctz6d.png" alt="loki-logs" /></p>

<h2>LogCLI</h2>

<p>If you prefer the cli to query logs, logcli is the command line client for loki, allows you to query logs from your terminal and has clients for linux, mac and windows.</p>

<p>Check the releases for the latest version:</p>

<ul>
<li><a href="https://github.com/grafana/loki/releases">https://github.com/grafana/loki/releases</a></li>
</ul>


<pre><code>$ wget https://github.com/grafana/loki/releases/download/v1.5.0/logcli-darwin-amd64.zip
$ unzip logcli-darwin-amd64.zip
$ mv logcli-darwin-amd64 /usr/local/bin/logcli
</code></pre>

<p>Set your environment details, in our case we dont have a username and password for loki:</p>

<pre><code>$ #export LOKI_USERNAME=${MYUSER}
$ #export LOKI_PASSWORD=${MYPASS}
$ export LOKI_ADDR=http://localhost:3001
</code></pre>

<p>We can view all our labels, let’s view all the job labels:</p>

<pre><code>$ logcli labels job
http://localhost:3001/loki/api/v1/label/job/values
dockerlogs
</code></pre>

<p>Let’s look at family apps nginx logs:</p>

<pre><code>$ logcli query '{job="dockerlogs"}'
http://localhost:3001/loki/api/v1/query_range?direction=BACKWARD&amp;end=1587727924005496000&amp;limit=30&amp;query=%7Bjob%3D%22dockerlogs%22%2C&amp;start=1587724324005496000
Common labels: {environment="development", owner="ruan", compose_service="my-nginx-service", job="dockerlogs", host="docker-desktop", compose_project="loki-nginx-docker"}
2020-08-13 17:08:40 192.168.32.1 - - [13/Aug/2020:15:08:40 +0000] "GET / HTTP/1.1" 200 612 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:79.0) Gecko/20100101 Firefox/79.0" "-"
</code></pre>

<p>We can also pipe that output to grep, awk, etc:</p>

<pre><code>$ logcli query '{job="dockerlogs"}' | grep GREP | awk -F 'X' '{print  $1}'
</code></pre>

<p>Supported arguments:</p>

<pre><code>$ logcli query --help
usage: logcli query [&lt;flags&gt;] &lt;query&gt;


Run a LogQL query.


Flags:
      --help             Show context-sensitive help (also try --help-long and --help-man).
      --version          Show application version.
  -q, --quiet            suppress everything but log lines
      --stats            show query statistics
  -o, --output=default   specify output mode [default, raw, jsonl]
  -z, --timezone=Local   Specify the timezone to use when formatting output timestamps [Local, UTC]
      --addr="http://localhost:3100"
                         Server address. Can also be set using LOKI_ADDR env var.
      --username=""      Username for HTTP basic auth. Can also be set using LOKI_USERNAME env var.
      --password=""      Password for HTTP basic auth. Can also be set using LOKI_PASSWORD env var.
      --ca-cert=""       Path to the server Certificate Authority. Can also be set using LOKI_CA_CERT_PATH env var.
      --tls-skip-verify  Server certificate TLS skip verify.
      --cert=""          Path to the client certificate. Can also be set using LOKI_CLIENT_CERT_PATH env var.
      --key=""           Path to the client certificate key. Can also be set using LOKI_CLIENT_KEY_PATH env var.
      --org-id=ORG-ID    org ID header to be substituted for auth
      --limit=30         Limit on number of entries to print.
      --since=1h         Lookback window.
      --from=FROM        Start looking for logs at this absolute time (inclusive)
      --to=TO            Stop looking for logs at this absolute time (exclusive)
      --step=STEP        Query resolution step width
      --forward          Scan forwards through logs.
      --no-labels        Do not print any labels
      --exclude-label=EXCLUDE-LABEL ...
                         Exclude labels given the provided key during output.
      --include-label=INCLUDE-LABEL ...
                         Include labels given the provided key during output.
      --labels-length=0  Set a fixed padding to labels
  -t, --tail             Tail the logs
      --delay-for=0      Delay in tailing by number of seconds to accumulate logs for re-ordering


Args:
  &lt;query&gt;  eg '{foo="bar",baz=~".*blip"} |~ ".*error.*"'
</code></pre>

<h2>Thank you</h2>

<p>I hope this was useful</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Graphing Covid-19 Stats With Grafana and Elasticsearch Using Python]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/04/26/graphing-covid-19-stats-with-grafana-and-elasticsearch-using-python/"/>
    <updated>2020-04-26T02:24:27+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/04/26/graphing-covid-19-stats-with-grafana-and-elasticsearch-using-python</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/80421197-62345180-88dc-11ea-9e0a-557199aaf613.png" alt="coronavirus-covid19-grafana-metrics" /></p>

<p>I stumbled upon a <a href="https://github.com/pomber/covid19/">github repository</a> that stores time-series data in json format of corona virus / covid19 statistics, which get updated daily.</p>

<p>I was curious to see data about my country and want to see how metrics will look like after our lockdown started, so I decided to consume that data with <strong>Python</strong> and the requests library, then ingest data about covid19 into <strong>Elasticsearch</strong> and the visualize the data with <strong>Grafana</strong>.</p>

<h2>Sample of the Data</h2>

<p>Let&rsquo;s have a peek at the data to determine how we will use it to write to Elasticsearch. Let&rsquo;s consume the data with python:</p>

<pre><code>&gt;&gt;&gt; import requests
&gt;&gt;&gt; import json
&gt;&gt;&gt; response = requests.get('https://pomber.github.io/covid19/timeseries.json').json()
</code></pre>

<p>Now let&rsquo;s determine the data type:</p>

<pre><code>&gt;&gt;&gt; type(response)
&lt;type 'dict'&gt;
</code></pre>

<p>Now as it&rsquo;s a dictionary, let&rsquo;s look at they keys:</p>

<pre><code>&gt;&gt;&gt; response.keys()
[u'Canada', u'Sao Tome and Principe', u'Lithuania', u'Cambodia', u'Ethiopia',....
</code></pre>

<p>So let&rsquo;s take a look how the data looks like if we do a lookup for Canada:</p>

<pre><code>&gt;&gt;&gt; type(response['Canada'])
&lt;type 'list'&gt;
</code></pre>

<p>As we can see it&rsquo;s a list, we can count how many items is in our list:</p>

<pre><code>&gt;&gt;&gt; len(response['Canada'])
94
</code></pre>

<p>Now let&rsquo;s peek at the data by accessing our first index of our list:</p>

<pre><code>&gt;&gt;&gt; response['Canada'][0]
{u'date': u'2020-1-22', u'confirmed': 0, u'recovered': 0, u'deaths': 0}
</code></pre>

<p>So our data will look like this:</p>

<pre><code>{
  [
    'Country Name': [
      {
        'date': '&lt;string&gt;', 
        'confirmed': '&lt;int&gt;', 
        'recovered': '&lt;int&gt;', 
        'deaths': '&lt;int&gt;'
      },
      {
        'date': '&lt;string&gt;',
        'confirmed': '&lt;int&gt;',
        'recovered': '&lt;int&gt;',
        'deaths': '&lt;int&gt;'
      },
    ],
    'Country Name': [
      ...
    ]
  ]
}
</code></pre>

<h2>Some issues we need to fix</h2>

<p>As you can see the date is displayed as <code>2020-1-22</code> instead of <code>2020-01-22</code>, I want to make it consistent as I will be ingesting the data with a <code>@timestamp</code> key which we will use the date from the returned data. So first we will need to convert that before we ingest the data.</p>

<p>The other thing I was thinking of is that, if for some reason we need to ingest this data again, we dont want to sit with duplicates (same document with different _id&rsquo;s), so for that I decided to generate a hash value that consist of the date and the country, so if the script run to ingest the data, it will use the same id for the specific document, which would just overwrite it, therefore we won&rsquo;t sit with duplicates.</p>

<p>So the idea is to ingest a document to elasticsearch like this:</p>

<pre><code>doc = {
    "_id": "sha_hash_value",
    "day": "2020-01-22",
    "timestamp": "@2020-01-22 00:00:00",
    "country": "CountryName",
    "confirmed": 0,
    "recovered": 0,
    "deaths": 0
}
</code></pre>

<h2>How we will ingest the data</h2>

<p>The first run will load all the data and ingest all the data up to the current day to elasticsearch. Once that is done, we will add code to our script to only ingest the most recent day&rsquo;s data into elasticsearch, which we will control with a cronjob.</p>

<p>Create a index with a mapping to let Elasticsearch know <code>timestamp</code> will be a date field:</p>

<pre><code>$ curl -XPUT -H 'Content-Type: application/json' \
  -u username:pass 'https://es.domain.com/coronastats' -d \
  '{"mappings": {"foo1": {"properties": {"timestamp" : {"type" : "date","format" : "yyyy-MM-dd HH:mm:ss"}}}}}'
</code></pre>

<p>Once our index is created, create the python script that will load the data, loop through each country&rsquo;s daily data and ingest it into elasticsearch:</p>

<pre><code class="python">#!/usr/bin/python
import requests
import datetime as dt
import json
import hashlib

url = 'https://pomber.github.io/covid19/timeseries.json'
elasticsearch_url = "https://es.domain.com"
elasticsearch_username = ""
elasticsearch_password = ""

api_response = requests.get(url).json()

def convert_datestamp(day):
    return str(dt.datetime.strptime(day, '%Y-%m-%d'))

def hash_function(country, date):
    string_to_hash = country + date
    hash_obj  = hashlib.sha1(string_to_hash.encode('utf-8'))
    hash_value = hash_obj.hexdigest()
    return hash_value

def map_es_doc(payload, country):
    doc = {
        "day": payload['date'],
        "timestamp": convert_datestamp(payload['date']),
        "country": country,
        "confirmed": payload['confirmed'],
        "recovered": payload['recovered'],
        "deaths": payload['deaths']
    }
    return doc

def ingest(doc_id, payload):
    response = requests.put(
        elasticsearch_url + '/coronastats/coronastats/' + doc_id,
        auth=(elasticsearch_username, elasticsearch_password),
        headers={'content-type': 'application/json'},
        json=payload
    )
    return response.status_code

for country in api_response.keys():
    try:
        for each_payload in api_response[country]:
            doc_id = hash_function(country, each_payload['date'])
            doc = map_es_doc(each_payload, country)
            response = ingest(doc_id, doc)
            print(response)
    except Exception as e:
        print(e)
</code></pre>

<p>Run the script to ingest all the data into elasticsearch. Now we will create the script that will run daily to only ingest the previous day&rsquo;s data, so that we only ingest the latest data and not all the data from scratch again.</p>

<p>I will create this file in <code>/opt/scripts/corona_covid19_ingest.py</code>:</p>

<pre><code>#!/usr/bin/python
import requests
import datetime as dt
import json
import hashlib

url = 'https://pomber.github.io/covid19/timeseries.json'
elasticsearch_url = "https://es.domain.com"
elasticsearch_username = ""
elasticsearch_password = ""

api_response = requests.get(url).json()

yesterdays_date = dt.date.today() - dt.timedelta(days=1)

def convert_datestamp(day):
    return str(dt.datetime.strptime(day, '%Y-%m-%d'))

def hash_function(country, date):
    string_to_hash = country + date
    hash_obj  = hashlib.sha1(string_to_hash.encode('utf-8'))
    hash_value = hash_obj.hexdigest()
    return hash_value

def map_es_doc(payload, country):
    doc = {
        "day": payload['date'],
        "timestamp": convert_datestamp(payload['date']),
        "country": country,
        "confirmed": payload['confirmed'],
        "recovered": payload['recovered'],
        "deaths": payload['deaths']
    }
    return doc

def ingest(doc_id, payload):
    response = requests.put(
        elasticsearch_url + '/coronastats/coronastats/' + doc_id,
        auth=(elasticsearch_username, elasticsearch_password),
        headers={'content-type': 'application/json'},
        json=payload
    )
    return response.status_code

for country in api_response.keys():
    try:
        for each_payload in api_response[country]:
            if convert_datestamp(each_payload['date']).split()[0] == str(yesterdays_date):
                print("ingesting latest data for {country}".format(country=country))
                doc_id = hash_function(country, each_payload['date'])
                doc = map_es_doc(each_payload, country)
                response = ingest(doc_id, doc)
                print(response)
    except Exception as e:
        print(e)
</code></pre>

<p>The only difference with this script is that it checks if the date is equals to yesterday&rsquo;s date, and if so the document will be prepared and ingested into elasticsearch. We will create a cronjob that runs this script every morning at 08:45.</p>

<p>First make the file executable:</p>

<pre><code>$ chmod +x /opt/scripts/corona_covid19_ingest.py
</code></pre>

<p>Run <code>crontab -e</code> and add the following</p>

<pre><code>45 8 * * * /opt/scripts/corona_covid19_ingest.py
</code></pre>

<h2>Visualize the Data with Grafana</h2>

<p>We will create this dashboard:</p>

<p><img src="https://user-images.githubusercontent.com/567298/80418135-35ca0680-88d7-11ea-83f6-3432a903333d.png" alt="corona-covid-19-dashboard" /></p>

<p>We need a elasticsearch datasource that points to the index that we ingest our data into. Head over to datasources, add a elasticsearch datasource and set the index to <code>coronastats</code> and add the timefield as <code>timestamp</code>.</p>

<p>We want to make the dashboard dynamic to have a <strong>&ldquo;country&rdquo;</strong> dropdown selector, for that go to the dashboard settings, select variable and add a country variable:</p>

<p><img src="https://user-images.githubusercontent.com/567298/80419463-7cb8fb80-88d9-11ea-959f-8f37ae3f6dc7.png" alt="covid19-dashboard-variables" /></p>

<p>First panel: &ldquo;Reported Cases per Day&rdquo;:</p>

<p><img src="https://user-images.githubusercontent.com/567298/80419572-af62f400-88d9-11ea-802e-7eeacb61ee19.png" alt="covid19-reported-cases" /></p>

<p>Second panel: &ldquo;Confirmed Cases&rdquo;:</p>

<p><img src="https://user-images.githubusercontent.com/567298/80419675-db7e7500-88d9-11ea-98a5-3aae4d9a6c87.png" alt="covid19-confirmed-cases" /></p>

<p>Third panel: &ldquo;Recovered Cases&rdquo;:</p>

<p><img src="https://user-images.githubusercontent.com/567298/80419750-fa7d0700-88d9-11ea-82a3-f26ff8c807ef.png" alt="covid19-recovered-cases" /></p>

<p>Now, if we select Italy, Spain and France as an example, we will see something like this:</p>

<p><img src="https://user-images.githubusercontent.com/567298/80419966-56479000-88da-11ea-8f30-39ac3da27007.png" alt="covid19-country-stats" /></p>

<h2>Thank You</h2>

<p>Although its pretty cool visualizing data, the issue that we are in at the moment with coronavirus / covid19 is really scary and we should all do our part to try and stay home, sanitize and try not to spread the virus. Together we can all do great things by reducing the spread of this virus.</p>

<p>Stay safe everyone.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deploy a Monitoring Stack on Docker Swarm With Grafana and Prometheus]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/09/05/deploy-a-monitoring-stack-on-docker-swarm-with-grafana-and-prometheus/"/>
    <updated>2019-09-05T00:07:52+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/09/05/deploy-a-monitoring-stack-on-docker-swarm-with-grafana-and-prometheus</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/57307750-696bb980-70e5-11e9-9b0b-73ad88bde6a3.png" alt="" /></p>

<p><a href="https://saythanks.io/to/ruanbekker"><img src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" alt="Say Thanks!" /></a> <a href="https://linux-hackers-slack.herokuapp.com/"><img src="https://linux-hackers-slack.herokuapp.com/badge.svg" alt="Slack Status" /></a> <a href="https://linux-hackers.slack.com/"><img src="https://img.shields.io/badge/chat-on_slack-orange.svg" alt="Chat on Slack" /></a> <a href="https://github.com/bekkerstacks/traefik"><img src="https://img.shields.io/github/followers/ruanbekker.svg?label=Follow&amp;style=social" alt="GitHub followers" /></a></p>

<p><a href="https://twitter.com/ruanbekker?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @ruanbekker</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>

<p>In this tutorial we will deploy a monitoring stack to docker swarm, that includes Grafana, Prometheus, Node-Exporter, cAdvisor and Alertmanager.</p>

<p>If you are looking for more information on Prometheus, have a look at my other <a href="https://blog.ruanbekker.com/blog/categories/prometheus/">Prometheus and Monitoring</a> blog posts.</p>

<h2>What you will get out of this</h2>

<p>Once you deployed the stacks, you will have the following:</p>

<ul>
<li>Access Grafana through Traefik reverse proxy</li>
<li>Node-Exporter to expose node level metrics</li>
<li>cAdvisor to expose container level metrics</li>
<li>Prometheus to scrape the exposed entpoints and ingest it into Prometheus</li>
<li>Prometheus for your Timeseries Database</li>
<li>Alertmanager for firing alerts on configured rules</li>
</ul>


<p>The compose file that I will provide will have pre-populated dashboards</p>

<h2>Deploy Traefik</h2>

<p>Get the traefik stack sources:</p>

<pre><code>$ git clone https://github.com/bekkerstacks/traefik
$ pushd traefik
</code></pre>

<p>Have a look at <a href="https://github.com/bekkerstacks/traefik/wiki/Deploy-Traefik-in-HTTPS-Mode">HTTPS Mode</a> if you want to deploy traefik on HTTPS, as I will use HTTP in this demonstration.</p>

<p>Set your domain and deploy the stack:</p>

<pre><code>$ DOMAIN=localhost PROTOCOL=http bash deploy.sh

Username for Traefik UI: demo
Password for Traefik UI: 
deploying traefik stack in http mode
Creating network public
Creating config proxy_traefik_htpasswd
Creating service proxy_traefik
Traefik UI is available at:
- http://traefik.localhost
</code></pre>

<p>Your traefik service should be running:</p>

<pre><code>$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
0wga71zbx1pe        proxy_traefik       replicated          1/1                 traefik:1.7.14      *:80-&gt;80/tcp
</code></pre>

<p>Switch back to the previous directory:</p>

<pre><code>$ popd
</code></pre>

<h2>Deploy the Monitoring Stack</h2>

<p>Get the sources:</p>

<pre><code>$ git clone https://github.com/bekkerstacks/monitoring-cpang
$ pushd monitoring-cpang
</code></pre>

<p>If you want to deploy the stack with no pre-configured dashboards, you would need to use <code>./docker-compose.html</code>, but in this case we will deploy the stack with pre-configured dashboards.</p>

<p>Set the domain and deploy the stack:</p>

<pre><code>$ docker stack deploy -c alt_versions/docker-compose_http_with_dashboards.yml mon

Creating network private
Creating config mon_grafana_config_datasource
Creating config mon_grafana_dashboard_prometheus
Creating config mon_grafana_dashboard_docker
Creating config mon_grafana_dashboard_nodes
Creating config mon_grafana_dashboard_blackbox
Creating config mon_alertmanager_config
Creating config mon_prometheus_config
Creating config mon_prometheus_rules
Creating service mon_blackbox-exporter
Creating service mon_alertmanager
Creating service mon_prometheus
Creating service mon_grafana
Creating service mon_cadvisor
Creating service mon_node-exporter
</code></pre>

<p>The endpoints is configured as <code>${service_name}.${DOMAIN}</code> so you will be able to access grafana on <code>http://grafana.localhost</code> as showed in my use-case.</p>

<p>Use <code>docker stack services mon</code> to see if all the tasks has checked into its desired count then access grafana on <code>http://grafana.${DOMAIN}</code></p>

<h2>Accessing Grafana</h2>

<p>Access Grafana on <code>http://grafana.${DOMAIN}</code> and logon with the user admin and the password admin:</p>

<p><img src="https://user-images.githubusercontent.com/50801771/64292266-4d303a00-cf6a-11e9-8a49-2ae05b1ed5c6.png" alt="image" /></p>

<p>You will be asked to reset the password:</p>

<p><img src="https://user-images.githubusercontent.com/50801771/64292291-5f11dd00-cf6a-11e9-8049-2abdbb0164f6.png" alt="image" /></p>

<p>You will then be directed to the ui:</p>

<p><img src="https://user-images.githubusercontent.com/50801771/64292317-705ae980-cf6a-11e9-928b-60b5dec7ea09.png" alt="image" /></p>

<p>From the top, when you list dashboards, you will see the 3 dashboards that was pre-configured:</p>

<p><img src="https://user-images.githubusercontent.com/50801771/64292334-7b157e80-cf6a-11e9-92c6-9e0698815ba7.png" alt="image" /></p>

<p>When looking at the Swarm Nodes Dashboard:</p>

<p><img src="https://user-images.githubusercontent.com/50801771/64297086-82da2080-cf74-11e9-8060-f0193bfaeb13.png" alt="image" /></p>

<p>The Swarm Services Dashboard:</p>

<p><img src="https://user-images.githubusercontent.com/50801771/64297656-a8ffc080-cf74-11e9-88a2-b4cec295aed5.png" alt="image" /></p>

<h2>Exploring Metrics in Prometheus</h2>

<p>Access prometheus on <code>http://prometheus.${DOMAIN}</code> and from the search input, you can start exploring though all the metrics that is available in prometheus:</p>

<p><img src="https://user-images.githubusercontent.com/50801771/64298324-74403900-cf75-11e9-99b7-559b02ef67b7.png" alt="image" /></p>

<p>If we search for <code>node_load15</code> and select graph, we can have a quick look on how the 15 minute load average looks like for the node where the stack is running on:</p>

<p><img src="https://user-images.githubusercontent.com/50801771/64298454-e31d9200-cf75-11e9-89bb-b6fe94470166.png" alt="image" /></p>

<p>Having a look at the alerts section:</p>

<p><img src="https://user-images.githubusercontent.com/50801771/64299172-7657c700-cf78-11e9-97bd-143e5fe87941.png" alt="image" /></p>

<h2>Resources</h2>

<p>For more information and configuration on the stack that we use, have a look at the wiki:
- <a href="https://github.com/bekkerstacks/monitoring-cpang/wiki">https://github.com/bekkerstacks/monitoring-cpang/wiki</a></p>

<p>The github repository:
- <a href="https://github.com/bekkerstacks/monitoring-cpang">https://github.com/bekkerstacks/monitoring-cpang</a></p>

<h2>Thank You</h2>

<p>Let me know what you think. If you liked my content, feel free to checkout my content on <strong><a href="https://ruan.dev/">ruan.dev</a></strong> or follow me on twitter at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong></p>

<p><a href="https://twitter.com/ruanbekker?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @ruanbekker</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Prometheus Series of Tutorials for Your Guide to Epic Metrics]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/05/17/prometheus-series-of-tutorials-for-your-guide-to-epic-metrics/"/>
    <updated>2019-05-17T14:24:40-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/05/17/prometheus-series-of-tutorials-for-your-guide-to-epic-metrics</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/57307750-696bb980-70e5-11e9-9b0b-73ad88bde6a3.png" alt="prometheus" /></p>

<p>This is a curated list of tutorials of prometheus, from installing prometheus, installing grafana, exporters, docker versions of the prometheus / grafana / node exporter stack, etc.</p>

<h2>The List</h2>

<ul>
<li>Install <a href="http://blog.ruanbekker.com/blog/2019/05/07/setup-prometheus-and-node-exporter-on-ubuntu-for-epic-monitoring/">Prometheus</a></li>
<li>Install <a href="http://blog.ruanbekker.com/blog/2019/05/07/setup-prometheus-and-node-exporter-on-ubuntu-for-epic-monitoring/">Node Exporter</a></li>
<li>Install <a href="http://blog.ruanbekker.com/blog/2019/05/17/install-pushgateway-to-expose-metrics-to-prometheus/">Pushgateway</a></li>
<li>Install <a href="http://blog.ruanbekker.com/blog/2019/05/17/install-grafana-to-visualize-your-metrics-from-datasources-such-as-prometheus-on-linux/">Grafana</a></li>
<li>Install <a href="http://blog.ruanbekker.com/blog/2019/05/17/install-alertmanager-to-alert-based-on-metrics-from-prometheus/">Alertmananger</a></li>
<li>Install <a href="http://blog.ruanbekker.com/blog/2019/05/17/install-blackbox-exporter-to-monitor-websites-with-prometheus/">Blackbox Exporter</a></li>
<li>Install <a href="">Docker Prometheus Grafana Stack</a></li>
</ul>


<p>This list will be updated as I publish more tutorials</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Install Grafana to Visualize Your Metrics From Datasources Such as Prometheus on Linux]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/05/17/install-grafana-to-visualize-your-metrics-from-datasources-such-as-prometheus-on-linux/"/>
    <updated>2019-05-17T12:08:02-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/05/17/install-grafana-to-visualize-your-metrics-from-datasources-such-as-prometheus-on-linux</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/57941411-2a045080-78cf-11e9-97f9-47fb8b75a722.png" alt="image" /></p>

<p>Grafana is a Open Source Dashboarding service that allows you to monitor, analyze and graph metrics from datasources such as prometheus, influxdb, elasticsearch, aws cloudwatch, and many more.</p>

<p>Not only is grafana amazing, its super pretty!</p>

<p>Example of how a dashboard might look like:</p>

<p><img width="1279" alt="E24B39B1-23C8-44C5-959D-6E6275F8FE99" src="https://user-images.githubusercontent.com/567298/57942872-d98ef200-78d2-11e9-9370-b130bcc222f7.png"></p>

<h2>What are we doing today</h2>

<p>In this tutorial we will setup grafana on linux. If you have not set up <a href="https://blog.ruanbekker.com/blog/2019/05/07/setup-prometheus-and-node-exporter-on-ubuntu-for-epic-monitoring/">prometheus</a>, follow <a href="https://blog.ruanbekker.com/blog/2019/05/07/setup-prometheus-and-node-exporter-on-ubuntu-for-epic-monitoring/">this blogpost</a> to install prometheus.</p>

<h2>Install Grafana</h2>

<p>I will be demonstrating how to install grafana on debian, if you have another operating system, head over to <a href="https://grafana.com/docs/installation/">grafana documentation</a> for other supported operating systems.</p>

<p>Get the gpg key:</p>

<pre><code>$ curl https://packages.grafana.com/gpg.key | sudo apt-key add -
</code></pre>

<p>Import the public keys:</p>

<pre><code>$ apt-key adv --keyserver keyserver.ubuntu.com --recv-keys  8C8C34C524098CB6 
</code></pre>

<p>Add the latest stable packages to your repository:</p>

<pre><code>$ add-apt-repository "deb https://packages.grafana.com/oss/deb stable main"
</code></pre>

<p>Install a pre-requirement package:</p>

<pre><code>$ apt install apt-transport-https -y
</code></pre>

<p>Update the repository index and install grafana:</p>

<pre><code>$ apt update &amp;&amp; sudo apt install grafana -y
</code></pre>

<p>Once grafana is installed, start the service:</p>

<pre><code>$ service grafana-server start
</code></pre>

<p>Then enable the service on boot:</p>

<pre><code>$ update-rc.d grafana-server defaults
</code></pre>

<p>If you want to control the service via systemd:</p>

<pre><code>$ systemctl daemon-reload
$ systemctl start grafana-server
$ systemctl status grafana-server
</code></pre>

<h2>Optional: Nginx Reverse Proxy</h2>

<p>If you want to front your grafana instance with a nginx reverse proxy:</p>

<pre><code>$ cat /etc/nginx/sites-enabled/grafana
server {
    listen 80;
    server_name grafana.domain.com;

    location / {
        proxy_pass http://127.0.0.1:3000/;
        proxy_redirect http://127.0.0.1:3000/ /;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Real-IP $remote_addr;
    }
</code></pre>

<p>Then restart nginx:</p>

<pre><code>$ systemctl restart nginx
</code></pre>

<h2>Access Grafana</h2>

<p>If you are accessing grafana directly, access grafana on <code>http://your-grafana-ip:3000/</code> and your username is <code>admin</code> and password <code>admin</code></p>

<h2>Dashboarding Tutorials</h2>

<p>Have a look at this screencast where the guys from grafana show you how to build dashboards:</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/sKNZMtoSHN4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


<p>Also have a look at their <a href="https://grafana.com/dashboards">public repository of dashboards</a></p>

<p>For more tutorials on prometheus and metrics have a look at <strong><a href="https://blog.ruanbekker.com/blog/categories/prometheus/">#prometheus</a></strong></p>
]]></content>
  </entry>
  
</feed>
