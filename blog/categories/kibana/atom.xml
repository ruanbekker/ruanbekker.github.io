<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Kibana | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/kibana/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2018-12-02T17:29:44-05:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Get Application Performance Metrics on Python Flask With Elastic APM on Kibana and Elasticsearch]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/11/11/get-application-performance-metrics-on-python-flask-with-elastic-apm-on-kibana-and-elasticsearch/"/>
    <updated>2018-11-11T13:09:18-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/11/11/get-application-performance-metrics-on-python-flask-with-elastic-apm-on-kibana-and-elasticsearch</id>
    <content type="html"><![CDATA[<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-banner.png" alt="" /></p>

<p>In this post we will setup a Python Flask Application which includes the APM Agent which will collect metrics, that gets pushed to the APM Server. If you have not setup the Elastic Stack with / or APM Server, you <a href="https://blog.ruanbekker.com/blog/2018/11/11/setup-apm-server-on-ubuntu-for-your-elastic-stack-to-get-insights-in-your-application-performance-metrics/">can follow this post</a> to setup the needed.</p>

<p>Then we will make a bunch of HTTP Requests to our Application and will go through the metrics per request type.</p>

<h2>Application Metrics</h2>

<p>Our Application will have the following Request Paths:</p>

<ul>
<li><code>/</code> - Returns static text</li>
<li><code>/delay</code> - random delays to simulate increased response latencies</li>
<li><code>/upstream</code> - get data from a upstream provider, if statements to provide dummy 200, 404 and 502 reponses to visualize</li>
<li><code>/5xx</code> - request path that will raise an exception so that we can see the error via apm</li>
<li><code>/sql-write</code> - inserts 5 rows into a sqlite database</li>
<li><code>/sql-read</code> - executes a select all from the database</li>
<li><code>/sql-group</code> - sql query to group all the cities and count them</li>
</ul>


<p>This is just simple request paths to demonstrate the metrics via APM (Application Performance Monitoring) on Kibana.</p>

<h2>Install Flask and APM Agent</h2>

<p>Create a virtual environment and install the dependencies:</p>

<pre><code class="bash">$ apt install python python-setuptools -y
$ easy_install pip
$ pip install virtualenv
$ pip install elastic-apm[flask]
$ pip install flask
</code></pre>

<p>For more info on <a href="https://www.elastic.co/guide/en/apm/agent/python/current/getting-started.html">APM Configuration</a>.</p>

<h2>Instrument a Bare Bones Python Flask app with APM:</h2>

<p>A Barebones app with APM Configured will look like this:</p>

<pre><code class="python">from flask import Flask, jsonify
from elasticapm.contrib.flask import ElasticAPM
from elasticapm.handlers.logging import LoggingHandler

app = Flask(__name__)
apm = ElasticAPM(app, server_url='http://localhost:8200', service_name='flask-app-1', logging=True)

@app.route('/')
def index():
    return jsonify({"message": "response ok"}), 200

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=80)
</code></pre>

<p>This will provide metrics on the <code>/</code> request path. In order to trace transaction ids from the metrics, we need to configure the index on Kibana. To do this, head over to Kibana, Management, Index Patterns, Add Index Pattern, <code>apm*</code>, select <code>@timestamp</code> as the time filter field name.</p>

<p>This will allow you to see the data when tracing the transaction id&rsquo;s via the Discover UI.</p>

<h2>Create the Python Flask App</h2>

<p>Create the Flask App with the request paths as mentioned in the beginning:</p>

<pre><code class="python">import sqlite3, requests, time, logging, random
from flask import Flask, jsonify
from elasticapm.contrib.flask import ElasticAPM
from elasticapm.handlers.logging import LoggingHandler

names = ['ruan', 'stefan', 'philip', 'norman', 'frank', 'pete', 'johnny', 'peter', 'adam']
cities = ['cape town', 'johannesburg', 'pretoria', 'dublin', 'kroonstad', 'bloemfontein', 'port elizabeth', 'auckland', 'sydney']
lastnames = ['smith', 'bekker', 'admams', 'phillips', 'james', 'adamson']

conn = sqlite3.connect('database.db')
conn.execute('CREATE TABLE IF NOT EXISTS people (name STRING, age INTEGER, surname STRING, city STRING)')
#sqlquery_write = conn.execute('INSERT INTO people VALUES("{}", "{}", "{}", "{}")'.format(random.choice(names), random.randint(18,40), random.choice(lastnames), random.choice(cities)))
seconds = [0.002, 0.003, 0.004, 0.01, 0.3, 0.2, 0.009, 0.015, 0.02, 0.225, 0.009, 0.001, 0.25, 0.030, 0.018]

app = Flask(__name__)
apm = ElasticAPM(app, server_url='http://localhost:8200', service_name='my-app-01', logging=False)

@app.route('/')
def index():
    return jsonify({"message": "response ok"})

@app.route('/delay')
def delay():
    time.sleep(random.choice(seconds))
    return jsonify({"message": "response delay"})

@app.route('/upstream')
def upstream():
    r = requests.get('https://api.ruanbekker.com/people').json()
    r.get('country')
    if r.get('country') == 'italy':
        return 'Italalia!', 200
    elif r.get('country') == 'canada':
        return 'Canada!', 502
    else:
        return 'Not Found', 404

@app.route('/5xx')
def fail_with_5xx():
    value = 'a' + 1
    return jsonify({"message": value})

@app.route('/sql-write')
def sqlw():
    conn = sqlite3.connect('database.db')
    conn.execute('INSERT INTO people VALUES("{}", "{}", "{}", "{}")'.format(random.choice(names), random.randint(18,40), random.choice(lastnames), random.choice(cities)))
    conn.execute('INSERT INTO people VALUES("{}", "{}", "{}", "{}")'.format(random.choice(names), random.randint(18,40), random.choice(lastnames), random.choice(cities)))
    conn.execute('INSERT INTO people VALUES("{}", "{}", "{}", "{}")'.format(random.choice(names), random.randint(18,40), random.choice(lastnames), random.choice(cities)))
    conn.execute('INSERT INTO people VALUES("{}", "{}", "{}", "{}")'.format(random.choice(names), random.randint(18,40), random.choice(lastnames), random.choice(cities)))
    conn.execute('INSERT INTO people VALUES("{}", "{}", "{}", "{}")'.format(random.choice(names), random.randint(18,40), random.choice(lastnames), random.choice(cities)))
    conn.commit()
    conn.close()
    return 'ok', 200

@app.route('/sql-read')
def sqlr():
    conn = sqlite3.connect('database.db')
    conn.row_factory = sqlite3.Row
    cur = conn.cursor()
    cur.execute('select * from people')
    rows = cur.fetchall()
    conn.close()
    return 'ok', 200

@app.route('/sql-group')
def slqg():
    conn = sqlite3.connect('database.db')
    conn.row_factory = sqlite3.Row
    cur = conn.cursor()
    cur.execute('select count(*) as num, city from people group by city')
    rows = cur.fetchall()
    conn.close()
    return 'ok', 200

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=80)
</code></pre>

<p>Run the app:</p>

<pre><code class="bash">$ python app.py
</code></pre>

<p>At this point, we wont have any data on APM as we need to make requests to our application. Let&rsquo;s make 10 HTTP GET Requests on the <code>/</code> Request Path:</p>

<pre><code class="bash">$ count=0 &amp;&amp; while [ $count -lt 10 ]; do curl http://application-routable-address:80/; sleep 1; count=$((count+1)); done
</code></pre>

<h2>Visualize the Root Request Path</h2>

<p>Head over to Kibana, Select APM and you will see something similar like below when selecting the timepicker to 15 minutes at the right top corner. This page will give you the overview of all your configured applications and the average response times over the selected time, transactions per minute, errors per minute etc:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-home-root.png" alt="" /></p>

<p>When you select your application, you will find the graphs on you response times and requests per minute, also a breakdown per request path:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-app-view-1.png" alt="" /></p>

<p>When selecting the request path, in this case <code>GET /</code>, you will find a breakdown of metrics only for that request and also the response time distribution for that request path, if you select frame from the response time distribution, it will filter the focus to that specific transaction.</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-req-view-1.png" alt="" /></p>

<p>When you scroll a bit down to the Transaction Sample section, you will find data about the request, response, system etc:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-app-transaction-1.png" alt="" /></p>

<p>From the Transaction Sample, you can select the View Transaction in Discover button, which will trace that transaction id on the Discover UI:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-transaction-disc-1.png" alt="" /></p>

<p>Increasing the http curl clients running simultaneously from different servers and increasing the time for 15 minutes to have more metrics will result in the screenshot below, notice the 6ms response time can easily be traced selecting it in the response time distribution, then discovering it in the UI, which will give you the raw data from that request:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-req-view-2.png" alt="" /></p>

<h2>Viewing Application Errors in APM</h2>

<p>Make a couple of requests to <code>/5xx</code>:</p>

<pre><code class="bash">$ curl http://application-routable-endpoint:80/5xx
</code></pre>

<p>Navigate to the app, select Errors, then you will see the exception details that was returned. Here we can see that in our code we tried to concatenate integers with strings:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-error-1.png" alt="" /></p>

<p>Furthermore we can select that error and it will provide us a direct view on where in our code the error gets generated:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-error-details-1.png" alt="" /></p>

<p>Pretty cool right?! You can also further select the library frames, which will take you to the lower level code that raised the exception. If this errors can be drilled down via the discover ui, to group by source address etc.</p>

<h2>Simulate Response Latencies:</h2>

<p>Make a couple of requests to the <code>/delay</code> request path, and you should see the increased response times from earlier:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-app-view-delay.png" alt="" /></p>

<h2>Requests where Database Calls are Executed</h2>

<p>The while loop to call random request paths:</p>

<pre><code class="bash">count=0 &amp;&amp; while [ $count -lt 1000 ]; 
do 
  curl -H "Host: my-za-server" -i http://x.x.x.x/sql-write; 
  curl -H "Host: my-za-server" -i http://x.x.x.x/sql-read; 
  curl -H "Host: my-za-server" -i http://x.x.x.x/sql-group; 
  curl -H "Host: my-eu-server" -i http://x.x.x.x/sql-write; 
  curl -H "Host: my-us-server" -i http://x.x.x.x/sql-write; 
  curl -H "Host: my-za-server" -i http://x.x.x.x/sql-write; 
  curl -H "Host: my-za-server" -i http://x.x.x.x/sql-write; 
  curl -H "Host: my-za-server" -i http://x.x.x.x/sql-read; 
  curl -H "Host: my-eu-server" -i http://x.x.x.x/sql-group; 
  curl -H "Host: my-us-server" -i http://x.x.x.x/sql-group; 
  curl -H "Host: my-za-server" -i http://x.x.x.x/sql-write; 
  curl -H "Host: my-za-server" -i http://x.x.x.x/sql-write; 
  curl -H "Host: my-eu-server" -i http://x.x.x.x/sql-group; 
  curl -H "Host: my-za-server" -i http://x.x.x.x/sql-group; 
  count=$((count+1)); 
done
</code></pre>

<p>When we look at our applications performance monitoring overview, we can see the writes provide more latencies as the group by&rsquo;s:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-db-view-1.png" alt="" /></p>

<p>The <code>/sql-write</code> request overview:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-db-sqlwrite-1.png" alt="" /></p>

<p>When selecting a transaction sample, we can see the timeline of each database call:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-db-sqlwrite-1-details.png" alt="" /></p>

<p>When looking at the <code>/sql-group</code> request overview, we can see that the response times increasing overtime, as more data is written to the database, it takes longer to read and group all the data from the database:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-db-sqlgroup-1.png" alt="" /></p>

<p>The transaction details shows the timeline of the database query from that request:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-db-sqlgroup-1-details.png" alt="" /></p>

<p>When you select the database select query on the timeline view, it will take you to the exact database query that was executed:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-db-sqlgroup-1-span.png" alt="" /></p>

<p>When we include a database call with a external request to a remote http endpoint, we will see something like:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-db-sqlread-ext.png" alt="" /></p>

<h2>Viewing 4xx and 5xx Response Codes</h2>

<p>From the application code we are returning 2xx, 4xx, and 5xx response codes for this demonstration to visualize them:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-app-response-codes.png" alt="" /></p>

<h2>Configuring more Applications</h2>

<p>Once more apps are configured, and they start serving traffic, they will start appearing on the APM UI as below:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-apps.png" alt="" /></p>

<p>APM is available for other languages as well and provides a getting started snippets from the APM UI. For more information on APM, have a look at their <a href="https://www.elastic.co/solutions/apm">Documentation</a></p>

<p>Hope this was useful.</p>

<script type="text/javascript">
  ( function() {
    if (window.CHITIKA === undefined) { window.CHITIKA = { 'units' : [] }; };
    var unit = {"calltype":"async[2]","publisher":"rbekker87","width":728,"height":90,"sid":"Chitika Default"};
    var placement_id = window.CHITIKA.units.length;
    window.CHITIKA.units.push(unit);
    document.write('<div id="chitikaAdBlock-' + placement_id + '"></div>');
}());
</script>


<script type="text/javascript" src="//cdn.chitika.net/getads.js" async></script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup APM Server on Ubuntu for Your Elastic Stack to Get Insights in Your Application Performance Metrics]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/11/11/setup-apm-server-on-ubuntu-for-your-elastic-stack-to-get-insights-in-your-application-performance-metrics/"/>
    <updated>2018-11-11T12:31:43-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/11/11/setup-apm-server-on-ubuntu-for-your-elastic-stack-to-get-insights-in-your-application-performance-metrics</id>
    <content type="html"><![CDATA[<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-overview.png" alt="" /></p>

<p>In this post we will setup the Elastic Stack with Elasticsearc, Kibana and APM . The APM Server (Application Performance Metrics) which will receive the metric data from the application side, and is then pushed to apm indices on Elasticsearch.</p>

<p>This will be a 2 post blog on APM:</p>

<ul>
<li>1) <a href="">Setup the Elastic Stack with Elasticsearch, Kibana and APM Server</a> - this post</li>
<li>2) <a href="https://blog.ruanbekker.com/blog/2018/11/11/get-application-performance-metrics-on-python-flask-with-elastic-apm-on-kibana-and-elasticsearch/">Setup a Python Flask application with the APM Agent</a></li>
</ul>


<h2>What is APM</h2>

<p>From their website APM is described as: &ldquo;Elastic APM is an application performance monitoring system built on the Elastic Stack. It allows you to monitor software services and applications in real time, collecting detailed performance information on response time for incoming requests, database queries, calls to caches, external HTTP requests, etc.&rdquo;</p>

<p>You get metrics like average, p99 response times etc, and also have insights when errors occur, it even allows you to look at the stacktrace, poinpointing on which line of your code it ocurred etc.</p>

<ul>
<li><a href="https://www.elastic.co/solutions/apm">More Info</a></li>
</ul>


<h2>APM Agents:</h2>

<p>The APM Agents will be loaded inside your application, application metrics will then be pushed to the APM Server (which we will setup in this post), which then gets pushed to Elasticsearch and is then consumed by Kibana.</p>

<p>At the time of writing, the APM Agents are supported in the following languages:</p>

<ul>
<li>Node.js</li>
<li>Django</li>
<li>Flask</li>
<li>Ruby on Rails</li>
<li>Rack</li>
<li>RUM</li>
<li>Golang</li>
<li>Java</li>
</ul>


<h2>Setup the Elastic Stack</h2>

<p>One thing to note, every service in your Elastic Stack needs to be running on the same version. In this post we will setup Elasticsearch, APM and Kibana all running on version <code>6.4.3</code></p>

<h2>Setup the Pre-Requirements:</h2>

<p>Elasticsearch depends on Java, se we will go ahead and setup the repositories:</p>

<pre><code class="bash">$ wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
$ apt-get install apt-transport-https -y
$ echo "deb https://artifacts.elastic.co/packages/6.x/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-6.x.list
$ apt update &amp;&amp; apt upgrade -y 
$ apt install openjdk-8-jdk -y
</code></pre>

<p>Verify that Java is installed:</p>

<pre><code class="bash">$ java -version
openjdk version "1.8.0_181"
OpenJDK Runtime Environment (build 1.8.0_181-8u181-b13-1ubuntu0.16.04.1-b13)
OpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)
</code></pre>

<p>Setup Kernel parameters for Elasticsearch:</p>

<pre><code class="bash">$ sysctl -w vm.max_map_count=262144
$ echo 'vm.max_map_count=262144' &gt;&gt; /etc/sysctl.conf
</code></pre>

<h2>Setup Elasticsearch:</h2>

<p>Search for the latest versions (when already having elasticsearch, either upgrade or install apm on the same version as elasticsearch/kibana):</p>

<pre><code class="bash">$ apt-cache madison elasticsearch
elasticsearch |      6.4.3 | https://artifacts.elastic.co/packages/6.x/apt stable/main amd64 Packages
elasticsearch |      6.4.2 | https://artifacts.elastic.co/packages/6.x/apt stable/main amd64 Packages
</code></pre>

<p>Install Elasticsearch:</p>

<pre><code class="bash">$ apt-get install elasticsearch=6.4.3 -y
</code></pre>

<p>Configure Elasticsearch to lock the memory on startup:</p>

<pre><code class="bash">$ sed -i 's/#bootstrap.memory_lock: true/bootstrap.memory_lock: true/g' /etc/elasticsearch/elasticsearch.yml
</code></pre>

<p>Enable Elasticsearch on startup and start the service:</p>

<pre><code>$ systemctl daemon-reload
$ systemctl enable elasticsearch.service
$ systemctl start elasticsearch.service
</code></pre>

<h2>Install Kibana:</h2>

<p>Install Kibana version <code>6.4.3</code>:</p>

<pre><code class="bash">$ apt install kibana=6.4.3 -y
</code></pre>

<p>For demonstration, I will configure Kibana to listen on all interfaces on port <code>5601</code>, but note this will enable access for everyone, you can [follow this blogpost] to setup a Nginx Reverse Proxy to upstream to localhost on port 5601.</p>

<p>Since this demonstration we are using Elasticsearch locally, so if you have a remote cluster, configuration can be applied where needed.</p>

<pre><code class="bash">$ sed -i 's/#server.host: "localhost"/server.host: "0.0.0.0"/'g /etc/kibana/kibana.yml
$ sed -i 's/#elasticsearch.url: "http:\/\/localhost:9200"/elasticsearch.url: "http:\/\/localhost:9200"/'g /etc/kibana/kibana.yml
</code></pre>

<p>Enable Kibana on startup and start the service:</p>

<pre><code class="bash">$ systemctl enable kibana.service
$ systemctl restart kibana.service
</code></pre>

<h2>Install the APM Server</h2>

<p>Install APM Server version <code>6.4.3</code>:</p>

<pre><code class="bash">$ apt install apm-server=6.4.3 -y
</code></pre>

<p>Since we have everything locally, the configuration can be kept as is, but if you need to configure the elasticsearch or kibana hosts, it can be done via <code>/etc/apm-server/apm-server.yml</code></p>

<p>Then once Kibana and Elasticsearch is started, load the mapping templates, enable and start the service:</p>

<pre><code class="bash">$ apm-server setup
$ systemctl enable apm-server.service
$ systemctl restart apm-server.service
</code></pre>

<p>Ensure all the services are running with <code>netstat -tulpn</code> and port <code>9200</code>, <code>9300</code>, <code>5601</code> and <code>8300</code> should be listening</p>

<h2>Access Your Elastic Stack</h2>

<p>Access Kibana on your routable endpoint on port <code>5601</code> and you should see something like this:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elastic-apm-startup.png" alt="" /></p>

<h2>Configuring a Application to push metrics to APM</h2>

<p>In the <a href="https://blog.ruanbekker.com/blog/2018/11/11/get-application-performance-metrics-on-python-flask-with-elastic-apm-on-kibana-and-elasticsearch/">next post</a> I will setup a Python Flask Application on APM</p>

<script type="text/javascript">
  ( function() {
    if (window.CHITIKA === undefined) { window.CHITIKA = { 'units' : [] }; };
    var unit = {"calltype":"async[2]","publisher":"rbekker87","width":728,"height":90,"sid":"Chitika Default"};
    var placement_id = window.CHITIKA.units.length;
    window.CHITIKA.units.push(unit);
    document.write('<div id="chitikaAdBlock-' + placement_id + '"></div>');
}());
</script>


<script type="text/javascript" src="//cdn.chitika.net/getads.js" async></script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running a 3 Node Elasticsearch Cluster With Docker Compose on Your Laptop for Testing]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/04/29/running-a-3-node-elasticsearch-cluster-with-docker-compose-on-your-laptop-for-testing/"/>
    <updated>2018-04-29T13:43:35-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/04/29/running-a-3-node-elasticsearch-cluster-with-docker-compose-on-your-laptop-for-testing</id>
    <content type="html"><![CDATA[<p>Having a Elasticsearch cluster on your laptop with Docker for testing is great. And in this post I will show you how quick and easy it is, to have a 3 node elasticsearch cluster running on docker for testing.</p>

<script id="mNCC" language="javascript">
    medianet_width = "728";
    medianet_height = "90";
    medianet_crid = "218284798";
    medianet_versionId = "3111299"; 
  </script>


<script src="//contextual.media.net/nmedianet.js?cid=8CUD78FSV"></script>


<h2>Pre-Requisites</h2>

<p>We need to set the <code>vm.max_map_count</code> kernel parameter:</p>

<pre><code class="bash">$ sudo sysctl -w vm.max_map_count=262144
</code></pre>

<p>To set this permanently, add it to <code>/etc/sysctl.conf</code> and reload with <code>sudo sysctl -p</code></p>

<h2>Docker Compose:</h2>

<p>The docker compose file that we will reference:</p>

<pre><code class="yaml">version: '2.2'
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.2.4
    container_name: elasticsearch
    environment:
      - cluster.name=docker-cluster
      - bootstrap.memory_lock=true
      - http.cors.enabled=true
      - http.cors.allow-origin=*
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - esdata1:/home/ruan/workspace/docker/elasticsearch/data
    ports:
      - 9200:9200
    networks:
      - esnet
  elasticsearch2:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.2.4
    container_name: elasticsearch2
    environment:
      - cluster.name=docker-cluster
      - bootstrap.memory_lock=true
      - http.cors.enabled=true
      - http.cors.allow-origin=*
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - "discovery.zen.ping.unicast.hosts=elasticsearch"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - esdata2:/home/ruan/workspace/docker/elasticsearch/data
    networks:
      - esnet
  elasticsearch3:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.2.4
    container_name: elasticsearch3
    environment:
      - cluster.name=docker-cluster
      - bootstrap.memory_lock=true
      - http.cors.enabled=true
      - http.cors.allow-origin=*
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - "discovery.zen.ping.unicast.hosts=elasticsearch"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - esdata3:/home/ruan/workspace/docker/elasticsearch/data
    networks:
      - esnet

  kibana:
    image: 'docker.elastic.co/kibana/kibana:6.3.2'
    container_name: kibana
    environment:
      SERVER_NAME: kibana.local
      ELASTICSEARCH_URL: http://elasticsearch:9200
    ports:
      - '5601:5601'
    networks:
      - esnet

  headPlugin:
    image: 'mobz/elasticsearch-head:5'
    container_name: head
    ports:
      - '9100:9100'
    networks:
      - esnet

volumes:
  esdata1:
    driver: local
  esdata2:
    driver: local
  esdata3:
    driver: local

networks:
  esnet:
</code></pre>

<p>Now make sure the paths exist that we referenced in the compose file, in my case <code>/home/ruan/workspace/docker/elasticsearch/data</code></p>

<h2>Deploy</h2>

<p>Deploy your elasticsearch cluster with docker compose:</p>

<pre><code class="bash">$ docker-compose up
</code></pre>

<p>This will run in the foreground, and you should see console output.</p>

<h2>Testing Elasticsearch</h2>

<p>Let&rsquo;s run a couple of queries, first up, check the cluster health api:</p>

<pre><code class="bash">$ curl http://127.0.0.1:9200/_cluster/health?pretty
{
  "cluster_name" : "docker-cluster",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 3,
  "number_of_data_nodes" : 3,
  "active_primary_shards" : 1,
  "active_shards" : 2,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 100.0
}
</code></pre>

<p>Create a index with replication count of 2:</p>

<pre><code class="bash">$ curl -H "Content-Type: application/json" -XPUT http://127.0.0.1:9200/test -d '{"number_of_replicas": 2}'
</code></pre>

<p>Ingest a document to elasticsearch:</p>

<pre><code class="bash">$ curl -H "Content-Type: application/json" -XPUT http://127.0.0.1:9200/test/docs/1 -d '{"name": "ruan"}'
{"_index":"test","_type":"docs","_id":"1","_version":1,"result":"created","_shards":{"total":3,"successful":3,"failed":0},"_seq_no":0,"_primary_term":1}
</code></pre>

<p>View the indices:</p>

<pre><code class="bash">$ curl http://127.0.0.1:9200/_cat/indices?v
health status index                       uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   test                        w4p2Q3fTR4uMSYBfpNVPqw   5   2          1            0      3.3kb          1.1kb
green  open   .monitoring-es-6-2018.04.29 W69lql-rSbORVfHZrj4vug   1   1       1601           38        4mb            2mb
</code></pre>

<h2>Kibana</h2>

<p>Kibana is also included in the stack and is accessible via <a href="http://localhost:5601/">http://localhost:5601/</a> and you it should look more or less like:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/kibana-local-home.png" alt="" /></p>

<h2>Elasticsearch Head UI</h2>

<p>I always prefer working directly with the RESTFul API, but if you would like to use a UI to interact with Elasticsearch, you can access it via <a href="http://localhost:9100/">http://localhost:9100/</a> and should look like this:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/elasticsearch-head-ui.png" alt="" /></p>

<h2>Deleting the Cluster:</h2>

<p>As its running in the foreground, you can just hit ctrl + c and as we persisted data in our compose, when you spin up the cluster again, the data will still be there.</p>

<h2>Resources:</h2>

<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html</a></li>
</ul>


<script type="text/javascript">
  ( function() {
    if (window.CHITIKA === undefined) { window.CHITIKA = { 'units' : [] }; };
    var unit = {"calltype":"async[2]","publisher":"rbekker87","width":728,"height":90,"sid":"Chitika Default"};
    var placement_id = window.CHITIKA.units.length;
    window.CHITIKA.units.push(unit);
    document.write('<div id="chitikaAdBlock-' + placement_id + '"></div>');
}());
</script>


<script type="text/javascript" src="//cdn.chitika.net/getads.js" async></script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nginx Reverse Proxy for Elasticsearch and Kibana 5 on AWS]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/09/16/nginx-reverse-proxy-for-elasticsearch-and-kibana-5-on-aws/"/>
    <updated>2017-09-16T17:24:32-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/09/16/nginx-reverse-proxy-for-elasticsearch-and-kibana-5-on-aws</id>
    <content type="html"><![CDATA[<p>As up untill today, there&rsquo;s currently no VPC Support for Amazon&rsquo;s Elasticsearch Service.</p>

<p>So for scenarios where you would like to allow private network traffic to Elasticsearch is impossible straight out of the box as Amazon&rsquo;s Elasticsearch Services, only sees Public Internet Traffic.</p>

<p>We will setup 2 configs, one for Kibana and one for Elasticsearch, each one having its own FQDN:</p>

<ul>
<li>Kibana: <code>http://kibana.domain.com</code></li>
<li>Elasticsearch: <code>http://elasticsearch.domain.com</code></li>
</ul>


<script id="mNCC" language="javascript">
    medianet_width = "728";
    medianet_height = "90";
    medianet_crid = "218284798";
    medianet_versionId = "3111299"; 
  </script>


<script src="//contextual.media.net/nmedianet.js?cid=8CUD78FSV"></script>


<h2>Workaround:</h2>

<p>There&rsquo;s a couple of workarounds, which includes:</p>

<ul>
<li>Nginx Reverse Proxy</li>
<li>NAT Gateway</li>
<li>Allow IAM Users/Roles</li>
</ul>


<p>Today we will tackle the Nginx Reverse Proxy Route.</p>

<p>The benefit of this, would be to associate an EIP to the Nginx EC2 Instnace, then whitelist your EIP with Elasticsearch, so the only traffic that will be accepted will be the traffic that is coming from the Nginx Instance. We will also apply an additional layer of security, in this case we will use HTTP Basic Authentication, then also authorize network sources on a Security Group level.</p>

<h2>Installing Nginx:</h2>

<p>In this case I am using Ubuntu 16.04, so we will need to install <code>nginx</code> and <code>apache2-utils</code> for creating the Basic HTTP Auth accounts.</p>

<pre><code class="bash">$ apt update &amp;&amp; apt upgrade -y
$ apt install nginx apache2-utils -y
</code></pre>

<h2>Configure Nginx:</h2>

<p>Our main config: <code>/etc/nginx/nginx.conf</code>:</p>

<pre><code class="bash /etc/nginx/nginx.conf">user www-data;
worker_processes auto;
pid /run/nginx.pid;
error_log /var/log/nginx/error.log;

events {
    worker_connections 1024;
}

http {

    # Basic Settings
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    server_names_hash_bucket_size 128;

    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # Logging Settings
        log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

    access_log /var/log/nginx/access.log main;

    # Gzip Settings
    gzip on;
    gzip_disable "msie6";

    # Elasticsearch and Kibana Configs
    include /etc/nginx/conf.d/elasticsearch.conf;
    include /etc/nginx/conf.d/kibana.conf;
}
</code></pre>

<p>Our <code>/etc/nginx/conf.d/elasticsearch.conf</code> configuration:</p>

<pre><code class="bash /etc/nginx/conf.d/elasticsearch.conf">server {

  listen 80;
  server_name elasticsearch.domain.com;

  # error logging
  error_log /var/log/nginx/elasticsearch_error.log;

  # authentication: elasticsearch
  auth_basic "Elasticsearch Auth";
  auth_basic_user_file /etc/nginx/.secrets_elasticsearch;

  location / {

    proxy_http_version 1.1;
    proxy_set_header Host https://search-elasticsearch-name.eu-west-1.es.amazonaws.com;
    proxy_set_header X-Real-IP &lt;ELASTIC-IP&gt;;
    proxy_set_header Connection "Keep-Alive";
    proxy_set_header Proxy-Connection "Keep-Alive";
    proxy_set_header Authorization "";

    proxy_pass https://search-elasticsearch-name.eu-west-1.es.amazonaws.com/;
    proxy_redirect https://search-elasticsearch-name.eu-west-1.es.amazonaws.com/ http://&lt;ELASTIC-IP&gt;/;

  }

  # ELB Health Checks
  location /status {
    root /usr/share/nginx/html/;
  }

}
</code></pre>

<p>Our <code>/etc/nginx/conf.d/kibana.conf</code> configuration:</p>

<pre><code class="bash /etc/nginx/conf.d/kibana.conf">server {

  listen 80;
  server_name kibana.domain.com;

  # error logging
  error_log /var/log/nginx/kibana_error.log;

  # authentication: kibana
  auth_basic "Kibana Auth";
  auth_basic_user_file /etc/nginx/.secrets_kibana;

  location / {

    proxy_http_version 1.1;
    proxy_set_header Host https://search.elasticsearch-name.eu-west-1.es.amazonaws.com;
    proxy_set_header X-Real-IP &lt;ELASTIC-IP&gt;;
    proxy_set_header Connection "Keep-Alive";
    proxy_set_header Proxy-Connection "Keep-Alive";
    proxy_set_header Authorization "";

    proxy_pass https://search.elasticsearch-name.eu-west-1.es.amazonaws.com/_plugin/kibana/;
    proxy_redirect https://search.elasticsearch-name.eu-west-1.es.amazonaws.com/_plugin/kibana/ http://&lt;ELASTIC-IP&gt;/kibana/;

  }

      location ~ (/app/kibana|/app/timelion|/bundles|/es_admin|/plugins|/api|/ui|/elasticsearch) {
         proxy_pass              https://search.elasticsearch-name.eu-west-1.es.amazonaws.com;
         proxy_set_header        Host $host;
         proxy_set_header        X-Real-IP $remote_addr;
         proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;
         proxy_set_header        X-Forwarded-Proto $scheme;
         proxy_set_header        X-Forwarded-Host $http_host;
         proxy_set_header    Authorization  "";
    }
}
</code></pre>

<p>Once you have replaced the elasticsearch endpoint and your EPI values, we can go ahead and create the auth accounts.</p>

<h2>Create User Accounts for HTTP Basic Auth</h2>

<p>Create the 2 accounts for authentication on kibana and elasticsearch:</p>

<pre><code class="bash">$ htpasswd -c /etc/nginx/.secrets_elasticsearch elasticsearch-admin
$ htpasswd -c /etc/nginx/.secrets_kibana kibana-admin
</code></pre>

<h2>Restart Nginx:</h2>

<p>Restart and enable Nginx on boot:</p>

<pre><code class="bash">$ systemctl enable nginx
$ systemctl restart nginx
</code></pre>

<p>Once your Nginx Service is running, you should be able to access Kibana and Elasticsearch using the credentials that you created.</p>

<h2>Resources:</h2>

<ul>
<li><a href="https://www.nginx.com/blog/tcp-load-balancing-udp-load-balancing-nginx-tips-tricks/">https://www.nginx.com/blog/tcp-load-balancing-udp-load-balancing-nginx-tips-tricks/</a></li>
<li><a href="https://www.elastic.co/blog/playing-http-tricks-nginx">https://www.elastic.co/blog/playing-http-tricks-nginx</a></li>
<li><a href="https://sysadmins.co.za/aws-access-kibana-5-behind-elb-via-nginx-reverse-proxy-on-custom-dns/">https://sysadmins.co.za/aws-access-kibana-5-behind-elb-via-nginx-reverse-proxy-on-custom-dns/</a></li>
</ul>


<script type="text/javascript">
  ( function() {
    if (window.CHITIKA === undefined) { window.CHITIKA = { 'units' : [] }; };
    var unit = {"calltype":"async[2]","publisher":"rbekker87","width":728,"height":90,"sid":"Chitika Default"};
    var placement_id = window.CHITIKA.units.length;
    window.CHITIKA.units.push(unit);
    document.write('<div id="chitikaAdBlock-' + placement_id + '"></div>');
}());
</script>


<script type="text/javascript" src="//cdn.chitika.net/getads.js" async></script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Run Kibana on Docker Swarm With Traefik]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/09/09/run-kibana-on-docker-swarm-with-traefik/"/>
    <updated>2017-09-09T18:33:12-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/09/09/run-kibana-on-docker-swarm-with-traefik</id>
    <content type="html"><![CDATA[<p>We will create a Kibana Service on Docker Swarm, that will sit behind a Traefik Reverse Proxy.</p>

<h2>Create the Overlay Network:</h2>

<pre><code class="bash">$ docker network create --driver overlay appnet
</code></pre>

<h2>Create the Traefik Service:</h2>

<pre><code class="bash">docker service create \
--name traefik \
--constraint 'node.role==manager' \
--publish 80:80 \
--publish 443:443 \
--publish 8080:8080 \
--mount type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock \
--network appnet \
traefik:camembert \
--docker --docker.swarmmode  \
--docker.domain=apps.domain.com \
--docker.watch \
--logLevel=DEBUG \
--web
</code></pre>

<h2>Set DNS:</h2>

<p>Set a wildcard <code>*.apps.domain.com</code> to resolve to <code>apps.domain.com</code>, where <code>apps.domain.com</code> resolves to your swarm addresses</p>

<h2>Create Kibana:</h2>

<p>Create a Kibana Service and set the <code>ELASTICSEARCH_URL</code> to your External Elasticsearch Endpoint, take note that it uses port <code>9200</code> by default.</p>

<pre><code class="bash">$ docker service create \
--name kibana \
--label 'traefik.port=5601' \
--network appnet \
--env KIBANA_ELASTICSEARCH_URL=elasticsearch.domain.com \
bitnami/kibana
</code></pre>

<h2>Access Kibana:</h2>

<p>Your Kibana endpoint will be available at: <code>http://kibana.apps.domain.com</code></p>

<h2>Resources:</h2>

<ul>
<li><a href="https://github.com/bitnami/bitnami-docker-kibana">https://github.com/bitnami/bitnami-docker-kibana</a></li>
<li><a href="https://docs.traefik.io/">https://docs.traefik.io/</a></li>
</ul>


<center>
    <script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Buy Me a Coffee', '#46b798', 'A6423ZIQ');kofiwidget2.draw();</script>
</center>

]]></content>
  </entry>
  
</feed>
