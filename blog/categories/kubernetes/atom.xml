<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Kubernetes | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/kubernetes/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2020-02-04T22:14:56+02:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[How to Deploy a Webapp on a AWS EKS Kubernetes Cluster]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/11/17/how-to-deploy-a-webapp-on-a-aws-eks-kubernetes-cluster/"/>
    <updated>2019-11-17T00:21:19+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/11/17/how-to-deploy-a-webapp-on-a-aws-eks-kubernetes-cluster</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/68999897-f59a3d00-08cf-11ea-83c7-8624e6048106.png" alt="kubernetes-eks-deploy-webapp" /></p>

<p><a href="https://saythanks.io/to/ruanbekker"><img src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" alt="Say Thanks!" /></a> <a href="https://linux-hackers-slack.herokuapp.com/"><img src="https://linux-hackers-slack.herokuapp.com/badge.svg" alt="Slack Status" /></a> <a href="https://linux-hackers.slack.com/"><img src="https://img.shields.io/badge/chat-on_slack-orange.svg" alt="Chat on Slack" /></a> <a href="https://github.com/ruanbekker"><img src="https://img.shields.io/github/followers/ruanbekker.svg?label=Follow&amp;style=social" alt="GitHub followers" /></a></p>

<p><a href="https://twitter.com/ruanbekker?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @ruanbekker</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>

<p>In our previous post, <a href="https://blog.ruanbekker.com/blog/2019/11/16/how-to-setup-a-aws-eks-kubernetes-cluster/">Part 1 - Setup a EKS Cluster</a> we went through the steps on how to Setup a EKS Cluster.</p>

<h2>What are we doing today</h2>

<p>In this post, we will deploy a sample web application to EKS and access our application using a ELB that EKS provides us.</p>

<h2>Deployment Manifests</h2>

<p>We will have two manifests that we will deploy to Kubernetes, a deployment manifest that will hold the information about our application and a service manifest that will hold the information about the service load balancer.</p>

<p>The deployment manifest, you will notice that we are specifying that we want 3 containers, we are using labels so that our service and deployment can find each other and we are using a basic http web application that will listen on port 8000 inside the container:</p>

<pre><code class="bash">$ cat deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-hostname-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: my-app-container
          image: ruanbekker/hostname
          ports:
          - name: http
            containerPort: 8000
</code></pre>

<p>The service manifest, you will notice that we are specifying <code>type: LoadBalancer</code> in our service manifest, this will tell EKS to provision a ELB for your application so that we can access our application from the internet.</p>

<p>You will see that the selector is specifying <code>my-app</code> which we also provided in our deployment.yml so that our service know where to find our backend application. We are also stating that the service is listening on port 80, and will forward its traffic to our deployment on port 8000:</p>

<pre><code class="bash">$ cat service.yml
apiVersion: v1
kind: Service
metadata:
  name: my-hostname-app-service
  labels:
    app: my-app
spec:
  ports:
  - port: 80
    targetPort: 8000
  selector:
    app: my-app
  type: LoadBalancer
</code></pre>

<h2>Deployment Time</h2>

<p>Deploy our application:</p>

<pre><code class="bash">$ kubectl apply -f deployment.yml
deployment.apps/my-hostname-app created
</code></pre>

<p>Deploy our service:</p>

<pre><code class="bash">$ kubectl apply -f service.yml
service/my-hostname-app-service created
</code></pre>

<p>Now when we look at our deployment, we should see that 3 replicas of our application is running:</p>

<pre><code class="bash">$ kubectl get deployments
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
my-hostname-app   3/3     3            3           4m38s
</code></pre>

<p>To see the pods of that deployment, look at the pods:</p>

<pre><code class="bash">$ kubectl get pods
NAME                               READY   STATUS    RESTARTS   AGE
my-hostname-app-5dcd48dfc5-2j8zm   1/1     Running   0          24s
my-hostname-app-5dcd48dfc5-58vkc   1/1     Running   0          24s
my-hostname-app-5dcd48dfc5-cmjwj   1/1     Running   0          24s
</code></pre>

<p>As we have more than one service in our EKS cluster, we can specify the labels that we have applied on our manifests to filter what we want to see (<code>app: my-app</code>):</p>

<pre><code class="bash">$ kubectl get service --selector app=my-app
NAME                      TYPE           CLUSTER-IP       EXTERNAL-IP                                                              PORT(S)        AGE
my-hostname-app-service   LoadBalancer   10.100.114.166   a460661ce089b11ea97cd06dd7513db6-669054126.eu-west-1.elb.amazonaws.com   80:30648/TCP   2m29s
</code></pre>

<p>As we can see EKS provisioned a ELB for us, and we can access the application by making a HTTP request:</p>

<pre><code class="bash">$ curl -i http://a460661ce089b11ea97cd06dd7513db6-669054126.eu-west-1.elb.amazonaws.com
HTTP/1.1 200 OK
Date: Sat, 16 Nov 2019 18:05:27 GMT
Content-Length: 43
Content-Type: text/plain; charset=utf-8

Hostname: my-hostname-app-5dcd48dfc5-2j8zm
</code></pre>

<h2>Scaling our Deployment</h2>

<p>Let&rsquo;s scale our deployment to 5 replicas:</p>

<pre><code class="bash">$ kubectl scale deployment/my-hostname-app --replicas 5
deployment.extensions/my-hostname-app scaled
</code></pre>

<p>After all the pods has been deployed, you should be able to see the 5 out of 5 pods that we provisioned, should be running:</p>

<pre><code class="bash">$ kubectl get deployments
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
my-hostname-app   5/5     5            5           5m7s
</code></pre>

<p>We can then also see the pods that our deployment is referencing:</p>

<pre><code class="bash">$ kubectl get pods
NAME                               READY   STATUS    RESTARTS   AGE
my-hostname-app-5dcd48dfc5-2j8zm   1/1     Running   0          6m8s
my-hostname-app-5dcd48dfc5-58vkc   1/1     Running   0          6m8s
my-hostname-app-5dcd48dfc5-cmjwj   1/1     Running   0          6m8s
my-hostname-app-5dcd48dfc5-m4xcq   1/1     Running   0          67s
my-hostname-app-5dcd48dfc5-zf6xl   1/1     Running   0          68s
</code></pre>

<h2>Further Reading on Kubernetes</h2>

<p>This is one amazing resource that covers a lot of kubernetes topics and will help you throughout your EKS journey:</p>

<ul>
<li><a href="https://eksworkshop.com/introduction/">EKSWorkshop</a></li>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/worker.html">Worker Nodes Documentation</a></li>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-guestbook.html">Guestbook Kubernetes Sample Application</a></li>
</ul>


<h2>Thank You</h2>

<p>Let me know what you think. If you liked my content, feel free to checkout my content on <strong><a href="https://ruan.dev/">ruan.dev</a></strong> or follow me on twitter at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong></p>

<center><script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Buy Me a Coffee', '#46b798', 'A6423ZIQ');kofiwidget2.draw();</script></center>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Setup a AWS EKS Kubernetes Cluster]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/11/16/how-to-setup-a-aws-eks-kubernetes-cluster/"/>
    <updated>2019-11-16T22:31:36+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/11/16/how-to-setup-a-aws-eks-kubernetes-cluster</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/68999066-b8c84900-08c3-11ea-9669-5c859590296c.png" alt="kubernetes-eks-aws-cluster" /></p>

<p><a href="https://saythanks.io/to/ruanbekker"><img src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" alt="Say Thanks!" /></a> <a href="https://linux-hackers-slack.herokuapp.com/"><img src="https://linux-hackers-slack.herokuapp.com/badge.svg" alt="Slack Status" /></a> <a href="https://linux-hackers.slack.com/"><img src="https://img.shields.io/badge/chat-on_slack-orange.svg" alt="Chat on Slack" /></a> <a href="https://github.com/ruanbekker"><img src="https://img.shields.io/github/followers/ruanbekker.svg?label=Follow&amp;style=social" alt="GitHub followers" /></a></p>

<p><a href="https://twitter.com/ruanbekker?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @ruanbekker</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>

<p>This will be a tutorial split up in two posts, where I will show you how to provision a EKS Cluster (Elastic Kubernetes Service) on AWS and in the <a href="https://blog.ruanbekker.com/blog/2019/11/17/how-to-deploy-a-webapp-on-a-aws-eks-kubernetes-cluster/">next post</a>, how to deploy a web application to your cluster (<a href="https://blog.ruanbekker.com/blog/2019/11/17/how-to-deploy-a-webapp-on-a-aws-eks-kubernetes-cluster/">Part2 - Deploy a Web App to EKS</a>.)</p>

<h2>And then came EKS</h2>

<p>As some of you may know, I&rsquo;m a massive AWS fan boy, and since AWS released their managed Kubernetes service, I was quite excited to test it out. A couple of months passed and I got the opportunity to test out on-the-job as we moved to Kubernetes.</p>

<p>A couple of moths has passed, and serving multiple production workloads on EKS, and I am really impressed with the service.</p>

<p>Amazon provides a vanilla Kubernetes version, they manage the master nodes and they have a extra component called the cloud controller that runs on the master nodes, which is the aws native component that talks to other aws services (as far as I can recall)</p>

<h2>What are we doing today</h2>

<p>We will cover this in this post:</p>

<table>
<thead>
<tr>
<th> <strong>Topic</strong>                                                    </th>
</tr>
</thead>
<tbody>
<tr>
<td> Deploy a EKS Cluster                                         </td>
</tr>
<tr>
<td> View the resources to see what was provisioned on AWS        </td>
</tr>
<tr>
<td> Interact with Kubernetes using kubectl                       </td>
</tr>
<tr>
<td> Terminate a Node and verify that the ASG replaces the node   </td>
</tr>
<tr>
<td> Scale down your worker nodes                                 </td>
</tr>
<tr>
<td> Run a pod on your cluster                                    </td>
</tr>
</tbody>
</table>


<p>In the <a href="https://blog.ruanbekker.com/blog/2019/11/17/how-to-deploy-a-webapp-on-a-aws-eks-kubernetes-cluster/">next post</a> we will deploy a web service to our EKS cluster.</p>

<h2>Install Pre-Requirements</h2>

<p>We require <code>awscli</code>, <code>eksctl</code> and <code>kubectl</code> before we continue. I will be installing this on MacOS, but you can have a look at the following links if you are using a different operating system:</p>

<ul>
<li><a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html">Install awscli</a></li>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html">Install eksctl</a></li>
<li><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">Install kubectl</a></li>
</ul>


<p>Install awscli:</p>

<pre><code class="bash">$ pip install awscli
</code></pre>

<p>Install kubectl:</p>

<pre><code>$ brew update
$ brew install kubernetes-cli
</code></pre>

<p>Install eksctl:</p>

<pre><code>$ brew tap weaveworks/tap
$ brew install weaveworks/tap/eksctl
</code></pre>

<h2>Deploy EKS</h2>

<p>Create a SSH key if you would like to SSH to your worker nodes:</p>

<pre><code>$ ssh-keygen -b 2048 -f ~/.ssh/eks -t rsa -q -N ""
</code></pre>

<p>Import your public key to EC2:</p>

<pre><code>$ aws --profile dev --region eu-west-1 ec2 import-key-pair --key-name "eks" --public-key-material file://~/.ssh/eks.pub
</code></pre>

<p>Provision your cluster using eksctl. This will deploy two cloudformation stacks, one for the kubernetes cluster, and one for the node group.</p>

<p>I am creating a kubernetes cluster with 3 nodes of instance type (t2.small) and using version 1.14:</p>

<pre><code class="bash">$ eksctl --profile dev --region eu-west-1 create cluster --name my-eks-cluster --version 1.14 --nodes 3 --node-type t2.small --ssh-public-key eks

[ℹ]  eksctl version 0.9.0
[ℹ]  using region eu-west-1
[ℹ]  setting availability zones to [eu-west-1a eu-west-1b eu-west-1c]
[ℹ]  subnets for eu-west-1a - public:192.168.0.0/19 private:192.168.96.0/19
[ℹ]  subnets for eu-west-1b - public:192.168.32.0/19 private:192.168.128.0/19
[ℹ]  subnets for eu-west-1c - public:192.168.64.0/19 private:192.168.160.0/19
[ℹ]  nodegroup "ng-f27f560e" will use "ami-059c6874350e63ca9" [AmazonLinux2/1.14]
[ℹ]  using Kubernetes version 1.14
[ℹ]  creating EKS cluster "my-eks-cluster" in "eu-west-1" region
[ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup
[ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=eu-west-1 --cluster=my-eks-cluster'
[ℹ]  CloudWatch logging will not be enabled for cluster "my-eks-cluster" in "eu-west-1"
[ℹ]  you can enable it with 'eksctl utils update-cluster-logging --region=eu-west-1 --cluster=my-eks-cluster'
[ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "my-eks-cluster" in "eu-west-1"
[ℹ]  2 sequential tasks: { create cluster control plane "my-eks-cluster", create nodegroup "ng-f27f560e" }
[ℹ]  building cluster stack "eksctl-my-eks-cluster-cluster"
[ℹ]  deploying stack "eksctl-my-eks-cluster-cluster"
[ℹ]  building nodegroup stack "eksctl-my-eks-cluster-nodegroup-ng-f27f560e"
[ℹ]  --nodes-min=3 was set automatically for nodegroup ng-f27f560e
[ℹ]  --nodes-max=3 was set automatically for nodegroup ng-f27f560e
[ℹ]  deploying stack "eksctl-my-eks-cluster-nodegroup-ng-f27f560e"
[+]  all EKS cluster resources for "my-eks-cluster" have been created
[+]  saved kubeconfig as "/Users/ruan/.kube/config"
[ℹ]  adding identity "arn:aws:iam::000000000000:role/eksctl-my-eks-cluster-nodegroup-n-NodeInstanceRole-SNVIW5C3J3SM" to auth ConfigMap
[ℹ]  nodegroup "ng-f27f560e" has 0 node(s)
[ℹ]  waiting for at least 3 node(s) to become ready in "ng-f27f560e"
[ℹ]  nodegroup "ng-f27f560e" has 3 node(s)
[ℹ]  node "ip-192-168-42-186.eu-west-1.compute.internal" is ready
[ℹ]  node "ip-192-168-75-87.eu-west-1.compute.internal" is ready
[ℹ]  node "ip-192-168-8-167.eu-west-1.compute.internal" is ready
[ℹ]  kubectl command should work with "/Users/ruan/.kube/config", try 'kubectl get nodes'
[+]  EKS cluster "my-eks-cluster" in "eu-west-1" region is ready
</code></pre>

<p>Now that our EKS cluster has been provisioned, let&rsquo;s browse through our AWS Management Console to understand what was provisioned.</p>

<h2>View the Provisioned Resources</h2>

<p>If we have a look at the Cloudformation stacks, we can see the two stacks that I mentioned previously:</p>

<p><img width="1057" alt="image" src="https://user-images.githubusercontent.com/567298/68996480-58c1aa80-08a3-11ea-95c1-0fcf0bc1863b.png"></p>

<p>Navigating to our EC2 Instances dashboard, we can see the three worker nodes that we provisioned. Remember that AWS manages the master nodes and we cant see them.</p>

<p><img width="1106" alt="image" src="https://user-images.githubusercontent.com/567298/68996520-ea311c80-08a3-11ea-8ea3-e9e481e4ba6f.png"></p>

<p>We have a ASG (Auto Scaling Group) associated with our worker nodes, nodegroup. We can make use of autoscaling and also have desired state, so we will test this out later where we will delete a worker node and verify if it gets replaced:</p>

<p><img width="1113" alt="image" src="https://user-images.githubusercontent.com/567298/68996551-2e242180-08a4-11ea-8df6-7b962b9aa03a.png"></p>

<h2>Navigate using Kubectl:</h2>

<p>Eksctl already applied the kubeconfig to <code>~/.kube/config</code>, so we can start using kubectl. Let&rsquo;s start by viewing the nodes:</p>

<pre><code>$ kubectl get nodes
NAME                                           STATUS   ROLES    AGE     VERSION
ip-192-168-42-186.eu-west-1.compute.internal   Ready    &lt;none&gt;   8m50s   v1.14.7-eks-1861c5
ip-192-168-75-87.eu-west-1.compute.internal    Ready    &lt;none&gt;   8m55s   v1.14.7-eks-1861c5
ip-192-168-8-167.eu-west-1.compute.internal    Ready    &lt;none&gt;   8m54s   v1.14.7-eks-1861c5
</code></pre>

<p>Viewing our pods from our <code>kube-system</code> namespace (we dont have any pods in our default namespace at the moment):</p>

<pre><code>$ kubectl get pods --namespace kube-system
NAME                       READY   STATUS    RESTARTS   AGE
aws-node-btfbk             1/1     Running   0          11m
aws-node-c6ktk             1/1     Running   0          11m
aws-node-wf8mc             1/1     Running   0          11m
coredns-759d6fc95f-ljxzf   1/1     Running   0          17m
coredns-759d6fc95f-s6lg6   1/1     Running   0          17m
kube-proxy-db46b           1/1     Running   0          11m
kube-proxy-ft4mc           1/1     Running   0          11m
kube-proxy-s5q2w           1/1     Running   0          11m
</code></pre>

<p>And our services from all our namespaces:</p>

<pre><code>$ kubectl get services --all-namespaces
NAMESPACE     NAME         TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE
default       kubernetes   ClusterIP   10.100.0.1    &lt;none&gt;        443/TCP         19m
kube-system   kube-dns     ClusterIP   10.100.0.10   &lt;none&gt;        53/UDP,53/TCP   19m
</code></pre>

<h2>Testing the ASG</h2>

<p>Let&rsquo;s view our current nodes in our cluster, then select the first node, delete it and verify if the ASG replaces that node.</p>

<p>First, view the nodes and select one node&rsquo;s address:</p>

<pre><code>$ kubectl get nodes
NAME                                           STATUS   ROLES    AGE   VERSION
ip-192-168-42-186.eu-west-1.compute.internal   Ready    &lt;none&gt;   37m   v1.14.7-eks-1861c5
ip-192-168-75-87.eu-west-1.compute.internal    Ready    &lt;none&gt;   37m   v1.14.7-eks-1861c5
ip-192-168-8-167.eu-west-1.compute.internal    Ready    &lt;none&gt;   37m   v1.14.7-eks-1861c5
</code></pre>

<p>Use the awscli to lookup the EC2 instance id, as we will need this id to delete the node:</p>

<pre><code>$ aws --profile dev ec2 describe-instances --query 'Reservations[*].Instances[?PrivateDnsName==`ip-192-168-42-186.eu-west-1.compute.internal`].[InstanceId][]' --output text
i-0d016de17a46d5178
</code></pre>

<p>Now that we have the EC2 instance id, delete the node:</p>

<pre><code>$ aws --profile dev ec2 terminate-instances --instance-id i-0d016de17a46d51782
{
    "TerminatingInstances": [
        {
            "CurrentState": {
                "Code": 32,
                "Name": "shutting-down"
            },
            "InstanceId": "i-0d016de17a46d5178",
            "PreviousState": {
                "Code": 16,
                "Name": "running"
            }
        }
    ]
}
</code></pre>

<p>Now that we have deleted the EC2 instance, view the nodes and you will see the node has been terminated:</p>

<pre><code>$ kubectl get nodes
NAME                                          STATUS   ROLES    AGE   VERSION
ip-192-168-75-87.eu-west-1.compute.internal   Ready    &lt;none&gt;   41m   v1.14.7-eks-1861c5
ip-192-168-8-167.eu-west-1.compute.internal   Ready    &lt;none&gt;   41m   v1.14.7-eks-1861c5
</code></pre>

<p>Allow about a minute so that the ASG can replace the node, and when you list again you will see that the ASG replaced the node :</p>

<pre><code>$ kubectl get nodes
NAME                                          STATUS   ROLES    AGE   VERSION
ip-192-168-42-61.eu-west-1.compute.internal   Ready    &lt;none&gt;   50s   v1.14.7-eks-1861c5
ip-192-168-75-87.eu-west-1.compute.internal   Ready    &lt;none&gt;   42m   v1.14.7-eks-1861c5
ip-192-168-8-167.eu-west-1.compute.internal   Ready    &lt;none&gt;   42m   v1.14.7-eks-1861c5
</code></pre>

<h2>Run a Pod</h2>

<p>Run a busybox pod on your EKS cluster:</p>

<pre><code>$ kubectl run --rm -it --generator run-pod/v1 my-busybox-pod --image busybox -- /bin/sh
</code></pre>

<p>You will be dropped into a shell:</p>

<pre><code>/ # busybox | head -1
BusyBox v1.31.1 (2019-10-28 18:40:01 UTC) multi-call binary.
</code></pre>

<p>And exit the shell:</p>

<pre><code>/ # exit
Session ended, resume using 'kubectl attach my-busybox-pod -c my-busybox-pod -i -t' command when the pod is running
pod "my-busybox-pod" deleted
</code></pre>

<h2>Scaling Nodes</h2>

<p>While I will not be covering auto-scaling in this post, we can manually scale the worker node count. Let&rsquo;s scale it down to 1 node.</p>

<p>First we need to get the EKS cluster name:</p>

<pre><code>$ eksctl --profile dev --region eu-west-1 get clusters
NAME        REGION
my-eks-cluster  eu-west-1
</code></pre>

<p>Then we need the node group id:</p>

<pre><code>$ eksctl --profile dev --region eu-west-1 get nodegroup --cluster my-eks-cluster
CLUSTER     NODEGROUP   CREATED         MIN SIZE    MAX SIZE    DESIRED CAPACITY    INSTANCE TYPE   IMAGE ID
my-eks-cluster  ng-f27f560e 2019-11-16T16:55:41Z    3       3       3           t2.small    ami-059c6874350e63ca9
</code></pre>

<p>Now that we have the node group id, we can scale the node count:</p>

<pre><code>$ eksctl --profile dev --region eu-west-1 scale nodegroup --cluster my-eks-cluster --nodes 1 ng-f27f560e

[ℹ]  scaling nodegroup stack "eksctl-my-eks-cluster-nodegroup-ng-f27f560e" in cluster eksctl-my-eks-cluster-cluster
[ℹ]  scaling nodegroup, desired capacity from 3 to 1, min size from 3 to 1
</code></pre>

<p>Now when we use kubectl to view the nodes, we will see we only have 1 worker node:</p>

<pre><code>$ kubectl get nodes
NAME                                          STATUS   ROLES    AGE   VERSION
ip-192-168-8-167.eu-west-1.compute.internal   Ready    &lt;none&gt;   73m   v1.14.7-eks-1861c5
</code></pre>

<h2>Clean Up</h2>

<p>If you want to follow along deploying a web application to your EKS cluster before we terminate the cluster, have a look at <a href="https://blog.ruanbekker.com/blog/2019/11/17/how-to-deploy-a-webapp-on-a-aws-eks-kubernetes-cluster/">Part 2 - EKS Tutorial</a> before continuing.</p>

<p>Once you are ready to terminate your EKS cluster, you can go ahead and terminate the cluster:</p>

<pre><code>$ eksctl --profile dev --region eu-west-1 delete cluster --name my-eks-cluster

[ℹ]  eksctl version 0.9.0
[ℹ]  using region eu-west-1
[ℹ]  deleting EKS cluster "my-eks-cluster"
[+]  kubeconfig has been updated
[ℹ]  cleaning up LoadBalancer services
[ℹ]  2 sequential tasks: { delete nodegroup "ng-f27f560e", delete cluster control plane "my-eks-cluster" [async] }
[ℹ]  will delete stack "eksctl-my-eks-cluster-nodegroup-ng-f27f560e"
[ℹ]  waiting for stack "eksctl-my-eks-cluster-nodegroup-ng-f27f560e" to get deleted
[ℹ]  will delete stack "eksctl-my-eks-cluster-cluster"
[+]  all cluster resources were deleted
</code></pre>

<h2>Further Reading on Kubernetes</h2>

<p>This is one amazing resource that covers a lot of kubernetes topics and will help you throughout your EKS journey:
- <a href="https://eksworkshop.com/introduction/">EKSWorkshop</a></p>

<h2>Thank You</h2>

<p>Let me know what you think. If you liked my content, feel free to checkout my content on <strong><a href="https://ruan.dev/">ruan.dev</a></strong> or follow me on twitter at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong></p>

<center><script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Buy Me a Coffee', '#46b798', 'A6423ZIQ');kofiwidget2.draw();</script></center>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup Traefik as an Ingress Controller on Kubernetes]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/06/10/setup-traefik-as-an-ingress-controller-on-kubernetes/"/>
    <updated>2019-06-10T22:21:36+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/06/10/setup-traefik-as-an-ingress-controller-on-kubernetes</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/59225379-db498e00-8bd0-11e9-9f20-62aecf915431.png" alt="image" /></p>

<p>If you have not provisioned a Kubernetes Cluster, you can <a href="https://blog.ruanbekker.com/blog/2019/06/10/testing-out-scaleways-kapsule-their-kubernetes-as-a-service-offering/">see this tutorial</a> on how to provision a Kubernetes Cluster on Scaleway</p>

<h2>What will we be doing</h2>

<p>In this tutorial we will setup Traefik as an Ingress Controller on Kubernetes and deploy a logos web app to our Kubernetes Cluster, using frontend rules to map subdomains to specific services.</p>

<p>We will have 3 subdomains, being mapped to containers from the below docker images:</p>

<pre><code>FQDN                     Image Name
- python.domain.com   -&gt; ruanbekker/logos:python
- openfaas.domain.com -&gt; ruanbekker/logos:openfaas
- rancher.domain.com  -&gt; ruanbekker/logos:rancher
</code></pre>

<h2>Get the sources</h2>

<p>If you would like to get the source code for this demonstration you can checkout this repository: <a href="https://github.com/ruanbekker/traefik-kubernetes-scaleway-demo">https://github.com/ruanbekker/traefik-kubernetes-scaleway-demo</a></p>

<pre><code>$ git clone https://github.com/ruanbekker/traefik-kubernetes-scaleway-demo
$ cd traefik-kubernetes-scaleway-demo
</code></pre>

<h2>Provision Traefik as an Ingress Controller</h2>

<p>Apply role based access control to authorize Traefik to use the Kubernetes API:</p>

<pre><code>$ kubectl apply -f traefik/01-traefik-rbac.yaml
clusterrole.rbac.authorization.k8s.io/traefik-ingress-controller created
clusterrolebinding.rbac.authorization.k8s.io/traefik-ingress-controller created
</code></pre>

<p>Consulting <a href="https://docs.traefik.io/user-guide/kubernetes/#deploy-traefik-using-a-deployment-or-daemonset">Traefik&rsquo;s</a> documentation, when deploying Traefik, it&rsquo;s possible to use a deployment or a demonset, not both. <a href="https://docs.traefik.io/user-guide/kubernetes/#deploy-traefik-using-a-deployment-or-daemonset">More details on why</a></p>

<p>I will go ahead and apply the Daemon Set:</p>

<pre><code>$ kubectl apply -f traefik/03-traefik-ds.yaml
serviceaccount/traefik-ingress-controller created
daemonset.extensions/traefik-ingress-controller created
service/traefik-ingress-service created
</code></pre>

<p>The Traefik UI Service will be associated with a FQDN, remember to set the FQDN for the endpoint, as example:</p>

<pre><code>$ cat traefik/04-traefik-ui.yaml
...
spec:
  rules:
  - host: traefik-ui.x-x-x-x-x.nodes.k8s.fr-par.scw.cloud
    http:
      paths:
      - path: /
...
</code></pre>

<p>Create the Traefik UI Service:</p>

<pre><code>$ kubectl apply -f traefik/04-traefik-ui.yaml
service/traefik-web-ui created
</code></pre>

<p>Traefik UI Ingress:</p>

<pre><code>$ kubectl apply -f traefik/05-traefik-ui-ingress.yaml
ingress.extensions/traefik-web-ui created
</code></pre>

<p>View the services:</p>

<pre><code>$ kubectl get services --namespace=kube-system
NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
coredns                   ClusterIP   x.x.x.x         &lt;none&gt;        53/UDP,53/TCP,9153/TCP   11h
heapster                  ClusterIP   x.x.x.x         &lt;none&gt;        80/TCP                   11h
kubernetes-dashboard      ClusterIP   x.x.x.x         &lt;none&gt;        443/TCP                  11h
metrics-server            ClusterIP   x.x.x.x         &lt;none&gt;        443/TCP                  11h
monitoring-influxdb       ClusterIP   x.x.x.x         &lt;none&gt;        8086/TCP                 11h
traefik-ingress-service   ClusterIP   x.x.x.x         &lt;none&gt;        80/TCP,8080/TCP          24m
traefik-web-ui            ClusterIP   x.x.x.x         &lt;none&gt;        80/TCP                   24m
</code></pre>

<h2>Deploy the Logo App to the Cluster</h2>

<p>We will deploy the logo app to our cluster:</p>

<pre><code>$ kubectl apply -f logos-app/logos-services.yaml
service/openfaas created
service/rancher created
service/python created
</code></pre>

<p>Create the deployment:</p>

<pre><code>$ kubectl apply -f logos-app/logos-deployments.yaml
deployment.extensions/openfaas created
deployment.extensions/rancher created
deployment.extensions/python created
</code></pre>

<p>Before creating the ingress for the logo&rsquo;s applications, we need to set the fqdn endpoints that we want to route traffic to as below as an example:</p>

<pre><code>$ cat logos-app/logos-ingress.yaml
...
spec:
  rules:
  - host: openfaas.x-x-x-x-x.nodes.k8s.fr-par.scw.cloud
    http:
      paths:
      - path: /
        backend:
          serviceName: openfaas
          servicePort: http
...
</code></pre>

<p>Create the ingress:</p>

<pre><code>$ kubectl apply -f logos-app/logos-ingress.yaml
ingress.extensions/logo created
</code></pre>

<p>After some time, have a look at the pods to get the status:</p>

<pre><code>$ kubectl get pods
NAME                                     READY   STATUS    RESTARTS   AGE
openfaas-cffdddc4-lvn5w                  1/1     Running   0          4m6s
openfaas-cffdddc4-wbcl6                  1/1     Running   0          4m6s
python-65ccf9c74b-8kmgp                  1/1     Running   0          4m6s
python-65ccf9c74b-dgnqb                  1/1     Running   0          4m6s
rancher-597b6b8554-mgcjr                 1/1     Running   0          4m6s
rancher-597b6b8554-mpk62                 1/1     Running   0          4m6s
</code></pre>

<h2>Navigating with Kubectl</h2>

<p>Show nodes:</p>

<pre><code>$ kubectl get nodes
NAME                                             STATUS   ROLES    AGE   VERSION
scw-k8s-mystifying-torvald-jovial-mclar-25a942   Ready    node     20h   v1.14.1
scw-k8s-mystifying-torvald-jovial-mclar-eaf1a2   Ready    node     20h   v1.14.1
scw-k8s-mystifying-torvalds-default-7f263aabab   Ready    master   20h   v1.14.1
</code></pre>

<p>Show services:</p>

<pre><code>$ kubectl get services
NAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)           AGE
kubernetes              ClusterIP   10.32.0.1      &lt;none&gt;        443/TCP           20h
openfaas                ClusterIP   10.41.47.185   &lt;none&gt;        80/TCP            9h
python                  ClusterIP   10.42.56.141   &lt;none&gt;        80/TCP            9h
rancher                 ClusterIP   10.32.41.218   &lt;none&gt;        80/TCP            9h
</code></pre>

<p>Show Pods:</p>

<p><em>To see pods from the kube-system namespace add -n kube-system</em></p>

<pre><code>$ kubectl get pods
NAME                                     READY   STATUS    RESTARTS   AGE
openfaas-cffdddc4-lvn5w                  1/1     Running   0          9h
openfaas-cffdddc4-wbcl6                  1/1     Running   0          9h
python-65ccf9c74b-8kmgp                  1/1     Running   0          9h
python-65ccf9c74b-dgnqb                  1/1     Running   0          9h
rancher-597b6b8554-mgcjr                 1/1     Running   0          9h
rancher-597b6b8554-mpk62                 1/1     Running   0          9h
</code></pre>

<p>Show deployments:</p>

<pre><code>$ kubectl get deployments -o wide
NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS  IMAGES                      SELECTOR
openfaas                2/2     2            2           9h    logo        ruanbekker/logos:openfaas   app=logo,task=openfaas
python                  2/2     2            2           9h    logo        ruanbekker/logos:python     app=logo,task=python
rancher                 2/2     2            2           9h    logo        ruanbekker/logos:rancher    app=logo,task=rancher
</code></pre>

<p>Show ingress:</p>

<pre><code>$ kubectl get ingress -o wide
NAME      HOSTS                                                          ADDRESS   PORTS   AGE
logo      openfaas.domain.com,rancher.domain.com,python.domain.com       80      9h
</code></pre>

<p>Show system ingress:</p>

<pre><code>$ kubectl get ingress -o wide -n kube-system
NAME             HOSTS                     ADDRESS   PORTS   AGE
traefik-web-ui   traefik-ui.domain.com               80      9h
</code></pre>

<h2>Access your Applications</h2>

<p>Access the Traefik-UI, and filter for one of the applications. Let&rsquo;s take OpenFaaS for an example:</p>

<p><img src="https://user-images.githubusercontent.com/567298/59177432-63418080-8b5c-11e9-8e54-20600508e510.png" alt="image" /></p>

<p>Access the OpenFaaS Page via the URL:</p>

<p><img src="https://user-images.githubusercontent.com/567298/59177206-a4856080-8b5b-11e9-8954-238590f18e5c.png" alt="image" /></p>

<h2>Resources</h2>

<ul>
<li><a href="https://docs.traefik.io/user-guide/kubernetes/">https://docs.traefik.io/user-guide/kubernetes/</a></li>
</ul>


<center>
        <script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Buy Me a Coffee', '#46b798', 'A6423ZIQ');kofiwidget2.draw();</script>
</center>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Testing Out Scaleways Kapsule Their Kubernetes as a Service Offering]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/06/10/testing-out-scaleways-kapsule-their-kubernetes-as-a-service-offering/"/>
    <updated>2019-06-10T18:28:45+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/06/10/testing-out-scaleways-kapsule-their-kubernetes-as-a-service-offering</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/59177864-a5b78d00-8b5d-11e9-931c-5b5dd4e81805.png" alt="" /></p>

<p>At this time of writing (2019.06.10) Scaleway&rsquo;s Kubernetes as a Service, named Kapsule is in Private Beta and got access and pretty stoked on how easy it is to provision a Kubernetes cluster.</p>

<h2>What are we doing today?</h2>

<p>In this tutorial I will show you how easy it is to provision a 3 node Kubernetes Cluster on Scaleway. In the upcoming tutorial, I will create traefik as an ingress controller and deploy applications to our cluster. <a href="https://github.com/ruanbekker/traefik-kubernetes-scaleway-demo">Github Repo Version available for now</a></p>

<h2>Provision a Kapsule Cluster</h2>

<p>Head over to Kapsule and provision a Kubernetes Cluster:</p>

<p><img src="https://user-images.githubusercontent.com/567298/59164353-e71f4c80-8b0b-11e9-8f5c-7c65db1af7b2.png" alt="" /></p>

<p>At this point in time, I will only create a one node &ldquo;cluster&rdquo;, as I want to show how to add pools after the intial creation.</p>

<p>After the cluster has been provisioned, you will get information about your endpoints from the Cluster Infromation Section, which we will need for our ingresses:</p>

<p><img src="https://user-images.githubusercontent.com/567298/59180685-df8c9180-8b65-11e9-82aa-05ee3cd42c78.png" alt="" /></p>

<p>Scroll down to download your config:</p>

<p><img src="https://user-images.githubusercontent.com/567298/59164356-f56d6880-8b0b-11e9-8c00-34dff0ba61fb.png" alt="" /></p>

<p>Move your config in place:</p>

<pre><code>$ mv ~/Downloads/kubeconfig-k8s-mystifying-torvalds.yaml ~/.kube/config
</code></pre>

<h2>Interact with your Cluster</h2>

<p>Test the connection by getting the info of your nodes in your kubernetes cluster:</p>

<pre><code>$ kubectl get node
NAME                                             STATUS    ROLES     AGE       VERSION
scw-k8s-mystifying-torvalds-default-7f263aabab   Ready     &lt;none&gt;    4m        v1.14.1
</code></pre>

<h2>Add more nodes:</h2>

<p>Provision another pool with 2 more nodes in our cluster:</p>

<p><img src="https://user-images.githubusercontent.com/567298/59164387-4e3d0100-8b0c-11e9-8633-b3fc680ac4cd.png" alt="" /></p>

<p>After the pool has been provisioned, verified that they have joined the cluster:</p>

<pre><code>$ kubectl get nodes
NAME                                             STATUS    ROLES     AGE       VERSION
scw-k8s-mystifying-torvald-jovial-mclar-25a942   Ready     &lt;none&gt;    2m        v1.14.1
scw-k8s-mystifying-torvald-jovial-mclar-eaf1a2   Ready     &lt;none&gt;    2m        v1.14.1
scw-k8s-mystifying-torvalds-default-7f263aabab   Ready     &lt;none&gt;    15m       v1.14.1
</code></pre>

<h2>Master / Node Capabilities</h2>

<p>Usually, I will label master nodes as master: <code>node-role.kubernetes.io/master</code> and worker nodes as nodes: <code>node-role.kubernetes.io/node</code> to allow container scheduling only on the worker nodes. But Scaleway manages this on their end and when you list your nodes, the nodes that you see are your &ldquo;worker&rdquo; nodes.</p>

<p>The master nodes are managed by Scaleway.</p>

<h2>Well Done Scaleway</h2>

<p>Just one more reason I really love Kapsule. Simplicity at its best, well done to <a href="https://scaleway.com">Scaleway</a>. I hope most of the people got access to private beta, but if not, im pretty sure they will keep the public informed on public release dates.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a 3 Node Kubernetes Cluster on Ubuntu]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/12/11/setup-a-3-node-kubernetes-cluster-on-ubuntu/"/>
    <updated>2017-12-11T16:31:47+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/12/11/setup-a-3-node-kubernetes-cluster-on-ubuntu</id>
    <content type="html"><![CDATA[<p><img src="https://kumorilabs.com/img/blog/kubernetes-logo.png" alt="" /></p>

<p>Setup a 3 Node Kubernetes Cluster on Ubuntu 16.04</p>

<h2>What is Kubernetes?</h2>

<p>As referenced from their <a href="https://kubernetes.io/">website</a>:</p>

<ul>
<li>&ldquo;Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.&rdquo;</li>
</ul>


<h2>Our Setup:</h2>

<p>For this setup I will be using 3 AWS EC2 Instances with Ubuntu 16.04. One node will act as the master node, and the other 2 nodes, will act as nodes, previously named minions.</p>

<p>We will deploy Kubernetes on all 3 nodes, the master will be the node where we will initialize our cluster, deploy our weave network, applications and we will execute the join command on the worker nodes to join the master to form the cluster.</p>

<h2>Deploy Kubernetes: Master</h2>

<p>The following commands will be used to install Kubernetes, it will be executed with root permissions:</p>

<pre><code class="bash">$ apt update &amp;&amp; sudo apt upgrade -y
$ sudo apt install docker.io apt-transport-https -qy
$ sudo apt update
$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
$ sudo su -c 'echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" &gt; /etc/apt/sources.list.d/app' root
$ apt update
$ sudo apt install kubelet kubeadm kubernetes-cni -y
</code></pre>

<p>Now we would like to set up the master by initializing the cluster:</p>

<pre><code class="bash">$ sudo kubeadm init --kubernetes-version stable-1.8
</code></pre>

<p>The output will provide you with instructions to setup the configurations for the master node, and provide you with a join token for your worker nodes, remember to make not of this token string, as we will need it later for our worker nodes. As your normal user, run the following to setup the config:</p>

<p>Remember to not run this as root, and as the normal user:</p>

<pre><code class="bash">$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre>

<p>Now we need to deploy a network for our pods:</p>

<pre><code class="bash">$ kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
</code></pre>

<p>Lets confirm if all our resources are in its desired state, a small snippet of the output will look like the one below:</p>

<pre><code class="bash">$ kubectl get all -n kube-system

...
NAME                                          READY     STATUS    RESTARTS   AGE
po/etcd-ip-172-31-40-211                      1/1       Running   0          6h
po/kube-apiserver-ip-172-31-40-211            1/1       Running   0          6h
</code></pre>

<p>Once all of the resources are in its desired state, we can head along to our worker nodes, to join them to the cluster</p>

<h2>Deploy Kubernetes: Worker Nodes</h2>

<p>As I have 2 worker nodes, we will need to deploy the following on both of our worker nodes, first to deploy Kubernetes on our nodes with root permission:</p>

<pre><code class="bash">$ apt update &amp;&amp; sudo apt upgrade -y
$ sudo apt install docker.io apt-transport-https -qy
$ sudo apt update
$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
$ sudo su -c 'echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" &gt; /etc/apt/sources.list.d/app' root
$ apt update
$ sudo apt install kubelet kubeadm kubernetes-cni -y
</code></pre>

<p>Once Kubernetes is installed, join the Master node by executing the join command:</p>

<pre><code class="bash">$ sudo kubeadm join --token 49abf7.247d663db97f8504 172.31.40.211:6443 --discovery-token-ca-cert-hash sha256:3a3b301cfbac0995c69a0115989ea384230470d6836ae0e13e71dbdf15ffbb48
</code></pre>

<p>Do the 2 steps on the other node, then head back to the master node.</p>

<h2>Verifying if All Nodes are Checked In</h2>

<p>To verify if all nodes are available and reachable in the cluster:</p>

<pre><code class="bash">$ kubectl get nodes
NAME               STATUS    ROLES     AGE       VERSION
ip-172-31-36-68    Ready     &lt;none&gt;    6h        v1.8.5
ip-172-31-40-211   Ready     master    6h        v1.8.5
ip-172-31-44-80    Ready     &lt;none&gt;    6h        v1.8.5
</code></pre>

<h2>Deploy Services to Kubernetes:</h2>

<p>Kubernetes has Awesome Examples on their <a href="https://github.com/kubernetes/kubernetes/tree/master/examples">Github Repository</a>.</p>

<p>Since the awesomeness of <a href="https://github.com/openfaas">OpenFaas</a>, I will deploy OpenFaas on Kubernetes:</p>

<pre><code class="bash">$ git clone https://github.com/openfaas/faas-netes
$ cd faas-netes
$ kubectl apply -f faas.yml,monitoring.yml,rbac.yml
</code></pre>

<p>Give it about a minute or so, then you should see the pods running in their desired state:</p>

<pre><code class="bash">$ kubectl get pods
NAME                           READY     STATUS    RESTARTS   AGE
alertmanager-77b4b476b-zxtcz   1/1       Running   0          4h
crypto-7d8b7f999c-7l85k        1/1       Running   0          1h
faas-netesd-64fb9b4dfb-hc8gh   1/1       Running   0          4h
gateway-69c9d949f-q57zh        1/1       Running   0          4h
prometheus-7fbfd8bfb8-d4cft    1/1       Running   0          4h
</code></pre>

<p>When we have the desired state, head over to the OpenFaas Gateway WebUI: <code>http://master-public-ip:31112/ui/</code>, select &ldquo;Deploy New Function&rdquo;, you can use your own function or select one from the store.</p>

<p>I am going to use Figlet from the store, once the pod has been deployed, select the function, enter any text into the request body and select invoke. I have used my name and surname, and turns out into:</p>

<pre><code class="bash"> ____                      ____       _    _             
|  _ \ _   _  __ _ _ __   | __ )  ___| | _| | _____ _ __ 
| |_) | | | |/ _` | '_ \  |  _ \ / _ \ |/ / |/ / _ \ '__|
|  _ &lt;| |_| | (_| | | | | | |_) |  __/   &lt;|   &lt;  __/ |   
|_| \_\\__,_|\__,_|_| |_| |____/ \___|_|\_\_|\_\___|_|   
</code></pre>

<h2>Resources:</h2>

<ul>
<li><a href="https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/">Kubernetes Overview</a></li>
<li><a href="https://kubernetes.io/docs/concepts/">Kubernetes Concepts</a></li>
<li><a href="https://blog.alexellis.io/tag/kubernetes/">Kubernetes Blogs</a></li>
<li><a href="https://blog.alexellis.io/tag/openfaas/">OpenFaas Blogs</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
