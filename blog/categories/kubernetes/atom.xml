<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Kubernetes | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/kubernetes/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2023-12-22T08:05:50-05:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[How to Use Cert-Manager DNS Challenge With Cloudflare on Kubernetes With Helm]]></title>
    <link href="https://blog.ruanbekker.com/blog/2023/12/22/how-to-use-cert-manager-dns-challenge-with-cloudflare-on-kubernetes-with-helm/"/>
    <updated>2023-12-22T08:04:02-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2023/12/22/how-to-use-cert-manager-dns-challenge-with-cloudflare-on-kubernetes-with-helm</id>
    <content type="html"><![CDATA[<p>In this tutorial, we will be issuing <a href="https://letsencrypt.org/docs/challenge-types/">Let&rsquo;s Encrypt</a> certificates using <a href="https://cert-manager.io/docs/">cert-manager</a> on <a href="https://kubernetes.io/">Kubernetes</a> and we will be using the <a href="https://letsencrypt.org/docs/challenge-types/#dns-01-challenge">DNS Challenge</a> with <a href="https://www.cloudflare.com/en-gb/">Cloudflare</a>.</p>

<p>The reason I am using DNS Challenge instead of HTTP Challenge is because the Kubernetes environment is local on my laptop and there isn&rsquo;t a direct HTTP route into my environment from the internet and I would like to not expose the endpoints to the public internet.</p>

<h2>Summary of what we will be doing</h2>

<p>We would like to have Let&rsquo;s Encrypt Certificates on our web application that will be issued by Cert-Manager using the DNS Challenge from CloudFlare.</p>

<p>Our ingress controller will be ingress-nginx and our endpoints will be private, as they will resolve to private IP addresses, hence the reason why we are using DNS validation instead of HTTP.</p>

<h2>Pre-Requisites</h2>

<p>To follow along in this tutorial you will need the following</p>

<ul>
<li><p><a href="https://blog.ruanbekker.com/blog/2022/09/20/kind-for-local-kubernetes-clusters/">https://blog.ruanbekker.com/blog/2022/09/20/kind-for-local-kubernetes-clusters/</a></p></li>
<li><p><a href="https://helm.sh/docs/intro/install/">Helm</a></p></li>
<li><p><a href="https://kubernetes.io/docs/tasks/tools/">Kubectl</a></p></li>
<li><p><a href="https://www.cloudflare.com/en-gb/">Cloudflare</a> Account</p></li>
<li><p>Patience (just kidding, I will try my best to make it easy)</p></li>
</ul>


<h2>Install a Kubernetes Cluster</h2>

<p>If you already have a Kubernetes Cluster, you can skip this step.</p>

<p>Define the <code>kind-config.yaml</code></p>

<pre><code class="yaml">kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  image: kindest/node:v1.26.6@sha256:6e2d8b28a5b601defe327b98bd1c2d1930b49e5d8c512e1895099e4504007adb
  extraPortMappings:
  - containerPort: 80
    hostPort: 80
    protocol: TCP
    listenAddress: "0.0.0.0"
  - containerPort: 443
    hostPort: 443
    protocol: TCP
</code></pre>

<p>Then create the cluster with <code>kind</code>:</p>

<pre><code class="bash">kind create cluster --name example --config kind-config.yaml
</code></pre>

<h2>Nginx Ingress Controller</h2>

<p>First we need to install a ingress controller and I am opting in to use ingress-nginx, so first we need to add the helm repository to our local repositories:</p>

<pre><code class="bash">helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
</code></pre>

<p>Then we need to update our repositories:</p>

<pre><code class="bash">helm repo update
</code></pre>

<p>Then we can install the helm release:</p>

<pre><code class="bash">helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
  --namespace ingress-nginx \
  --create-namespace \
  --set controller.kind=DaemonSet \
  --set controller.hostPort.enabled=true \
  --set controller.ingressClass=nginx
</code></pre>

<p>You can view all the default values from their GitHub repository where the chart is hosted:</p>

<ul>
<li><a href="https://github.com/kubernetes/ingress-nginx/blob/main/charts/ingress-nginx/values.yaml"><strong>https://github.com/kubernetes/ingress-nginx/blob/main/charts/ingress-nginx/values.yaml</strong></a></li>
</ul>


<p>Once the release has been deployed, you should see the ingress-nginx pod running under the <code>ingress-nginx</code> namespace:</p>

<pre><code class="bash">kubectl get pods -n ingress-nginx
</code></pre>

<h2>Cert-Manager</h2>

<p>The next step is to install cert-manager using helm, first add the repository:</p>

<pre><code class="bash">helm repo add jetstack https://charts.jetstack.io
</code></pre>

<p>Update the repositories:</p>

<pre><code class="bash">helm repo update
</code></pre>

<p>Then install the cert-manager release:</p>

<pre><code class="bash">helm upgrade --install cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --create-namespace \
  --version v1.13.1 \
  --set installCRDs=true
</code></pre>

<h2>Cloudflare API Token</h2>

<p>We need to grant Cert-Manager access to make DNS changes on our Cloudflare account for DNS validation on our behalf, and in order to do that, we need to create a Cloudflare API Token.</p>

<p>As per the <a href="https://cert-manager.io/docs/configuration/acme/dns01/cloudflare/#api-tokens">cert-manager documentation</a>, from your profile select <a href="https://dash.cloudflare.com/profile/api-tokens">API Tokens</a>, create an API Token and select <code>Edit Zone DNS</code> template.</p>

<p>Then select the following:</p>

<ul>
<li><p>Permissions:</p>

<ul>
<li><p>Zone: DNS -&gt; Edit</p></li>
<li><p>Zone: Zone -&gt; Read</p></li>
</ul>
</li>
<li><p>Zone Resources:</p>

<ul>
<li>Include -&gt; All Zones</li>
</ul>
</li>
</ul>


<p><img src="https://gitlab.com/bekker-space/workshops/ingress-nginx/uploads/c19d741352f767a1bfb97ef4fd716364/image.png%20align=" title="left" alt="" /></p>

<p>Then create the token and save the value somewhere safe, as we will be using it in the next step.</p>

<h2>Cert-Manager ClusterIssuer</h2>

<p>First, we need to create a Kubernetes secret with the API Token that we created in the previous step.</p>

<pre><code class="bash">kubectl create secret generic cloudflare-api-key-secret \
  --from-literal=api-key=[YOUR_CLOUDFLARE_API_KEY]
</code></pre>

<p>Then create the <code>clusterissuer.yaml</code></p>

<pre><code class="yaml">apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-dns01-issuer
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: you@example.com  # your email address for updates
    privateKeySecretRef:
      name: letsencrypt-dns01-private-key
    solvers:
    - dns01:
        cloudflare:
          email: you@example.com # your cloudflare account email address
          apiTokenSecretRef:
            name: cloudflare-api-key-secret
            key: api-key
</code></pre>

<p>Then create the cluster issuer:</p>

<pre><code class="bash">kubectl apply -f clusterissuer.yaml
</code></pre>

<h2>Request a Certificate</h2>

<p>Now that we have our <code>ClusterIssuer</code> created, we can request a certificate. In my scenario, I have a domain <code>example.com</code> which is hosted on CloudFlare and I would like to create a wildcard certificate on the sub-domain <code>*.workshop.example.com</code></p>

<p>Certificates are scoped on a namespace level, and ClusterIssuer&rsquo;s are cluster-wide, therefore I am prefixing my certificate with the namespace (just my personal preference).</p>

<pre><code class="yaml">apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: default-workshop-certificate
  namespace: default
spec:
  secretName: default-workshop-example-tls
  issuerRef:
    name: letsencrypt-dns01-issuer
    kind: ClusterIssuer
  commonName: workshop.example.com
  dnsNames:
  - workshop.example.com
  - '*.workshop.example.com'
</code></pre>

<p>Before we create the certificate on CloudFlare, I have created private DNS to the names mentioned in the manifest above like the following:</p>

<pre><code class="bash">- workshop.example.com -&gt; A Record -&gt; 10.5.24.254
- *.workshop.example.com -&gt; CNAME -&gt; workshop.example.com
</code></pre>

<p>In the DNS configuration mentioned above, to explain why I am creating 2 entries:</p>

<ul>
<li><p><code>10.2.24.254</code> - This is my LoadBalancer IP Address</p></li>
<li><p>I have a static DNS entry to the name <code>workshop.example.com</code> so if my LoadBalancer IP Address ever change, I can just change this address</p></li>
<li><p>I am creating a wildcard DNS entry for <code>*.workshop.example.com</code> and I am creating a CNAME record for it to resolve to <code>workshop.example.com</code> so it will essentially respond to the LoadBalancer IP.</p></li>
<li><p>So lets say I create <code>test1.workshop.example.com</code> and <code>test2.workshop.example.com</code> then it will resolve to the LoadBalancer IP in <code>workshop.example.com</code> and as mentioned before, if the LoadBalancer IP ever changes, I only have to update the A Record of <code>workshop.example.com</code></p></li>
</ul>


<p>Then after DNS was created, I went ahead and created the certificate:</p>

<pre><code class="bash">kubectl apply -f certificate.yaml
</code></pre>

<p>You can view the progress by viewing the certificate status by running:</p>

<pre><code class="bash">kubectl get certificate -n default
</code></pre>

<h2>Specify the Certificate in your Ingress</h2>

<p>Let&rsquo;s deploy a <code>nginx</code> web server deployment and I have concatenated the following in one manifest called <code>deployment.yaml</code>:</p>

<ul>
<li><p><code>Deployment</code></p></li>
<li><p><code>Service</code></p></li>
<li><p><code>Ingress</code></p></li>
</ul>


<pre><code class="yaml">---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-web
  namespace: default
  labels:
    app: nginx-web
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx-web
  template:
    metadata:
      labels:
        app: nginx-web
    spec:
      containers:
      - name: nginx
        image: nginx:1.19
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-web-service
  namespace: default
  labels:
    app: nginx-web
spec:
  type: ClusterIP
  ports:
  - port: 80
    targetPort: 80
  selector:
    app: nginx-web
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-web-ingress
  namespace: default
  annotations:
    kubernetes.io/ingress.class: "nginx"
spec:
  rules:
  - host: nginx.workshop.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nginx-web-service
            port:
              number: 80
  tls:
  - hosts:
    - nginx.workshop.example.com
    secretName: default-workshop-example-tls
</code></pre>

<p>A few important things to notice on the ingress resource:</p>

<ul>
<li><p><code>host</code> the host needs to match the certificate</p></li>
<li><p><code>secretName</code> the secret needs to match the secret defined in the certificate</p></li>
</ul>


<p>Then create the deployment:</p>

<pre><code class="bash">kubectl apply -f deployment.yaml
</code></pre>

<h2>Ensure DNS Challenges are successful</h2>

<p>Ensure that cert-manager can set DNS-01 challenge records correctly, if you encounter issues, you can inspect the cert-manager pod logs.</p>

<p>To view the pods for cert-manager:</p>

<pre><code class="bash">kubectl get pods -n cert-manager
</code></pre>

<p>Then view the logs using:</p>

<pre><code class="bash">kubectl logs -f pod &lt;pod-id&gt; -n cert-manager
</code></pre>

<h2>Test</h2>

<p>You can open up a browser and access the ingress on your browser, in my case it would be <code>https://nginx.workshop.example.com</code> and verify that you have a certificate issued from Lets Encrypt.</p>

<h2>Thank You</h2>

<p>Thanks for reading, if you enjoy my content please feel free to follow me on <a href="https://twitter.com/ruanbekker"><strong>Twitter -</strong></a> @<a href="@ruanbekker">@ruanbekker</a> or visit me on my <a href="https://ruan.dev/"><strong>website -</strong></a> <a href="http://ruan.dev"><strong>ruan.dev</strong></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Deploy Ingress-Nginx Controller on Kubernetes With Helm]]></title>
    <link href="https://blog.ruanbekker.com/blog/2023/12/22/how-to-deploy-ingress-nginx-controller-on-kubernetes-with-helm/"/>
    <updated>2023-12-22T07:56:22-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2023/12/22/how-to-deploy-ingress-nginx-controller-on-kubernetes-with-helm</id>
    <content type="html"><![CDATA[<p>In this tutorial we will deploy the <a href="https://github.com/kubernetes/ingress-nginx">ingress-nginx</a> controller on kubernetes.</p>

<h2>Pre-Requisites</h2>

<p>I will be using kind to run a kubernetes cluster locally, if you want to follow along, have a look at my previous post on how to install <a href="https://kubernetes.io/docs/tasks/tools/">kubectl</a> and kind and the basic usage of kind:</p>

<ul>
<li><a href="https://blog.ruanbekker.com/blog/2022/09/20/kind-for-local-kubernetes-clusters/">https://blog.ruanbekker.com/blog/2022/09/20/kind-for-local-kubernetes-clusters/</a></li>
</ul>


<p>You will also need <a href="https://helm.sh/docs/intro/install/">helm</a> to deploy the ingress-nginx release from their helm charts, you can see their documentation on how to install it:</p>

<ul>
<li><a href="https://helm.sh/docs/intro/install/">https://helm.sh/docs/intro/install/</a></li>
</ul>


<h2>Create the Kubernetes Cluster</h2>

<p>First we will define the kind configuration which will expose port 80 locally in a file name <code>kind-config.yaml</code></p>

<pre><code class="yaml">kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  image: kindest/node:v1.25.11@sha256:227fa11ce74ea76a0474eeefb84cb75d8dad1b08638371ecf0e86259b35be0c8
  extraPortMappings:
  - containerPort: 80
    hostPort: 80
    protocol: TCP
    listenAddress: "0.0.0.0"
  - containerPort: 443
    hostPort: 443
    protocol: TCP
  kubeadmConfigPatches:
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: "ingress-ready=true"
</code></pre>

<p>Then go ahead and create the kubernetes cluster:</p>

<pre><code class="bash">kind create cluster --name workshop --config kind-config.yaml
</code></pre>

<h2>Install ingress-nginx using Helm</h2>

<p>Install the ingress-nginx helm chart, by first adding the repository:</p>

<pre><code class="bash">helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
</code></pre>

<p>Then update your local repositories:</p>

<pre><code class="bash">helm repo update
</code></pre>

<p>Then install the helm release, and set a couple of overrides.</p>

<p>The reason we use NodePort is because our kubernetes cluster runs on docker containers, and from our kind config we have exposed port 80 locally, we are using the NodePort service so that we can make an HTTP request to port 80 to traverse to the port of the service:</p>

<pre><code class="bash">helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
  --namespace ingress-nginx --create-namespace \
  --set controller.kind=DaemonSet \
  --set controller.hostPort.enabled=true \
  --set controller.ingressClass=nginx
</code></pre>

<p>You can view all the default values from their GitHub repository where the chart is hosted:</p>

<ul>
<li><a href="https://github.com/kubernetes/ingress-nginx/blob/main/charts/ingress-nginx/values.yaml">https://github.com/kubernetes/ingress-nginx/blob/main/charts/ingress-nginx/values.yaml</a></li>
</ul>


<p>Once the release has been deployed, you should see the ingress-nginx pod running under the <code>ingress-nginx</code> namespace:</p>

<pre><code class="bash">kubectl get pods -n ingress-nginx
</code></pre>

<h2>Deploy a Web Application</h2>

<p>We will create 3 files:</p>

<ul>
<li><p><code>example/deployment.yaml</code></p></li>
<li><p><code>example/service.yaml</code></p></li>
<li><p><code>example/ingress.yaml</code></p></li>
</ul>


<p>Create the example directory:</p>

<pre><code class="bash">mkdir example
</code></pre>

<p>Our <code>example/deployment.yaml</code></p>

<pre><code class="yaml">---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: webapp
  name: webapp
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - image: ruanbekker/web-center-name-v2
        name: webapp
        ports:
        - name: http
          containerPort: 5000
        env:
        - name: APP_TITLE
          value: "Runs on Kind"
        resources:
          requests:
            memory: "64Mi"
            cpu: "250m"
          limits:
            memory: "256Mi"
            cpu: "1000m"
</code></pre>

<p>Our <code>example/service.yaml</code></p>

<pre><code class="yaml">---
apiVersion: v1
kind: Service
metadata:
  name: webapp
  namespace: default
spec:
  type: ClusterIP
  selector:
    app: webapp
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 5000
</code></pre>

<p>Our <code>example/ingress.yaml</code></p>

<pre><code class="yaml">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: webapp
  namespace: default
spec:
  ingressClassName: nginx
  rules:
    - host: example.127.0.0.1.nip.io
      http:
        paths:
          - pathType: Prefix
            backend:
              service:
                name: webapp
                port:
                  number: 80
            path: /
</code></pre>

<p>In summary, we are creating a deployment with a pod that listens on port 5000, and then we are creating a service with port 80 that will forward its connections to the container port of 5000.</p>

<p>Then we define our ingress that will match our hostname and forward its connections to our service on port 80, and also notice that we are defining our ingress class name, which we have set in our helm values.</p>

<p>Deploy this example with kubectl:</p>

<pre><code class="bash">kubectly apply -f example/
</code></pre>

<p>Now you can access the web application at <a href="http://example.127.0.0.1.nip.io">http://example.127.0.0.1.nip.io</a></p>

<h2>Teardown</h2>

<p>You can delete the resources that we&rsquo;ve created using:</p>

<pre><code class="bash">kubectl delete -f example/
</code></pre>

<p>Delete the cluster using:</p>

<pre><code class="bash">kind delete cluster --name workshop
</code></pre>

<h2>Thank You</h2>

<p>Thanks for reading, if you enjoy my content please feel free to follow me on <a href="https://twitter.com/ruanbekker"><strong>Twitter - @ruanbekker</strong></a> or visit me on my <a href="https://ruan.dev/"><strong>website -</strong></a> <a href="http://ruan.dev"><strong>ruan.dev</strong></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Manage Helm Releases With Terraform]]></title>
    <link href="https://blog.ruanbekker.com/blog/2023/03/09/manage-helm-releases-with-terraform/"/>
    <updated>2023-03-09T16:15:47-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2023/03/09/manage-helm-releases-with-terraform</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/224163430-34e18f11-2182-4d2b-b7ab-f4683c187719.png" alt="helm-releases-with-terraform" /></p>

<p>In this post we will use terraform to deploy a helm release to kubernetes.</p>

<h2>Kubernetes</h2>

<p>For this demonstration I will be using <a href="https://kind.sigs.k8s.io/">kind</a> to deploy a local Kubernetes cluster to the operating system that I am running this on, which will be Ubuntu Linux. For a more in-depth tutorial on Kind, you can see my post on <a href="https://blog.ruanbekker.com/blog/2022/09/20/kind-for-local-kubernetes-clusters/">Kind for Local Kubernetes Clusters</a>.</p>

<h2>Installing the Pre-Requirements</h2>

<p>We will be installing terraform, docker, kind and kubectl on Linux.</p>

<p>Install terraform:</p>

<pre><code class="bash">wget https://releases.hashicorp.com/terraform/1.3.0/terraform_1.3.0_linux_amd64.zip
unzip terraform_1.3.0_linux_amd64.zip
rm terraform_1.3.0_linux_amd64.zip
mv terraform /usr/bin/terraform
</code></pre>

<p>Verify that terraform has been installed:</p>

<pre><code class="bash">terraform -version
</code></pre>

<p>Which in my case returns:</p>

<pre><code class="bash">Terraform v1.3.0
on linux_amd64
</code></pre>

<p>Install Docker on Linux (be careful to curl pipe bash - trust the scripts that you are running):</p>

<pre><code class="bash">curl https://get.docker.com | bash
</code></pre>

<p>Then running <code>docker ps</code> should return:</p>

<pre><code class="bash">CONTAINER ID   IMAGE        COMMAND         CREATED          STATUS          PORTS       NAMES
</code></pre>

<p>Install kind on Linux:</p>

<pre><code class="bash">apt update
curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.17.0/kind-linux-amd64
chmod +x ./kind
sudo mv ./kind /usr/local/bin/kind
</code></pre>

<p>Then to verify that kind was installed with <code>kind --version</code> should return:</p>

<pre><code class="bash">kind version 0.17.0
</code></pre>

<p>Create a kubernetes cluster using kind:</p>

<pre><code class="bash">kind create cluster --name rbkr --image kindest/node:v1.24.0
</code></pre>

<p>Now install <a href="https://kubernetes.io/docs/tasks/tools/">kubectl</a>:</p>

<pre><code class="bash">curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
</code></pre>

<p>Then to verify that kubectl was installed:</p>

<pre><code class="bash">kubectl version --client
</code></pre>

<p>Which in my case returns:</p>

<pre><code class="bash">Client Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.1", GitCommit:"8f94681cd294aa8cfd3407b8191f6c70214973a4", GitTreeState:"clean", BuildDate:"2023-01-18T15:58:16Z", GoVersion:"go1.19.5", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
</code></pre>

<p>Now we can test if kubectl can communicate with the kubernetes api server:</p>

<pre><code class="bash">kubectl get nodes
</code></pre>

<p>In my case it returns:</p>

<pre><code class="bash">NAME                 STATUS   ROLES           AGE     VERSION
rbkr-control-plane   Ready    control-plane   6m20s   v1.24.0
</code></pre>

<h2>Terraform</h2>

<p>Now that our pre-requirements are sorted we can configure terraform to communicate with kubernetes. For that to happen, we need to consult the <a href="https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs">terraform kubernetes provider</a>&rsquo;s documentation.</p>

<p>As per their documentation they provide us with this snippet:</p>

<pre><code>terraform {
  required_providers {
    kubernetes = {
      source = "hashicorp/kubernetes"
      version = "2.18.0"
    }
  }
}

provider "kubernetes" {
  # Configuration options
}
</code></pre>

<p>And from their <a href="https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs">main</a> page, it gives us a couple of options to configure the provider and the easiest is probably to read the <code>~/.kube/config</code> configuration file.</p>

<p>But in cases where you have multiple configurations in your kube config file, this might not be ideal, and I like to be precise, so I will extract the client certificate, client key and cluster ca certificate and endpoint from our <code>~/.kube/config</code> file.</p>

<p>If we run <code>cat ~/.kube/config</code> we will see something like this:</p>

<pre><code class="yaml">apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRU......FURS0tLS0tCg==
    server: https://127.0.0.1:40305
  name: kind-rbkr
contexts:
- context:
    cluster: kind-rbkr
    user: kind-rbkr
  name: kind-rbkr
current-context: kind-rbkr
kind: Config
preferences: {}
users:
- name: kind-rbkr
  user:
    client-certificate-data: LS0tLS1CRX......FURS0tLS0tCg==
    client-key-data: LS0tLS1CRUejhKWUk2N2.....S0tCg==
</code></pre>

<p>First we will create a directory for our certificates:</p>

<pre><code class="bash">mkdir ~/certs
</code></pre>

<p>I have truncated my kube config for readability, but for our first file <code>certs/client-cert.pem</code> we will copy the value of <code>client-certificate-data:</code>, which will look something like this:</p>

<pre><code class="bash">cat certs/client-cert.pem
LS0tLS1CRX......FURS0tLS0tCg==
</code></pre>

<p>Then we will copy the contents of <code>client-key-data:</code> into <code>certs/client-key.pem</code> and then lastly the content of <code>certificate-authority-data:</code> into <code>certs/cluster-ca-cert.pem</code>.</p>

<p>So then we should have the following files inside our <code>certs/</code> directory:</p>

<pre><code class="bash">tree certs/
certs/
├── client-cert.pem
├── client-key.pem
└── cluster-ca-cert.pem

0 directories, 3 files
</code></pre>

<p>Now make them read only:</p>

<pre><code class="bash">chmod 400 ~/certs/*
</code></pre>

<p>Now that we have that we can start writing our terraform configuration. In <code>providers.tf</code>:</p>

<pre><code>terraform {
  required_providers {
    kubernetes = {
      source = "hashicorp/kubernetes"
      version = "2.18.0"
    }
  }
}

provider "kubernetes" {
  host                   = "https://127.0.0.1:40305"
  client_certificate     = base64decode(file("~/certs/client-cert.pem"))
  client_key             = base64decode(file("~/certs/client-key.pem"))
  cluster_ca_certificate = base64decode(file("~/certs/cluster-ca-cert.pem"))
}
</code></pre>

<p>Your host might look different to mine, but you can find your host endpoint in <code>~/.kube/config</code>.</p>

<p>For a simple test we can list all our namespaces to ensure that our configuration is working. In a file called <code>namespaces.tf</code>, we can populate the following:</p>

<pre><code>data "kubernetes_all_namespaces" "allns" {}

output "all-ns" {
  value = data.kubernetes_all_namespaces.allns.namespaces
}
</code></pre>

<p>Now we need to initialize terraform so that it can download the providers:</p>

<pre><code class="bash">terraform init
</code></pre>

<p>Then we can run a plan which will reveal our namespaces:</p>

<pre><code class="bash">terraform plan

data.kubernetes_all_namespaces.allns: Reading...
data.kubernetes_all_namespaces.allns: Read complete after 0s [id=a0ff7e83ffd7b2d9953abcac9f14370e842bdc8f126db1b65a18fd09faa3347b]

Changes to Outputs:
  + all-ns = [
      + "default",
      + "kube-node-lease",
      + "kube-public",
      + "kube-system",
      + "local-path-storage",
    ]
</code></pre>

<p>We can now remove our <code>namespaces.tf</code> as our test worked:</p>

<pre><code class="bash">rm namespaces.tf
</code></pre>

<h2>Helm Releases with Terraform</h2>

<p>We will need two things, we need to consult the <a href="https://registry.terraform.io/providers/hashicorp/helm/latest/docs/resources/release">terraform helm release provider</a> documentation and we also need to consult the helm chart documentation which we are interested in.</p>

<p>In my previous post I wrote about <a href="https://blog.ruanbekker.com/blog/2023/01/24/everything-you-need-to-know-about-helm/">Everything you need to know about Helm</a> and I used the <a href="https://artifacthub.io/packages/helm/bitnami/nginx">Bitnami Nginx Helm Chart</a>, so we will use that one again.</p>

<p>As we are working with helm releases, we need to configure the helm provider, I will just extend my configuration from my previous provider config in <code>providers.tf</code>:</p>

<pre><code>terraform {
  required_providers {
    kubernetes = {
      source = "hashicorp/kubernetes"
      version = "2.18.0"
    }
    helm = {
      source = "hashicorp/helm"
      version = "2.9.0"
    }
  }
}

provider "kubernetes" {
  host                   = "https://127.0.0.1:40305"
  client_certificate     = base64decode(file("~/certs/client-cert.pem"))
  client_key             = base64decode(file("~/certs/client-key.pem"))
  cluster_ca_certificate = base64decode(file("~/certs/cluster-ca-cert.pem"))
}

provider "helm" {
  kubernetes {
    host                   = "https://127.0.0.1:40305"
    client_certificate     = base64decode(file("~/certs/client-cert.pem"))
    client_key             = base64decode(file("~/certs/client-key.pem"))
    cluster_ca_certificate = base64decode(file("~/certs/cluster-ca-cert.pem"))
  }
}
</code></pre>

<p>We will create three terraform files:</p>

<pre><code class="bash">touch {main,outputs,variables}.tf
</code></pre>

<p>And our values yaml in <code>helm-chart/nginx/values.yaml</code>:</p>

<pre><code class="bash">mkdir -p helm-chart/nginx
</code></pre>

<p>Then you can copy the values file from <a href="https://artifacthub.io/packages/helm/bitnami/nginx?modal=values">https://artifacthub.io/packages/helm/bitnami/nginx?modal=values</a> into <code>helm-chart/nginx/values.yaml</code>.</p>

<p>In our <code>main.tf</code> I will use two ways to override values in our <code>values.yaml</code> using <code>set</code> and <code>templatefile</code>. The reason for the templatefile, is when we want to fetch a value and want to replace the content with our values file, it could be used when we retrieve a value from a data source as an example. In my example im just using a variable.</p>

<p>We will have the following:</p>

<pre><code>resource "helm_release" "nginx" {
  name             = var.release_name
  version          = var.chart_version
  namespace        = var.namespace
  create_namespace = var.create_namespace
  chart            = var.chart_name
  repository       = var.chart_repository_url
  dependency_update = true
  reuse_values      = true
  force_update      = true
  atomic              = var.atomic

  set {
    name  = "image.tag"
    value = "1.23.3-debian-11-r3"
  }

  set {
    name  = "service.type"
    value = "ClusterIP"
  }

  values = [
    templatefile("${path.module}/helm-chart/nginx/values.yaml", {
      NAME_OVERRIDE   = var.release_name
    }
  )]

}
</code></pre>

<p>As you can see we are referencing a <code>NAME_OVERRIDE</code> in our <code>values.yaml</code>, I have cleaned up the values file to the following:</p>

<pre><code class="yaml">nameOverride: "${NAME_OVERRIDE}"

## ref: https://hub.docker.com/r/bitnami/nginx/tags/
image:
  registry: docker.io
  repository: bitnami/nginx
  tag: 1.23.3-debian-11-r3
</code></pre>

<p>The <code>NAME_OVERRIDE</code> must be in a <code>${}</code> format.</p>

<p>In our <code>variables.tf</code> we will have the following:</p>

<pre><code>variable "release_name" {
  type        = string
  default     = "nginx"
  description = "The name of our release."
}

variable "chart_repository_url" {
  type        = string
  default     = "https://charts.bitnami.com/bitnami"
  description = "The chart repository url."
}

variable "chart_name" {
  type        = string
  default     = "nginx"
  description = "The name of of our chart that we want to install from the repository."
}

variable "chart_version" {
  type        = string
  default     = "13.2.20"
  description = "The version of our chart."
}

variable "namespace" {
  type        = string
  default     = "apps"
  description = "The namespace where our release should be deployed into."
}

variable "create_namespace" {
  type        = bool
  default     = true
  description = "If it should create the namespace if it doesnt exist."
}

variable "atomic" {
  type        = bool
  default     = false
  description = "If it should wait until release is deployed."
}
</code></pre>

<p>And lastly our <code>outputs.tf</code>:</p>

<pre><code>output "metadata" {
  value = helm_release.nginx.metadata
}
</code></pre>

<p>Now that we have all our configuration ready, we can initialize terraform:</p>

<pre><code class="bash">terraform init
</code></pre>

<p>Then we can run a plan to see what terraform wants to deploy:</p>

<pre><code class="bash">terraform plan
</code></pre>

<p>The plan output shows the following:</p>

<pre><code class="bash">Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  # helm_release.nginx will be created
  + resource "helm_release" "nginx" {
      + atomic                     = false
      + chart                      = "nginx"
      + cleanup_on_fail            = false
      + create_namespace           = true
      + dependency_update          = false
      + disable_crd_hooks          = false
      + disable_openapi_validation = false
      + disable_webhooks           = false
      + force_update               = false
      + id                         = (known after apply)
      + lint                       = false
      + manifest                   = (known after apply)
      + max_history                = 0
      + metadata                   = (known after apply)
      + name                       = "nginx"
      + namespace                  = "apps"
      + pass_credentials           = false
      + recreate_pods              = false
      + render_subchart_notes      = true
      + replace                    = false
      + repository                 = "https://charts.bitnami.com/bitnami"
      + reset_values               = false
      + reuse_values               = false
      + skip_crds                  = false
      + status                     = "deployed"
      + timeout                    = 300
      + values                     = [
          + &lt;&lt;-EOT
                nameOverride: "nginx"

                ## ref: https://hub.docker.com/r/bitnami/nginx/tags/
                image:
                  registry: docker.io
                  repository: bitnami/nginx
                  tag: 1.23.3-debian-11-r3
            EOT,
        ]
      + verify                     = false
      + version                    = "13.2.20"
      + wait                       = false
      + wait_for_jobs              = false

      + set {
          + name  = "image.tag"
          + value = "1.23.3-debian-11-r3"
        }
    }

Plan: 1 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + metadata = (known after apply)
</code></pre>

<p>Once we are happy with our plan, we can run a apply:</p>

<pre><code class="bash">terraform apply 

Plan: 1 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + metadata = (known after apply)

Do you want to perform these actions?
  Terraform will perform the actions described above.
  Only 'yes' will be accepted to approve.

  Enter a value: yes

helm_release.nginx: Creating...
helm_release.nginx: Still creating... [10s elapsed]

metadata = tolist([
  {
    "app_version" = "1.23.3"
    "chart" = "nginx"
    "name" = "nginx"
    "namespace" = "apps"
    "revision" = 1
    "values" = "{\"image\":{\"registry\":\"docker.io\",\"repository\":\"bitnami/nginx\",\"tag\":\"1.23.3-debian-11-r3\"},\"nameOverride\":\"nginx\"}"
    "version" = "13.2.20"
  },
])
</code></pre>

<p>Then we can verify if the pod is running:</p>

<pre><code class="bash">kubectl get pods -n apps
NAME                    READY   STATUS    RESTARTS   AGE
nginx-59bdc6465-xdbfh   1/1     Running   0          2m35s
</code></pre>

<h2>Importing Helm Releases into Terraform State</h2>

<p>If you have an existing helm release that was deployed with helm and you want to transfer the ownership to terraform, you first need to write the terraform code, then import the resources into terraform state using:</p>

<pre><code class="bash">terraform import helm_release.nginx apps/nginx
</code></pre>

<p>Where the last argument is <code>&lt;namespace&gt;/&lt;release-name&gt;</code>. Once that is imported you can run terraform plan and apply.</p>

<p>If you want to discover all helm releases managed by helm you can use:</p>

<pre><code class="bash">kubectl get all -A -l app.kubernetes.io/managed-by=Helm
</code></pre>

<h2>Thank You</h2>

<p>Thanks for reading, feel free to check out my <a href="https://ruan.dev/">website</a>, feel free to subscribe to my <a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a> or follow me at <a href="https://twitter.com/ruanbekker">@ruanbekker</a> on Twitter.</p>

<ul>
<li>Linktree: <a href="https://go.ruan.dev/links">https://go.ruan.dev/links</a></li>
<li>Patreon: <a href="https://go.ruan.dev/patreon">https://go.ruan.dev/patreon</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Everything You Need to Know About Helm]]></title>
    <link href="https://blog.ruanbekker.com/blog/2023/01/24/everything-you-need-to-know-about-helm/"/>
    <updated>2023-01-24T16:02:22-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2023/01/24/everything-you-need-to-know-about-helm</id>
    <content type="html"><![CDATA[<p><img width="965" alt="image" src="https://user-images.githubusercontent.com/567298/214427983-29601304-9930-40b6-bbc6-e2ce68c04c23.png"></p>

<p>Helm, its one amazing piece of software that I use multiple times per day!</p>

<h2>What is Helm?</h2>

<p>You can think of helm as a package manager for kubernetes, but in fact its much more than that.</p>

<p>Think about it in the following way:</p>

<ul>
<li>Kubernetes Package Manager</li>
<li>Way to templatize your applications (this is the part im super excited about)</li>
<li>Easy way to install applications to your kubernetes cluster</li>
<li>Easy way to do upgrades to your applications</li>
<li>Websites such as artifacthub.io provides a nice interface to lookup any application an how to install or upgrade that application.</li>
</ul>


<h2>How does Helm work?</h2>

<p>Helm uses your kubernetes config to connect to your kubernetes cluster. In most cases it utilises the config defined by the <code>KUBECONFIG</code> environment variable, which in most cases points to <code>~/kube/config</code>.</p>

<p>If you want to follow along, you can view the following blog post to provision a kubernetes cluster locally:</p>

<ul>
<li><a href="https://blog.ruanbekker.com/blog/2022/09/20/kind-for-local-kubernetes-clusters/">https://blog.ruanbekker.com/blog/2022/09/20/kind-for-local-kubernetes-clusters/</a></li>
</ul>


<p>Once you have provisioned your kubernetes cluster locally, you can proceed to <a href="https://helm.sh/docs/intro/install/">install helm</a>, I will make the assumption that you are using Mac:</p>

<pre><code class="bash">brew install helm
</code></pre>

<p>Once helm has been installed, you can test the installation by listing any helm releases, by running:</p>

<pre><code class="bash">helm list
</code></pre>

<h2>Helm Charts</h2>

<p>Helm uses a packaging format called charts, which is a collection of files that describes a related set of kubernetes resources. A sinlge helm chart m
ight be used to deploy something simple such as a deployment or something complex that deploys a deployment, ingress, horizontal pod autoscaler, etc.</p>

<h2>Using Helm to deploy applications</h2>

<p>So let&rsquo;s assume that we have our kubernetes cluster deployed, and now we are ready to deploy some applications to kubernetes, but we are unsure on how we would do that.</p>

<p>Let&rsquo;s assume we want to install Nginx.</p>

<p>First we would navigate to <a href="https://artifacthub.io">artifacthub.io</a>, which is a repository that holds a bunch of helm charts and the information on how to deploy helm charts to our cluster.</p>

<p>Then we would search for Nginx, which would ultimately let us land on:</p>

<ul>
<li><a href="https://artifacthub.io/packages/helm/bitnami/nginx">https://artifacthub.io/packages/helm/bitnami/nginx</a></li>
</ul>


<p>On this view, we have super useful information such as how to use this helm chart, the default values, etc.</p>

<p>Now that we have identified the chart that we want to install, we can have a look at their readme, which will indicate how to install the chart:</p>

<pre><code class="bash">$ helm repo add my-repo https://charts.bitnami.com/bitnami
$ helm install my-release my-repo/nginx
</code></pre>

<p>But before we do that, if we think about it, we add a repository, then before we install a release, we could first find information such as the release versions, etc.</p>

<p>So the way I would do it, is to first add the repository:</p>

<pre><code class="bash">$ helm repo add bitnami https://charts.bitnami.com/bitnami
</code></pre>

<p>Then since we have added the repository, we can update our repository to ensure that we have the latest release versions:</p>

<pre><code class="bash">$ helm repo update
</code></pre>

<p>Now that we have updated our local repositories, we want to find the release versions, and we can do that by listing the repository in question. For example, if we don&rsquo;t know the application name, we can search by the repository name:</p>

<pre><code class="bash">$ helm search repo bitnami/ --versions
</code></pre>

<p>In this case we will get an output of all the applications that is currently being hosted by Bitnami.</p>

<p>If we know the repository and the release name, we can extend our search by using:</p>

<pre><code class="bash">$ helm search repo bitnami/nginx --versions
</code></pre>

<p>In this case we get an output of all the Nginx release versions that is currently hosted by Bitnami.</p>

<h2>Installing a Helm Release</h2>

<p>Now that we have received a response from <code>helm search repo</code>, we can see that we have different release versions, as example:</p>

<pre><code class="bash">NAME                                CHART VERSION   APP VERSION DESCRIPTION
bitnami/nginx                       13.2.22         1.23.3      NGINX Open Source is a web server that can be a...
bitnami/nginx                       13.2.21         1.23.3      NGINX Open Source is a web server that can be a...
</code></pre>

<p>For each helm chart, the chart has default values which means, when we install the helm release it will use the default values which is defined by the helm chart.</p>

<p>We have the concept of overriding the default values with a yaml configuration file we usually refer to <code>values.yaml</code>, that we can define the values that we want to override our default values with.</p>

<p>To get the current default values, we can use <code>helm show values</code>, which will look like the following:</p>

<pre><code class="bash">$ helm show values bitnami/nginx --version 13.2.22
</code></pre>

<p>That will output to standard out, but we can redirect the output to a file using the following:</p>

<pre><code class="bash">$ helm show values bitnami/nginx --version 13.2.22 &gt; nginx-values.yaml
</code></pre>

<p>Now that we have redirected the output to <code>nginx-values.yaml</code>, we can inspect the default values using <code>cat nginx-values.yaml</code>, and any values that we see that we want to override, we can edit the yaml file and once we are done we can save it.</p>

<p>Now that we have our override values, we can install a release to our kubernetes cluster.</p>

<p>Let&rsquo;s assume we want to install nginx to our cluster under the name <code>my-nginx</code> and we want to deploy it to the namespace called <code>web-servers</code>:</p>

<pre><code class="bash">$ helm upgrade --install my-nginx bitnami/nginx --values nginx-values.yaml --namespace web-servers --create-namespace --version 13.2.22
</code></pre>

<p>In the example above, we defined the following:</p>

<ul>
<li><code>upgrade --install</code>                          - meaning we are installing a release, if already exists, do an upgrade</li>
<li><code>my-nginx</code>                                   - use the release name <code>my-nginx</code></li>
<li><code>bitnami/nginx</code>                              - use the repository and chart named nginx</li>
<li><code>--values nginx-values.yaml</code>                 - define the values file with the overrides</li>
<li><code>--namespace web-servers --create-namespace</code> - define the namespace where the release will be installed to, and create the namespace if not exists</li>
<li><code>--version 13.2.22</code>                          - specify the version of the chart to be installed</li>
</ul>


<h2>Information about the release</h2>

<p>We can view information about our release by running:</p>

<pre><code class="bash">$ helm list -n web-servers
</code></pre>

<h2>Creating your own helm charts</h2>

<p>It&rsquo;s very common to create your own helm charts when you follow a common pattern in a microservice architecture or something else, where you only want to override specific values such as the container image, etc.</p>

<p>In this case we can create our own helm chart using:</p>

<pre><code class="bash">$ mkdir ~/charts
$ cd ~/charts
$ helm create my-chart
</code></pre>

<p>This will create a scaffoliding project with the required information that we need to create our own helm chart. If we look at a tree view, it will look like the following:</p>

<pre><code class="bash">$ tree . 
.
└── my-chart
    ├── Chart.yaml
    ├── charts
    ├── templates
    │   ├── NOTES.txt
    │   ├── _helpers.tpl
    │   ├── deployment.yaml
    │   ├── hpa.yaml
    │   ├── ingress.yaml
    │   ├── service.yaml
    │   ├── serviceaccount.yaml
    │   └── tests
    │       └── test-connection.yaml
    └── values.yaml

4 directories, 10 files
</code></pre>

<p>This example chart can already be used, to see what this chart will produce when running it with helm, we can use the <code>helm template</code> command:</p>

<pre><code class="bash">$ cd my-chart
$ helm template example . --values values.yaml
</code></pre>

<p>The output will be something like the following:</p>

<pre><code class="yaml">---
# Source: my-chart/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: example-my-chart
  labels:
    helm.sh/chart: my-chart-0.1.0
    app.kubernetes.io/name: my-chart
    app.kubernetes.io/instance: example
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  template:
    spec:
      containers:
        - name: my-chart
          image: "nginx:1.16.0"
          ...
---
...
</code></pre>

<p>In our example it will create a service account, service, deployment, etc.</p>

<p>As you can see the <code>spec.template.spec.containers[].image</code> is set to <code>nginx:1.16.0</code>, and to see how that was computed, we can have a look at <code>templates/deployment.yaml</code>:</p>

<script src="https://gist.github.com/ruanbekker/908dfeef90ef6edf8d2e40dc6c49bebf.js"></script>


<p>As you can see in <code>image:</code> section we have <code>.Values.image.repository</code> and <code>.Values.image.tag</code>, and those values are being retrieved from the <code>values.yaml</code> file, and when we look at the <code>values.yaml</code> file:</p>

<pre><code class="yaml">image:
  repository: nginx
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""
</code></pre>

<p>If we want to override the image repository and image tag, we can update the <code>values.yaml</code> file to lets say:</p>

<pre><code class="yaml">image:
  repository: busybox
  tag: latest
  pullPolicy: IfNotPresent
</code></pre>

<p>When we run our helm template command again, we can see that the computed values changed to what we want:</p>

<pre><code class="bash">$ helm template example . --values values.yaml
---
# Source: my-chart/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: example-my-chart
spec:
  replicas: 1
  template:
    spec:
      containers:
        - name: my-chart
          image: "busybox:latest"
          imagePullPolicy: IfNotPresent
      ...
</code></pre>

<p>Another way is to use <code>--set</code>:</p>

<pre><code class="bash">$ helm template example . --values values.yaml --set image.repository=ruanbekker/containers,image.tag=curl
spec:
  template:
    spec:
      containers:
        - name: my-chart
          image: "ruanbekker/containers:curl"
      ...
</code></pre>

<p>The template subcommand provides a great way to debug your charts. To learn more about helm charts, view their <a href="https://helm.sh/docs/topics/charts/">documentation</a>.</p>

<h2>Publish your Helm Chart to ChartMuseum</h2>

<p><a href="https://chartmuseum.com/">ChartMuseum</a> is an open-source Helm Chart Repository server written in Go.</p>

<p>Running chartmuseum demonstration will be done locally on my workstation using Docker. To run the server:</p>

<pre><code class="bash">$ docker run --rm -it \
  -p 8080:8080 \
  -e DEBUG=1 \
  -e STORAGE=local \
  -e STORAGE_LOCAL_ROOTDIR=/charts \
  -v $(pwd)/charts:/charts \
  ghcr.io/helm/chartmuseum:v0.14.0
</code></pre>

<p>Now that ChartMuseum is running, we will need to install a helm plugin called <code>helm-push</code> which helps to push charts to our chartmusuem repository:</p>

<pre><code class="bash">$ helm plugin install https://github.com/chartmuseum/helm-push
</code></pre>

<p>We can verify if our plugin was installed:</p>

<pre><code class="bash">$ helm plugin list
NAME        VERSION DESCRIPTION
cm-push     0.10.3  Push chart package to ChartMuseum
</code></pre>

<p>Now we add our chartmuseum helm chart repository, which we will call <code>cm-local</code>:</p>

<pre><code class="bash">$ helm repo add cm-local http://localhost:8080/
</code></pre>

<p>We can list our helm repository:</p>

<pre><code class="bash">$ helm repo list
NAME                    URL
cm-local                http://localhost:8080/
</code></pre>

<p>Now that our helm repository has been added, we can push our helm chart to our helm chart repository. Ensure that we are in our chart repository directory, where the <code>Chart.yaml</code> file should be in our current directory. We need this file as it holds metadata about our chart.</p>

<p>We can view the <code>Chart.yaml</code>:</p>

<pre><code class="yaml">apiVersion: v2
name: my-chart
description: A Helm chart for Kubernetes
type: application
version: 0.1.0
appVersion: "1.16.0"
</code></pre>

<p>Push the helm chart to chartmuseum:</p>

<pre><code class="bash">$ helm cm-push . http://localhost:8080/ --version 0.0.1
Pushing my-chart-0.0.1.tgz to http://localhost:8080/...
Done.
</code></pre>

<p>Now we should update our repositories so that we can get the latest changes:</p>

<pre><code class="bash">$ helm repo update
</code></pre>

<p>Now we can list the charts under our repository:</p>

<pre><code class="bash">$ helm search repo cm-local/
NAME                CHART VERSION   APP VERSION DESCRIPTION
cm-local/my-chart   0.0.1           1.16.0      A Helm chart for Kubernetes
</code></pre>

<p>We can now get the values for our helm chart by running:</p>

<pre><code class="bash">$ helm show values cm-local/my-chart
</code></pre>

<p>This returns the values yaml that we can use for our chart, so let&rsquo;s say you want to output the values yaml so that we can use to to deploy a release we can do:</p>

<pre><code class="bash">$ helm show values cm-local/my-chart &gt; my-values.yaml
</code></pre>

<p>Now when we want to deploy a release, we can do:</p>

<pre><code class="bash">$ helm upgrade --install my-release cm-local/my-chart --values my-values.yaml --namespace test --create-namespace --version 0.0.1
</code></pre>

<p>After the release was deployed, we can list the releases by running:</p>

<pre><code class="bash">$ helm list
</code></pre>

<p>And to view the release history:</p>

<pre><code class="bash">$ helm history my-release
</code></pre>

<h2>Resources</h2>

<p>Please find the following information with regards to Helm documentation:
- <a href="https://helm.sh/docs/">helm docs</a>
- <a href="https://helm.sh/docs/chart_template_guide/">helm cart template guide</a></p>

<p>If you need a kubernetes cluster and you would like to run this locally, find the following documentation in order to do that:
- <a href="https://blog.ruanbekker.com/blog/2022/09/20/kind-for-local-kubernetes-clusters/">using kind for local kubernetes clusters</a></p>

<h2>Thank You</h2>

<p>Thanks for reading, feel free to check out my <a href="https://ruan.dev/">website</a>, feel free to subscribe to my <a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a> or follow me at <a href="https://twitter.com/ruanbekker">@ruanbekker</a> on Twitter.</p>

<ul>
<li>Linktree: <a href="https://go.ruan.dev/links">https://go.ruan.dev/links</a></li>
<li>Patreon: <a href="https://go.ruan.dev/patreon">https://go.ruan.dev/patreon</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[KinD for Local Kubernetes Clusters]]></title>
    <link href="https://blog.ruanbekker.com/blog/2022/09/20/kind-for-local-kubernetes-clusters/"/>
    <updated>2022-09-20T02:18:16-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2022/09/20/kind-for-local-kubernetes-clusters</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/191189852-44f2fd39-7ad7-4d0a-a36b-c2889a838649.png" alt="kubernetes-kind" /></p>

<p>In this tutorial we will demonstrate how to use KinD (Kubernetes in Docker) to provision local kubernetes clusters for local development.</p>

<p><em>Updated at</em>: <em>2023-12-22</em></p>

<h2>About</h2>

<p>KinD uses container images to run as &ldquo;nodes&rdquo;, so spinning up and tearing down clusters becomes really easy or running multiple or different versions, is as easy as pointing to a different container image.</p>

<p>Configuration such as node count, ports, volumes, image versions can either be controlled via the command line or via configuration, more information on that can be found on their documentation:</p>

<ul>
<li><a href="https://kind.sigs.k8s.io/docs/user/quick-start/">https://kind.sigs.k8s.io/docs/user/quick-start/</a></li>
<li><a href="https://kind.sigs.k8s.io/docs/user/configuration/">https://kind.sigs.k8s.io/docs/user/configuration/</a></li>
</ul>


<h2>Installation</h2>

<p>Follow the <a href="https://kind.sigs.k8s.io/docs/user/quick-start/#installing-with-a-package-manager">docs</a> for more information, but for mac:</p>

<pre><code class="bash">brew install kind
</code></pre>

<p>To verify if kind was installed, you can run:</p>

<pre><code class="bash">kind version
</code></pre>

<h2>Create a Cluster</h2>

<p>Create the cluster with command line arguments, such as cluster name, the container image:</p>

<pre><code class="bash">kind create cluster --name cluster-1 --image kindest/node:v1.26.6
</code></pre>

<p>And the output will look something like this:</p>

<pre><code class="bash">Creating cluster "cluster-1" ...
 ✓ Ensuring node image (kindest/node:v1.26.6) 🖼
 ✓ Preparing nodes 📦
 ✓ Writing configuration 📜
 ✓ Starting control-plane 🕹️
 ✓ Installing CNI 🔌
 ✓ Installing StorageClass 💾
Set kubectl context to "kind-cluster-1"
You can now use your cluster with:

kubectl cluster-info --context kind-cluster-1

Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community 🙂
</code></pre>

<p>Then you can interact with the cluster using:</p>

<pre><code class="bash">kubectl get nodes --context kind-cluster-1
</code></pre>

<p>Then delete the cluster using:</p>

<pre><code class="bash">kind delete cluster --name kind-cluster-1
</code></pre>

<p>I <strong>highly recommend</strong> installing <a href="https://github.com/ahmetb/kubectx">kubectx</a>, which makes it easy to switch between kubernetes contexts.</p>

<h2>Create a Cluster with Config</h2>

<p>If you would like to define your cluster configuration as config, you can create a file <code>default-config.yaml</code> with the following as a 2 node cluster, and specifying version 1.24.0:</p>

<pre><code class="yaml">---
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  image: kindest/node:v1.26.6@sha256:6e2d8b28a5b601defe327b98bd1c2d1930b49e5d8c512e1895099e4504007adb
- role: worker
  image: kindest/node:v1.26.6@sha256:6e2d8b28a5b601defe327b98bd1c2d1930b49e5d8c512e1895099e4504007adb
</code></pre>

<p>Then create the cluster and point the config:</p>

<pre><code class="bash">kind create cluster --name kind-cluster --config default-config.yaml
</code></pre>

<h2>Interact with the Cluster</h2>

<p>View the cluster info:</p>

<pre><code class="bash">kubectl cluster-info --context kind-kind-cluster
</code></pre>

<p>View cluster contexts:</p>

<pre><code class="bash">kubectl config get-contexts
</code></pre>

<p>Use context:</p>

<pre><code class="bash">kubectl config use-context kind-kind-cluster
</code></pre>

<p>View nodes:</p>

<pre><code class="bash">kubectl get nodes -o wide

NAME                         STATUS   ROLES           AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION      CONTAINER-RUNTIME
kind-cluster-control-plane   Ready    control-plane   2m11s   v1.26.6   172.20.0.5    &lt;none&gt;        Ubuntu 21.10   5.10.104-linuxkit   containerd://1.6.4
kind-cluster-worker          Ready    &lt;none&gt;          108s    v1.26.6   172.20.0.4    &lt;none&gt;        Ubuntu 21.10   5.10.104-linuxkit   containerd://1.6.4
</code></pre>

<h2>Deploy Sample Application</h2>

<p>We will create a deployment, a service and port-forward to our service to access our application. You can also specify port configuration to your cluster so that you don&rsquo;t need to port-forward, which you can find in their <a href="https://kind.sigs.k8s.io/docs/user/configuration/#extra-port-mappings">port mappings documentation</a></p>

<p>I will be using the following commands to generate the manifests, but will also add them to this post:</p>

<pre><code class="bash">kubectl create deployment hostname --namespace default --replicas 2 --image ruanbekker/containers:hostname --port 8080 --dry-run=client -o yaml &gt; hostname-deployment.yaml
kubectl expose deployment hostname --namespace default --port=80 --target-port=8080 --name=hostname-http --dry-run=client -o yaml &gt; hostname-service.yaml
</code></pre>

<p>The manifest:</p>

<pre><code class="yaml">---
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: hostname
  name: hostname
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: hostname
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: hostname
    spec:
      containers:
      - image: ruanbekker/containers:hostname
        name: containers
        ports:
        - containerPort: 8080
        resources: {}
status: {}
---
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: hostname
  name: hostname-http
  namespace: default
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: hostname
status:
  loadBalancer: {}
</code></pre>

<p>Then apply them with:</p>

<pre><code class="bash">kubectl apply -f &lt;name-of-manifest&gt;.yaml
</code></pre>

<p>Or if you used kubectl to create them:</p>

<pre><code class="bash">kubectl apply -f hostname-deployment.yaml
kubectl apply -f hostname-service.yaml
</code></pre>

<p>You can then view your resources with:</p>

<pre><code class="bash">kubectl get deployment,pod,service

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/hostname   2/2     2            2           9m27s

NAME                            READY   STATUS    RESTARTS   AGE
pod/hostname-7ff58c5644-67vhq   1/1     Running   0          9m27s
pod/hostname-7ff58c5644-wjjbw   1/1     Running   0          9m27s

NAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
service/hostname-http   ClusterIP   10.96.218.58   &lt;none&gt;        80/TCP    5m48s
service/kubernetes      ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP   24m
</code></pre>

<p>Port forward to your service:</p>

<pre><code class="bash">kubectl port-forward svc/hostname-http 8080:80
</code></pre>

<p>Then access your application:</p>

<pre><code class="bash">curl http://localhost:8080/

Hostname: hostname-7ff58c5644-wjjbw
</code></pre>

<h2>Delete Kind Cluster</h2>

<p>View the clusters:</p>

<pre><code class="bash">kind get clusters
</code></pre>

<p>Delete a cluster:</p>

<pre><code class="bash">kind delete cluster --name kind-cluster
</code></pre>

<h2>Additional Configs</h2>

<p>If you want more configuration options, you can look at their documentation:</p>

<ul>
<li><a href="https://kind.sigs.k8s.io/docs/user/configuration/">https://kind.sigs.k8s.io/docs/user/configuration/</a></li>
</ul>


<p>But one more example that I like using, is to define the port mappings:</p>

<pre><code class="yaml">kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  image: kindest/node:v1.26.6@sha256:6e2d8b28a5b601defe327b98bd1c2d1930b49e5d8c512e1895099e4504007adb
  extraPortMappings:
  - containerPort: 80
    hostPort: 80
    protocol: TCP
    listenAddress: "0.0.0.0"
  - containerPort: 443
    hostPort: 443
    protocol: TCP
  kubeadmConfigPatches:
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: "ingress-ready=true"
</code></pre>

<h2>Extras</h2>

<p>I highly recommend using <code>kubectx</code> to switch contexts and <code>kubens</code> to set the default namespace, and aliases:</p>

<pre><code class="bash">alias k=kubectl
alias kx=kubectx
alias kns=kubens
</code></pre>

<h2>Thank You</h2>

<p>Thanks for reading, feel free to check out my <a href="https://ruan.dev/">website</a>, feel free to subscribe to my <a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a> or follow me at <a href="https://twitter.com/ruanbekker">@ruanbekker</a> on Twitter.</p>

<ul>
<li>Linktree: <a href="https://go.ruan.dev/links">https://go.ruan.dev/links</a></li>
<li>Patreon: <a href="https://go.ruan.dev/patreon">https://go.ruan.dev/patreon</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
