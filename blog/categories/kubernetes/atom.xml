<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Kubernetes | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/kubernetes/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2023-03-05T02:24:19-05:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Everything You Need to Know About Helm]]></title>
    <link href="https://blog.ruanbekker.com/blog/2023/01/24/everything-you-need-to-know-about-helm/"/>
    <updated>2023-01-24T16:02:22-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2023/01/24/everything-you-need-to-know-about-helm</id>
    <content type="html"><![CDATA[<p><img width="965" alt="image" src="https://user-images.githubusercontent.com/567298/214427983-29601304-9930-40b6-bbc6-e2ce68c04c23.png"></p>

<p>Helm, its one amazing piece of software that I use multiple times per day!</p>

<h2>What is Helm?</h2>

<p>You can think of helm as a package manager for kubernetes, but in fact its much more than that.</p>

<p>Think about it in the following way:</p>

<ul>
<li>Kubernetes Package Manager</li>
<li>Way to templatize your applications (this is the part im super excited about)</li>
<li>Easy way to install applications to your kubernetes cluster</li>
<li>Easy way to do upgrades to your applications</li>
<li>Websites such as artifacthub.io provides a nice interface to lookup any application an how to install or upgrade that application.</li>
</ul>


<h2>How does Helm work?</h2>

<p>Helm uses your kubernetes config to connect to your kubernetes cluster. In most cases it utilises the config defined by the <code>KUBECONFIG</code> environment variable, which in most cases points to <code>~/kube/config</code>.</p>

<p>If you want to follow along, you can view the following blog post to provision a kubernetes cluster locally:</p>

<ul>
<li><a href="https://blog.ruanbekker.com/blog/2022/09/20/kind-for-local-kubernetes-clusters/">https://blog.ruanbekker.com/blog/2022/09/20/kind-for-local-kubernetes-clusters/</a></li>
</ul>


<p>Once you have provisioned your kubernetes cluster locally, you can proceed to <a href="https://helm.sh/docs/intro/install/">install helm</a>, I will make the assumption that you are using Mac:</p>

<pre><code class="bash">brew install helm
</code></pre>

<p>Once helm has been installed, you can test the installation by listing any helm releases, by running:</p>

<pre><code class="bash">helm list
</code></pre>

<h2>Helm Charts</h2>

<p>Helm uses a packaging format called charts, which is a collection of files that describes a related set of kubernetes resources. A sinlge helm chart m
ight be used to deploy something simple such as a deployment or something complex that deploys a deployment, ingress, horizontal pod autoscaler, etc.</p>

<h2>Using Helm to deploy applications</h2>

<p>So let&rsquo;s assume that we have our kubernetes cluster deployed, and now we are ready to deploy some applications to kubernetes, but we are unsure on how we would do that.</p>

<p>Let&rsquo;s assume we want to install Nginx.</p>

<p>First we would navigate to <a href="https://artifacthub.io">artifacthub.io</a>, which is a repository that holds a bunch of helm charts and the information on how to deploy helm charts to our cluster.</p>

<p>Then we would search for Nginx, which would ultimately let us land on:</p>

<ul>
<li><a href="https://artifacthub.io/packages/helm/bitnami/nginx">https://artifacthub.io/packages/helm/bitnami/nginx</a></li>
</ul>


<p>On this view, we have super useful information such as how to use this helm chart, the default values, etc.</p>

<p>Now that we have identified the chart that we want to install, we can have a look at their readme, which will indicate how to install the chart:</p>

<pre><code class="bash">$ helm repo add my-repo https://charts.bitnami.com/bitnami
$ helm install my-release my-repo/nginx
</code></pre>

<p>But before we do that, if we think about it, we add a repository, then before we install a release, we could first find information such as the release versions, etc.</p>

<p>So the way I would do it, is to first add the repository:</p>

<pre><code class="bash">$ helm repo add bitnami https://charts.bitnami.com/bitnami
</code></pre>

<p>Then since we have added the repository, we can update our repository to ensure that we have the latest release versions:</p>

<pre><code class="bash">$ helm repo update
</code></pre>

<p>Now that we have updated our local repositories, we want to find the release versions, and we can do that by listing the repository in question. For example, if we don&rsquo;t know the application name, we can search by the repository name:</p>

<pre><code class="bash">$ helm search repo bitnami/ --versions
</code></pre>

<p>In this case we will get an output of all the applications that is currently being hosted by Bitnami.</p>

<p>If we know the repository and the release name, we can extend our search by using:</p>

<pre><code class="bash">$ helm search repo bitnami/nginx --versions
</code></pre>

<p>In this case we get an output of all the Nginx release versions that is currently hosted by Bitnami.</p>

<h2>Installing a Helm Release</h2>

<p>Now that we have received a response from <code>helm search repo</code>, we can see that we have different release versions, as example:</p>

<pre><code class="bash">NAME                                CHART VERSION   APP VERSION DESCRIPTION
bitnami/nginx                       13.2.22         1.23.3      NGINX Open Source is a web server that can be a...
bitnami/nginx                       13.2.21         1.23.3      NGINX Open Source is a web server that can be a...
</code></pre>

<p>For each helm chart, the chart has default values which means, when we install the helm release it will use the default values which is defined by the helm chart.</p>

<p>We have the concept of overriding the default values with a yaml configuration file we usually refer to <code>values.yaml</code>, that we can define the values that we want to override our default values with.</p>

<p>To get the current default values, we can use <code>helm show values</code>, which will look like the following:</p>

<pre><code class="bash">$ helm show values bitnami/nginx --version 13.2.22
</code></pre>

<p>That will output to standard out, but we can redirect the output to a file using the following:</p>

<pre><code class="bash">$ helm show values bitnami/nginx --version 13.2.22 &gt; nginx-values.yaml
</code></pre>

<p>Now that we have redirected the output to <code>nginx-values.yaml</code>, we can inspect the default values using <code>cat nginx-values.yaml</code>, and any values that we see that we want to override, we can edit the yaml file and once we are done we can save it.</p>

<p>Now that we have our override values, we can install a release to our kubernetes cluster.</p>

<p>Let&rsquo;s assume we want to install nginx to our cluster under the name <code>my-nginx</code> and we want to deploy it to the namespace called <code>web-servers</code>:</p>

<pre><code class="bash">$ helm upgrade --install my-nginx bitnami/nginx --values nginx-values.yaml --namespace web-servers --create-namespace --version 13.2.22
</code></pre>

<p>In the example above, we defined the following:</p>

<ul>
<li><code>upgrade --install</code>                          - meaning we are installing a release, if already exists, do an upgrade</li>
<li><code>my-nginx</code>                                   - use the release name <code>my-nginx</code></li>
<li><code>bitnami/nginx</code>                              - use the repository and chart named nginx</li>
<li><code>--values nginx-values.yaml</code>                 - define the values file with the overrides</li>
<li><code>--namespace web-servers --create-namespace</code> - define the namespace where the release will be installed to, and create the namespace if not exists</li>
<li><code>--version 13.2.22</code>                          - specify the version of the chart to be installed</li>
</ul>


<h2>Information about the release</h2>

<p>We can view information about our release by running:</p>

<pre><code class="bash">$ helm list -n web-servers
</code></pre>

<h2>Creating your own helm charts</h2>

<p>It&rsquo;s very common to create your own helm charts when you follow a common pattern in a microservice architecture or something else, where you only want to override specific values such as the container image, etc.</p>

<p>In this case we can create our own helm chart using:</p>

<pre><code class="bash">$ mkdir ~/charts
$ cd ~/charts
$ helm create my-chart
</code></pre>

<p>This will create a scaffoliding project with the required information that we need to create our own helm chart. If we look at a tree view, it will look like the following:</p>

<pre><code class="bash">$ tree . 
.
‚îî‚îÄ‚îÄ my-chart
    ‚îú‚îÄ‚îÄ Chart.yaml
    ‚îú‚îÄ‚îÄ charts
    ‚îú‚îÄ‚îÄ templates
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ NOTES.txt
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ _helpers.tpl
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ deployment.yaml
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ hpa.yaml
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ingress.yaml
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ service.yaml
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ serviceaccount.yaml
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ tests
    ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ test-connection.yaml
    ‚îî‚îÄ‚îÄ values.yaml

4 directories, 10 files
</code></pre>

<p>This example chart can already be used, to see what this chart will produce when running it with helm, we can use the <code>helm template</code> command:</p>

<pre><code class="bash">$ cd my-chart
$ helm template example . --values values.yaml
</code></pre>

<p>The output will be something like the following:</p>

<pre><code class="yaml">---
# Source: my-chart/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: example-my-chart
  labels:
    helm.sh/chart: my-chart-0.1.0
    app.kubernetes.io/name: my-chart
    app.kubernetes.io/instance: example
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  template:
    spec:
      containers:
        - name: my-chart
          image: "nginx:1.16.0"
          ...
---
...
</code></pre>

<p>In our example it will create a service account, service, deployment, etc.</p>

<p>As you can see the <code>spec.template.spec.containers[].image</code> is set to <code>nginx:1.16.0</code>, and to see how that was computed, we can have a look at <code>templates/deployment.yaml</code>:</p>

<script src="https://gist.github.com/ruanbekker/908dfeef90ef6edf8d2e40dc6c49bebf.js"></script>


<p>As you can see in <code>image:</code> section we have <code>.Values.image.repository</code> and <code>.Values.image.tag</code>, and those values are being retrieved from the <code>values.yaml</code> file, and when we look at the <code>values.yaml</code> file:</p>

<pre><code class="yaml">image:
  repository: nginx
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""
</code></pre>

<p>If we want to override the image repository and image tag, we can update the <code>values.yaml</code> file to lets say:</p>

<pre><code class="yaml">image:
  repository: busybox
  tag: latest
  pullPolicy: IfNotPresent
</code></pre>

<p>When we run our helm template command again, we can see that the computed values changed to what we want:</p>

<pre><code class="bash">$ helm template example . --values values.yaml
---
# Source: my-chart/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: example-my-chart
spec:
  replicas: 1
  template:
    spec:
      containers:
        - name: my-chart
          image: "busybox:latest"
          imagePullPolicy: IfNotPresent
      ...
</code></pre>

<p>Another way is to use <code>--set</code>:</p>

<pre><code class="bash">$ helm template example . --values values.yaml --set image.repository=ruanbekker/containers,image.tag=curl
spec:
  template:
    spec:
      containers:
        - name: my-chart
          image: "ruanbekker/containers:curl"
      ...
</code></pre>

<p>The template subcommand provides a great way to debug your charts. To learn more about helm charts, view their <a href="https://helm.sh/docs/topics/charts/">documentation</a>.</p>

<h2>Publish your Helm Chart to ChartMuseum</h2>

<p><a href="https://chartmuseum.com/">ChartMuseum</a> is an open-source Helm Chart Repository server written in Go.</p>

<p>Running chartmuseum demonstration will be done locally on my workstation using Docker. To run the server:</p>

<pre><code class="bash">$ docker run --rm -it \
  -p 8080:8080 \
  -e DEBUG=1 \
  -e STORAGE=local \
  -e STORAGE_LOCAL_ROOTDIR=/charts \
  -v $(pwd)/charts:/charts \
  ghcr.io/helm/chartmuseum:v0.14.0
</code></pre>

<p>Now that ChartMuseum is running, we will need to install a helm plugin called <code>helm-push</code> which helps to push charts to our chartmusuem repository:</p>

<pre><code class="bash">$ helm plugin install https://github.com/chartmuseum/helm-push
</code></pre>

<p>We can verify if our plugin was installed:</p>

<pre><code class="bash">$ helm plugin list
NAME        VERSION DESCRIPTION
cm-push     0.10.3  Push chart package to ChartMuseum
</code></pre>

<p>Now we add our chartmuseum helm chart repository, which we will call <code>cm-local</code>:</p>

<pre><code class="bash">$ helm repo add cm-local http://localhost:8080/
</code></pre>

<p>We can list our helm repository:</p>

<pre><code class="bash">$ helm repo list
NAME                    URL
cm-local                http://localhost:8080/
</code></pre>

<p>Now that our helm repository has been added, we can push our helm chart to our helm chart repository. Ensure that we are in our chart repository directory, where the <code>Chart.yaml</code> file should be in our current directory. We need this file as it holds metadata about our chart.</p>

<p>We can view the <code>Chart.yaml</code>:</p>

<pre><code class="yaml">apiVersion: v2
name: my-chart
description: A Helm chart for Kubernetes
type: application
version: 0.1.0
appVersion: "1.16.0"
</code></pre>

<p>Push the helm chart to chartmuseum:</p>

<pre><code class="bash">$ helm cm-push . http://localhost:8080/ --version 0.0.1
Pushing my-chart-0.0.1.tgz to http://localhost:8080/...
Done.
</code></pre>

<p>Now we should update our repositories so that we can get the latest changes:</p>

<pre><code class="bash">$ helm repo update
</code></pre>

<p>Now we can list the charts under our repository:</p>

<pre><code class="bash">$ helm search repo cm-local/
NAME                CHART VERSION   APP VERSION DESCRIPTION
cm-local/my-chart   0.0.1           1.16.0      A Helm chart for Kubernetes
</code></pre>

<p>We can now get the values for our helm chart by running:</p>

<pre><code class="bash">$ helm show values cm-local/my-chart
</code></pre>

<p>This returns the values yaml that we can use for our chart, so let&rsquo;s say you want to output the values yaml so that we can use to to deploy a release we can do:</p>

<pre><code class="bash">$ helm show values cm-local/my-chart &gt; my-values.yaml
</code></pre>

<p>Now when we want to deploy a release, we can do:</p>

<pre><code class="bash">$ helm upgrade --install my-release cm-local/my-chart --values my-values.yaml --namespace test --create-namespace --version 0.0.1
</code></pre>

<p>After the release was deployed, we can list the releases by running:</p>

<pre><code class="bash">$ helm list
</code></pre>

<p>And to view the release history:</p>

<pre><code class="bash">$ helm history my-release
</code></pre>

<h2>Resources</h2>

<p>Please find the following information with regards to Helm documentation:
- <a href="https://helm.sh/docs/">helm docs</a>
- <a href="https://helm.sh/docs/chart_template_guide/">helm cart template guide</a></p>

<p>If you need a kubernetes cluster and you would like to run this locally, find the following documentation in order to do that:
- <a href="https://blog.ruanbekker.com/blog/2022/09/20/kind-for-local-kubernetes-clusters/">using kind for local kubernetes clusters</a></p>

<h2>Thank You</h2>

<p>Thanks for reading, feel free to check out my <a href="https://ruan.dev/">website</a>, feel free to subscribe to my <a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a> or follow me at <a href="https://twitter.com/ruanbekker">@ruanbekker</a> on Twitter.</p>

<ul>
<li>Linktree: <a href="https://go.ruan.dev/links">https://go.ruan.dev/links</a></li>
<li>Patreon: <a href="https://go.ruan.dev/patreon">https://go.ruan.dev/patreon</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[KinD for Local Kubernetes Clusters]]></title>
    <link href="https://blog.ruanbekker.com/blog/2022/09/20/kind-for-local-kubernetes-clusters/"/>
    <updated>2022-09-20T02:18:16-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2022/09/20/kind-for-local-kubernetes-clusters</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/191189852-44f2fd39-7ad7-4d0a-a36b-c2889a838649.png" alt="kubernetes-kind" /></p>

<p>In this tutorial we will demonstrate how to use KinD (Kubernetes in Docker) to provision local kubernetes clusters for local development.</p>

<h2>About</h2>

<p>KinD uses container images to run as &ldquo;nodes&rdquo;, so spinning up and tearing down clusters becomes really easy or running multiple or different versions, is as easy as pointing to a different container image.</p>

<p>Configuration such as node count, ports, volumes, image versions can either be controlled via the command line or via configuration, more information on that can be found on their documentation:</p>

<ul>
<li><a href="https://kind.sigs.k8s.io/docs/user/quick-start/">https://kind.sigs.k8s.io/docs/user/quick-start/</a></li>
<li><a href="https://kind.sigs.k8s.io/docs/user/configuration/">https://kind.sigs.k8s.io/docs/user/configuration/</a></li>
</ul>


<h2>Installation</h2>

<p>Follow the <a href="https://kind.sigs.k8s.io/docs/user/quick-start/#installing-with-a-package-manager">docs</a> for more information, but for mac:</p>

<pre><code class="bash">brew install kind
</code></pre>

<p>To verify if kind was installed, you can run:</p>

<pre><code class="bash">kind version
</code></pre>

<h2>Create a Cluster</h2>

<p>Create the cluster with command line arguments, such as cluster name, the container image:</p>

<pre><code class="bash">kind create cluster --name cluster-1 --image kindest/node:v1.24.0
</code></pre>

<p>And the output will look something like this:</p>

<pre><code class="bash">Creating cluster "cluster-1" ...
 ‚úì Ensuring node image (kindest/node:v1.24.0) üñº
 ‚úì Preparing nodes üì¶
 ‚úì Writing configuration üìú
 ‚úì Starting control-plane üïπÔ∏è
 ‚úì Installing CNI üîå
 ‚úì Installing StorageClass üíæ
Set kubectl context to "kind-cluster-1"
You can now use your cluster with:

kubectl cluster-info --context kind-cluster-1

Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community üôÇ
</code></pre>

<p>I <strong>highly recommend</strong> installing <a href="https://github.com/ahmetb/kubectx">kubectx</a>, which makes it easy to switch between kubernetes contexts.</p>

<h2>Create a Cluster with Config</h2>

<p>If you would like to define your cluster configuration as config, you can create a file <code>default-config.yaml</code> with the following as a 2 node cluster, and specifying version 1.24.0:</p>

<pre><code class="yaml">---
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  image: kindest/node:v1.24.0@sha256:0866296e693efe1fed79d5e6c7af8df71fc73ae45e3679af05342239cdc5bc8e
- role: worker
  image: kindest/node:v1.24.0@sha256:0866296e693efe1fed79d5e6c7af8df71fc73ae45e3679af05342239cdc5bc8e
</code></pre>

<p>Then create the cluster and point the config:</p>

<pre><code class="bash">kind create cluster --name kind-cluster --config default-config.yaml
</code></pre>

<h2>Interact with the Cluster</h2>

<p>View the cluster info:</p>

<pre><code class="bash">kubectl cluster-info --context kind-kind-cluster
</code></pre>

<p>View cluster contexts:</p>

<pre><code class="bash">kubectl config get-contexts
</code></pre>

<p>Use context:</p>

<pre><code class="bash">kubectl config use-context kind-kind-cluster
</code></pre>

<p>View nodes:</p>

<pre><code class="bash">kubectl get nodes -o wide

NAME                         STATUS   ROLES           AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION      CONTAINER-RUNTIME
kind-cluster-control-plane   Ready    control-plane   2m11s   v1.24.0   172.20.0.5    &lt;none&gt;        Ubuntu 21.10   5.10.104-linuxkit   containerd://1.6.4
kind-cluster-worker          Ready    &lt;none&gt;          108s    v1.24.0   172.20.0.4    &lt;none&gt;        Ubuntu 21.10   5.10.104-linuxkit   containerd://1.6.4
</code></pre>

<h2>Deploy Sample Application</h2>

<p>We will create a deployment, a service and port-forward to our service to access our application. You can also specify port configuration to your cluster so that you don&rsquo;t need to port-forward, which you can find in their <a href="https://kind.sigs.k8s.io/docs/user/configuration/#extra-port-mappings">port mappings documentation</a></p>

<p>I will be using the following commands to generate the manifests, but will also add them to this post:</p>

<pre><code class="bash">kubectl create deployment hostname --namespace default --replicas 2 --image ruanbekker/containers:hostname --port 8080 --dry-run=client -o yaml &gt; hostname-deployment.yaml
kubectl expose deployment hostname --namespace default --port=80 --target-port=8080 --name=hostname-http --dry-run=client -o yaml &gt; hostname-service.yaml
</code></pre>

<p>The manifest:</p>

<pre><code class="yaml">---
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: hostname
  name: hostname
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: hostname
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: hostname
    spec:
      containers:
      - image: ruanbekker/containers:hostname
        name: containers
        ports:
        - containerPort: 8080
        resources: {}
status: {}
---
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: hostname
  name: hostname-http
  namespace: default
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: hostname
status:
  loadBalancer: {}
</code></pre>

<p>Then apply them with:</p>

<pre><code class="bash">kubectl apply -f &lt;name-of-manifest&gt;.yaml
</code></pre>

<p>Or if you used kubectl to create them:</p>

<pre><code class="bash">kubectl apply -f hostname-deployment.yaml
kubectl apply -f hostname-service.yaml
</code></pre>

<p>You can then view your resources with:</p>

<pre><code class="bash">kubectl get deployment,pod,service

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/hostname   2/2     2            2           9m27s

NAME                            READY   STATUS    RESTARTS   AGE
pod/hostname-7ff58c5644-67vhq   1/1     Running   0          9m27s
pod/hostname-7ff58c5644-wjjbw   1/1     Running   0          9m27s

NAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
service/hostname-http   ClusterIP   10.96.218.58   &lt;none&gt;        80/TCP    5m48s
service/kubernetes      ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP   24m
</code></pre>

<p>Port forward to your service:</p>

<pre><code class="bash">kubectl port-forward svc/hostname-http 8080:80
</code></pre>

<p>Then access your application:</p>

<pre><code class="bash">curl http://localhost:8080/

Hostname: hostname-7ff58c5644-wjjbw
</code></pre>

<h2>Delete Kind Cluster</h2>

<p>View the clusters:</p>

<pre><code class="bash">kind get clusters
</code></pre>

<p>Delete a cluster:</p>

<pre><code class="bash">kind delete cluster --name kind-cluster
</code></pre>

<h2>Extras</h2>

<p>I highly recommend using <code>kubectx</code> to switch contexts and <code>kubens</code> to set the default namespace, and aliases:</p>

<pre><code class="bash">alias k=kubectl
alias kx=kubectx
alias kns=kubens
</code></pre>

<h2>Thank You</h2>

<p>Thanks for reading, feel free to check out my <a href="https://ruan.dev/">website</a>, feel free to subscribe to my <a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a> or follow me at <a href="https://twitter.com/ruanbekker">@ruanbekker</a> on Twitter.</p>

<ul>
<li>Linktree: <a href="https://go.ruan.dev/links">https://go.ruan.dev/links</a></li>
<li>Patreon: <a href="https://go.ruan.dev/patreon">https://go.ruan.dev/patreon</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Persistent Volumes With K3d Kubernetes]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/02/21/persistent-volumes-with-k3d-kubernetes/"/>
    <updated>2020-02-21T00:07:48+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/02/21/persistent-volumes-with-k3d-kubernetes</id>
    <content type="html"><![CDATA[<p>With k3d we can mount the host to container path, and with persistent volumes we can set a hostPath for our persistent volumes. With k3d, all the nodes will be using the same volume mapping which maps back to the host.</p>

<p>We will test the data persistence by writing a file inside a container, kill the pod, then exec into the pod again and test if the data persisted</p>

<h2>The k3d Cluster</h2>

<p>Create the directory on the host where we will persist the data:</p>

<pre><code>&gt; mkdir -p /tmp/k3dvol
</code></pre>

<p>Create the cluster:</p>

<pre><code>&gt; k3d create --name "k3d-cluster" --volume /tmp/k3dvol:/tmp/k3dvol --publish "80:80" --workers 2
&gt; export KUBECONFIG="$(k3d get-kubeconfig --name='k3d-cluster')"
</code></pre>

<p>Our application will be a busybox container which will keep running with a ping command, map the persistent volume to <code>/data</code> inside the pod.</p>

<p>Our <code>app.yml</code></p>

<pre><code>apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/tmp/k3dvol"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: echo
spec:
  selector:
    matchLabels:
      app: echo
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: echo
    spec:
      volumes:
        - name: task-pv-storage
          persistentVolumeClaim:
            claimName: task-pv-claim
      containers:
      - image: busybox
        name: echo
        volumeMounts:
          - mountPath: "/data"
            name: task-pv-storage
        command: ["ping", "127.0.0.1"]
</code></pre>

<p>Deploy the workload:</p>

<pre><code>&gt; kubectl apply -f app.yml
persistentvolume/task-pv-volume created
persistentvolumeclaim/task-pv-claim created
deployment.apps/echo created
</code></pre>

<p>View the persistent volumes:</p>

<pre><code>&gt; kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS   REASON   AGE
task-pv-volume                             1Gi        RWO            Retain           Bound    default/task-pv-claim    manual                  6s
</code></pre>

<p>View the Persistent Volume Claims:</p>

<pre><code>&gt; kubectl get pvc
NAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
task-pv-claim    Bound    task-pv-volume                             1Gi        RWO            manual         11s
</code></pre>

<p>View the pods:</p>

<pre><code>&gt; kubectl get pods
NAME                   READY   STATUS    RESTARTS   AGE
echo-58fd7d9b6-x4rxj   1/1     Running   0          16s
</code></pre>

<p>Exec into the pod:</p>

<pre><code>&gt; kubectl exec -it echo-58fd7d9b6-x4rxj sh
/ # df -h
Filesystem                Size      Used Available Use% Mounted on
overlay                  58.4G     36.1G     19.3G  65% /
osxfs                   233.6G    139.7G     86.3G  62% /data
/dev/sda1                58.4G     36.1G     19.3G  65% /etc/hosts
/dev/sda1                58.4G     36.1G     19.3G  65% /dev/termination-log
/dev/sda1                58.4G     36.1G     19.3G  65% /etc/hostname
/dev/sda1                58.4G     36.1G     19.3G  65% /etc/resolv.conf
</code></pre>

<p>Write the hostname of the current pod to the persistent volume path:</p>

<pre><code>/ # echo $(hostname)
echo-58fd7d9b6-x4rxj
/ # echo $(hostname) &gt; /data/hostname.txt
/ # exit
</code></pre>

<p>Exit the pod and read the content from the host (workstation/laptop):</p>

<pre><code>&gt; cat /tmp/k3dvol/hostname.txt
echo-58fd7d9b6-x4rxj
</code></pre>

<p>Look at the host where the pod is running on:</p>

<pre><code>&gt; kubectl get nodes -o wide
NAME                       STATUS   ROLES    AGE   VERSION        INTERNAL-IP    EXTERNAL-IP   OS-IMAGE   KERNEL-VERSION     CONTAINER-RUNTIME
k3d-k3d-cluster-server     Ready    master   13m   v1.17.2+k3s1   192.168.32.2   &lt;none&gt;        Unknown    4.9.184-linuxkit   containerd://1.3.3-k3s1
k3d-k3d-cluster-worker-1   Ready    &lt;none&gt;   13m   v1.17.2+k3s1   192.168.32.4   &lt;none&gt;        Unknown    4.9.184-linuxkit   containerd://1.3.3-k3s1
k3d-k3d-cluster-worker-0   Ready    &lt;none&gt;   13m   v1.17.2+k3s1   192.168.32.3   &lt;none&gt;        Unknown    4.9.184-linuxkit   containerd://1.3.3-k3s1
</code></pre>

<p>Delete the pod:</p>

<pre><code>&gt; kubectl delete pod/echo-58fd7d9b6-x4rxj
pod "echo-58fd7d9b6-x4rxj" deleted
</code></pre>

<p>Wait until the pod is rescheduled again and verify if the pod is running on a different node:</p>

<pre><code>&gt; kubectl get pods -o wide
NAME                   READY   STATUS    RESTARTS   AGE   IP          NODE                       NOMINATED NODE   READINESS GATES
echo-58fd7d9b6-fkvbs   1/1     Running   0          35s   10.42.2.9   k3d-k3d-cluster-worker-1   &lt;none&gt;           &lt;none&gt;
</code></pre>

<p>Exec into the new pod:</p>

<pre><code>&gt; kubectl exec -it echo-58fd7d9b6-fkvbs sh
</code></pre>

<p>View if the data is persisted:</p>

<pre><code>/ # hostname
echo-58fd7d9b6-fkvbs

/ # cat /data/hostname.txt
echo-58fd7d9b6-x4rxj
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Traefik Ingress for OpenFaas on Kubernetes (K3d)]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/02/17/traefik-ingress-for-openfaas-on-kubernetes-k3d/"/>
    <updated>2020-02-17T23:36:33+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/02/17/traefik-ingress-for-openfaas-on-kubernetes-k3d</id>
    <content type="html"><![CDATA[<p>In this post we will deploy <a href="https://www.openfaas.com/">OpenFaas</a> on Kubernetes locally using <a href="https://github.com/alexellis/k3sup">k3sup</a> and <a href="https://github.com/rancher/k3d">k3d</a>, then deploy a Traefik Ingress so that we can access the OpenFaas Gateway on HTTP over the standard port 80.</p>

<p>K3d is a amazing wrapper that deploys a k3s cluster on docker, and k3sup makes it very easy to provision OpenFaas to your Kubernetes cluster.</p>

<h2>Deploy a Kubernetes Cluster</h2>

<p>If you have not installed k3d, you can install k3d on mac with brew:</p>

<pre><code>$ brew install k3d
</code></pre>

<p>We will deploy our cluster with 2 worker nodes and publish port 80 to the containers port 80:</p>

<pre><code>$ k3d create --name="demo" --workers="2" --publish="80:80"
</code></pre>

<p>Point the kubeconfig to the location that k3d generated:</p>

<pre><code>$ export KUBECONFIG="$(k3d get-kubeconfig --name='demo')"
</code></pre>

<h2>Deploy OpenFaas</h2>

<p>First we need to get k3sup:</p>

<pre><code>$ curl -sLfS https://get.k3sup.dev | sudo sh
</code></pre>

<p>Once k3sup is installed, deploy OpenFaas to your cluster:</p>

<pre><code>$ k3sup app install openfaas
</code></pre>

<p>Give it a minute or so and check if everything is running:</p>

<pre><code>$ kubectl get pods -n openfaas
NAMESPACE     NAME                                 READY   STATUS      RESTARTS   AGE
openfaas      alertmanager-546f66b6c6-qtb69        1/1     Running     0          5m
openfaas      basic-auth-plugin-79b9878b7b-7vlln   1/1     Running     0          4m59s
openfaas      faas-idler-db8cd9c7d-8xfpp           1/1     Running     2          4m57s
openfaas      gateway-7dcc6d694d-dmvqn             2/2     Running     0          4m56s
openfaas      nats-d6d574749-rt9vw                 1/1     Running     0          4m56s
openfaas      prometheus-d99669d9b-mfxc8           1/1     Running     0          4m53s
openfaas      queue-worker-75f44b56b9-mhhbv        1/1     Running     0          4m52s
</code></pre>

<h2>Traefik Ingress</h2>

<p>In my scenario, I am using <code>openfaas.localdns.xyz</code> which resolves to <code>127.0.0.1</code>. Next we need to know to which service to route the traffic to, we can find that by:</p>

<pre><code>$ kubectl get svc/gateway -n openfaas
NAME      TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
gateway   ClusterIP   10.43.174.57   &lt;none&gt;        8080/TCP   23m
</code></pre>

<p>Below is our ingress.yml:</p>

<pre><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: openfaas-gateway-ingress
  namespace: openfaas
  annotations:
    kubernetes.io/ingress.class: traefik
spec:
  rules:
  - host: openfaas.localdns.xyz
    http:
      paths:
      - backend:
          serviceName: gateway
          servicePort: 8080
</code></pre>

<p>Apply the ingress:</p>

<pre><code>$ kubectl apply -f ingress.yml
ingress.extensions/openfaas-gateway-ingress created
</code></pre>

<p>We can the verify that our ingress is visible:</p>

<pre><code>$ kubectl get ingress -n openfaas
NAMESPACE   NAME                       HOSTS               ADDRESS      PORTS   AGE
openfaas    openfaas-gateway-ingress   openfaas.co.local   172.25.0.4   80      28s
</code></pre>

<h2>OpenFaas CLI</h2>

<p>Install the OpenFaas CLI:</p>

<pre><code>$ curl -SLsf https://cli.openfaas.com | sudo sh
</code></pre>

<p>Export the <code>OPENFAAS_URL</code> to our ingress endpoint and <code>OPENFAAS_PREFIX</code> for your dockerhub username:</p>

<pre><code>$ export OPENFAAS_URL=http://openfaas.localdns.xyz
$ export OPENFAAS_PREFIX=ruanbekker # change to your username
</code></pre>

<p>Get your credentials for the OpenFaas Gateway and login with the OpenFaas CLI:</p>

<pre><code>$ PASSWORD=$(kubectl get secret -n openfaas basic-auth -o jsonpath="{.data.basic-auth-password}" | base64 --decode; echo)
$ echo -n $PASSWORD | faas-cli login --username admin --password-stdin
</code></pre>

<h2>Deploy a Function</h2>

<p>Deploy the figlet function as an example:</p>

<pre><code>$ faas-cli store deploy figlet

Deployed. 202 Accepted.
URL: http://openfaas.localdns.xyz/function/figlet
</code></pre>

<p>Invoke the function:</p>

<pre><code>$ curl http://openfaas.localdns.xyz/function/figlet -d 'hello, world'
 _          _ _                             _     _
| |__   ___| | | ___    __      _____  _ __| | __| |
| '_ \ / _ \ | |/ _ \   \ \ /\ / / _ \| '__| |/ _` |
| | | |  __/ | | (_) |   \ V  V / (_) | |  | | (_| |
|_| |_|\___|_|_|\___( )   \_/\_/ \___/|_|  |_|\__,_|
                    |/
</code></pre>

<h2>Delete the Cluster</h2>

<p>Delete your k3d Kubernetes Cluster:</p>

<pre><code>$ k3d delete --name demo
</code></pre>

<h2>Thank You</h2>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Install OpenFaas on K3d Kubernetes]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/02/12/install-openfaas-on-k3d-kubernetes/"/>
    <updated>2020-02-12T00:57:47+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/02/12/install-openfaas-on-k3d-kubernetes</id>
    <content type="html"><![CDATA[<p>In this post we will deploy i<a href="https://www.openfaas.com">openfaas</a> on kubernetes (<a href="https://github.com/rancher/k3d">k3d</a>)</p>

<h2>Kubernetes on k3d</h2>

<p>k3d is a helper tool that provisions a kubernetes distribution, called k3s on docker. To deploy a kubernetes cluster on k3d, you can follow <a href="https://blog.ruanbekker.com/blog/2020/02/12/lightweight-development-kubernetes-options-k3d/">this blog post</a></p>

<h2>Deploy a 3 Node Kubernetes Cluster</h2>

<p>Using k3d, let&rsquo;s deploy a kubernetes cluster:</p>

<pre><code class="bash">$ k3d create --name="demo" --workers="2" --publish="80:80"
</code></pre>

<p>Export the kubeconfig:</p>

<pre><code class="bash">$ export KUBECONFIG="$(k3d get-kubeconfig --name='demo')"
</code></pre>

<p>Verify that you are able to communicate with your kubernetes cluster:</p>

<pre><code class="bash">$ kubectl get nodes
</code></pre>

<h2>Deploy OpenFaas</h2>

<p>First we need to get <a href="https://k3sup.dev">k3sup</a> :</p>

<pre><code class="bash">$ curl -sLfS https://get.k3sup.dev | sudo sh
</code></pre>

<p>Once k3sup is installed, deploy openfaas to your cluster:</p>

<pre><code class="bash">$ k3sup app install openfaas
</code></pre>

<p>Give it a minute or so and check if everything is running:</p>

<pre><code class="bash">$ kubectl get pods -n openfaas
NAMESPACE     NAME                                 READY   STATUS      RESTARTS   AGE
openfaas      alertmanager-546f66b6c6-qtb69        1/1     Running     0          5m
openfaas      basic-auth-plugin-79b9878b7b-7vlln   1/1     Running     0          4m59s
openfaas      faas-idler-db8cd9c7d-8xfpp           1/1     Running     2          4m57s
openfaas      gateway-7dcc6d694d-dmvqn             2/2     Running     0          4m56s
openfaas      nats-d6d574749-rt9vw                 1/1     Running     0          4m56s
openfaas      prometheus-d99669d9b-mfxc8           1/1     Running     0          4m53s
openfaas      queue-worker-75f44b56b9-mhhbv        1/1     Running     0          4m52s
</code></pre>

<p>Install the openfaas-cli:</p>

<pre><code class="bash">$ curl -SLsf https://cli.openfaas.com | sudo sh
</code></pre>

<p>In a screen session, forward port 8080 to the gateway service:</p>

<pre><code class="bash">$ screen -S portfwd-process -m -d sh -c "kubectl port-forward -n openfaas svc/gateway 8080:8080"
</code></pre>

<p>Expose the gateway password as an environment variable:</p>

<pre><code class="bash">$ PASSWORD=$(kubectl get secret -n openfaas basic-auth -o jsonpath="{.data.basic-auth-password}" | base64 --decode; echo)
</code></pre>

<p>Then login to the gateway:</p>

<pre><code class="bash">$ echo -n $PASSWORD | faas-cli login --username admin --password-stdin
</code></pre>

<h2>Deploy a OpenFaas Function</h2>

<p>To list all the functions:</p>

<pre><code class="bash">$ faas-cli store list
</code></pre>

<p>To deploy the figlet function:</p>

<pre><code class="bash">$ faas-cli store deploy figlet

Deployed. 202 Accepted.
URL: http://127.0.0.1:8080/function/figlet
</code></pre>

<p>List your deployed functions:</p>

<pre><code class="bash">$ faas-cli list
Function                        Invocations     Replicas
figlet                          0               1
</code></pre>

<p>Invoke your function:</p>

<pre><code class="bash">$ curl http://127.0.0.1:8080/function/figlet -d 'hello, world'
 _          _ _                             _     _
| |__   ___| | | ___    __      _____  _ __| | __| |
| '_ \ / _ \ | |/ _ \   \ \ /\ / / _ \| '__| |/ _` |
| | | |  __/ | | (_) |   \ V  V / (_) | |  | | (_| |
|_| |_|\___|_|_|\___( )   \_/\_/ \___/|_|  |_|\__,_|
                    |/
</code></pre>

<h2>Delete your Cluster</h2>

<p>When you are done, delete your kubernetes cluster:</p>

<pre><code class="bash">$ k3d delete --name demo
</code></pre>

<h2>Thank You</h2>

<p>Thank you for reading. If you like my content, feel free to visit me at <strong><a href="https://ruan.dev/">ruan.dev</a></strong> or follow me on twitter at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong></p>

<p><a href="https://twitter.com/ruanbekker"><img src="https://user-images.githubusercontent.com/567298/71188576-e2410f80-2289-11ea-8667-08f0c14ab7b5.png" alt="" /></a></p>

<p><a href="https://ko-fi.com/A6423ZIQ"><img src="https://www.ko-fi.com/img/githubbutton_sm.svg" alt="ko-fi" /></a></p>
]]></content>
  </entry>
  
</feed>
