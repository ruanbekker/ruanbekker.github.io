<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Kubernetes | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/kubernetes/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2023-01-17T10:24:03-05:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[KinD for Local Kubernetes Clusters]]></title>
    <link href="https://blog.ruanbekker.com/blog/2022/09/20/kind-for-local-kubernetes-clusters/"/>
    <updated>2022-09-20T02:18:16-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2022/09/20/kind-for-local-kubernetes-clusters</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/191189852-44f2fd39-7ad7-4d0a-a36b-c2889a838649.png" alt="kubernetes-kind" /></p>

<p>In this tutorial we will demonstrate how to use KinD (Kubernetes in Docker) to provision local kubernetes clusters for local development.</p>

<h2>About</h2>

<p>KinD uses container images to run as &ldquo;nodes&rdquo;, so spinning up and tearing down clusters becomes really easy or running multiple or different versions, is as easy as pointing to a different container image.</p>

<p>Configuration such as node count, ports, volumes, image versions can either be controlled via the command line or via configuration, more information on that can be found on their documentation:</p>

<ul>
<li><a href="https://kind.sigs.k8s.io/docs/user/quick-start/">https://kind.sigs.k8s.io/docs/user/quick-start/</a></li>
<li><a href="https://kind.sigs.k8s.io/docs/user/configuration/">https://kind.sigs.k8s.io/docs/user/configuration/</a></li>
</ul>


<h2>Installation</h2>

<p>Follow the <a href="https://kind.sigs.k8s.io/docs/user/quick-start/#installing-with-a-package-manager">docs</a> for more information, but for mac:</p>

<pre><code class="bash">brew install kind
</code></pre>

<p>To verify if kind was installed, you can run:</p>

<pre><code class="bash">kind version
</code></pre>

<h2>Create a Cluster</h2>

<p>Create the cluster with command line arguments, such as cluster name, the container image:</p>

<pre><code class="bash">kind create cluster --name cluster-1 --image kindest/node:v1.24.0
</code></pre>

<p>And the output will look something like this:</p>

<pre><code class="bash">Creating cluster "cluster-1" ...
 ‚úì Ensuring node image (kindest/node:v1.24.0) üñº
 ‚úì Preparing nodes üì¶
 ‚úì Writing configuration üìú
 ‚úì Starting control-plane üïπÔ∏è
 ‚úì Installing CNI üîå
 ‚úì Installing StorageClass üíæ
Set kubectl context to "kind-cluster-1"
You can now use your cluster with:

kubectl cluster-info --context kind-cluster-1

Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community üôÇ
</code></pre>

<p>I <strong>highly recommend</strong> installing <a href="https://github.com/ahmetb/kubectx">kubectx</a>, which makes it easy to switch between kubernetes contexts.</p>

<h2>Create a Cluster with Config</h2>

<p>If you would like to define your cluster configuration as config, you can create a file <code>default-config.yaml</code> with the following as a 2 node cluster, and specifying version 1.24.0:</p>

<pre><code class="yaml">---
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  image: kindest/node:v1.24.0@sha256:0866296e693efe1fed79d5e6c7af8df71fc73ae45e3679af05342239cdc5bc8e
- role: worker
  image: kindest/node:v1.24.0@sha256:0866296e693efe1fed79d5e6c7af8df71fc73ae45e3679af05342239cdc5bc8e
</code></pre>

<p>Then create the cluster and point the config:</p>

<pre><code class="bash">kind create cluster --name kind-cluster --config default-config.yaml
</code></pre>

<h2>Interact with the Cluster</h2>

<p>View the cluster info:</p>

<pre><code class="bash">kubectl cluster-info --context kind-kind-cluster
</code></pre>

<p>View cluster contexts:</p>

<pre><code class="bash">kubectl config get-contexts
</code></pre>

<p>Use context:</p>

<pre><code class="bash">kubectl config use-context kind-kind-cluster
</code></pre>

<p>View nodes:</p>

<pre><code class="bash">kubectl get nodes -o wide

NAME                         STATUS   ROLES           AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION      CONTAINER-RUNTIME
kind-cluster-control-plane   Ready    control-plane   2m11s   v1.24.0   172.20.0.5    &lt;none&gt;        Ubuntu 21.10   5.10.104-linuxkit   containerd://1.6.4
kind-cluster-worker          Ready    &lt;none&gt;          108s    v1.24.0   172.20.0.4    &lt;none&gt;        Ubuntu 21.10   5.10.104-linuxkit   containerd://1.6.4
</code></pre>

<h2>Deploy Sample Application</h2>

<p>We will create a deployment, a service and port-forward to our service to access our application. You can also specify port configuration to your cluster so that you don&rsquo;t need to port-forward, which you can find in their <a href="https://kind.sigs.k8s.io/docs/user/configuration/#extra-port-mappings">port mappings documentation</a></p>

<p>I will be using the following commands to generate the manifests, but will also add them to this post:</p>

<pre><code class="bash">kubectl create deployment hostname --namespace default --replicas 2 --image ruanbekker/containers:hostname --port 8080 --dry-run=client -o yaml &gt; hostname-deployment.yaml
kubectl expose deployment hostname --namespace default --port=80 --target-port=8080 --name=hostname-http --dry-run=client -o yaml &gt; hostname-service.yaml
</code></pre>

<p>The manifest:</p>

<pre><code class="yaml">---
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: hostname
  name: hostname
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: hostname
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: hostname
    spec:
      containers:
      - image: ruanbekker/containers:hostname
        name: containers
        ports:
        - containerPort: 8080
        resources: {}
status: {}
---
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: hostname
  name: hostname-http
  namespace: default
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: hostname
status:
  loadBalancer: {}
</code></pre>

<p>Then apply them with:</p>

<pre><code class="bash">kubectl apply -f &lt;name-of-manifest&gt;.yaml
</code></pre>

<p>Or if you used kubectl to create them:</p>

<pre><code class="bash">kubectl apply -f hostname-deployment.yaml
kubectl apply -f hostname-service.yaml
</code></pre>

<p>You can then view your resources with:</p>

<pre><code class="bash">kubectl get deployment,pod,service

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/hostname   2/2     2            2           9m27s

NAME                            READY   STATUS    RESTARTS   AGE
pod/hostname-7ff58c5644-67vhq   1/1     Running   0          9m27s
pod/hostname-7ff58c5644-wjjbw   1/1     Running   0          9m27s

NAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
service/hostname-http   ClusterIP   10.96.218.58   &lt;none&gt;        80/TCP    5m48s
service/kubernetes      ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP   24m
</code></pre>

<p>Port forward to your service:</p>

<pre><code class="bash">kubectl port-forward svc/hostname-http 8080:80
</code></pre>

<p>Then access your application:</p>

<pre><code class="bash">curl http://localhost:8080/

Hostname: hostname-7ff58c5644-wjjbw
</code></pre>

<h2>Delete Kind Cluster</h2>

<p>View the clusters:</p>

<pre><code class="bash">kind get clusters
</code></pre>

<p>Delete a cluster:</p>

<pre><code class="bash">kind delete cluster --name kind-cluster
</code></pre>

<h2>Extras</h2>

<p>I highly recommend using <code>kubectx</code> to switch contexts and <code>kubens</code> to set the default namespace, and aliases:</p>

<pre><code class="bash">alias k=kubectl
alias kx=kubectx
alias kns=kubens
</code></pre>

<h2>Thank You</h2>

<p>Thanks for reading, feel free to check out my <a href="https://ruan.dev/">website</a>, feel free to subscribe to my <a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a> or follow me at <a href="https://twitter.com/ruanbekker">@ruanbekker</a> on Twitter.</p>

<ul>
<li>Linktree: <a href="https://go.ruan.dev/links">https://go.ruan.dev/links</a></li>
<li>Patreon: <a href="https://go.ruan.dev/patreon">https://go.ruan.dev/patreon</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Persistent Volumes With K3d Kubernetes]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/02/21/persistent-volumes-with-k3d-kubernetes/"/>
    <updated>2020-02-21T00:07:48+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/02/21/persistent-volumes-with-k3d-kubernetes</id>
    <content type="html"><![CDATA[<p>With k3d we can mount the host to container path, and with persistent volumes we can set a hostPath for our persistent volumes. With k3d, all the nodes will be using the same volume mapping which maps back to the host.</p>

<p>We will test the data persistence by writing a file inside a container, kill the pod, then exec into the pod again and test if the data persisted</p>

<h2>The k3d Cluster</h2>

<p>Create the directory on the host where we will persist the data:</p>

<pre><code>&gt; mkdir -p /tmp/k3dvol
</code></pre>

<p>Create the cluster:</p>

<pre><code>&gt; k3d create --name "k3d-cluster" --volume /tmp/k3dvol:/tmp/k3dvol --publish "80:80" --workers 2
&gt; export KUBECONFIG="$(k3d get-kubeconfig --name='k3d-cluster')"
</code></pre>

<p>Our application will be a busybox container which will keep running with a ping command, map the persistent volume to <code>/data</code> inside the pod.</p>

<p>Our <code>app.yml</code></p>

<pre><code>apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/tmp/k3dvol"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: echo
spec:
  selector:
    matchLabels:
      app: echo
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: echo
    spec:
      volumes:
        - name: task-pv-storage
          persistentVolumeClaim:
            claimName: task-pv-claim
      containers:
      - image: busybox
        name: echo
        volumeMounts:
          - mountPath: "/data"
            name: task-pv-storage
        command: ["ping", "127.0.0.1"]
</code></pre>

<p>Deploy the workload:</p>

<pre><code>&gt; kubectl apply -f app.yml
persistentvolume/task-pv-volume created
persistentvolumeclaim/task-pv-claim created
deployment.apps/echo created
</code></pre>

<p>View the persistent volumes:</p>

<pre><code>&gt; kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS   REASON   AGE
task-pv-volume                             1Gi        RWO            Retain           Bound    default/task-pv-claim    manual                  6s
</code></pre>

<p>View the Persistent Volume Claims:</p>

<pre><code>&gt; kubectl get pvc
NAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
task-pv-claim    Bound    task-pv-volume                             1Gi        RWO            manual         11s
</code></pre>

<p>View the pods:</p>

<pre><code>&gt; kubectl get pods
NAME                   READY   STATUS    RESTARTS   AGE
echo-58fd7d9b6-x4rxj   1/1     Running   0          16s
</code></pre>

<p>Exec into the pod:</p>

<pre><code>&gt; kubectl exec -it echo-58fd7d9b6-x4rxj sh
/ # df -h
Filesystem                Size      Used Available Use% Mounted on
overlay                  58.4G     36.1G     19.3G  65% /
osxfs                   233.6G    139.7G     86.3G  62% /data
/dev/sda1                58.4G     36.1G     19.3G  65% /etc/hosts
/dev/sda1                58.4G     36.1G     19.3G  65% /dev/termination-log
/dev/sda1                58.4G     36.1G     19.3G  65% /etc/hostname
/dev/sda1                58.4G     36.1G     19.3G  65% /etc/resolv.conf
</code></pre>

<p>Write the hostname of the current pod to the persistent volume path:</p>

<pre><code>/ # echo $(hostname)
echo-58fd7d9b6-x4rxj
/ # echo $(hostname) &gt; /data/hostname.txt
/ # exit
</code></pre>

<p>Exit the pod and read the content from the host (workstation/laptop):</p>

<pre><code>&gt; cat /tmp/k3dvol/hostname.txt
echo-58fd7d9b6-x4rxj
</code></pre>

<p>Look at the host where the pod is running on:</p>

<pre><code>&gt; kubectl get nodes -o wide
NAME                       STATUS   ROLES    AGE   VERSION        INTERNAL-IP    EXTERNAL-IP   OS-IMAGE   KERNEL-VERSION     CONTAINER-RUNTIME
k3d-k3d-cluster-server     Ready    master   13m   v1.17.2+k3s1   192.168.32.2   &lt;none&gt;        Unknown    4.9.184-linuxkit   containerd://1.3.3-k3s1
k3d-k3d-cluster-worker-1   Ready    &lt;none&gt;   13m   v1.17.2+k3s1   192.168.32.4   &lt;none&gt;        Unknown    4.9.184-linuxkit   containerd://1.3.3-k3s1
k3d-k3d-cluster-worker-0   Ready    &lt;none&gt;   13m   v1.17.2+k3s1   192.168.32.3   &lt;none&gt;        Unknown    4.9.184-linuxkit   containerd://1.3.3-k3s1
</code></pre>

<p>Delete the pod:</p>

<pre><code>&gt; kubectl delete pod/echo-58fd7d9b6-x4rxj
pod "echo-58fd7d9b6-x4rxj" deleted
</code></pre>

<p>Wait until the pod is rescheduled again and verify if the pod is running on a different node:</p>

<pre><code>&gt; kubectl get pods -o wide
NAME                   READY   STATUS    RESTARTS   AGE   IP          NODE                       NOMINATED NODE   READINESS GATES
echo-58fd7d9b6-fkvbs   1/1     Running   0          35s   10.42.2.9   k3d-k3d-cluster-worker-1   &lt;none&gt;           &lt;none&gt;
</code></pre>

<p>Exec into the new pod:</p>

<pre><code>&gt; kubectl exec -it echo-58fd7d9b6-fkvbs sh
</code></pre>

<p>View if the data is persisted:</p>

<pre><code>/ # hostname
echo-58fd7d9b6-fkvbs

/ # cat /data/hostname.txt
echo-58fd7d9b6-x4rxj
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Traefik Ingress for OpenFaas on Kubernetes (K3d)]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/02/17/traefik-ingress-for-openfaas-on-kubernetes-k3d/"/>
    <updated>2020-02-17T23:36:33+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/02/17/traefik-ingress-for-openfaas-on-kubernetes-k3d</id>
    <content type="html"><![CDATA[<p>In this post we will deploy <a href="https://www.openfaas.com/">OpenFaas</a> on Kubernetes locally using <a href="https://github.com/alexellis/k3sup">k3sup</a> and <a href="https://github.com/rancher/k3d">k3d</a>, then deploy a Traefik Ingress so that we can access the OpenFaas Gateway on HTTP over the standard port 80.</p>

<p>K3d is a amazing wrapper that deploys a k3s cluster on docker, and k3sup makes it very easy to provision OpenFaas to your Kubernetes cluster.</p>

<h2>Deploy a Kubernetes Cluster</h2>

<p>If you have not installed k3d, you can install k3d on mac with brew:</p>

<pre><code>$ brew install k3d
</code></pre>

<p>We will deploy our cluster with 2 worker nodes and publish port 80 to the containers port 80:</p>

<pre><code>$ k3d create --name="demo" --workers="2" --publish="80:80"
</code></pre>

<p>Point the kubeconfig to the location that k3d generated:</p>

<pre><code>$ export KUBECONFIG="$(k3d get-kubeconfig --name='demo')"
</code></pre>

<h2>Deploy OpenFaas</h2>

<p>First we need to get k3sup:</p>

<pre><code>$ curl -sLfS https://get.k3sup.dev | sudo sh
</code></pre>

<p>Once k3sup is installed, deploy OpenFaas to your cluster:</p>

<pre><code>$ k3sup app install openfaas
</code></pre>

<p>Give it a minute or so and check if everything is running:</p>

<pre><code>$ kubectl get pods -n openfaas
NAMESPACE     NAME                                 READY   STATUS      RESTARTS   AGE
openfaas      alertmanager-546f66b6c6-qtb69        1/1     Running     0          5m
openfaas      basic-auth-plugin-79b9878b7b-7vlln   1/1     Running     0          4m59s
openfaas      faas-idler-db8cd9c7d-8xfpp           1/1     Running     2          4m57s
openfaas      gateway-7dcc6d694d-dmvqn             2/2     Running     0          4m56s
openfaas      nats-d6d574749-rt9vw                 1/1     Running     0          4m56s
openfaas      prometheus-d99669d9b-mfxc8           1/1     Running     0          4m53s
openfaas      queue-worker-75f44b56b9-mhhbv        1/1     Running     0          4m52s
</code></pre>

<h2>Traefik Ingress</h2>

<p>In my scenario, I am using <code>openfaas.localdns.xyz</code> which resolves to <code>127.0.0.1</code>. Next we need to know to which service to route the traffic to, we can find that by:</p>

<pre><code>$ kubectl get svc/gateway -n openfaas
NAME      TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
gateway   ClusterIP   10.43.174.57   &lt;none&gt;        8080/TCP   23m
</code></pre>

<p>Below is our ingress.yml:</p>

<pre><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: openfaas-gateway-ingress
  namespace: openfaas
  annotations:
    kubernetes.io/ingress.class: traefik
spec:
  rules:
  - host: openfaas.localdns.xyz
    http:
      paths:
      - backend:
          serviceName: gateway
          servicePort: 8080
</code></pre>

<p>Apply the ingress:</p>

<pre><code>$ kubectl apply -f ingress.yml
ingress.extensions/openfaas-gateway-ingress created
</code></pre>

<p>We can the verify that our ingress is visible:</p>

<pre><code>$ kubectl get ingress -n openfaas
NAMESPACE   NAME                       HOSTS               ADDRESS      PORTS   AGE
openfaas    openfaas-gateway-ingress   openfaas.co.local   172.25.0.4   80      28s
</code></pre>

<h2>OpenFaas CLI</h2>

<p>Install the OpenFaas CLI:</p>

<pre><code>$ curl -SLsf https://cli.openfaas.com | sudo sh
</code></pre>

<p>Export the <code>OPENFAAS_URL</code> to our ingress endpoint and <code>OPENFAAS_PREFIX</code> for your dockerhub username:</p>

<pre><code>$ export OPENFAAS_URL=http://openfaas.localdns.xyz
$ export OPENFAAS_PREFIX=ruanbekker # change to your username
</code></pre>

<p>Get your credentials for the OpenFaas Gateway and login with the OpenFaas CLI:</p>

<pre><code>$ PASSWORD=$(kubectl get secret -n openfaas basic-auth -o jsonpath="{.data.basic-auth-password}" | base64 --decode; echo)
$ echo -n $PASSWORD | faas-cli login --username admin --password-stdin
</code></pre>

<h2>Deploy a Function</h2>

<p>Deploy the figlet function as an example:</p>

<pre><code>$ faas-cli store deploy figlet

Deployed. 202 Accepted.
URL: http://openfaas.localdns.xyz/function/figlet
</code></pre>

<p>Invoke the function:</p>

<pre><code>$ curl http://openfaas.localdns.xyz/function/figlet -d 'hello, world'
 _          _ _                             _     _
| |__   ___| | | ___    __      _____  _ __| | __| |
| '_ \ / _ \ | |/ _ \   \ \ /\ / / _ \| '__| |/ _` |
| | | |  __/ | | (_) |   \ V  V / (_) | |  | | (_| |
|_| |_|\___|_|_|\___( )   \_/\_/ \___/|_|  |_|\__,_|
                    |/
</code></pre>

<h2>Delete the Cluster</h2>

<p>Delete your k3d Kubernetes Cluster:</p>

<pre><code>$ k3d delete --name demo
</code></pre>

<h2>Thank You</h2>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Install OpenFaas on K3d Kubernetes]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/02/12/install-openfaas-on-k3d-kubernetes/"/>
    <updated>2020-02-12T00:57:47+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/02/12/install-openfaas-on-k3d-kubernetes</id>
    <content type="html"><![CDATA[<p>In this post we will deploy i<a href="https://www.openfaas.com">openfaas</a> on kubernetes (<a href="https://github.com/rancher/k3d">k3d</a>)</p>

<h2>Kubernetes on k3d</h2>

<p>k3d is a helper tool that provisions a kubernetes distribution, called k3s on docker. To deploy a kubernetes cluster on k3d, you can follow <a href="https://blog.ruanbekker.com/blog/2020/02/12/lightweight-development-kubernetes-options-k3d/">this blog post</a></p>

<h2>Deploy a 3 Node Kubernetes Cluster</h2>

<p>Using k3d, let&rsquo;s deploy a kubernetes cluster:</p>

<pre><code class="bash">$ k3d create --name="demo" --workers="2" --publish="80:80"
</code></pre>

<p>Export the kubeconfig:</p>

<pre><code class="bash">$ export KUBECONFIG="$(k3d get-kubeconfig --name='demo')"
</code></pre>

<p>Verify that you are able to communicate with your kubernetes cluster:</p>

<pre><code class="bash">$ kubectl get nodes
</code></pre>

<h2>Deploy OpenFaas</h2>

<p>First we need to get <a href="https://k3sup.dev">k3sup</a> :</p>

<pre><code class="bash">$ curl -sLfS https://get.k3sup.dev | sudo sh
</code></pre>

<p>Once k3sup is installed, deploy openfaas to your cluster:</p>

<pre><code class="bash">$ k3sup app install openfaas
</code></pre>

<p>Give it a minute or so and check if everything is running:</p>

<pre><code class="bash">$ kubectl get pods -n openfaas
NAMESPACE     NAME                                 READY   STATUS      RESTARTS   AGE
openfaas      alertmanager-546f66b6c6-qtb69        1/1     Running     0          5m
openfaas      basic-auth-plugin-79b9878b7b-7vlln   1/1     Running     0          4m59s
openfaas      faas-idler-db8cd9c7d-8xfpp           1/1     Running     2          4m57s
openfaas      gateway-7dcc6d694d-dmvqn             2/2     Running     0          4m56s
openfaas      nats-d6d574749-rt9vw                 1/1     Running     0          4m56s
openfaas      prometheus-d99669d9b-mfxc8           1/1     Running     0          4m53s
openfaas      queue-worker-75f44b56b9-mhhbv        1/1     Running     0          4m52s
</code></pre>

<p>Install the openfaas-cli:</p>

<pre><code class="bash">$ curl -SLsf https://cli.openfaas.com | sudo sh
</code></pre>

<p>In a screen session, forward port 8080 to the gateway service:</p>

<pre><code class="bash">$ screen -S portfwd-process -m -d sh -c "kubectl port-forward -n openfaas svc/gateway 8080:8080"
</code></pre>

<p>Expose the gateway password as an environment variable:</p>

<pre><code class="bash">$ PASSWORD=$(kubectl get secret -n openfaas basic-auth -o jsonpath="{.data.basic-auth-password}" | base64 --decode; echo)
</code></pre>

<p>Then login to the gateway:</p>

<pre><code class="bash">$ echo -n $PASSWORD | faas-cli login --username admin --password-stdin
</code></pre>

<h2>Deploy a OpenFaas Function</h2>

<p>To list all the functions:</p>

<pre><code class="bash">$ faas-cli store list
</code></pre>

<p>To deploy the figlet function:</p>

<pre><code class="bash">$ faas-cli store deploy figlet

Deployed. 202 Accepted.
URL: http://127.0.0.1:8080/function/figlet
</code></pre>

<p>List your deployed functions:</p>

<pre><code class="bash">$ faas-cli list
Function                        Invocations     Replicas
figlet                          0               1
</code></pre>

<p>Invoke your function:</p>

<pre><code class="bash">$ curl http://127.0.0.1:8080/function/figlet -d 'hello, world'
 _          _ _                             _     _
| |__   ___| | | ___    __      _____  _ __| | __| |
| '_ \ / _ \ | |/ _ \   \ \ /\ / / _ \| '__| |/ _` |
| | | |  __/ | | (_) |   \ V  V / (_) | |  | | (_| |
|_| |_|\___|_|_|\___( )   \_/\_/ \___/|_|  |_|\__,_|
                    |/
</code></pre>

<h2>Delete your Cluster</h2>

<p>When you are done, delete your kubernetes cluster:</p>

<pre><code class="bash">$ k3d delete --name demo
</code></pre>

<h2>Thank You</h2>

<p>Thank you for reading. If you like my content, feel free to visit me at <strong><a href="https://ruan.dev/">ruan.dev</a></strong> or follow me on twitter at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong></p>

<p><a href="https://twitter.com/ruanbekker"><img src="https://user-images.githubusercontent.com/567298/71188576-e2410f80-2289-11ea-8667-08f0c14ab7b5.png" alt="" /></a></p>

<p><a href="https://ko-fi.com/A6423ZIQ"><img src="https://www.ko-fi.com/img/githubbutton_sm.svg" alt="ko-fi" /></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Lightweight Development Kubernetes Options: K3d]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/02/12/lightweight-development-kubernetes-options-k3d/"/>
    <updated>2020-02-12T00:27:00+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/02/12/lightweight-development-kubernetes-options-k3d</id>
    <content type="html"><![CDATA[<p>In this post we will cover a lightweight development kubernetes called, &ldquo;<a href="https://github.com/rancher/k3d">k3d</a>&rdquo; which we will deploy on a mac.</p>

<h2>What is k3d</h2>

<p><a href="https://github.com/rancher/k3d">k3d</a> is a binary that provisions a <a href="https://github.com/rancher/k3s">k3s</a> kubernetes cluster on docker</p>

<h2>Pre-Requirements</h2>

<p>You will require docker and we will be using brew to install k3d on a mac.</p>

<h2>Install k3d</h2>

<p>Installing k3d is as easy as:</p>

<pre><code class="bash">$ brew install k3d
</code></pre>

<p>Verify your installation:</p>

<pre><code>$ k3d --version
k3d version v1.3.1
</code></pre>

<h2>Deploy a 3 Node Cluster</h2>

<p>Using k3d, we will deploy a 3 node k3s cluster:</p>

<pre><code class="bash">$ k3d create --name="demo" --workers="2" --publish="80:80"
</code></pre>

<p>This will deploy a master and 2 worker nodes and we will also publish our host port 80 to our container port 80 (k3s comes default with traefik)</p>

<p>Set your kubeconfig:</p>

<pre><code class="bash">$ export KUBECONFIG="$(k3d get-kubeconfig --name='demo')"
</code></pre>

<p>Test it out by listing your nodes:</p>

<pre><code class="bash">$ kubectl get nodes
NAME                STATUS   ROLES    AGE    VERSION
k3d-demo-server     Ready    master   102s   v1.14.6-k3s.1
k3d-demo-worker-0   Ready    worker   102s   v1.14.6-k3s.1
k3d-demo-worker-1   Ready    worker   102s   v1.14.6-k3s.1
</code></pre>

<p>That was easy right?</p>

<h2>Deploy a Sample App</h2>

<p>We will deploy a simple golang web application that will return the container name upon a http request. We will also make use of the traefik ingress for demonstration.</p>

<p>Our deployment manifest that I will save as <code>app.yml</code>:</p>

<pre><code class="yaml">apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: k3s-demo
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: k3d-demo
  template:
    metadata:
      labels:
        app: k3d-demo
    spec:
      containers:
      - name: k3d-demo
        image: ruanbekker/hostname:latest
---
apiVersion: v1
kind: Service
metadata:
  name: k3d-demo
  namespace: default
spec:
  ports:
  - name: http
    targetPort: 8000
    port: 80
  selector:
    app: k3d-demo
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: k3d-demo
  annotations:
    kubernetes.io/ingress.class: "traefik"

spec:
  rules:
  - host: k3d-demo.example.org
    http:
      paths:
      - path: /
        backend:
          serviceName: k3d-demo
          servicePort: http
</code></pre>

<p>Deploy our application:</p>

<pre><code class="bash">$ kubectl apply -f app.yml
deployment.extensions/k3s-demo created
service/k3d-demo created
ingress.extensions/k3d-demo created
</code></pre>

<p>Verify that the pods are running:</p>

<pre><code class="bash">$ kubectl get pods
NAME                       READY   STATUS    RESTARTS   AGE
k3s-demo-f76d866b9-dv5z9   1/1     Running   0          10s
k3s-demo-f76d866b9-qxltk   1/1     Running   0          10s
</code></pre>

<p>Make a http request:</p>

<pre><code class="bash">$ curl -H "Host: k3d-demo.example.org" http://localhost
Hostname: k3d-demo-f76d866b9-qxltk
</code></pre>

<h2>Deleting your Cluster</h2>

<p>To delete your cluster:</p>

<pre><code class="bash">$ k3d delete --name demo
</code></pre>

<h2>Thank You</h2>

<p>Thank you for reading. If you like my content, feel free to visit me at <strong><a href="https://ruan.dev/">ruan.dev</a></strong> or follow me on twitter at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong></p>

<p><a href="https://twitter.com/ruanbekker"><img src="https://user-images.githubusercontent.com/567298/71188576-e2410f80-2289-11ea-8667-08f0c14ab7b5.png" alt="" /></a></p>

<p><a href="https://ko-fi.com/A6423ZIQ"><img src="https://www.ko-fi.com/img/githubbutton_sm.svg" alt="ko-fi" /></a></p>
]]></content>
  </entry>
  
</feed>
