<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Logstash | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/logstash/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2023-01-29T12:50:43-05:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Nginx Analysis Dashboard Using Grafana and Elasticsearch]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/04/28/nginx-analysis-dashboard-using-grafana-and-elasticsearch/"/>
    <updated>2020-04-28T20:07:22+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/04/28/nginx-analysis-dashboard-using-grafana-and-elasticsearch</id>
    <content type="html"><![CDATA[<p>In this post we will be setting up a <strong>analytical dashboard</strong> using <strong>grafana</strong> to visualize our <strong>nginx access logs</strong>.</p>

<p><img width="1212" alt="grafana-nginx-elasticsearch-prometheus" src="https://user-images.githubusercontent.com/567298/80539136-48ac0c00-89a7-11ea-869d-597da4fa4d92.png"></p>

<p>In this tutorial I will be using my other blog <code>sysadmins.co.za</code> which is being served on nginx. We will also be setting up the other components such as filebeat, logstash, elasticsearch and redis, which require if you would like to follow along.</p>

<h2>The End Result</h2>

<p>We will be able to analyze our Nginx Access logs to answer questions such as:</p>

<ul>
<li>Whats the Top 10 Countries accessing your website in the last 24 hours</li>
<li>Who&rsquo;s the Top 10 Referers?</li>
<li>Whats the most popular page for the past 24 hours?</li>
<li>How does the percentage of 200&rsquo;s vs 404&rsquo;s look like?</li>
<li>Ability to view results based on status code</li>
<li>Everyone loves a World Map to view hotspots</li>
</ul>


<p>At the end of the tutorial, your dashboard will look similar to this:</p>

<p><img width="1123" alt="grafana-elasticsearch-nginx-dashboard" src="https://user-images.githubusercontent.com/567298/80523974-32925180-898f-11ea-96b6-e8e559655745.png"></p>

<h2>High Level Overview</h2>

<p>Our infrastructure will require Nginx with Filebeat, Redis, Logstash, Elasticsearch and Grafana and will look like this:</p>

<p><img width="871" alt="grafana-elasticsearch-logs-setup" src="https://user-images.githubusercontent.com/567298/80526020-6d49b900-8992-11ea-9a39-67331ccc3808.png"></p>

<p>I will drill down how everything is connected:</p>

<ol>
<li>Nginx has a custom <code>log_format</code> that we define, that will write to <code>/var/log/nginx/access_json.log</code>, which will be picked up by <strong>Filebeat</strong> as a input.</li>
<li>and <strong>Filebeat</strong> has an output that pushes the data to <strong>Redis</strong></li>
<li><strong>Logstash</strong> is configured with <strong>Redis</strong> as an input with configured filter section to transform the data and outputs to <strong>Elasticsearch</strong></li>
<li>From <strong>Grafana</strong> we have a configured <strong>Elasticsearch</strong> datasource</li>
<li>Use the grafana template to build this awesome dashboard on Grafana</li>
</ol>


<p>But first, a massive thank you to <a href="https://www.akiraka.net">akiraka</a> for templatizing this dashboard and made it available on <a href="https://grafana.com/orgs/akiraka">grafana</a></p>

<h2>Let&rsquo;s build all the things</h2>

<p>I will be using LXD to run my system/server containers (running ubuntu 18), but you can use a vps, cloud instance, multipass, virtualbox, or anything to host your servers that we will be deploying redis, logstash, etc.</p>

<p>Servers provisioned for this setup:</p>

<ul>
<li>Nginx</li>
<li>Redis</li>
<li>Logstash</li>
<li>Elasticsearch</li>
<li>Grafana</li>
<li>Prometheus</li>
</ul>


<h2>Elasticsearch</h2>

<p>If you don&rsquo;t have a cluster running already, you can follow <strong><a href="https://blog.ruanbekker.com/blog/2019/04/02/setup-a-5-node-highly-available-elasticsearch-cluster/">this tutorial</a></strong> which will help you deploy a HA Elasticsearch Cluster, or if you prefer docker, you can follow <strong><a href="https://blog.ruanbekker.com/blog/2018/04/29/running-a-3-node-elasticsearch-cluster-with-docker-compose-on-your-laptop-for-testing/">this tutorial</a></strong></p>

<h2>Redis</h2>

<p>For our in-memory data store, I will be securing my redis installation with a password as well.</p>

<p>Install redis:</p>

<pre><code>$ apt update &amp;&amp; apt install redis-server -y
</code></pre>

<p>Generate a password:</p>

<pre><code>$ openssl rand -base64 36
9V5YlWvm8WuC4n1KZLYUEbLruLJLNJEnDzhu4WnAIfgxMmlv
</code></pre>

<p>In your redis config <code>/etc/redis/redis.conf</code>, you need to change the following:</p>

<pre><code>...
bind 0.0.0.0
port 6379
daemonize yes
supervised systemd
requirepass 9V5YlWvm8WuC4n1KZLYUEbLruLJLNJEnDzhu4WnAIfgxMmlv
...
</code></pre>

<p>Restart redis to activate your changes:</p>

<pre><code>$ systemctl restart redis.service
</code></pre>

<p>and then set and get a key using your password:</p>

<pre><code>$ redis-cli -a "9V5YlWvm8WuC4n1KZLYUEbLruLJLNJEnDzhu4WnAIfgxMmlv" set test ok
$ redis-cli -a "9V5YlWvm8WuC4n1KZLYUEbLruLJLNJEnDzhu4WnAIfgxMmlv" get test
ok
</code></pre>

<h2>Logstash</h2>

<p>On the logstash server, install the requirements:</p>

<pre><code>$ apt update &amp;&amp; apt install wget apt-transport-https default-jre -y
$ wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | apt-key add -
$ echo "deb https://artifacts.elastic.co/packages/7.x/apt stable main" | tee -a /etc/apt/sources.list.d/elastic-7.x.list
</code></pre>

<p>Now the repository for elastic is setup now we need to update and install logstash:</p>

<pre><code>$ apt update &amp;&amp; apt install logstash -y
</code></pre>

<p>Once logstash is installed, we need to provide logstash with a configuration, in our scenario we will have a input for redis, a filter section to transform and output as elasticsearch.</p>

<p>Just make sure of the following:</p>

<ul>
<li>Populate the connection details of redis (we will define the key in filebeat later)</li>
<li>Ensure that <code>GeoLite2-City.mmdb</code> is in the path that I have under filter</li>
<li>Populate the connectiond details of Elasticsearch and choose a suitable index name, we will need to provide that index name in Grafana later</li>
</ul>


<p>Create the config: <code>/etc/logstash/conf.d/logs.conf</code> and my config will look like the following. (<a href="https://grafana.com/grafana/dashboards/11190">config source</a>)</p>

<pre><code>input {
  redis {
    data_type =&gt;"list"
    key =&gt;"nginx_logs"
    host =&gt;"10.47.127.37"
    port =&gt; 6379
    password =&gt; "9V5YlWvm8WuC4n1KZLYUEbLruLJLNJEnDzhu4WnAIfgxMmlv"
    db =&gt; 0
  }
}

filter {
  geoip {
    target =&gt; "geoip"
    source =&gt; "client_ip"
    database =&gt; "/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-filter-geoip-6.0.3-java/vendor/GeoLite2-City.mmdb"
    add_field =&gt; [ "[geoip][coordinates]", "%{[geoip][longitude]}" ]
    add_field =&gt; [ "[geoip][coordinates]", "%{[geoip][latitude]}" ]
    remove_field =&gt; ["[geoip][latitude]", "[geoip][longitude]", "[geoip][country_code]", "[geoip][country_code2]", "[geoip][country_code3]", "[geoip][timezone]", "[geoip][continent_code]", "[geoip][region_code]"]
  }
  mutate {
    convert =&gt; [ "size", "integer" ]
    convert =&gt; [ "status", "integer" ]
    convert =&gt; [ "responsetime", "float" ]
    convert =&gt; [ "upstreamtime", "float" ]
    convert =&gt; [ "[geoip][coordinates]", "float" ]
    remove_field =&gt; [ "ecs","agent","host","cloud","@version","input","logs_type" ]
  }
  useragent {
    source =&gt; "http_user_agent"
    target =&gt; "ua"
    remove_field =&gt; [ "[ua][minor]","[ua][major]","[ua][build]","[ua][patch]","[ua][os_minor]","[ua][os_major]" ]
  }
}
output {
  elasticsearch {
    hosts =&gt; ["10.47.127.132", "10.47.127.199", "10.47.127.130"]
    #user =&gt; "myusername"
    #password =&gt; "mypassword"
    index =&gt; "logstash-nginx-sysadmins-%{+YYYY.MM.dd}"
  }
}
</code></pre>

<h2>Nginx</h2>

<p>On our nginx server we will install nginx and filebeat, then configure nginx to log to a custom log format, and configure filebeat to read the logs and push it to redis.</p>

<p>Installing nginx:</p>

<pre><code>$ apt update &amp;&amp; apt install nginx -y
</code></pre>

<p>Installing <a href="https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-installation.html">filebeat</a>:</p>

<pre><code>$ curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.6.2-amd64.deb
$ dpkg -i filebeat-7.6.2-amd64.deb
</code></pre>

<p>Next we will configure nginx to log to a seperate file with a custom log format to include data such as the, request method, upstream response time, hostname, remote address, etc.</p>

<p>Under the <code>http</code> directive in your <code>/etc/nginx/nginx.conf</code>, configure the <code>log_format</code> and <code>access_log</code>:</p>

<pre><code>http {
...
        log_format json_logs '{"@timestamp":"$time_iso8601","host":"$hostname",'
                            '"server_ip":"$server_addr","client_ip":"$remote_addr",'
                            '"xff":"$http_x_forwarded_for","domain":"$host",'
                            '"url":"$uri","referer":"$http_referer",'
                            '"args":"$args","upstreamtime":"$upstream_response_time",'
                            '"responsetime":"$request_time","request_method":"$request_method",'
                            '"status":"$status","size":"$body_bytes_sent",'
                            '"request_body":"$request_body","request_length":"$request_length",'
                            '"protocol":"$server_protocol","upstreamhost":"$upstream_addr",'
                            '"file_dir":"$request_filename","http_user_agent":"$http_user_agent"'
                            '}';

        access_log  /var/log/nginx/access_json.log  json_logs;
...
}
</code></pre>

<p>Restart nginx to activate the changes:</p>

<pre><code>$ systemctl restart nginx
</code></pre>

<p>Next we need to configure filebeat to read from our nginx access logs and configure the output to redis. Edit the filebeat config:</p>

<pre><code>$ vim /etc/filebeat/filebeat.yml
</code></pre>

<p>And configure filebeat with the following and make sure to change the values where you need to:</p>

<pre><code># config source: akiraka.net
# filebeat input 
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/nginx/access_json.log
  json.keys_under_root: true
  json.overwrite_keys: true
  json.add_error_key: true

# filebeat modules 
filebeat.config.modules:
  # remove the escape character before the wildcard below
  path: ${path.config}/modules.d/\*.yml
  reload.enabled: false

# elasticsearch template settings
setup.template.settings:
  index.number_of_shards: 3

# redis output
output.redis:
  hosts: ["10.47.127.140:6379"]
  password: "9V5YlWvm8WuC4n1KZLYUEbLruLJLNJEnDzhu4WnAIfgxMmlv"
  key: "nginx_logs"
  # ^ this key needs to be the same as the configured key on logstash 
  db: 0
  timeout: 5
</code></pre>

<p>Restart filebeat:</p>

<pre><code>$ systemctl restart filebeat
</code></pre>

<p>When you make a request to your nginx server, you should see a similar logline like below:</p>

<pre><code>$ tail -n1 /var/log/nginx/access_elg.log
{"@timestamp":"2020-04-28T20:05:03+00:00","host":"sysadmins-blog","server_ip":"10.68.100.89","client_ip":"x.x.x.x","xff":"x.x.x.x","domain":"sysadmins.co.za","url":"/","referer":"-","args":"-","upstreamtime":"0.310","responsetime":"0.312","request_method":"GET","status":"200","size":"4453","request_body":"-","request_length":"519","protocol":"HTTP/1.1","upstreamhost":"127.0.0.1:2369","file_dir":"/var/www/web/root/","http_user_agent":"Mozilla/5.0"}
</code></pre>

<h2>Grafana</h2>

<p>On the grafana server, install grafana:</p>

<pre><code>$ apt update &amp;&amp; apt install apt-transport-https software-properties-common wget -y
$ wget -q -O - https://packages.grafana.com/gpg.key | apt-key add -
$ add-apt-repository "deb https://packages.grafana.com/oss/deb stable main"
$ apt update &amp;&amp; apt install grafana -y
</code></pre>

<p>Now we need to install a couple of grafana plugins that we require for our dashboards:</p>

<pre><code>$ grafana-cli plugins install grafana-worldmap-panel
$ grafana-cli plugins install grafana-clock-panel
$ grafana-cli plugins install grafana-piechart-panel
</code></pre>

<p>Now reload systemd and restart grafana:</p>

<pre><code>$ systemctl daemon-reload
$ systemctl restart grafana-server
</code></pre>

<p>If you would like to setup nginx as a reverse proxy to grafana, you can have a look at <strong><a href="https://blog.ruanbekker.com/blog/2019/05/17/install-grafana-to-visualize-your-metrics-from-datasources-such-as-prometheus-on-linux/">this blogpost</a></strong> on how to do that.</p>

<h2>Prometheus</h2>

<p>If you don&rsquo;t have Prometheus installed already, you can view my <a href="https://blog.ruanbekker.com/blog/2019/05/07/setup-prometheus-and-node-exporter-on-ubuntu-for-epic-monitoring/">blogpost</a> on setting up Prometheus.</p>

<h2>Verifying</h2>

<p>To verify if everything works as expected, make a request to your nginx server, then have a look if your index count on elasticsearch increases:</p>

<pre><code>$ curl http://elasticsearch-endpoint-address:9200/_cat/indices/logstash-*?v
health status index                               uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   logstash-nginx-x-2020.04.28 SWbHCer-TeOcw6bi_695Xw   5   1      58279            0     32.6mb         16.3mb
</code></pre>

<p>If you dont, make sure that all the processes are running on the servers, and that each server is able to reach each other on the targeted ports.</p>

<h2>The Fun Part: Dashboarding</h2>

<p>Now that we have everything in place, the fun part is to build the dashboards, first we need to configure elasticsearch as our datasource and specify the index we want to read from. Open grafana on <code>http://ip.of.grafana.server:3000</code>, default user and password is admin.</p>

<p>Select config on the left and select datasources, add a datasource, select elasticsearch and specify your datasource name, mine is <strong>es-nginx</strong> in this example, the <strong>url</strong> of your elasticsearch endpoint, if you have secured your elasticsearch cluster with authentication, provide the auth, then provide your index name as as provided in logstash.</p>

<p>My configured index will look like <code>logstash-nginx-sysadmins-YYYY-MM-dd</code>, therefore I specified index name as <code>logstash-nginx-sysadmins-*</code> and my timefield as <code>@timestamp</code>, the version, and select save and test, which would look like this:</p>

<p><img width="569" alt="AC025E20-38D0-4676-B576-9F5932913BA1" src="https://user-images.githubusercontent.com/567298/80538019-48ab0c80-89a5-11ea-8f4f-a30384991ab9.png"></p>

<p>Now we will import our dashboard template (Once again a massive thank you to <a href="https://grafana.com/grafana/dashboards/11190">Shenxiang, Qingkong and Ruixi</a> which made this template available!), head over to dashboards and select import, then provide the ID: <code>11190</code>, after that it will prompt what your dashboard needs to be named and you need to select your Elasticsearch and Prometheus datasource.</p>

<p>The description of the panels is in Chinese, if you would like it in english, I have translated mine to english and made the dashboard json available in <a href="https://gist.githubusercontent.com/ruanbekker/699fca31ebd7223b675d0acd25ea84bc/raw/316a015a0464989117cd72a1e8e056854d582178/nginx_grafana_dashboard_11190_eng.json">this gist</a></p>

<h2>Tour of our Dashboard Panels</h2>

<p>Looking at our hotspot map:</p>

<p><img width="1212" alt="grafana" src="https://user-images.githubusercontent.com/567298/80539136-48ac0c00-89a7-11ea-869d-597da4fa4d92.png"></p>

<p>The summary and top 10 pages:</p>

<p><img width="1243" alt="76E8CBE1-4B03-4226-8041-B98879BAD66A" src="https://user-images.githubusercontent.com/567298/80540596-e86a9980-89a9-11ea-924d-29f777a7c15a.png"></p>

<p>Page views, historical trends:</p>

<p><img width="1239" alt="grafana-page-views" src="https://user-images.githubusercontent.com/567298/80539728-4eeeb800-89a8-11ea-959e-5e2915387b7b.png"></p>

<p>Top 10 referers and table data of our logs:</p>

<p><img width="1235" alt="B17C4F55-DF91-4EA0-9669-C237FF560459" src="https://user-images.githubusercontent.com/567298/80540381-772ae680-89a9-11ea-9067-61cd519c9d8a.png"></p>

<h2>Thank You</h2>

<p>I hope this was useful, if you have any issues with this feel free to reach out to me. If you like my work, please feel free to share this post, follow me on Twitter at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong> or visit me on my <strong><a href="https://ruan.dev">website</a></strong></p>

<p><a href="https://ko-fi.com/A6423ZIQ"><img src="https://www.ko-fi.com/img/githubbutton_sm.svg" alt="ko-fi" /></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Reindex Elasticsearch Indices With Logstash]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/09/08/reindex-elasticsearch-indices-with-logstash/"/>
    <updated>2019-09-08T13:00:59+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/09/08/reindex-elasticsearch-indices-with-logstash</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/59209960-ca872100-8bac-11e9-8672-8c6af502afe0.png" alt="logstash" /></p>

<p>In this tutorial I will show you how to reindex daily indices to a monthly index on Elasticsearch using Logstash</p>

<h2>Use Case</h2>

<p>In this scenario we have filebeat indices which have a low document count and would like to aggregate the daily indices into a bigger index, which will be a monthly index. So reindexing from <code>"filebeat-2019.08.*"</code> to <code>"filebeat-monthly-2019.08"</code>.</p>

<h2>Overview of our Setup</h2>

<p>Here we can see all the indices that we would like to read from"</p>

<pre><code>$ curl 10.37.117.130:9200/_cat/indices/filebeat-2019.08.*?v
health status index               uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   filebeat-2019.08.28 qoKiHUjQT5eNVF_wjLi9fA   5   1         17            0    295.4kb        147.7kb
green  open   filebeat-2019.08.27 8PWngqFdRPKLEnrCCiw6xA   5   1        301            0    900.9kb          424kb
green  open   filebeat-2019.08.29 PiG2ma8zSbSt6sSg7soYPA   5   1         24            0    400.2kb          196kb
green  open   filebeat-2019.08.31 XSWZvqQDR0CugD23y6_iaA   5   1         27            0    451.5kb        222.1kb
green  open   filebeat-2019.08.30 u_Hr9fA5RtOtpabNGUmSpw   5   1         18            0    326.1kb          163kb
</code></pre>

<p>I have 3 nodes in my elasticsearch cluster:</p>

<pre><code>$ curl 10.37.117.130:9200/_cat/nodes?v
ip            heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
10.37.117.132           56          56   5    0.47    0.87     1.10 mdi       -      elasticsearch-01
10.37.117.130           73          56   4    0.47    0.87     1.10 mdi       -      elasticsearch-03
10.37.117.199           29          56   4    0.47    0.87     1.10 mdi       *      elasticsearch-02
</code></pre>

<p>As elasticsearch create 5 primary shards by default, I want to override this behavior to creating 3 primary shards. I will be using a template, so whenever a index get created with the index pattern `&ldquo;<em>-monthly-</em>&rdquo;, it will apply the settings to create 3 primary shards and 1 replica shards:</p>

<pre><code>$ curl -H 'Content-Type: application/json' -XPUT 10.37.117.130:9200/_template/monthly -d '
{"index_patterns": ["*-monthly-*"], "order": -1, "settings": {"number_of_shards": "3", "number_of_replicas": "1"}}
'
</code></pre>

<h2>Logstash Configuration</h2>

<p>Our logstash configuration which we will use, will read from elasticsearch and the index pattern which we want to read from. Then our ouput configuration instructs where to write the data to:</p>

<pre><code>$ cat /tmp/logstash/logstash.conf
input {
  elasticsearch {
    hosts =&gt; [ "http://10.37.117.132:9200" ]
    index =&gt; "filebeat-2019.08.*"
    size =&gt; 500
    scroll =&gt; "5m"
    docinfo =&gt; true
  }
}

output {
  elasticsearch {
    hosts =&gt; ["http://10.37.117.199:9200"]
    index =&gt; "filebeat-monthly-2019.08"
    document_id =&gt; "%{[@metadata][_id]}"
  }
  stdout {
    codec =&gt; "dots"
  }
}
</code></pre>

<h2>Reindex the Data</h2>

<p>I will be using docker to run logstash, and map the configuration to the configuration directory inside the container:</p>

<pre><code>$ sudo docker run --rm -it -v /tmp/logstash:/usr/share/logstash/pipeline docker.elastic.co/logstash/logstash-oss:6.2.4
[2019-09-08T10:57:36,170][INFO ][logstash.pipeline        ] Pipeline started successfully {:pipeline_id=&gt;"main", :thread=&gt;"#&lt;Thread:0x7db57d5f run&gt;"}
[2019-09-08T10:57:36,325][INFO ][logstash.agent           ] Pipelines running {:count=&gt;1, :pipelines=&gt;["main"]}
...
[2019-09-08T10:57:39,359][INFO ][logstash.pipeline        ] Pipeline has terminated {:pipeline_id=&gt;"main", :thread=&gt;"#&lt;Thread:0x7db57d5f run&gt;"}
</code></pre>

<p>Review that the data was reindexed:</p>

<pre><code>$ curl 10.37.117.130:9200/_cat/indices/*filebeat-*08*?v
health status index                    uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   filebeat-2019.08.28      qoKiHUjQT5eNVF_wjLi9fA   5   1         17            0    295.4kb        147.7kb
green  open   filebeat-2019.08.29      PiG2ma8zSbSt6sSg7soYPA   5   1         24            0    400.2kb          196kb
green  open   filebeat-2019.08.30      u_Hr9fA5RtOtpabNGUmSpw   5   1         18            0    326.1kb          163kb
green  open   filebeat-2019.08.27      8PWngqFdRPKLEnrCCiw6xA   5   1        301            0    900.9kb          424kb
green  open   filebeat-2019.08.31      XSWZvqQDR0CugD23y6_iaA   5   1         27            0    451.5kb        222.1kb
green  open   filebeat-monthly-2019.08 VZD8iDjfTfeyP-SWB9l2Pg   3   1        387            0    577.8kb        274.7kb
</code></pre>

<p>Once we are happy with what we are seeing, we can delete the source data:</p>

<pre><code>$ curl -XDELETE "10.37.117.130:9200/filebeat-2019.08.*"
{"acknowledged":true}
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a Logstash Server for Amazon Elasticsearch Service and Auth With IAM]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/06/04/setup-a-logstash-server-for-amazon-elasticsearch-service-and-auth-with-iam/"/>
    <updated>2019-06-04T17:46:27-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/06/04/setup-a-logstash-server-for-amazon-elasticsearch-service-and-auth-with-iam</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/59209960-ca872100-8bac-11e9-8672-8c6af502afe0.png" alt="logstash" /></p>

<p>As many of you might know, when you deploy a ELK stack on Amazon Web Services, you only get E and K in the ELK stack, which is Elasticsearch and Kibana. Here we will be dealing with Logstash on EC2.</p>

<h2>What will we be doing</h2>

<p>In this tutorial we will setup a Logstash Server on EC2, setup a IAM Role and Autenticate Requests to Elasticsearch with an IAM Role, setup Nginx so that logstash can ship logs to Elasticsearch.</p>

<p>I am not fond of working with access key&rsquo;s and secret keys, and if I can stay away from handling secret information the better. So instead of creating a access key and secret key for logstash, we will instead create a IAM Policy that will allow the actions to Elasticsearch, associate that policy to an IAM Role, set EC2 as a trusted entity and strap that IAM Role to the EC2 Instance.</p>

<p>Then we will allow the IAM Role ARN to the Elasticsearch Policy, then when Logstash makes requests against Elasticsearch, it will use the IAM Role to assume temporary credentials to authenticate. That way we don&rsquo;t have to deal with keys. But I mean you can create access keys if that is your preferred method, I&rsquo;m just not a big fan of keeping secret keys.</p>

<p>The benefit of authenticating with IAM, allows you to remove a reverse proxy that is another hop to the path of your target.</p>

<h2>Create the IAM Policy:</h2>

<p>Create a IAM Policy that will allow actions to Elasticsearch:</p>

<pre><code>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "es:ESHttpHead",
                "es:ESHttpPost",
                "es:ESHttpGet",
                "es:ESHttpPut"
            ],
            "Resource": "arn:aws:es:eu-west-1:0123456789012:domain/my-es-domain"
        }
    ]
}
</code></pre>

<p>Create Role logstash-system-es with &ldquo;ec2.amazonaws.com&rdquo; as trusted entity in trust the relationship and associate the above policy to the role.</p>

<h2>Authorize your Role in Elasticsearch Policy</h2>

<p>Head over to your Elasticsearch Domain and configure your Elasticsearch Policy to include your IAM Role to grant requests to your Domain:</p>

<pre><code>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "arn:aws:iam::0123456789012:role/logstash-system-es"
        ]
      },
      "Action": "es:*",
      "Resource": "arn:aws:es:eu-west-1:0123456789012:domain/my-es-domain/*"
    }
  ]
}
</code></pre>

<h2>Install Logstash on EC2</h2>

<p>I will be using Ubuntu Server 18. Update the repositories and install dependencies:</p>

<pre><code>$ apt update &amp;&amp; apt upgrade -y
$ apt install build-essential apt-transport-https -y
$ wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
$ echo "deb https://artifacts.elastic.co/packages/6.x/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-6.x.list
$ apt update
</code></pre>

<p>As logstash requires Java, install the the Java OpenJDK Runtime Environment:</p>

<pre><code>$ apt install default-jre -y
</code></pre>

<p>Verify that Java is installed:</p>

<pre><code>$ java -version
openjdk version "11.0.3" 2019-04-16
OpenJDK Runtime Environment (build 11.0.3+7-Ubuntu-1ubuntu218.04.1)
OpenJDK 64-Bit Server VM (build 11.0.3+7-Ubuntu-1ubuntu218.04.1, mixed mode, sharing)
</code></pre>

<p>Now, install logstash and enable the service on boot:</p>

<pre><code>$ apt install logstash -y
$ systemctl enable logstash.service
$ service logstash stop
</code></pre>

<h2>Install the Amazon ES Logstash Output Plugin</h2>

<p>For us to be able to authenticate using IAM, we should use the Amazon-ES Logstash Output Plugin. Update and install the plugin:</p>

<pre><code>$ /usr/share/logstash/bin/logstash-plugin update
$ /usr/share/logstash/bin/logstash-plugin install logstash-output-amazon_es
</code></pre>

<h2>Configure Logstash</h2>

<p>I like to split up my configuration in 3 parts, (input, filter, output).</p>

<p>Let&rsquo;s create the input configuration: <code>/etc/logstash/conf.d/10-input.conf</code></p>

<pre><code>input {
  file {
    path =&gt; "/var/log/nginx/access.log"
    start_position =&gt; "beginning"
  }
}
</code></pre>

<p>Our filter configuration: <code>/etc/logstash/conf.d/20-filter.conf</code></p>

<pre><code>filter {
  grok {
    match =&gt; { "message" =&gt; "%{HTTPD_COMMONLOG}" }
  }
  mutate {
    add_field =&gt; {
      "custom_field1" =&gt; "hello from: %{host}"
    }
  }
}
</code></pre>

<p>And lastly, our output configuration: <code>/etc/logstash/conf.d/30-outputs.conf</code>:</p>

<pre><code>output {
  amazon_es {
      hosts =&gt; ["my-es-domain.abcdef.eu-west-1.es.amazonaws.com"]
      index =&gt; "new-logstash-%{+YYYY.MM.dd}"
      region =&gt; "eu-west-1"
      aws_access_key_id =&gt; ''
      aws_secret_access_key =&gt; ''
  }
}
</code></pre>

<p>Note that the <code>aws_</code> directives has been left empty as that seems to be the way it needs to be set when using roles. Authentication will be assumed via the Role which is associated to the EC2 Instance.</p>

<p>If you are using access keys, you can populate them there.</p>

<h2>Start Logstash</h2>

<p>Start logstash:</p>

<pre><code>$ service logstash start
</code></pre>

<p>Tail the logs to see if logstash starts up correctly, it should look more or less like this:</p>

<pre><code>$ tail -f /var/log/logstash/logstash-plain.log

[2019-06-04T16:38:12,087][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=&gt;"6.8.0"}
[2019-06-04T16:38:14,480][INFO ][logstash.pipeline        ] Starting pipeline {:pipeline_id=&gt;"main", "pipeline.workers"=&gt;2, "pipeline.batch.size"=&gt;125, "pipeline.batch.delay"=&gt;50}
[2019-06-04T16:38:15,226][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=&gt;{:removed=&gt;[], :added=&gt;[https://search-my-es-domain-xx.eu-west-1.es.amazonaws.com:443/]}}
[2019-06-04T16:38:15,234][INFO ][logstash.outputs.elasticsearch] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=&gt;https://search-my-es-domain-xx.eu-west-1.es.amazonaws.com:443/, :path=&gt;"/"}
</code></pre>

<h2>Install Nginx</h2>

<p>As you noticed, I have specified <code>/var/log/nginx/access.log</code> as my input file for logstash, as we will test logstash by shipping nginx access logs to Elasticsearch Service.</p>

<p>Install Nginx:</p>

<pre><code>$ apt install nginx -y
</code></pre>

<p>Start the service:</p>

<pre><code>$ systemctl restart nginx 
$ systemctl enable nginx
</code></pre>

<p>Make a GET request on your Nginx Web Server and inspect the log on Kibana, where it should look like this:</p>

<p><img src="https://user-images.githubusercontent.com/567298/58917559-4dc8f280-8727-11e9-9e9d-7950217abe34.png" alt="" /></p>
]]></content>
  </entry>
  
</feed>
