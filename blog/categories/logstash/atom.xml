<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Logstash | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/logstash/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2019-12-04T22:38:25+02:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Reindex Elasticsearch Indices With Logstash]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/09/08/reindex-elasticsearch-indices-with-logstash/"/>
    <updated>2019-09-08T13:00:59+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/09/08/reindex-elasticsearch-indices-with-logstash</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/59209960-ca872100-8bac-11e9-8672-8c6af502afe0.png" alt="logstash" /></p>

<p>In this tutorial I will show you how to reindex daily indices to a monthly index on Elasticsearch using Logstash</p>

<h2>Use Case</h2>

<p>In this scenario we have filebeat indices which have a low document count and would like to aggregate the daily indices into a bigger index, which will be a monthly index. So reindexing from <code>"filebeat-2019.08.*"</code> to <code>"filebeat-monthly-2019.08"</code>.</p>

<h2>Overview of our Setup</h2>

<p>Here we can see all the indices that we would like to read from"</p>

<pre><code>$ curl 10.37.117.130:9200/_cat/indices/filebeat-2019.08.*?v
health status index               uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   filebeat-2019.08.28 qoKiHUjQT5eNVF_wjLi9fA   5   1         17            0    295.4kb        147.7kb
green  open   filebeat-2019.08.27 8PWngqFdRPKLEnrCCiw6xA   5   1        301            0    900.9kb          424kb
green  open   filebeat-2019.08.29 PiG2ma8zSbSt6sSg7soYPA   5   1         24            0    400.2kb          196kb
green  open   filebeat-2019.08.31 XSWZvqQDR0CugD23y6_iaA   5   1         27            0    451.5kb        222.1kb
green  open   filebeat-2019.08.30 u_Hr9fA5RtOtpabNGUmSpw   5   1         18            0    326.1kb          163kb
</code></pre>

<p>I have 3 nodes in my elasticsearch cluster:</p>

<pre><code>$ curl 10.37.117.130:9200/_cat/nodes?v
ip            heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
10.37.117.132           56          56   5    0.47    0.87     1.10 mdi       -      elasticsearch-01
10.37.117.130           73          56   4    0.47    0.87     1.10 mdi       -      elasticsearch-03
10.37.117.199           29          56   4    0.47    0.87     1.10 mdi       *      elasticsearch-02
</code></pre>

<p>As elasticsearch create 5 primary shards by default, I want to override this behavior to creating 3 primary shards. I will be using a template, so whenever a index get created with the index pattern `&ldquo;<em>-monthly-</em>&rdquo;, it will apply the settings to create 3 primary shards and 1 replica shards:</p>

<pre><code>$ curl -H 'Content-Type: application/json' -XPUT 10.37.117.130:9200/_template/monthly -d '
{"index_patterns": ["*-monthly-*"], "order": -1, "settings": {"number_of_shards": "3", "number_of_replicas": "1"}}
'
</code></pre>

<h2>Logstash Configuration</h2>

<p>Our logstash configuration which we will use, will read from elasticsearch and the index pattern which we want to read from. Then our ouput configuration instructs where to write the data to:</p>

<pre><code>$ cat /tmp/logstash/logstash.conf
input {
  elasticsearch {
    hosts =&gt; [ "http://10.37.117.132:9200" ]
    index =&gt; "filebeat-2019.08.*"
    size =&gt; 500
    scroll =&gt; "5m"
    docinfo =&gt; true
  }
}

output {
  elasticsearch {
    hosts =&gt; ["http://10.37.117.199:9200"]
    index =&gt; "filebeat-monthly-2019.08"
    document_id =&gt; "%{[@metadata][_id]}"
  }
  stdout {
    codec =&gt; "dots"
  }
}
</code></pre>

<h2>Reindex the Data</h2>

<p>I will be using docker to run logstash, and map the configuration to the configuration directory inside the container:</p>

<pre><code>$ sudo docker run --rm -it -v /tmp/logstash:/usr/share/logstash/pipeline docker.elastic.co/logstash/logstash-oss:6.2.4
[2019-09-08T10:57:36,170][INFO ][logstash.pipeline        ] Pipeline started successfully {:pipeline_id=&gt;"main", :thread=&gt;"#&lt;Thread:0x7db57d5f run&gt;"}
[2019-09-08T10:57:36,325][INFO ][logstash.agent           ] Pipelines running {:count=&gt;1, :pipelines=&gt;["main"]}
...
[2019-09-08T10:57:39,359][INFO ][logstash.pipeline        ] Pipeline has terminated {:pipeline_id=&gt;"main", :thread=&gt;"#&lt;Thread:0x7db57d5f run&gt;"}
</code></pre>

<p>Review that the data was reindexed:</p>

<pre><code>$ curl 10.37.117.130:9200/_cat/indices/*filebeat-*08*?v
health status index                    uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   filebeat-2019.08.28      qoKiHUjQT5eNVF_wjLi9fA   5   1         17            0    295.4kb        147.7kb
green  open   filebeat-2019.08.29      PiG2ma8zSbSt6sSg7soYPA   5   1         24            0    400.2kb          196kb
green  open   filebeat-2019.08.30      u_Hr9fA5RtOtpabNGUmSpw   5   1         18            0    326.1kb          163kb
green  open   filebeat-2019.08.27      8PWngqFdRPKLEnrCCiw6xA   5   1        301            0    900.9kb          424kb
green  open   filebeat-2019.08.31      XSWZvqQDR0CugD23y6_iaA   5   1         27            0    451.5kb        222.1kb
green  open   filebeat-monthly-2019.08 VZD8iDjfTfeyP-SWB9l2Pg   3   1        387            0    577.8kb        274.7kb
</code></pre>

<p>Once we are happy with what we are seeing, we can delete the source data:</p>

<pre><code>$ curl -XDELETE "10.37.117.130:9200/filebeat-2019.08.*"
{"acknowledged":true}
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a Logstash Server for Amazon Elasticsearch Service and Auth With IAM]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/06/04/setup-a-logstash-server-for-amazon-elasticsearch-service-and-auth-with-iam/"/>
    <updated>2019-06-04T23:46:27+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/06/04/setup-a-logstash-server-for-amazon-elasticsearch-service-and-auth-with-iam</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/59209960-ca872100-8bac-11e9-8672-8c6af502afe0.png" alt="logstash" /></p>

<p>As many of you might know, when you deploy a ELK stack on Amazon Web Services, you only get E and K in the ELK stack, which is Elasticsearch and Kibana. Here we will be dealing with Logstash on EC2.</p>

<h2>What will we be doing</h2>

<p>In this tutorial we will setup a Logstash Server on EC2, setup a IAM Role and Autenticate Requests to Elasticsearch with an IAM Role, setup Nginx so that logstash can ship logs to Elasticsearch.</p>

<p>I am not fond of working with access key&rsquo;s and secret keys, and if I can stay away from handling secret information the better. So instead of creating a access key and secret key for logstash, we will instead create a IAM Policy that will allow the actions to Elasticsearch, associate that policy to an IAM Role, set EC2 as a trusted entity and strap that IAM Role to the EC2 Instance.</p>

<p>Then we will allow the IAM Role ARN to the Elasticsearch Policy, then when Logstash makes requests against Elasticsearch, it will use the IAM Role to assume temporary credentials to authenticate. That way we don&rsquo;t have to deal with keys. But I mean you can create access keys if that is your preferred method, I&rsquo;m just not a big fan of keeping secret keys.</p>

<p>The benefit of authenticating with IAM, allows you to remove a reverse proxy that is another hop to the path of your target.</p>

<h2>Create the IAM Policy:</h2>

<p>Create a IAM Policy that will allow actions to Elasticsearch:</p>

<pre><code>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "es:ESHttpHead",
                "es:ESHttpPost",
                "es:ESHttpGet",
                "es:ESHttpPut"
            ],
            "Resource": "arn:aws:es:eu-west-1:0123456789012:domain/my-es-domain"
        }
    ]
}
</code></pre>

<p>Create Role logstash-system-es with &ldquo;ec2.amazonaws.com&rdquo; as trusted entity in trust the relationship and associate the above policy to the role.</p>

<h2>Authorize your Role in Elasticsearch Policy</h2>

<p>Head over to your Elasticsearch Domain and configure your Elasticsearch Policy to include your IAM Role to grant requests to your Domain:</p>

<pre><code>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "arn:aws:iam::0123456789012:role/logstash-system-es"
        ]
      },
      "Action": "es:*",
      "Resource": "arn:aws:es:eu-west-1:0123456789012:domain/my-es-domain/*"
    }
  ]
}
</code></pre>

<h2>Install Logstash on EC2</h2>

<p>I will be using Ubuntu Server 18. Update the repositories and install dependencies:</p>

<pre><code>$ apt update &amp;&amp; apt upgrade -y
$ apt install build-essential apt-transport-https -y
$ wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
$ echo "deb https://artifacts.elastic.co/packages/6.x/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-6.x.list
$ apt update
</code></pre>

<p>As logstash requires Java, install the the Java OpenJDK Runtime Environment:</p>

<pre><code>$ apt install default-jre -y
</code></pre>

<p>Verify that Java is installed:</p>

<pre><code>$ java -version
openjdk version "11.0.3" 2019-04-16
OpenJDK Runtime Environment (build 11.0.3+7-Ubuntu-1ubuntu218.04.1)
OpenJDK 64-Bit Server VM (build 11.0.3+7-Ubuntu-1ubuntu218.04.1, mixed mode, sharing)
</code></pre>

<p>Now, install logstash and enable the service on boot:</p>

<pre><code>$ apt install logstash -y
$ systemctl enable logstash.service
$ service logstash stop
</code></pre>

<h2>Install the Amazon ES Logstash Output Plugin</h2>

<p>For us to be able to authenticate using IAM, we should use the Amazon-ES Logstash Output Plugin. Update and install the plugin:</p>

<pre><code>$ /usr/share/logstash/bin/logstash-plugin update
$ /usr/share/logstash/bin/logstash-plugin install logstash-output-amazon_es
</code></pre>

<h2>Configure Logstash</h2>

<p>I like to split up my configuration in 3 parts, (input, filter, output).</p>

<p>Let&rsquo;s create the input configuration: <code>/etc/logstash/conf.d/10-input.conf</code></p>

<pre><code>input {
  file {
    path =&gt; "/var/log/nginx/access.log"
    start_position =&gt; "beginning"
  }
}
</code></pre>

<p>Our filter configuration: <code>/etc/logstash/conf.d/20-filter.conf</code></p>

<pre><code>filter {
  grok {
    match =&gt; { "message" =&gt; "%{HTTPD_COMMONLOG}" }
  }
  mutate {
    add_field =&gt; {
      "custom_field1" =&gt; "hello from: %{host}"
    }
  }
}
</code></pre>

<p>And lastly, our output configuration: <code>/etc/logstash/conf.d/30-outputs.conf</code>:</p>

<pre><code>output {
  amazon_es {
      hosts =&gt; ["my-es-domain.abcdef.eu-west-1.es.amazonaws.com"]
      index =&gt; "new-logstash-%{+YYYY.MM.dd}"
      region =&gt; "eu-west-1"
      aws_access_key_id =&gt; ''
      aws_secret_access_key =&gt; ''
  }
}
</code></pre>

<p>Note that the <code>aws_</code> directives has been left empty as that seems to be the way it needs to be set when using roles. Authentication will be assumed via the Role which is associated to the EC2 Instance.</p>

<p>If you are using access keys, you can populate them there.</p>

<h2>Start Logstash</h2>

<p>Start logstash:</p>

<pre><code>$ service logstash start
</code></pre>

<p>Tail the logs to see if logstash starts up correctly, it should look more or less like this:</p>

<pre><code>$ tail -f /var/log/logstash/logstash-plain.log

[2019-06-04T16:38:12,087][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=&gt;"6.8.0"}
[2019-06-04T16:38:14,480][INFO ][logstash.pipeline        ] Starting pipeline {:pipeline_id=&gt;"main", "pipeline.workers"=&gt;2, "pipeline.batch.size"=&gt;125, "pipeline.batch.delay"=&gt;50}
[2019-06-04T16:38:15,226][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=&gt;{:removed=&gt;[], :added=&gt;[https://search-my-es-domain-xx.eu-west-1.es.amazonaws.com:443/]}}
[2019-06-04T16:38:15,234][INFO ][logstash.outputs.elasticsearch] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=&gt;https://search-my-es-domain-xx.eu-west-1.es.amazonaws.com:443/, :path=&gt;"/"}
</code></pre>

<h2>Install Nginx</h2>

<p>As you noticed, I have specified <code>/var/log/nginx/access.log</code> as my input file for logstash, as we will test logstash by shipping nginx access logs to Elasticsearch Service.</p>

<p>Install Nginx:</p>

<pre><code>$ apt install nginx -y
</code></pre>

<p>Start the service:</p>

<pre><code>$ systemctl restart nginx 
$ systemctl enable nginx
</code></pre>

<p>Make a GET request on your Nginx Web Server and inspect the log on Kibana, where it should look like this:</p>

<p><img src="https://user-images.githubusercontent.com/567298/58917559-4dc8f280-8727-11e9-9e9d-7950217abe34.png" alt="" /></p>
]]></content>
  </entry>
  
</feed>
