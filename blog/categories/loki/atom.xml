<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Loki | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/loki/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2020-11-11T14:40:41+00:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Deploy Loki on Multipass]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/11/11/deploy-loki-on-multipass/"/>
    <updated>2020-11-11T14:19:05+00:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/11/11/deploy-loki-on-multipass</id>
    <content type="html"><![CDATA[<p><img src="https://sysadmins.co.za/content/images/size/w1600/2020/11/loki-banner-2.png" alt="" /></p>

<p>In this post I will demonstrate how to deploy Grafana Labs&rsquo;s <strong>Loki</strong> on <strong>Multipass</strong> using cloud-init so that you can run your own dev environment and run a couple of queries to get you started.</p>

<h2>About</h2>

<p>If you haven&rsquo;t heard of <a href="https://multipass.run/">Multipass</a>, it allows you to run Ubuntu VMs on your Mac or Windows workstation.</p>

<p>If you haven&rsquo;t heard of <a href="https://grafana.com/oss/loki/">Loki</a>, as described by Grafana Labs: <em>&ldquo;Loki is a horizontally-scalable, highly-available, multi-tenant log aggregation system inspired by Prometheus.&rdquo;</em></p>

<h2>Install Multipass</h2>

<p>Head over to <a href="https://multipass.run/">multipass.run</a> to get the installer for your operating system, and if you are curious about Multipass, I wrote a beginners guide on Multipass which can be <strong><a href="https://sysadmins.co.za/getting-started-with-multipass-vms/">found here</a></strong></p>

<h2>Cloud Init for Loki</h2>

<p>We will be making use of <strong><a href="https://cloudinit.readthedocs.io/en/latest/">cloud-init</a></strong> to bootstrap <strong><a href="https://github.com/grafana/loki/releases/tag/v2.0.0">Loki v2.0.0</a></strong> to our multipass instance.</p>

<p>V2.0.0 is the current release of the time of writing, so depending on the time when you read this, have a look at the <a href="https://github.com/grafana/loki/releases">Loki Releases</a> page for the latest version and adjust the cloud-init.yml according to the version if it differs from the one I&rsquo;m mentioning.</p>

<p>(Optional) If you want to use SSH to your Multipass VM, you can use your existing SSH key or generate a new one, if you want to create a new key, you can <a href="https://sysadmins.co.za/getting-started-with-multipass-vms/">follow this post</a></p>

<p>Copy your public key, in my case <code>~/.ssh/id_rsa.pub</code> and paste it under the ssh <code>authorized_keys</code> section.</p>

<p>Our <code>cloud-init.yml</code> has a couple of sections, but to break it down it will do the following:</p>

<ul>
<li>We provide it our public ssh key so that we can ssh with our private key</li>
<li>Updates the index repository</li>
<li>Installs the packages, unzip and wget</li>
<li>Creates the loki systemd unit file and places it under /etc/systemd/system/</li>
<li>When the vm boots it will create the user loki and creates the loki etc directory</li>
<li>Once that completes, we are downloading the loki, logcli and promtail binaries from github</li>
</ul>


<pre><code class="yaml">#cloud-config
ssh_authorized_keys:
  - ssh-rsa AAAA...Ha9 your-comment

package_update: true

packages:
 - unzip
 - wget

write_files:
  - content: |-
      [Unit]
      Description=Loki
      User=loki
      Group=loki
      Wants=network-online.target
      After=network-online.target
      [Service]
      Type=simple
      Restart=on-failure
      ExecStart=/usr/local/bin/loki -config.file /etc/loki/loki-local-config.yaml
      [Install]
      WantedBy=multi-user.target

    owner: root:root
    path: /etc/systemd/system/loki.service
    permissions: '0644'

bootcmd:
  - useradd --no-create-home --shell /bin/false loki
  - mkdir /etc/loki
  - chown -R loki:loki /etc/loki

runcmd:
 - for app in loki logcli promtail; do wget "https://github.com/grafana/loki/releases/download/v2.0.0/${app}-linux-amd64.zip"; done
 - for app in loki logcli promtail; do unzip "${app}-linux-amd64.zip"; done
 - for app in loki logcli promtail; do mv "${app}-linux-amd64" /usr/local/bin/${app}; done
 - for app in loki logcli promtail; do rm -f "${app}-linux-amd64.zip"; done
 - wget https://raw.githubusercontent.com/grafana/loki/v2.0.0/cmd/loki/loki-local-config.yaml
 - mv ./loki-local-config.yaml /etc/loki/loki-local-config.yaml
 - chown loki:loki /etc/loki/loki-local-config.yaml
 - systemctl daemon-reload
 - systemctl start loki
 - sleep 5
 - echo "this is a test" | promtail --stdin --client.url http://localhost:3100/loki/api/v1/push --client.external-labels=app=cli -server.disable
</code></pre>

<p>You will notice that the VM will have <code>loki</code>, <code>logcli</code> and <code>promtail</code> available on it, so you will have an environment to use all of them together.</p>

<p>As you can see once we start loki, we are piping <code>this is a test</code> to Loki using Promtail, so that we can verify that the data is visible in Loki. That step is not required, but just added it to this demo.</p>

<h2>Deploy Loki on Multipass</h2>

<p>We will provision a Multipass VM using the Ubuntu Focal distribution and spec our VM with 1 CPU, 512MB of Memory and 1GB of disk and then bootstrap our installation of Loki using cloud-init:</p>

<pre><code class="bash">$ multipass launch focal \
  --name loki \
  --cpus 1 \
  --mem 512m \
  --disk 1G \
  --cloud-init cloud-init.yml

Creating: loki
Waiting for initialization to complete 
Launched: loki
</code></pre>

<p>We can validate if our Multipass VM is running:</p>

<pre><code class="bash">$ multipass list
Name                    State             IPv4             Image
loki                    Running           192.168.64.19    Ubuntu 20.04 LTS
</code></pre>

<h2>Test Loki inside the VM</h2>

<p>First we will exec into the VM (or SSH), then we will test out Loki inside the VM since we already have logcli available:</p>

<pre><code class="bash">$ multipass exec loki -- bash
To run a command as administrator (user "root"), use "sudo &lt;command&gt;".
See "man sudo_root" for details.

ubuntu@loki:~$
</code></pre>

<p>Remembered in our cloud-init, we instructed this command to run:</p>

<pre><code class="bash">echo "this is a test" | promtail --stdin --client.url http://localhost:3100/loki/api/v1/push --client.external-labels=app=cli -server.disable
</code></pre>

<p>So if we use logcli, we can inspect our visible labels:</p>

<pre><code>$ logcli --quiet labels
__name__
app
hostname
job
</code></pre>

<p>And as we expect, we will see the app label from the <code>--client.external-labels=app=cli</code> argument that we passed. We can also look at the values for a given label:</p>

<pre><code class="bash">$ logcli --quiet labels app
cli
</code></pre>

<p>Now let&rsquo;s query our logs using the label selector: <code>{app="cli"}</code>:</p>

<pre><code class="bash">$ logcli --quiet --output raw query '{app="cli"}'
this is a test
</code></pre>

<p>If we remove the extra arguments, we will see more verbose output like the following:</p>

<pre><code class="bash">$ logcli query '{app="cli"}'

http://localhost:3100/loki/api/v1/query_range?direction=BACKWARD&amp;end=1605092055756745122&amp;limit=30&amp;query=%7Bapp%3D%22cli%22%7D&amp;start=1605088455756745122
Common labels: {app="cli", hostname="loki", job="stdin"}
2020-11-11T12:45:20+02:00 {} this is a test
http://localhost:3100/loki/api/v1/query_range?direction=BACKWARD&amp;end=1605091520778438972&amp;limit=30&amp;query=%7Bapp%3D%22cli%22%7D&amp;start=1605088455756745122
Common labels: {app="cli", hostname="loki", job="stdin"}
</code></pre>

<p>We can pipe some more output to Loki:</p>

<pre><code class="bash">$ echo "this is another test" | promtail --stdin --client.url http://localhost:3100/loki/api/v1/push --client.external-labels=app=cli -server.disable
</code></pre>

<p>And querying our logs:</p>

<pre><code class="bash">$ logcli --quiet --output raw query '{app="cli"}'
this is another test
this is a test
</code></pre>

<h2>Testing Loki Outside our VM</h2>

<p>Let&rsquo;s exit the VM and test Loki from our local workstation, first you will need to get the logcli for your OS, head over to the <a href="https://github.com/grafana/loki/releases">releases</a> page and get the binary of your choice.</p>

<p>I will be demonstrating using a mac:</p>

<pre><code class="bash">$ wget https://github.com/grafana/loki/releases/download/v2.0.0/logcli-darwin-amd64.zip
$ unzip logcli-darwin-amd64.zip
$ sudo mv logcli-darwin-amd64 /usr/local/bin/logcli
$ rm -f logcli-darwin-amd64.zip
</code></pre>

<p>Now we need to tell logcli where our Loki server resides, so let&rsquo;s get the IP address of Loki:</p>

<pre><code class="bash">$ multipass info --all --format json | jq -r '.info.loki.ipv4[]'
192.168.64.19
</code></pre>

<p>We can either set the Loki host as an environment variable:</p>

<pre><code class="bash">$ export LOKI_ADDR=http://192.168.64.19
</code></pre>

<p>or you can specify it using the <code>--addr</code> argument:</p>

<pre><code class="bash">$ logcli --addr="http://192.168.64.19:3100"
</code></pre>

<p>For the sake of simplicity and not having to type the <code>--addr</code> the whole time, I will be setting the Loki address as an environment variable:</p>

<pre><code class="bash">$ export LOKI_ADDR="http://$(multipass info --all --format json | jq -r '.info.loki.ipv4[]'):3100"
</code></pre>

<p>And when we inspect our labels using logcli, we can see that we are getting our labels from Loki on our Multipass VM:</p>

<pre><code class="bash">$ logcli labels
http://192.168.64.19:3100/loki/api/v1/labels?end=1605093229877731000&amp;start=1605089629877731000
__name__
app
hostname
job
</code></pre>

<h2>Write Logs to Loki using the Loki Docker Driver</h2>

<p>We have used promtail before to pipe logs to Loki and in this example we will be making use of the Loki Docker Logging Plugin to write data to Loki.</p>

<p>If you have docker installed, install the Loki plugin:</p>

<pre><code class="bash">$ docker plugin install \
  grafana/loki-docker-driver:latest \
  --alias loki \
  --grant-all-permissions
</code></pre>

<p>Now we will use a docker container to echo stdout to the loki docker driver, which will send the output to Loki.</p>

<p>Let&rsquo;s alias a command loki_echo that we will use to send our output to the docker container:</p>

<pre><code class="bash">$ alias 'loki_echo=docker run --rm -it --log-driver loki --log-opt loki-url="http://192.168.64.19:3100/loki/api/v1/push" --log-opt loki-external-labels="app=echo-container" busybox echo'
</code></pre>

<p>So every time we run <code>loki_echo {string}</code> we will run a docker container from the busybox image and pass the <code>{string}</code> as an argument to the echo command inside the container, which will be sent to the loki log driver and land up in Loki.</p>

<p>Let&rsquo;s push 100 log events to Loki:</p>

<pre><code class="bash">$ count=0
$ while [ ${count} != 100 ]
  do 
    for color in red blue white silver green;
    do 
      loki_echo "there are ${RANDOM} items of ${color} available";
      count=$((count+1))
    done
  done

there are 26890 items of green available
there are 14856 items of red available
there are 31162 items of blue available
there are 23993 items of white available
there are 22310 items of silver available
there are 10700 items of green available
there are 14077 items of red available
there are 20642 items of blue available
there are 31576 items of white available
there are 26053 items of silver available
there are 2973 items of green available
there are 2203 items of red available
there are 8557 items of blue available
...
</code></pre>

<p>We can verify how many log events we have with:</p>

<pre><code>$ logcli query '{app="echo-container"}' --quiet --limit 200 --output raw | wc -l
100
</code></pre>

<p>To see how many logs we have with the line &ldquo;blue&rdquo; in it:</p>

<pre><code class="bash">$ logcli query '{app="echo-container"} |= "blue"' --quiet --limit 200 --output raw | wc -l
20
</code></pre>

<p>Let&rsquo;s look for logs with blue or green and limit the results to 5:</p>

<pre><code class="bash">$ logcli query '{app="echo-container"} |~ "items of (blue|green)"' --quiet --limit 5 --output raw
there are 28985 items of green available
there are 10289 items of blue available
there are 12316 items of green available
there are 23775 items of blue available
there are 20 items of green available
</code></pre>

<h2>Teardown</h2>

<p>If you followed along, you can terminate your Multipass VM with:</p>

<pre><code class="bash">$ multipass delete --purge loki
</code></pre>

<p>You can get the example code in my <strong><a href="https://github.com/ruanbekker/multipassfiles/tree/master/loki">multipassfiles github repository</a></strong></p>

<h2>Thanks</h2>

<p>Thanks for reading, if you like my content, check out my <strong><a href="https://ruan.dev">website</a></strong> or follow me at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Setup Alerting With Loki]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/11/06/how-to-setup-alerting-with-loki/"/>
    <updated>2020-11-06T15:13:53+00:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/11/06/how-to-setup-alerting-with-loki</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/98380823-bd948880-2051-11eb-8ab4-c8d5f5d3e612.png" alt="image" /></p>

<p>Recently Grafana Labs announced <strong><a href="https://grafana.com/blog/2020/10/28/loki-2.0-released-transform-logs-as-youre-querying-them-and-set-up-alerts-within-loki/">Loki v2</a></strong> and its awesome! Definitely check out their blog post on more details.</p>

<p>Loki has a index option called <strong>boltdb-shipper</strong>, which allows you to run Loki with only a object store and you <strong>no longer need a dedicated index store</strong> such as DynamoDB. You can extract labels from log lines at query time, which is CRAZY! And I really like how they&rsquo;ve implemented it, you can parse, filter and format like mad. I really like that.</p>

<p>And then generating alerts from any query, which we will go into today. Definitely check out <a href="https://grafana.com/blog/2020/10/28/loki-2.0-released-transform-logs-as-youre-querying-them-and-set-up-alerts-within-loki/">this blogpost</a> and <a href="https://grafana.com/blog/2020/11/04/video-top-three-features-of-the-new-loki-2.0/">this video</a> for more details on the features of Loki v2.</p>

<h2>What will we be doing today</h2>

<p>In this tutorial we will setup a alert using the Loki local ruler to alert us when we have <strong>high number of log events coming in</strong>. For example, let&rsquo;s say someone has debug logging enabled in their application and we want to send a alert to slack when it breaches the threshold.</p>

<p>I will simulate this with a <code>http-client</code> container which runs <code>curl</code> in a while loop to fire a bunch of http requests against the nginx container which logs to Loki, so we can see how the alerting works, and in this scenario we will alert to Slack.</p>

<p>And after that we will stop our http-client container to see how the alarm resolves when the log rate comes down again.</p>

<p>All the components are available in the <code>docker-compose.yml</code> on my <a href="https://github.com/ruanbekker/loki-alerts-docker">github repository</a></p>

<h2>Components</h2>

<p>Let&rsquo;s break it down and start with the loki config:</p>

<pre><code>...
ruler:
  storage:
    type: local
    local:
      directory: /etc/loki/rules
  rule_path: /tmp/loki/rules-temp
  alertmanager_url: http://alertmanager:9093
  ring:
    kvstore:
      store: inmemory
  enable_api: true
  enable_alertmanager_v2: true
</code></pre>

<p>In the section of the loki config, I will be making use of the local ruler and map my alert rules under <code>/etc/loki/rules/</code> and we are also defining our alertmanager instance where these alerts should be shipped to.</p>

<p>In my rule definition <code>/etc/loki/rules/demo/rules.yml</code>:</p>

<pre><code>groups:
  - name: rate-alerting
    rules:
      - alert: HighLogRate
        expr: |
          sum by (compose_service)
            (rate({job="dockerlogs"}[1m]))
            &gt; 60
        for: 1m
        labels:
            severity: warning
            team: devops
            category: logs
        annotations:
            title: "High LogRate Alert"
            description: "something is logging a lot"
            impact: "impact"
            action: "action"
            dashboard: "https://grafana.com/service-dashboard"
            runbook: "https://wiki.com"
            logurl: "https://grafana.com/log-explorer"
</code></pre>

<p>In my expression, I am using LogQL to return per second rate of all my docker logs within the last minute per compose service for my dockerlogs job and we are specifying that it should alert when the threshold is above 60.</p>

<p>As you can see I have a couple of <strong>labels and annotations</strong>, which becomes <strong>very useful</strong> when you have dashboard links, runbooks etc and you would like to map that to your alert. I am doing the mapping in my <code>alertmanager.yml</code> config:</p>

<pre><code>route:
...
  receiver: 'default-catchall-slack'
  routes:
  - match:
      severity: warning
    receiver: warning-devops-slack
    routes:
    - match_re:
        team: .*(devops).*
      receiver: warning-devops-slack

receivers:
...
- name: 'warning-devops-slack'
  slack_configs:
    - send_resolved: true
      channel: '__SLACK_CHANNEL__'
      title: ':fire::white_check_mark: []  '
      text: &gt;-
        
          *Description:* 
          *Severity:* ``
          *Graph:* &lt;|:chart_with_upwards_trend:&gt;&lt;|:chart_with_upwards_trend:&gt; *Dashboard:* &lt;|:bar_chart:&gt; *Runbook:* &lt;|:spiral_note_pad:&gt;
          *Details:*
           - *:* ``
          
           - *Impact*: 
           - *Receiver*: warning--slack
        
</code></pre>

<p>As you can see, when my alert matches nothing it will go to my catchall receiver, but when my label contains <code>devops</code> and the route the alert to my <code>warning-devops-slack</code> receiver, and then we will be parsing our labels and annotations to include the values in our alarm on slack.</p>

<h2>Demo</h2>

<p>Enough with the background details, and it&rsquo;s time to get into the action.</p>

<p>All the code for this demonstration will be available in my github repository: <strong><a href="https://github.com/ruanbekker/loki-alerts-docker">github.com/ruanbekker/loki-alerts-docker</a></strong></p>

<p>The docker-compose will have a container of <strong>grafana</strong>, <strong>alertmanager</strong>, <strong>loki</strong>, <strong>nginx</strong> and a <strong>http-client</strong>.</p>

<p>The http-client is curl in a while loop that will just make a bunch of http requests against the nginx container, which will be logging to loki.</p>

<h2>Get the source</h2>

<p>Get the code from my github repository:</p>

<pre><code>$ git clone https://github.com/ruanbekker/loki-alerts-docker
$ cd loki-alerts-docker
</code></pre>

<p>You will need to replace the slack webhook url and the slack channel where you want your alerts to be sent to. This will take the environment variables and replace the values in <code>config/alertmanager.yml</code> (always check out the script first, before executing it)</p>

<pre><code>$ SLACK_WEBHOOK_URL="https://hooks.slack.com/services/xx/xx/xx" SLACK_CHANNEL="#notifications" ./parse_configs.sh
</code></pre>

<p>You can double check by running <code>cat config/alertmanager.yml</code>, once you are done, boot the stack:</p>

<pre><code>$ docker-compose up -d
</code></pre>

<p>Open up grafana:</p>

<pre><code>$ open http://grafana.localdns.xyz:3000
</code></pre>

<p>Use the initial user and password combination <code>admin/admin</code> and then reset your password:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98379039-7efdce80-204f-11eb-9c8a-3ed12a63cb14.png" alt="image" /></p>

<p>Browse for your labels on the log explorer section, in my example it will be <code>{job="dockerlogs"}</code>:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98379172-ace31300-204f-11eb-8e6c-3cf073afe771.png" alt="image" /></p>

<p>When we select our job=&ldquo;dockerlogs&rdquo; label, we will see our logs:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98379288-c71cf100-204f-11eb-911c-043a983bae6d.png" alt="image" /></p>

<p>As I explained earlier the query that we will be running in our ruler, can be checked what the rate currently is:</p>

<pre><code>sum by (compose_project, compose_service) (rate({job="dockerlogs"}[1m]))
</code></pre>

<p>Which will look like this:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98379765-54604580-2050-11eb-9c90-5e0adf2bb586.png" alt="image" /></p>

<p>In the configured expression in our ruler config, we have set to alarm once the value goes above 60, we can validate this by running:</p>

<pre><code>sum by (compose_project, compose_service) (rate({job="dockerlogs"}[1m])) &gt; 60
</code></pre>

<p>And we can verify that this is the case, and by now it should be alarming:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98379900-84a7e400-2050-11eb-87d0-ae52617d195e.png" alt="image" /></p>

<p>Head over to alertmanager:</p>

<pre><code>$ open http://alertmanager.localdns.xyz:9093
</code></pre>

<p>We can see alertmanager is showing the alarm:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98380013-af923800-2050-11eb-8585-f7489bf722cb.png" alt="image" /></p>

<p>When we head over to slack, we can see our notification:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98380158-de101300-2050-11eb-8d73-20828124fab5.png" alt="image" /></p>

<p>So let&rsquo;s stop our http client:</p>

<pre><code>$ docker-compose stop http-client
Stopping http-client ... done
</code></pre>

<p>Then we can see the logging stopped:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98380907-e0bf3800-2051-11eb-99c3-b3b9ac22bba5.png" alt="image" /></p>

<p>And in slack, we should see that the alarm recovered and we should see the notification:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98381360-722eaa00-2052-11eb-8bb4-07cdc8ffa7ee.png" alt="image" /></p>

<p>Then you can terminate your stack:</p>

<pre><code>$ docker-compose down
</code></pre>

<p>Pretty epic stuff right? I really love how cost effective Loki is as logging use to be so expensive to run and especially maintain, Grafana Labs are really doing some epic work and my hat goes off to them.</p>

<h2>Thanks</h2>

<p>I hope you found this useful, feel free to reach out to me on Twitter <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong> or visit me on my website <strong><a href="https://ruan.dev">ruan.dev</a></strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running Loki Behind Nginx Reverse Proxy]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/10/29/running-loki-behind-nginx-reverse-proxy/"/>
    <updated>2020-10-29T08:29:13+00:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/10/29/running-loki-behind-nginx-reverse-proxy</id>
    <content type="html"><![CDATA[<p>In this tutorial I will demonstrate how to run <strong>Loki v2.0.0</strong> behind a <strong>Nginx Reverse Proxy</strong> with basic http authentication enabled on Nginx and what to do to configure Nginx for <strong>websockets</strong>, which is required when you want to use <strong>tail in logcli</strong> via Nginx.</p>

<h2>Assumptions</h2>

<p>My environment consists of a AWS Application LoadBalancer with a Host entry and a Target Group associated to port 80 of my Nginx/Loki EC2 instance.</p>

<p>Health checks to my EC2 instance are being performed to <code>instance:80/ready</code></p>

<p>I have a S3 bucket and a DynamoDB table already running in my account which Loki will use. But <strong>NOTE</strong> that boltdb-shipper is now production ready since <a href="https://github.com/grafana/loki/blob/v2.0.0/CHANGELOG.md#20">v2.0.0</a>, which is awesome, because now you only require a object store such as S3, so you don&rsquo;t need DynamoDB.</p>

<p>More information on this topic can be found under their <a href="https://github.com/grafana/loki/blob/v2.0.0/CHANGELOG.md#20">changelog</a></p>

<h2>What can you expect from this blogpost</h2>

<p>We will go through the following topics:</p>

<ul>
<li>Install Loki v2.0.0 and Nginx</li>
<li>Configure HTTP Basic Authentication to Loki&rsquo;s API Endpoints</li>
<li>Bypass HTTP Basic Authentication to the <code>/ready</code> endpoint for our Load Balancer to perform healthchecks</li>
<li>Enable Nginx to upgrade websocket connections so that we can use <code>logcli --tail</code></li>
<li>Test out access to Loki via our Nginx Reverse Proxy</li>
<li>Install and use LogCLI</li>
</ul>


<h2>Install Software</h2>

<p>First we will install <code>nginx</code> and <code>apache2-utils</code>. In my use-case I will be using Ubuntu 20 as my operating system:</p>

<pre><code>$ sudo apt update &amp;&amp; sudp apt install nginx apache2-utils -y
</code></pre>

<p>Next we will install Loki v2.0.0, if you are upgrading from a previous version of Loki, I would recommend checking out the <a href="https://github.com/grafana/loki/releases/tag/v2.0.0">upgrade guide</a> mentioned on their releases page.</p>

<p>Download the package:</p>

<pre><code>$ curl -O -L "https://github.com/grafana/loki/releases/download/v2.0.0/loki-linux-amd64.zip"
</code></pre>

<p>Unzip the archive:</p>

<pre><code>$ unzip loki-linux-amd64.zip
</code></pre>

<p>Move the binary to your $PATH:</p>

<pre><code>$ sudo mv loki-linux-amd64 /usr/local/bin/loki
</code></pre>

<p>And ensure that the binary is executable:</p>

<pre><code>$ sudo chmod a+x /usr/local/bin/loki
</code></pre>

<h2>Configuration</h2>

<p>Create the user that will be responsible for running loki:</p>

<pre><code>$ useradd --no-create-home --shell /bin/false loki
</code></pre>

<p>Create the directory where we will place the loki configuration:</p>

<pre><code>$ mkdir /etc/loki
</code></pre>

<p>Create the loki configuration file:</p>

<pre><code>$ cat /etc/loki/loki-config.yml
auth_enabled: false

server:
  http_listen_port: 3100
  http_listen_address: 127.0.0.1
  http_server_read_timeout: 1000s
  http_server_write_timeout: 1000s
  http_server_idle_timeout: 1000s
  log_level: info

ingester:
  lifecycler:
    address: 127.0.0.1
    ring:
      kvstore:
        store: inmemory
      replication_factor: 1
    final_sleep: 0s
  chunk_encoding: snappy
  chunk_idle_period: 1h
  chunk_target_size: 1048576
  chunk_retain_period: 30s
  max_transfer_retries: 0

# https://grafana.com/docs/loki/latest/configuration/#schema_config
schema_config:
  configs:
    - from: 2020-05-15
      store: aws
      object_store: s3
      schema: v11
      index:
        prefix: loki-logging-index

storage_config:
  aws:
    http_config:
      idle_conn_timeout: 90s
      response_header_timeout: 0s
    s3: s3://myak:mysk@eu-west-1/loki-logs-datastore

    dynamodb:
      dynamodb_url: dynamodb://myak:mysk@eu-west-1

limits_config:
  enforce_metric_name: false
  reject_old_samples: true
  reject_old_samples_max_age: 168h
  ingestion_rate_mb: 30
  ingestion_burst_size_mb: 60

# https://grafana.com/docs/loki/latest/operations/storage/retention/
# To avoid querying of data beyond the retention period, max_look_back_period config in chunk_store_config
# must be set to a value less than or equal to what is set in table_manager.retention_period
chunk_store_config:
  max_look_back_period: 720h

# https://grafana.com/docs/loki/latest/operations/storage/retention/
table_manager:
  retention_deletes_enabled: true
  retention_period: 720h
  chunk_tables_provisioning:
    inactive_read_throughput: 10
    inactive_write_throughput: 10
    provisioned_read_throughput: 50
    provisioned_write_throughput: 20
  index_tables_provisioning:
    inactive_read_throughput: 10
    inactive_write_throughput: 10
    provisioned_read_throughput: 50
    provisioned_write_throughput: 20
</code></pre>

<p>Apply permissions so that the loki user has access to it&rsquo;s configuration:</p>

<pre><code>$ chown -R loki:loki /etc/loki
</code></pre>

<p>Create a systemd unit file:</p>

<pre><code>$ cat /etc/systemd/system/loki.service
[Unit]
Description=Loki
Wants=network-online.target
After=network-online.target

[Service]
User=loki
Group=loki
Type=simple
Restart=on-failure
ExecStart=/usr/local/bin/loki -config.file /etc/loki/loki-config.yml

[Install]
WantedBy=multi-user.target
</code></pre>

<p>Create the main nginx config:</p>

<pre><code>$ cat /etc/nginx/nginx.conf
user www-data;
worker_processes auto;
pid /run/nginx.pid;
include /etc/nginx/modules-enabled/*.conf;
worker_rlimit_nofile 100000;

events {
        worker_connections 4000;
        use epoll;
        multi_accept on;
}

http {

    # basic settings
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
        open_file_cache_valid 30s;
        open_file_cache_min_uses 2;
        open_file_cache_errors on;

    include /etc/nginx/mime.types;
    default_type application/octet-stream;

        # ssl settings
    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;
    ssl_prefer_server_ciphers on;

    # websockets config
    map $http_upgrade $connection_upgrade {
            default upgrade;
            '' close;
        }

    # logging settings
    access_log off;
    access_log /var/log/nginx/access.log;
    error_log /var/log/nginx/error.log;

    # gzip settings
    gzip on;
        gzip_min_length 10240;
        gzip_comp_level 1;
        gzip_vary on;
        gzip_disable msie6;
        gzip_proxied expired no-cache no-store private auth;
        gzip_types
        text/css
        text/javascript
        text/xml
        text/plain
        text/x-component
        application/javascript
        application/x-javascript
        application/json
    application/xml
        application/rss+xml
    application/atom+xml
        font/truetype
        font/opentype
        application/vnd.ms-fontobject
        image/svg+xml;
        reset_timedout_connection on;
        client_body_timeout 10;
        send_timeout 2;
        keepalive_requests 100000;

        # virtual host configs
    include /etc/nginx/conf.d/loki.conf;
}
</code></pre>

<p>Create the virtual host config:</p>

<pre><code>$ cat /etc/nginx/conf.d/loki.conf
upstream loki {
  server 127.0.0.1:3100;
  keepalive 15;
}

server {
  listen 80;
  server_name loki.localdns.xyz;

  auth_basic "loki auth";
  auth_basic_user_file /etc/nginx/passwords;

  location / {
    proxy_read_timeout 1800s;
    proxy_connect_timeout 1600s;
    proxy_pass http://loki;
    proxy_http_version 1.1;
    proxy_set_header Upgrade $http_upgrade;
    proxy_set_header Connection $connection_upgrade;
    proxy_set_header Connection "Keep-Alive";
    proxy_set_header Proxy-Connection "Keep-Alive";
    proxy_redirect off;
  }

  location /ready {
    proxy_pass http://loki;
    proxy_http_version 1.1;
    proxy_set_header Connection "Keep-Alive";
    proxy_set_header Proxy-Connection "Keep-Alive";
    proxy_redirect off;
    auth_basic "off";
  }
}
</code></pre>

<p>As you&rsquo;ve noticed, we are providing a <code>auth_basic_user_file</code> to <code>/etc/nginx/passwords</code>, so let&rsquo;s create a user that we will be using to authenticate against loki:</p>

<pre><code>$ htpasswd -c /etc/nginx/passwords lokiisamazing
</code></pre>

<h2>Enable and Start Services</h2>

<p>Because we created a systemd unit file, we need to reload the systemd daemon:</p>

<pre><code>$ sudo systemctl daemon-reload
</code></pre>

<p>Then enable nginx and loki on boot:</p>

<pre><code>$ sudo systemctl enable nginx
$ sudo systemctl enable loki
</code></pre>

<p>Then start or restart both services:</p>

<pre><code>$ sudo systemctl restart nginx
$ sudo systemctl restart loki
</code></pre>

<p>You should see both ports, 80 and 3100 are listening:</p>

<pre><code>$ sudo netstat -tulpn | grep -E '(3100|80)'
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      8949/nginx: master
tcp        0      0 127.0.0.1:3100          0.0.0.0:*               LISTEN      23498/loki
</code></pre>

<h2>Test Access</h2>

<p>You will notice that I have a <code>/ready</code> endpoint that I am proxy passing to loki, which bypasses authentication, this has been setup for my AWS Application Load Balancer&rsquo;s Target Group to perform health checks against.</p>

<p>We can verify if we are getting a 200 response code without passing authentication:</p>

<pre><code>$ curl -i http://loki.localdns.xyz/ready
HTTP/1.1 200 OK
Server: nginx/1.14.0 (Ubuntu)
Date: Thu, 29 Oct 2020 09:15:52 GMT
Content-Type: text/plain; charset=utf-8
Content-Length: 6
Connection: keep-alive
X-Content-Type-Options: nosniff

ready
</code></pre>

<p>If we try to make a request to Loki&rsquo;s labels API endpoint, you will notice that we are returned with a 401 unauthorized response:</p>

<pre><code>$ curl -i http://loki.localdns.xyz/loki/api/v1/labels
HTTP/1.1 401 Unauthorized
Server: nginx/1.14.0 (Ubuntu)
Date: Thu, 29 Oct 2020 09:16:52 GMT
Content-Type: text/html
Content-Length: 204
Connection: keep-alive
WWW-Authenticate: Basic realm="loki auth"
</code></pre>

<p>So let&rsquo;s access the labels API endpoint by passing our basic auth credentials. To leave no leaking passwords behind, create a file and save your password content in that file:</p>

<pre><code>$ vim /tmp/.pass
-&gt; then enter your password and save the file &lt;-
</code></pre>

<p>Expose the content as an environment variable:</p>

<pre><code>$ pass=$(cat /tmp/.pass)
</code></pre>

<p>Now make a request to Loki&rsquo;s labels endpoint by passing authentication:</p>

<pre><code>$ curl -i -u lokiisawesome:$pass http://loki.localdns.xyz/loki/api/v1/labels
HTTP/1.1 200 OK
Server: nginx/1.14.0 (Ubuntu)
Date: Thu, 29 Oct 2020 09:20:20 GMT
Content-Type: application/json; charset=UTF-8
Content-Length: 277
Connection: keep-alive

{"status":"success","data":["__name__","aws_account","cluster_name","container_name","environment","filename","job","service","team"]}
</code></pre>

<p>Then ensure that your remove the password file:</p>

<pre><code>$ rm -rf /tmp/.pass
</code></pre>

<p>And unset your pass environment variable, to clean up your tracks:</p>

<pre><code>$ unset pass
</code></pre>

<h2>LogCLI</h2>

<p>Now for my favorite part, using logcli to interact with Loki, but more specifically using <code>--tail</code> as it requires websockets, nginx will now be able to upgrade those connections:</p>

<p>Install logcli, in my case I am using a mac, so I will be using darwin:</p>

<pre><code>$ wget https://github.com/grafana/loki/releases/download/v2.0.0/logcli-darwin-amd64.zip
$ unzip logcli-darwin-amd64.zip
$ mv logcli-darwin-amd64 /usr/local/bin/logcli
</code></pre>

<p>Set your environment variables for logcli:</p>

<pre><code>$ export LOKI_ADDR=https://loki.yourdomain.com # im doing ssl termination on the aws alb
$ export LOKI_USERNAME=lokiisawesome
$ export LOKI_PASSWORD=$pass 
</code></pre>

<p>Now for that sweetness of tailing ALL THE LOGS!! :-D . Let&rsquo;s first discover the label that we want to select:</p>

<pre><code>$ logcli labels --quiet container_name | grep deadman
ecs-deadmanswitch-4-deadmanswitch-01234567890abcdefghi
</code></pre>

<p>Then tail for the win!</p>

<pre><code>$ logcli query --quiet --output raw --tail '{job="prod/dockerlogs", container_name=~"ecs-deadmanswitch.*"}'
time="2020-10-29T09:03:36Z" level=info msg="timerID: xxxxxxxxxxxxxxxxxxxx"
time="2020-10-29T09:03:36Z" level=info msg="POST - /ping/xxxxxxxxxxxxxxxxxxx"
</code></pre>

<p>Awesome right?</p>

<p><img src="https://media.giphy.com/media/3ohzdIuqJoo8QdKlnW/giphy.gif" alt="" /></p>

<h2>Thank You</h2>

<p>Hope that you found this useful, make sure to follow Grafana&rsquo;s blog for more awesome content:</p>

<ul>
<li><a href="https://grafana.com/blog/">https://grafana.com/blog/</a></li>
</ul>


<p>If you liked this content, please make sure to share or come say hi on my website or twitter:</p>

<ul>
<li><a href="https://ruan.dev">w: ruan.dev</a></li>
<li><a href="https://ruan.dev">t: @ruanbekker</a></li>
</ul>


<p>For other content of mine on Loki:</p>

<ul>
<li><a href="https://blog.ruanbekker.com/blog/categories/loki/">https://blog.ruanbekker.com/blog/categories/loki/</a></li>
<li><a href="https://github.com/ruanbekker/docker-loki-distributed-minio">https://github.com/ruanbekker/docker-loki-distributed-minio</a></li>
<li><a href="https://github.com/ruanbekker/loki-docker-nginx-example">https://github.com/ruanbekker/loki-docker-nginx-example</a></li>
<li><a href="https://github.com/ruanbekker/loki-minio-docker">https://github.com/ruanbekker/loki-minio-docker</a></li>
<li><a href="https://github.com/ruanbekker/cheatsheets/tree/master/loki">https://github.com/ruanbekker/cheatsheets/tree/master/loki</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started on Logging With Loki Using Docker]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/08/13/getting-started-on-logging-with-loki-using-docker/"/>
    <updated>2020-08-13T13:39:28+00:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/08/13/getting-started-on-logging-with-loki-using-docker</id>
    <content type="html"><![CDATA[<p>Logging with Loki is AMAZING!</p>

<p>In the past couple of months i&rsquo;ve been working a lot with logging, but more specifically logging with loki. As most of my metrics reside in prometheus, I use grafana quite extensively and logging was always the one that stood out a bit as I pushed my logs to elasticsearch and consumed them from grafana. Which worked pretty well, but the maintenance and resource costs was a bit too much for what I was looking for.</p>

<p>And then grafana released Loki, which is like prometheus, but for logs. And that was just super, exactly what I was looking for. For my use case, I was looking for something that can be consumed by grafana as a presentation layer, central based so I can push all sorts of logs, and want a easy way to grep for logs and a bonus would be to have a cli tool.</p>

<p>And Loki checked all those boxes!</p>

<div class="tenor-gif-embed" data-postid="7644619" data-share-method="host" data-width="100%" data-aspect-ratio="1.1971153846153846"><a href="https://tenor.com/view/oh-yeah-gif-7644619">Oh Yeah Parks And Recreation GIF</a> from <a href="https://tenor.com/search/ohyeah-gifs">Ohyeah GIFs</a></div>


<script type="text/javascript" async src="https://tenor.com/embed.js"></script>


<h2>What can you expect from this blog</h2>

<p>In this post will be a getting started guide to Loki, we will provision Loki, Grafana and Nginx using Docker to get our environment up and running, so that we can push our nginx container logs to the loki datasource, and access the logs via grafana.</p>

<p>We will then generate some logs so that we can show a couple of query examples using the log query language (LogQL) and use the LogCLI to access our logs via cli.</p>

<p>In a <a href="">future post</a>, I will demonstrate how to setup Loki for a non-docker deployment.</p>

<h2>Some useful information about Loki</h2>

<p>Let&rsquo;s first talk about Loki compared with Elasticsearch, as they are not the same:</p>

<ol>
<li>Loki does not index the text of the logs, instead grouping entries into streams and index those with labels</li>
<li>Things like full text search engines tokenizes your text into k/v pairs and gets written to an inverted index, which over time in my opinion gets complex to maintain, expensive to scale, storage retention, etc.</li>
<li>Loki is advertised as easy to scale, affordable to operate as it uses DynamoDB for Indexing and S3 for Storage</li>
<li>When using Loki, you may need to forget what you know and look to see how the problem can be solved differently with parallelization. Lokiâ€™s superpower is breaking up queries into small pieces and dispatching them in parallel so that you can query huge amounts of log data in small amounts of time.</li>
</ol>


<p>If we look at the <strong>Loki Log Model</strong>, we can see that the timestamp and the labels are indexed and the content of the logs are not indexed:</p>

<p><img src="https://img.sysadmins.co.za/cpr6n7.png" alt="loki" /></p>

<p>A <strong>log stream</strong> is a stream of log entries with the same exact label set:</p>

<p><img src="https://img.sysadmins.co.za/el6djk.png" alt="loki" /></p>

<p>For the <strong>storage</strong> side, inside each chunk, log entries are sorted by timestamp. Loki only indexes minimum and maximum timestamps of a chunk. Storage options support local storage, AWS S3, Google Cloud Storage and Azure</p>

<p><img src="https://img.sysadmins.co.za/959pjw.png" alt="loki" /></p>

<p>For <strong>chunks and querying</strong>, chunks are filled per stream and they are flushed of a few criterias such as age and size:</p>

<p><img src="https://img.sysadmins.co.za/ekm8cy.png" alt="loki" /></p>

<p>And one of the most important parts are the <strong>labels</strong>, labels define the stream and therefore its very important.</p>

<p>High cardinality is bad for labels, as something like a IP address can reduce your performance a lot, as it will create a stream for every unique IP label.</p>

<p>Static defined labels such as environment, hostnames are good, you can read more up about it <a href="https://grafana.com/blog/2020/04/21/how-labels-in-loki-can-make-log-queries-faster-and-easier/">here</a></p>

<p>Here is a info graphic on how one log line can be split up into 36 streams:</p>

<p><img src="https://img.sysadmins.co.za/g119oq.png" alt="" /></p>

<p>So with that being said, <strong>good labels</strong> will be considered as cluster, job, namespace, environment, etc where as <strong>bad labels</strong> will be things such as userid, ip address, url path, etc</p>

<h2>Selecting logstreams with Loki</h2>

<p>Selecting logstreams, is done by using <strong>label matchers</strong> and <strong>filter expressions</strong>, such as this example:</p>

<pre><code>{job="dockerlogs", environment="development"} |= "POST" |~ "196.35.64.+"
</code></pre>

<p>Label Matchers and Filter Expressions support:</p>

<ul>
<li><code>=</code> Contains string</li>
<li><code>!=</code> Does not contain string</li>
<li><code>=~</code> Matches regular expression</li>
<li><code>!~</code> Does not match regular expression</li>
</ul>


<h2>Supported Clients</h2>

<p>At the moment of writing, loki supports the following log clients:</p>

<ul>
<li>Promtail (tails logs and ships to Loki)</li>
<li>Docker Driver</li>
<li>Fluentd</li>
<li>Fluent Bit</li>
<li>Logstash</li>
</ul>


<p>We will be going into more detail on using promtail in a <a href="">future post</a>, but you can read more up about it <a href="https://github.com/grafana/loki/tree/master/cmd">here</a></p>

<h2>Loki in Action</h2>

<p>Time to get to the fun part, clone my <a href="https://github.com/ruanbekker/loki-docker-nginx-example">github repo</a>:</p>

<pre><code>$ git clone https://github.com/ruanbekker/loki-docker-nginx-example
$ cd loki-docker-nginx-example
</code></pre>

<p>You can inspect the docker-compose.yml:</p>

<pre><code>$ cat docker-compose.yml
version: "3.4"

services:
  my-nginx-service:
    image: nginx
    container_name: my-nginx-service
    ports:
      - 8000:80
    environment:
      - FOO=bar
    logging:
      driver: loki
      options:
        loki-url: http://localhost:3100/loki/api/v1/push
        loki-external-labels: job=dockerlogs,owner=ruan,environment=development

  grafana:
    image: grafana/grafana:7.1.1
    volumes:
    - ./config/datasource.yml:/etc/grafana/provisioning/datasources/datasource.yml
    ports:
    - "3000:3000"

  loki:
   image: grafana/loki:v1.3.0
   volumes:
     - ./config/loki.yaml:/etc/config/loki.yaml
   entrypoint:
     - /usr/bin/loki
     - -config.file=/etc/config/loki.yaml
   ports:
     - "3100:3100"
</code></pre>

<p>As you can see loki will be the datasource where we will be pushing our logs to from our nginx container and we are defining our logging section where it should find loki and we are also setting labels to that log stream using <code>loki-external-labels</code>. Then we are using grafana to auto configure the loki datasource from the <code>./config/datasource.yml</code> section so that we can visualize our logs.</p>

<p>If you don&rsquo;t want to define the logging section per container, you can always set the defaults using the <code>/etc/docker/daemon.json</code> by following <a href="https://grafana.com/docs/loki/latest/clients/docker-driver/configuration/#change-the-default-logging-driver">this guide</a></p>

<p>Let&rsquo;s boot up our stack:</p>

<pre><code>$ docker-compose up
</code></pre>

<p>After everything is up, you should be able to access nginx by visiting: <code>http://nginx.localdns.xyz:8000/</code>, after you received a response, visit Grafana on <code>http://grafana.localdns.xyz:3000</code> using the username and password: <code>admin/admin</code>.</p>

<p>If you head over to datasources, you should see the loki datasource which was provisioned for you:</p>

<p><img src="https://img.sysadmins.co.za/tyn0ny.png" alt="loki-grafana" /></p>

<p>When you head to the left on explore and you select the loki datasource on <code>http://grafana.localdns.xyz:3000/explore</code> you should see the following:</p>

<p><img src="https://img.sysadmins.co.za/5kp07m.png" alt="loki-grafana" /></p>

<p>You will see that grafana discovers logstreams with the label <code>job</code> as you can see that our <code>job="dockerlogs"</code> can be seen there. We can either click on it, select the log labels from the left and browse the label we want to select or manually enter the query.</p>

<p>I will be using the query manually:</p>

<pre><code>{job="dockerlogs"}
</code></pre>

<p>So now we will get all the logs that has that label associated and as you can see, we see our request that we made:</p>

<p><img src="https://img.sysadmins.co.za/gra0oe.png" alt="nginx-grafana-loki" /></p>

<p>We can see one error due to the favicon.ico that it could not find, but let&rsquo;s first inspect our first log line:</p>

<p><img src="https://img.sysadmins.co.za/6dbuqn.png" alt="loki" /></p>

<p>Here we can see the labels assigned to that log event, which we can include in our query, like if we had multiple services and different environments, we can use a query like the following to only see logs for a specific service and environment:</p>

<pre><code>{job="dockerlogs", environment="development", compose_service="my-nginx-service"}
</code></pre>

<p>In the example above we used the selectors to select the logs we want to see, now we can use our filter expressions, to &ldquo;grep&rdquo; our logs.</p>

<p>Let&rsquo;s say we want to focus only on one service, and we want to filter for any logs with GET requests, so first we select to service then apply the filter expression:</p>

<pre><code>{compose_service="my-nginx-service"} |= "GET"
</code></pre>

<p><img src="https://img.sysadmins.co.za/vv609g.png" alt="loki-logs" /></p>

<p>As you can see we can see the ones we were looking for, we can also chain them, so we want to se GET&rsquo;s and errors:</p>

<pre><code>{compose_service="my-nginx-service"} |= "GET" |= "error"
</code></pre>

<p>And lets say for some reason we only want to see the logs that comes from a 192.168.32 subnet:</p>

<pre><code>{compose_service="my-nginx-service"} |= "GET" |= "error" |~ "192.168.32."
</code></pre>

<p>But we dont want to see requests from &ldquo;nginx.localdns.xyz&rdquo;:</p>

<pre><code>{compose_service="my-nginx-service"} |= "GET" |= "error" |~ "192.168.32." != "nginx.localdns.xyz"
</code></pre>

<p>Make two extra get requests to &ldquo;foo.localdns.xyz:8000&rdquo; and &ldquo;bar.localdns.xyz:8000&rdquo; and then we change the query to say that we only want to see errors and hostnames coming from the 2 requests that we made:</p>

<pre><code>{compose_service="my-nginx-service"} |= "error" |~ "(foo|bar).localdns.xyz"
</code></pre>

<p>If we expand one of the log lines, we can do a ad-hoc analysis to see the percentage of logs by source for example:</p>

<p><img src="https://img.sysadmins.co.za/9ctz6d.png" alt="loki-logs" /></p>

<h2>LogCLI</h2>

<p>If you prefer the cli to query logs, logcli is the command line client for loki, allows you to query logs from your terminal and has clients for linux, mac and windows.</p>

<p>Check the releases for the latest version:</p>

<ul>
<li><a href="https://github.com/grafana/loki/releases">https://github.com/grafana/loki/releases</a></li>
</ul>


<pre><code>$ wget https://github.com/grafana/loki/releases/download/v1.5.0/logcli-darwin-amd64.zip
$ unzip logcli-darwin-amd64.zip
$ mv logcli-darwin-amd64 /usr/local/bin/logcli
</code></pre>

<p>Set your environment details, in our case we dont have a username and password for loki:</p>

<pre><code>$ #export LOKI_USERNAME=${MYUSER}
$ #export LOKI_PASSWORD=${MYPASS}
$ export LOKI_ADDR=http://localhost:3001
</code></pre>

<p>We can view all our labels, letâ€™s view all the job labels:</p>

<pre><code>$ logcli labels job
http://localhost:3001/loki/api/v1/label/job/values
dockerlogs
</code></pre>

<p>Letâ€™s look at family apps nginx logs:</p>

<pre><code>$ logcli query '{job="dockerlogs"}'
http://localhost:3001/loki/api/v1/query_range?direction=BACKWARD&amp;end=1587727924005496000&amp;limit=30&amp;query=%7Bjob%3D%22dockerlogs%22%2C&amp;start=1587724324005496000
Common labels: {environment="development", owner="ruan", compose_service="my-nginx-service", job="dockerlogs", host="docker-desktop", compose_project="loki-nginx-docker"}
2020-08-13 17:08:40 192.168.32.1 - - [13/Aug/2020:15:08:40 +0000] "GET / HTTP/1.1" 200 612 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:79.0) Gecko/20100101 Firefox/79.0" "-"
</code></pre>

<p>We can also pipe that output to grep, awk, etc:</p>

<pre><code>$ logcli query '{job="dockerlogs"}' | grep GREP | awk -F 'X' '{print  $1}'
</code></pre>

<p>Supported arguments:</p>

<pre><code>$ logcli query --help
usage: logcli query [&lt;flags&gt;] &lt;query&gt;


Run a LogQL query.


Flags:
      --help             Show context-sensitive help (also try --help-long and --help-man).
      --version          Show application version.
  -q, --quiet            suppress everything but log lines
      --stats            show query statistics
  -o, --output=default   specify output mode [default, raw, jsonl]
  -z, --timezone=Local   Specify the timezone to use when formatting output timestamps [Local, UTC]
      --addr="http://localhost:3100"
                         Server address. Can also be set using LOKI_ADDR env var.
      --username=""      Username for HTTP basic auth. Can also be set using LOKI_USERNAME env var.
      --password=""      Password for HTTP basic auth. Can also be set using LOKI_PASSWORD env var.
      --ca-cert=""       Path to the server Certificate Authority. Can also be set using LOKI_CA_CERT_PATH env var.
      --tls-skip-verify  Server certificate TLS skip verify.
      --cert=""          Path to the client certificate. Can also be set using LOKI_CLIENT_CERT_PATH env var.
      --key=""           Path to the client certificate key. Can also be set using LOKI_CLIENT_KEY_PATH env var.
      --org-id=ORG-ID    org ID header to be substituted for auth
      --limit=30         Limit on number of entries to print.
      --since=1h         Lookback window.
      --from=FROM        Start looking for logs at this absolute time (inclusive)
      --to=TO            Stop looking for logs at this absolute time (exclusive)
      --step=STEP        Query resolution step width
      --forward          Scan forwards through logs.
      --no-labels        Do not print any labels
      --exclude-label=EXCLUDE-LABEL ...
                         Exclude labels given the provided key during output.
      --include-label=INCLUDE-LABEL ...
                         Include labels given the provided key during output.
      --labels-length=0  Set a fixed padding to labels
  -t, --tail             Tail the logs
      --delay-for=0      Delay in tailing by number of seconds to accumulate logs for re-ordering


Args:
  &lt;query&gt;  eg '{foo="bar",baz=~".*blip"} |~ ".*error.*"'
</code></pre>

<h2>Thank you</h2>

<p>I hope this was useful</p>
]]></content>
  </entry>
  
</feed>
