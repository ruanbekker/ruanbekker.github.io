<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Monitoring | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/monitoring/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2022-06-18T22:57:04-04:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Setup TLS and Basic Authentication on Node Exporter for Prometheus]]></title>
    <link href="https://blog.ruanbekker.com/blog/2021/10/10/setup-basic-authentication-on-node-exporter-and-prometheus/"/>
    <updated>2021-10-10T16:50:17-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2021/10/10/setup-basic-authentication-on-node-exporter-and-prometheus</id>
    <content type="html"><![CDATA[<p>I had a public VPS server that I wanted to scrape node-exporter metrics from, but my Prometheus instance was behind a Dynamic IP address, so to allow only my prometheus instance to scrape my Node Exporter instance, was a bit difficult, since the IP keep changing and I had to update my iptables firewall rules.</p>

<p>In this tutorial I will show you how to setup TLS and Basic Authentication on Node Exporter, and how to configure prometheus to pass the auhtentication to successfully scrape the node exporter metrics endpoint.</p>

<h2>Install Node Exporter</h2>

<p>On the node-exporter host, set the environment variables for the version, user and directory path where node exporter will be installed::</p>

<pre><code class="bash">$ NODE_EXPORTER_VERSION="1.1.2"
$ NODE_EXPORTER_USER="node_exporter"
$ BIN_DIRECTORY="/usr/local/bin"
</code></pre>

<p>Download and place the node-exporter binary in place:</p>

<pre><code class="bash">$ wget https://github.com/prometheus/node_exporter/releases/download/v${NODE_EXPORTER_VERSION}/node_exporter-${NODE_EXPORTER_VERSION}.linux-amd64.tar.gz
$ tar -xf node_exporter-${NODE_EXPORTER_VERSION}.linux-amd64.tar.gz
$ cp node_exporter-${NODE_EXPORTER_VERSION}.linux-amd64/node_exporter ${BIN_DIRECTORY}/
$ chown ${NODE_EXPORTER_USER}:${NODE_EXPORTER_USER} ${BIN_DIRECTORY}/node_exporter
$ rm -rf node_exporter-${NODE_EXPORTER_VERSION}.linux-amd64*
$ mkdir /etc/node-exporter
</code></pre>

<h2>Configuration</h2>

<p>Create a self-signed cert for node-exporter:</p>

<pre><code class="bash">$ openssl req -new -newkey rsa:2048 -days 365 -nodes -x509 -keyout node_exporter.key -out node_exporter.crt -subj "/C=ZA/ST=CT/L=SA/O=VPN/CN=localhost" -addext "subjectAltName = DNS:localhost"
</code></pre>

<p>Move the certs into the directory we created:</p>

<pre><code class="bash">$ mv node_exporter.* /etc/node-exporter/
</code></pre>

<p>Install htpasswd so that we can generate a password hash with bcrypt, which will prompt you for a password that we are setting for the prometheus user::</p>

<pre><code class="bash">$ apt install apache2-utils
$ htpasswd -nBC 10 "" | tr -d ':\n'; echo
</code></pre>

<p>Now populate the config for node-exporter:</p>

<pre><code class="bash">$ cat /etc/node-exporter/config.yml
tls_server_config:
  cert_file: node_exporter.crt
  key_file: node_exporter.key
basic_auth_users:
  prometheus: &lt;the-output-value-of-htpasswd&gt;
</code></pre>

<p>Change the ownership of the node exporter directory:</p>

<pre><code class="bash">$ chown -R ${NODE_EXPORTER_USER}:${NODE_EXPORTER_USER} /etc/node-exporter
</code></pre>

<p>Then create the systemd unit file:</p>

<pre><code>$ cat &gt; /etc/systemd/system/node_exporter.service &lt;&lt; EOF
[Unit]
Description=Node Exporter
Wants=network-online.target
After=network-online.target
StartLimitIntervalSec=500
StartLimitBurst=5
[Service]
User=${NODE_EXPORTER_USER}
Group=${NODE_EXPORTER_USER}
Type=simple
Restart=on-failure
RestartSec=5s
ExecStart=${BIN_DIRECTORY}/node_exporter --web.config=/etc/node-exporter/config.yml
[Install]
WantedBy=multi-user.target
EOF
</code></pre>

<p>Reload systemd and start node-exporter</p>

<pre><code class="bash">$ systemctl daemon-reload
$ systemctl enable node_exporter
$ systemctl restart node_exporter
</code></pre>

<h2>Prometheus Config</h2>

<p>Copy the <code>/etc/node-exporter/node_exporter.crt</code> from the node-exporter node to prometheus-node, then in the <code>/etc/prometheus/prometheus.yml</code> config:</p>

<pre><code class="yaml">scrape_configs:
  - job_name: 'node-exporter-tls'
    scheme: https
    basic_auth:
      username: prometheus
      password: &lt;the-plain-text-password&gt;
    tls_config:
      ca_file: node_exporter.crt
      insecure_skip_verify: true
    static_configs:
    - targets: ['node-exporter-ip:9100']
      labels:
        instance: friendly-instance-name
</code></pre>

<p>After you restart prometheus, you should see the metrics in prometheus' tsdb of the node exporter target that we are scraping.</p>

<h2>Thank You</h2>

<p>Thanks for reading, if you like my content, check out my <strong><a href="https://ruan.dev">website</a></strong> or follow me at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Graphing Covid-19 Stats With Grafana and Elasticsearch Using Python]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/04/26/graphing-covid-19-stats-with-grafana-and-elasticsearch-using-python/"/>
    <updated>2020-04-26T02:24:27+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/04/26/graphing-covid-19-stats-with-grafana-and-elasticsearch-using-python</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/80421197-62345180-88dc-11ea-9e0a-557199aaf613.png" alt="coronavirus-covid19-grafana-metrics" /></p>

<p>I stumbled upon a <a href="https://github.com/pomber/covid19/">github repository</a> that stores time-series data in json format of corona virus / covid19 statistics, which get updated daily.</p>

<p>I was curious to see data about my country and want to see how metrics will look like after our lockdown started, so I decided to consume that data with <strong>Python</strong> and the requests library, then ingest data about covid19 into <strong>Elasticsearch</strong> and the visualize the data with <strong>Grafana</strong>.</p>

<h2>Sample of the Data</h2>

<p>Let&rsquo;s have a peek at the data to determine how we will use it to write to Elasticsearch. Let&rsquo;s consume the data with python:</p>

<pre><code>&gt;&gt;&gt; import requests
&gt;&gt;&gt; import json
&gt;&gt;&gt; response = requests.get('https://pomber.github.io/covid19/timeseries.json').json()
</code></pre>

<p>Now let&rsquo;s determine the data type:</p>

<pre><code>&gt;&gt;&gt; type(response)
&lt;type 'dict'&gt;
</code></pre>

<p>Now as it&rsquo;s a dictionary, let&rsquo;s look at they keys:</p>

<pre><code>&gt;&gt;&gt; response.keys()
[u'Canada', u'Sao Tome and Principe', u'Lithuania', u'Cambodia', u'Ethiopia',....
</code></pre>

<p>So let&rsquo;s take a look how the data looks like if we do a lookup for Canada:</p>

<pre><code>&gt;&gt;&gt; type(response['Canada'])
&lt;type 'list'&gt;
</code></pre>

<p>As we can see it&rsquo;s a list, we can count how many items is in our list:</p>

<pre><code>&gt;&gt;&gt; len(response['Canada'])
94
</code></pre>

<p>Now let&rsquo;s peek at the data by accessing our first index of our list:</p>

<pre><code>&gt;&gt;&gt; response['Canada'][0]
{u'date': u'2020-1-22', u'confirmed': 0, u'recovered': 0, u'deaths': 0}
</code></pre>

<p>So our data will look like this:</p>

<pre><code>{
  [
    'Country Name': [
      {
        'date': '&lt;string&gt;', 
        'confirmed': '&lt;int&gt;', 
        'recovered': '&lt;int&gt;', 
        'deaths': '&lt;int&gt;'
      },
      {
        'date': '&lt;string&gt;',
        'confirmed': '&lt;int&gt;',
        'recovered': '&lt;int&gt;',
        'deaths': '&lt;int&gt;'
      },
    ],
    'Country Name': [
      ...
    ]
  ]
}
</code></pre>

<h2>Some issues we need to fix</h2>

<p>As you can see the date is displayed as <code>2020-1-22</code> instead of <code>2020-01-22</code>, I want to make it consistent as I will be ingesting the data with a <code>@timestamp</code> key which we will use the date from the returned data. So first we will need to convert that before we ingest the data.</p>

<p>The other thing I was thinking of is that, if for some reason we need to ingest this data again, we dont want to sit with duplicates (same document with different _id&rsquo;s), so for that I decided to generate a hash value that consist of the date and the country, so if the script run to ingest the data, it will use the same id for the specific document, which would just overwrite it, therefore we won&rsquo;t sit with duplicates.</p>

<p>So the idea is to ingest a document to elasticsearch like this:</p>

<pre><code>doc = {
    "_id": "sha_hash_value",
    "day": "2020-01-22",
    "timestamp": "@2020-01-22 00:00:00",
    "country": "CountryName",
    "confirmed": 0,
    "recovered": 0,
    "deaths": 0
}
</code></pre>

<h2>How we will ingest the data</h2>

<p>The first run will load all the data and ingest all the data up to the current day to elasticsearch. Once that is done, we will add code to our script to only ingest the most recent day&rsquo;s data into elasticsearch, which we will control with a cronjob.</p>

<p>Create a index with a mapping to let Elasticsearch know <code>timestamp</code> will be a date field:</p>

<pre><code>$ curl -XPUT -H 'Content-Type: application/json' \
  -u username:pass 'https://es.domain.com/coronastats' -d \
  '{"mappings": {"foo1": {"properties": {"timestamp" : {"type" : "date","format" : "yyyy-MM-dd HH:mm:ss"}}}}}'
</code></pre>

<p>Once our index is created, create the python script that will load the data, loop through each country&rsquo;s daily data and ingest it into elasticsearch:</p>

<pre><code class="python">#!/usr/bin/python
import requests
import datetime as dt
import json
import hashlib

url = 'https://pomber.github.io/covid19/timeseries.json'
elasticsearch_url = "https://es.domain.com"
elasticsearch_username = ""
elasticsearch_password = ""

api_response = requests.get(url).json()

def convert_datestamp(day):
    return str(dt.datetime.strptime(day, '%Y-%m-%d'))

def hash_function(country, date):
    string_to_hash = country + date
    hash_obj  = hashlib.sha1(string_to_hash.encode('utf-8'))
    hash_value = hash_obj.hexdigest()
    return hash_value

def map_es_doc(payload, country):
    doc = {
        "day": payload['date'],
        "timestamp": convert_datestamp(payload['date']),
        "country": country,
        "confirmed": payload['confirmed'],
        "recovered": payload['recovered'],
        "deaths": payload['deaths']
    }
    return doc

def ingest(doc_id, payload):
    response = requests.put(
        elasticsearch_url + '/coronastats/coronastats/' + doc_id,
        auth=(elasticsearch_username, elasticsearch_password),
        headers={'content-type': 'application/json'},
        json=payload
    )
    return response.status_code

for country in api_response.keys():
    try:
        for each_payload in api_response[country]:
            doc_id = hash_function(country, each_payload['date'])
            doc = map_es_doc(each_payload, country)
            response = ingest(doc_id, doc)
            print(response)
    except Exception as e:
        print(e)
</code></pre>

<p>Run the script to ingest all the data into elasticsearch. Now we will create the script that will run daily to only ingest the previous day&rsquo;s data, so that we only ingest the latest data and not all the data from scratch again.</p>

<p>I will create this file in <code>/opt/scripts/corona_covid19_ingest.py</code>:</p>

<pre><code>#!/usr/bin/python
import requests
import datetime as dt
import json
import hashlib

url = 'https://pomber.github.io/covid19/timeseries.json'
elasticsearch_url = "https://es.domain.com"
elasticsearch_username = ""
elasticsearch_password = ""

api_response = requests.get(url).json()

yesterdays_date = dt.date.today() - dt.timedelta(days=1)

def convert_datestamp(day):
    return str(dt.datetime.strptime(day, '%Y-%m-%d'))

def hash_function(country, date):
    string_to_hash = country + date
    hash_obj  = hashlib.sha1(string_to_hash.encode('utf-8'))
    hash_value = hash_obj.hexdigest()
    return hash_value

def map_es_doc(payload, country):
    doc = {
        "day": payload['date'],
        "timestamp": convert_datestamp(payload['date']),
        "country": country,
        "confirmed": payload['confirmed'],
        "recovered": payload['recovered'],
        "deaths": payload['deaths']
    }
    return doc

def ingest(doc_id, payload):
    response = requests.put(
        elasticsearch_url + '/coronastats/coronastats/' + doc_id,
        auth=(elasticsearch_username, elasticsearch_password),
        headers={'content-type': 'application/json'},
        json=payload
    )
    return response.status_code

for country in api_response.keys():
    try:
        for each_payload in api_response[country]:
            if convert_datestamp(each_payload['date']).split()[0] == str(yesterdays_date):
                print("ingesting latest data for {country}".format(country=country))
                doc_id = hash_function(country, each_payload['date'])
                doc = map_es_doc(each_payload, country)
                response = ingest(doc_id, doc)
                print(response)
    except Exception as e:
        print(e)
</code></pre>

<p>The only difference with this script is that it checks if the date is equals to yesterday&rsquo;s date, and if so the document will be prepared and ingested into elasticsearch. We will create a cronjob that runs this script every morning at 08:45.</p>

<p>First make the file executable:</p>

<pre><code>$ chmod +x /opt/scripts/corona_covid19_ingest.py
</code></pre>

<p>Run <code>crontab -e</code> and add the following</p>

<pre><code>45 8 * * * /opt/scripts/corona_covid19_ingest.py
</code></pre>

<h2>Visualize the Data with Grafana</h2>

<p>We will create this dashboard:</p>

<p><img src="https://user-images.githubusercontent.com/567298/80418135-35ca0680-88d7-11ea-83f6-3432a903333d.png" alt="corona-covid-19-dashboard" /></p>

<p>We need a elasticsearch datasource that points to the index that we ingest our data into. Head over to datasources, add a elasticsearch datasource and set the index to <code>coronastats</code> and add the timefield as <code>timestamp</code>.</p>

<p>We want to make the dashboard dynamic to have a <strong>&ldquo;country&rdquo;</strong> dropdown selector, for that go to the dashboard settings, select variable and add a country variable:</p>

<p><img src="https://user-images.githubusercontent.com/567298/80419463-7cb8fb80-88d9-11ea-959f-8f37ae3f6dc7.png" alt="covid19-dashboard-variables" /></p>

<p>First panel: &ldquo;Reported Cases per Day&rdquo;:</p>

<p><img src="https://user-images.githubusercontent.com/567298/80419572-af62f400-88d9-11ea-802e-7eeacb61ee19.png" alt="covid19-reported-cases" /></p>

<p>Second panel: &ldquo;Confirmed Cases&rdquo;:</p>

<p><img src="https://user-images.githubusercontent.com/567298/80419675-db7e7500-88d9-11ea-98a5-3aae4d9a6c87.png" alt="covid19-confirmed-cases" /></p>

<p>Third panel: &ldquo;Recovered Cases&rdquo;:</p>

<p><img src="https://user-images.githubusercontent.com/567298/80419750-fa7d0700-88d9-11ea-82a3-f26ff8c807ef.png" alt="covid19-recovered-cases" /></p>

<p>Now, if we select Italy, Spain and France as an example, we will see something like this:</p>

<p><img src="https://user-images.githubusercontent.com/567298/80419966-56479000-88da-11ea-8f30-39ac3da27007.png" alt="covid19-country-stats" /></p>

<h2>Thank You</h2>

<p>Although its pretty cool visualizing data, the issue that we are in at the moment with coronavirus / covid19 is really scary and we should all do our part to try and stay home, sanitize and try not to spread the virus. Together we can all do great things by reducing the spread of this virus.</p>

<p>Stay safe everyone.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nginx Metrics on Prometheus With the Nginx Log Exporter]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/04/25/nginx-metrics-on-prometheus-with-the-nginx-log-exporter/"/>
    <updated>2020-04-25T01:42:35+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/04/25/nginx-metrics-on-prometheus-with-the-nginx-log-exporter</id>
    <content type="html"><![CDATA[<p>In this post we will setup a nginx log exporter for prometeus to get metrics of our nginx web server, such as number of requests per method, status code, processed bytes etc. Then we will configure prometheus to scrape our nginx metric endpoint and also create a basic dashbaord to visualize our data.</p>

<p>If you follow along on this tutorial, it assumes that you have <a href="https://blog.ruanbekker.com/blog/2019/05/07/setup-prometheus-and-node-exporter-on-ubuntu-for-epic-monitoring/">Prometheus</a> and <a href="https://blog.ruanbekker.com/blog/2019/05/17/install-grafana-to-visualize-your-metrics-from-datasources-such-as-prometheus-on-linux/">Grafana</a> up and running. But if not the embedded links will take you to the blog posts to set it up.</p>

<h2>Nginx Webserver</h2>

<p>Install nginx:</p>

<pre><code>$ apt update
$ apt install nginx -y
</code></pre>

<p>Configure your nginx server&rsquo;s log format to match the nginx log exporter&rsquo;s expected format, we will name it custom:</p>

<pre><code>  log_format custom   '$remote_addr - $remote_user [$time_local] '
                      '"$request" $status $body_bytes_sent '
                      '"$http_referer" "$http_user_agent" "$http_x_forwarded_for"';
</code></pre>

<p>Edit your main nginx config:</p>

<pre><code>$ vim /etc/nginx/nginx.conf
</code></pre>

<p>This is how my complete config looks like:</p>

<pre><code>user www-data;
worker_processes auto;
pid /run/nginx.pid;
# remote the escape char if you are going to use this config
include /etc/nginx/modules-enabled/\*.conf;

events {
  worker_connections 768;
}

http {

  # basic config
  sendfile on;
  tcp_nopush on;
  tcp_nodelay on;
  keepalive_timeout 65;
  types_hash_max_size 2048;
  include /etc/nginx/mime.types;
  default_type application/octet-stream;

  # ssl config
  ssl_protocols TLSv1 TLSv1.1 TLSv1.2; 
  ssl_prefer_server_ciphers on;

  # logging config
  log_format custom   '$remote_addr - $remote_user [$time_local] '
                      '"$request" $status $body_bytes_sent '
                      '"$http_referer" "$http_user_agent" "$http_x_forwarded_for"';

  access_log /var/log/nginx/access.log custom;
  error_log /var/log/nginx/error.log;

  # gzip
  gzip on;

  # virtual host config
  include /etc/nginx/conf.d/myapp.conf;

}
</code></pre>

<p>I will delete the default host config:</p>

<pre><code>$ rm -rf /etc/nginx/sites-enabled/default
</code></pre>

<p>And then create my <code>/etc/nginx/conf.d/myapp.conf</code> as referenced in my main config, with the following:</p>

<pre><code>server {

  listen 80 default_server;
  # remove the escape char if you are going to use this config
  server_name \_;

  root /var/www/html;
  index index.html index.htm index.nginx-debian.html;

  location / {
    try_files $uri $uri/ =404;
  }

}
</code></pre>

<p>When you make a GET request to your server, you should see something like this in your access log:</p>

<pre><code>10x.1x.2x.1x - - [25/Apr/2020:00:31:11 +0000] "GET / HTTP/1.1" 200 396 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15" "-"
</code></pre>

<h2>Nginx Log Exporter</h2>

<p>Head over to the <a href="https://github.com/martin-helmich/prometheus-nginxlog-exporter/releases">prometheus-nginxlog-exporter releases</a> page and get the latest version, in the time of writing it is v1.4.0:</p>

<pre><code>$ wget https://github.com/martin-helmich/prometheus-nginxlog-exporter/releases/download/v1.4.0/prometheus-nginxlog-exporter
</code></pre>

<p>Make it executable and move it to your path:</p>

<pre><code>$ chmod +x prometheus-nginxlog-exporter
$ mv prometheus-nginxlog-exporter /usr/bin/prometheus-nginxlog-exporter
</code></pre>

<p>Create the directory where we will place our config for our exporter:</p>

<pre><code>$ mkdir /etc/prometheus
</code></pre>

<p>Create the config file:</p>

<pre><code>$ vim /etc/prometheus/nginxlog_exporter.yml
</code></pre>

<p>You can follow the instructions from <a href="https://github.com/martin-helmich/prometheus-nginxlog-exporter">github.com/prometheus-nginxlog-exporter</a> for more information on configuration, but I will be using the following config:</p>

<pre><code>listen:
  port: 4040
  address: "0.0.0.0"

consul:
  enable: false

namespaces:
  - name: myapp
    format: "$remote_addr - $remote_user [$time_local] \"$request\" $status $body_bytes_sent \"$http_referer\" \"$http_user_agent\" \"$http_x_forwarded_for\""
    source:
      files:
        - /var/log/nginx/access.log
    labels:
      service: "myapp"
      environment: "production"
      hostname: "myapp.example.com"
    histogram_buckets: [.005, .01, .025, .05, .1, .25, .5, 1, 2.5, 5, 10]
</code></pre>

<p>Create the systemd unit file:</p>

<pre><code>$ vim /etc/systemd/system/nginxlog_exporter.service
</code></pre>

<p>And my configuration that I will be using:</p>

<pre><code>[Unit]
Description=Prometheus Log Exporter
Wants=network-online.target
After=network-online.target

[Service]
User=root
Group=root
Type=simple
ExecStart=/usr/bin/prometheus-nginxlog-exporter -config-file /etc/prometheus/nginxlog_exporter.yml

[Install]
WantedBy=multi-user.target
</code></pre>

<p>Reload systemd and enable the service on boot:</p>

<pre><code>$ systemctl daemon-reload
$ systemctl enable nginxlog_exporter
</code></pre>

<p>Restart the service:</p>

<pre><code>$ systemctl restart nginxlog_exporter
</code></pre>

<p>Ensure that the service is running:</p>

<pre><code>$ systemctl status nginxlog_exporter

● nginxlog_exporter.service - Prometheus Log Exporter
   Loaded: loaded (/etc/systemd/system/nginxlog_exporter.service; disabled; vendor preset: enabled)
   Active: active (running) since Sat 2020-04-25 00:50:06 UTC; 5s ago
 Main PID: 4561 (prometheus-ngin)
    Tasks: 7 (limit: 2317)
   CGroup: /system.slice/nginxlog_exporter.service
           └─4561 /usr/bin/prometheus-nginxlog-exporter -config-file /etc/prometheus/nginxlog_exporter.yml

Apr 25 00:50:06 nginx-log-exporter systemd[1]: Started Prometheus Log Exporter.
Apr 25 00:50:06 nginx-log-exporter prometheus-nginxlog-exporter[4561]: loading configuration file /etc/prometheus/nginxlog_exporter.yml
Apr 25 00:50:06 nginx-log-exporter prometheus-nginxlog-exporter[4561]: using configuration {Listen:{Port:4040 Address:0.0.0.0} Consul:{Enable:false Address: Datacenter: Scheme: Toke
Apr 25 00:50:06 nginx-log-exporter prometheus-nginxlog-exporter[4561]: starting listener for namespace myapp
Apr 25 00:50:06 nginx-log-exporter prometheus-nginxlog-exporter[4561]: running HTTP server on address 0.0.0.0:4040
Apr 25 00:50:06 nginx-log-exporter prometheus-nginxlog-exporter[4561]: 2020/04/25 00:50:06 Seeked /var/log/nginx/access.log - &amp;{Offset:0 Whence:2}
</code></pre>

<h2>Test the exporter</h2>

<p>Make a couple of requests against your webserver:</p>

<pre><code>$ for each in {1..10}; do curl http://78.141.211.49 ; done
</code></pre>

<p>So prometheus will now scrape the exporter http endpoint (<code>:4040/metrics</code>) and push the returned values into prometheus. But to get a feel on how the metrics look like, make a request to the metrics endpoint:</p>

<pre><code>$ curl http://localhost:4040/metrics
...
# HELP myapp_http_response_count_total Amount of processed HTTP requests
# TYPE myapp_http_response_count_total counter
myapp_http_response_count_total{environment="production",hostname="myapp.example.com",method="GET",service="myapp",status="200"} 10
myapp_http_response_count_total{environment="production",hostname="myapp.example.com",method="POST",service="myapp",status="404"} 1
# HELP myapp_http_response_size_bytes Total amount of transferred bytes
# TYPE myapp_http_response_size_bytes counter
myapp_http_response_size_bytes{environment="production",hostname="myapp.example.com",method="GET",service="myapp",status="200"} 6120
myapp_http_response_size_bytes{environment="production",hostname="myapp.example.com",method="POST",service="myapp",status="404"} 152
# HELP myapp_parse_errors_total Total number of log file lines that could not be parsed
# TYPE myapp_parse_errors_total counter
myapp_parse_errors_total 0
...
</code></pre>

<p>As you can see we are getting metrics such as response count total, response size, errors, etc.</p>

<h2>Configure Prometheus</h2>

<p>Let&rsquo;s configure prometheus to scrape this endpoint. Head over to your prometheus instance, and edit your prometheus config:</p>

<pre><code>$ vim /etc/prometheus/prometheus.yml
</code></pre>

<p>Note that in my config I have 2 endpoints that I am scraping, the prometheus endpoint which exists and I will be adding the nginx endpoint, so in full, this is how my config will look like:</p>

<pre><code>global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'prometheus'
    scrape_interval: 5s
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'nginx'
    scrape_interval: 15s
    static_configs:
      - targets: ['ip.of.nginx.exporter:4040']
</code></pre>

<p>Restart prometheus:</p>

<pre><code>$ systemctl restart prometheus
</code></pre>

<p>To verify that the exporter is working as expected, head over to your prometheus ui on port 9090, and query <code>up{}</code> to see if your exporters are returning 1:</p>

<p><img width="1280" alt="image" src="https://user-images.githubusercontent.com/567298/80267654-7b51be00-86a2-11ea-98e2-a48a5c2a1e4f.png"></p>

<p>We can then query prometheus with <code>myapp_http_response_count_total{service="myapp"}</code> to see the response counts:</p>

<p><img width="1273" alt="image" src="https://user-images.githubusercontent.com/567298/80267823-590c7000-86a3-11ea-9098-28e37e7941d7.png"></p>

<h2>Dashboarding in Grafana</h2>

<p>If you don&rsquo;t have Grafana installed, you can look at my <a href="https://blog.ruanbekker.com/blog/2019/05/17/install-grafana-to-visualize-your-metrics-from-datasources-such-as-prometheus-on-linux/">Grafana Installation</a> post to get that up and running.</p>

<p>If you have not created the Prometheus datasource, on Grafana, head over to the configuration section on your left, select Datasources, add a Prometheus datasource and add the following (this is assuming grafana runs on the prometheus node - which is fine for testing):</p>

<p><img width="592" alt="image" src="https://user-images.githubusercontent.com/567298/80267986-48a8c500-86a4-11ea-9046-3fba601d41cf.png"></p>

<p>Create a new dashboard and add a new panel:</p>

<p><img width="605" alt="image" src="https://user-images.githubusercontent.com/567298/80267884-b3a5cc00-86a3-11ea-8624-797e5310de80.png"></p>

<p>Let&rsquo;s query our data to show us HTTP Method and Status code per 30s: <code>rate(myapp_http_response_count_total{service="myapp"}[$__interval])</code></p>

<p><img width="1271" alt="image" src="https://user-images.githubusercontent.com/567298/80269073-e607f700-86ac-11ea-8d42-4814084dfb4a.png"></p>

<h2>Thank You</h2>

<p>Hope you found this helpful, if you haven&rsquo;t seen my other posts on Prometheus, have a look at the following:</p>

<ul>
<li><a href="https://blog.ruanbekker.com/blog/2019/05/07/setup-prometheus-and-node-exporter-on-ubuntu-for-epic-monitoring/">Setup Prometheus</a></li>
<li><a href="https://blog.ruanbekker.com/blog/2019/05/17/install-grafana-to-visualize-your-metrics-from-datasources-such-as-prometheus-on-linux/">Setup Grafana</a></li>
<li><a href="https://blog.ruanbekker.com/blog/2019/05/07/setup-prometheus-and-node-exporter-on-ubuntu-for-epic-monitoring/">Setup Node Exporter</a></li>
<li><a href="https://blog.ruanbekker.com/blog/2019/05/17/install-blackbox-exporter-to-monitor-websites-with-prometheus/">Setup Blackbox Exporter</a></li>
<li><a href="https://blog.ruanbekker.com/blog/2019/05/17/install-alertmanager-to-alert-based-on-metrics-from-prometheus/">Setup Alertmanager</a></li>
<li><a href="https://blog.ruanbekker.com/blog/2019/05/17/install-pushgateway-to-expose-metrics-to-prometheus/">Setup Pushgateway</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deploy a Monitoring Stack on Docker Swarm With Grafana and Prometheus]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/09/05/deploy-a-monitoring-stack-on-docker-swarm-with-grafana-and-prometheus/"/>
    <updated>2019-09-05T00:07:52+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/09/05/deploy-a-monitoring-stack-on-docker-swarm-with-grafana-and-prometheus</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/57307750-696bb980-70e5-11e9-9b0b-73ad88bde6a3.png" alt="" /></p>

<p><a href="https://saythanks.io/to/ruanbekker"><img src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" alt="Say Thanks!" /></a> <a href="https://linux-hackers-slack.herokuapp.com/"><img src="https://linux-hackers-slack.herokuapp.com/badge.svg" alt="Slack Status" /></a> <a href="https://linux-hackers.slack.com/"><img src="https://img.shields.io/badge/chat-on_slack-orange.svg" alt="Chat on Slack" /></a> <a href="https://github.com/bekkerstacks/traefik"><img src="https://img.shields.io/github/followers/ruanbekker.svg?label=Follow&amp;style=social" alt="GitHub followers" /></a></p>

<p><a href="https://twitter.com/ruanbekker?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @ruanbekker</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>

<p>In this tutorial we will deploy a monitoring stack to docker swarm, that includes Grafana, Prometheus, Node-Exporter, cAdvisor and Alertmanager.</p>

<p>If you are looking for more information on Prometheus, have a look at my other <a href="https://blog.ruanbekker.com/blog/categories/prometheus/">Prometheus and Monitoring</a> blog posts.</p>

<h2>What you will get out of this</h2>

<p>Once you deployed the stacks, you will have the following:</p>

<ul>
<li>Access Grafana through Traefik reverse proxy</li>
<li>Node-Exporter to expose node level metrics</li>
<li>cAdvisor to expose container level metrics</li>
<li>Prometheus to scrape the exposed entpoints and ingest it into Prometheus</li>
<li>Prometheus for your Timeseries Database</li>
<li>Alertmanager for firing alerts on configured rules</li>
</ul>


<p>The compose file that I will provide will have pre-populated dashboards</p>

<h2>Deploy Traefik</h2>

<p>Get the traefik stack sources:</p>

<pre><code>$ git clone https://github.com/bekkerstacks/traefik
$ pushd traefik
</code></pre>

<p>Have a look at <a href="https://github.com/bekkerstacks/traefik/wiki/Deploy-Traefik-in-HTTPS-Mode">HTTPS Mode</a> if you want to deploy traefik on HTTPS, as I will use HTTP in this demonstration.</p>

<p>Set your domain and deploy the stack:</p>

<pre><code>$ DOMAIN=localhost PROTOCOL=http bash deploy.sh

Username for Traefik UI: demo
Password for Traefik UI: 
deploying traefik stack in http mode
Creating network public
Creating config proxy_traefik_htpasswd
Creating service proxy_traefik
Traefik UI is available at:
- http://traefik.localhost
</code></pre>

<p>Your traefik service should be running:</p>

<pre><code>$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
0wga71zbx1pe        proxy_traefik       replicated          1/1                 traefik:1.7.14      *:80-&gt;80/tcp
</code></pre>

<p>Switch back to the previous directory:</p>

<pre><code>$ popd
</code></pre>

<h2>Deploy the Monitoring Stack</h2>

<p>Get the sources:</p>

<pre><code>$ git clone https://github.com/bekkerstacks/monitoring-cpang
$ pushd monitoring-cpang
</code></pre>

<p>If you want to deploy the stack with no pre-configured dashboards, you would need to use <code>./docker-compose.html</code>, but in this case we will deploy the stack with pre-configured dashboards.</p>

<p>Set the domain and deploy the stack:</p>

<pre><code>$ docker stack deploy -c alt_versions/docker-compose_http_with_dashboards.yml mon

Creating network private
Creating config mon_grafana_config_datasource
Creating config mon_grafana_dashboard_prometheus
Creating config mon_grafana_dashboard_docker
Creating config mon_grafana_dashboard_nodes
Creating config mon_grafana_dashboard_blackbox
Creating config mon_alertmanager_config
Creating config mon_prometheus_config
Creating config mon_prometheus_rules
Creating service mon_blackbox-exporter
Creating service mon_alertmanager
Creating service mon_prometheus
Creating service mon_grafana
Creating service mon_cadvisor
Creating service mon_node-exporter
</code></pre>

<p>The endpoints is configured as <code>${service_name}.${DOMAIN}</code> so you will be able to access grafana on <code>http://grafana.localhost</code> as showed in my use-case.</p>

<p>Use <code>docker stack services mon</code> to see if all the tasks has checked into its desired count then access grafana on <code>http://grafana.${DOMAIN}</code></p>

<h2>Accessing Grafana</h2>

<p>Access Grafana on <code>http://grafana.${DOMAIN}</code> and logon with the user admin and the password admin:</p>

<p><img src="https://user-images.githubusercontent.com/50801771/64292266-4d303a00-cf6a-11e9-8a49-2ae05b1ed5c6.png" alt="image" /></p>

<p>You will be asked to reset the password:</p>

<p><img src="https://user-images.githubusercontent.com/50801771/64292291-5f11dd00-cf6a-11e9-8049-2abdbb0164f6.png" alt="image" /></p>

<p>You will then be directed to the ui:</p>

<p><img src="https://user-images.githubusercontent.com/50801771/64292317-705ae980-cf6a-11e9-928b-60b5dec7ea09.png" alt="image" /></p>

<p>From the top, when you list dashboards, you will see the 3 dashboards that was pre-configured:</p>

<p><img src="https://user-images.githubusercontent.com/50801771/64292334-7b157e80-cf6a-11e9-92c6-9e0698815ba7.png" alt="image" /></p>

<p>When looking at the Swarm Nodes Dashboard:</p>

<p><img src="https://user-images.githubusercontent.com/50801771/64297086-82da2080-cf74-11e9-8060-f0193bfaeb13.png" alt="image" /></p>

<p>The Swarm Services Dashboard:</p>

<p><img src="https://user-images.githubusercontent.com/50801771/64297656-a8ffc080-cf74-11e9-88a2-b4cec295aed5.png" alt="image" /></p>

<h2>Exploring Metrics in Prometheus</h2>

<p>Access prometheus on <code>http://prometheus.${DOMAIN}</code> and from the search input, you can start exploring though all the metrics that is available in prometheus:</p>

<p><img src="https://user-images.githubusercontent.com/50801771/64298324-74403900-cf75-11e9-99b7-559b02ef67b7.png" alt="image" /></p>

<p>If we search for <code>node_load15</code> and select graph, we can have a quick look on how the 15 minute load average looks like for the node where the stack is running on:</p>

<p><img src="https://user-images.githubusercontent.com/50801771/64298454-e31d9200-cf75-11e9-89bb-b6fe94470166.png" alt="image" /></p>

<p>Having a look at the alerts section:</p>

<p><img src="https://user-images.githubusercontent.com/50801771/64299172-7657c700-cf78-11e9-97bd-143e5fe87941.png" alt="image" /></p>

<h2>Resources</h2>

<p>For more information and configuration on the stack that we use, have a look at the wiki:
- <a href="https://github.com/bekkerstacks/monitoring-cpang/wiki">https://github.com/bekkerstacks/monitoring-cpang/wiki</a></p>

<p>The github repository:
- <a href="https://github.com/bekkerstacks/monitoring-cpang">https://github.com/bekkerstacks/monitoring-cpang</a></p>

<h2>Thank You</h2>

<p>Let me know what you think. If you liked my content, feel free to checkout my content on <strong><a href="https://ruan.dev/">ruan.dev</a></strong> or follow me on twitter at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong></p>

<p><a href="https://twitter.com/ruanbekker?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @ruanbekker</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Prometheus Series of Tutorials for Your Guide to Epic Metrics]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/05/17/prometheus-series-of-tutorials-for-your-guide-to-epic-metrics/"/>
    <updated>2019-05-17T14:24:40-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/05/17/prometheus-series-of-tutorials-for-your-guide-to-epic-metrics</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/57307750-696bb980-70e5-11e9-9b0b-73ad88bde6a3.png" alt="prometheus" /></p>

<p>This is a curated list of tutorials of prometheus, from installing prometheus, installing grafana, exporters, docker versions of the prometheus / grafana / node exporter stack, etc.</p>

<h2>The List</h2>

<ul>
<li>Install <a href="http://blog.ruanbekker.com/blog/2019/05/07/setup-prometheus-and-node-exporter-on-ubuntu-for-epic-monitoring/">Prometheus</a></li>
<li>Install <a href="http://blog.ruanbekker.com/blog/2019/05/07/setup-prometheus-and-node-exporter-on-ubuntu-for-epic-monitoring/">Node Exporter</a></li>
<li>Install <a href="http://blog.ruanbekker.com/blog/2019/05/17/install-pushgateway-to-expose-metrics-to-prometheus/">Pushgateway</a></li>
<li>Install <a href="http://blog.ruanbekker.com/blog/2019/05/17/install-grafana-to-visualize-your-metrics-from-datasources-such-as-prometheus-on-linux/">Grafana</a></li>
<li>Install <a href="http://blog.ruanbekker.com/blog/2019/05/17/install-alertmanager-to-alert-based-on-metrics-from-prometheus/">Alertmananger</a></li>
<li>Install <a href="http://blog.ruanbekker.com/blog/2019/05/17/install-blackbox-exporter-to-monitor-websites-with-prometheus/">Blackbox Exporter</a></li>
<li>Install <a href="">Docker Prometheus Grafana Stack</a></li>
</ul>


<p>This list will be updated as I publish more tutorials</p>
]]></content>
  </entry>
  
</feed>
