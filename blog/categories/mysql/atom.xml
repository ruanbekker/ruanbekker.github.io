<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Mysql | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/mysql/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2017-12-11T09:35:13-05:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Rejoining or Bootstrapping MySQL Galera Cluster Nodes After Shutdown]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/12/10/rejoining-or-bootstrapping-mysql-galera-cluster-nodes-after-shutdown/"/>
    <updated>2017-12-10T18:03:44-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/12/10/rejoining-or-bootstrapping-mysql-galera-cluster-nodes-after-shutdown</id>
    <content type="html"><![CDATA[<p>I have a 3 Node MySQL Galera Cluster that faced a shutdown on all 3 nodes at the same time, luckily this is only a testing environment, but at that time it was down and did not want to start up.</p>

<h2>Issues Faced</h2>

<p>When trying to start MySQL the only error visible was:</p>

<pre><code class="bash">$ /etc/init.d/mysql restart
 * MySQL server PID file could not be found!
Starting MySQL
........ * The server quit without updating PID file (/var/run/mysqld/mysqld.pid).
 * Failed to restart server.
</code></pre>

<p>At that time I can see that the galera port is started, but not mysql:</p>

<pre><code class="bash">$ ps aux | grep mysql
root     23580  0.0  0.0   4508  1800 pts/0    S    00:37   0:00 /bin/sh /usr/bin/mysqld_safe --datadir=/var/lib/mysql --pid-file=/var/run/mysqld/mysqld.pid
mysql    24144  0.7 22.2 1185116 455660 pts/0  Sl   00:38   0:00 /usr/sbin/mysqld --basedir=/usr --datadir=/var/lib/mysql --plugin-dir=/usr/lib/mysql/plugin --user=mysql --log-error=/var/log/mysql/error.log --pid-file=/var/run/mysqld/mysqld.pid --socket=/var/run/mysqld/mysqld.sock --port=3306 --wsrep_start_position=long:string

$ netstat -tulpn
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:4567            0.0.0.0:*               LISTEN      25507/mysqld
</code></pre>

<h2>Why?</h2>

<p>More in detail is explained on a <a href="https://severalnines.com/blog/how-bootstrap-mysqlmariadb-galera-cluster">SeveralNines Blog Post</a>, but due to the fact that all the nodes left the cluster, one of the nodes needs to be started as a referencing point, before the other nodes can rejoin or bootstrapped to the cluster.</p>

<h2>Rejoining the Cluster</h2>

<p>Consult the blog for more information, but from my end, I had a look at the node with the highest seqno and then updated <code>safe_to_bootstrap</code> to <code>1</code>:</p>

<pre><code class="bash">$ cat /var/lib/mysql/grastate.dat
# GALERA saved state
version: 2.1
uuid:    e9f9cf6a-87a1-11e7-9fb4-52612b906897
seqno:   123512
safe_to_bootstrap: 1
</code></pre>

<p>Then made sure that no mysql processes are running, then did a bootstrap:</p>

<pre><code class="bash">$ /etc/init.d/mysql bootstrap
Bootstrapping the cluster
Starting MySQL
</code></pre>

<p>Then restarted mysql on the other nodes.</p>

<h2>Verifying</h2>

<p>To verify that all your nodes has checked in, I have 3 nodes:</p>

<pre><code class="sql">mysql&gt; SHOW STATUS LIKE 'wsrep_%';
+------------------------------+---------------------------------------------------+
| Variable_name                | Value                                             |
+------------------------------+---------------------------------------------------+
| wsrep_local_recv_queue_avg   | 0.000000                                          |
| wsrep_local_state_comment    | Synced                                            |
| wsrep_incoming_addresses     | 10.3.132.91:3306,10.4.1.201:3306,10.4.113.21:3306 |
| wsrep_evs_state              | OPERATIONAL                                       |
| wsrep_cluster_size           | 3                                                 |
| wsrep_cluster_status         | Primary                                           |
| wsrep_connected              | ON                                                |
+------------------------------+---------------------------------------------------+
</code></pre>

<p>or a shorter version:</p>

<pre><code class="sql">mysql&gt; SHOW GLOBAL STATUS LIKE 'wsrep_cluster_size';
+------------------------------+---------------------------------------------------+
| Variable_name                | Value                                             |
+------------------------------+---------------------------------------------------+
| wsrep_cluster_size           | 3                                                 |
+------------------------------+---------------------------------------------------+
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Use Docker Secrets With MySQL on Docker Swarm]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/11/23/use-docker-secrets-with-mysql-on-docker-swarm/"/>
    <updated>2017-11-23T16:55:15-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/11/23/use-docker-secrets-with-mysql-on-docker-swarm</id>
    <content type="html"><![CDATA[<p><img src="http://obj-cache.cloud.ruanbekker.com/docker-logo.png" alt="" /></p>

<p>Today we will use Docker Secrets, more specifically store our MySQL Passwords in Secrets, which will be passed to our containers, so that we don&rsquo;t use clear text passwords in our Compose files.</p>

<h2>What is Docker Secrets:</h2>

<p>In Docker, Docker Secrets are encrypted during transit and at rest in a Docker Swarm Cluster. The great thing about Docker Secrets is that you can manage these secrets from a central place, and the fact that it encrypts the data and transfers the data securely to the containers that needs the secrets. So you authorize which containers needs access to these secrets.</p>

<p>So instead of setting the MySQL Root Passwords in clear text, you will create the secrets, then in your docker-compose file, you will reference the secret name.</p>

<h2>Deploy MySQL with Docker Secrets</h2>

<p>We will deploy a Stack that contains MySQL and Adminer (WebUI for MySQL).</p>

<p>We will make the MySQL Service Persistent by setting a constraint to only run on the Manager node, as we will create the volume path on the host, and then map the host to the container so that the container can have persistent data. We will also create secrets for our MySQL Service so that we dont expose any plaintext passwords in our compose file.</p>

<p>Our Docker Compose file:</p>

<pre><code class="yaml docker-compose.yml">version: '3.3'

services:
  db:
    image: mysql
    secrets:
      - db_root_password
      - db_dba_password
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
      resources:
        reservations:
          memory: 128M
        limits:
          memory: 256M
    ports:
      - 3306:3306
    environment:
      MYSQL_USER: dba
      MYSQL_DATABASE: mydb
      MYSQL_ROOT_PASSWORD_FILE: /run/secrets/db_root_password
      MYSQL_PASSWORD_FILE: /run/secrets/db_dba_password
    networks:
      - appnet
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - type: bind
        source: /opt/docker/volumes/mysql
        target: /var/lib/mysql

  adminer:
    image: adminer
    ports:
      - 8080:8080
    networks:
      - appnet

secrets:
  db_root_password:
    external: true
  db_dba_password:
    external: true

networks:
  appnet:
    external: true
</code></pre>

<h2>Dependencies:</h2>

<p>As we specified our secrets and networks as external resources, it needs to exist before we deploy our stack. We also need to create the directory for our mysql data, as the data will be mapped from our host to our container.</p>

<p>Create the Overlay Network:</p>

<pre><code class="bash">$ docker network create --driver overlay appnet
</code></pre>

<p>Create the Secrets:</p>

<pre><code class="bash">$ openssl rand -base64 12 | docker secret create db_root_password -
$ openssl rand -base64 12 | docker secret create db_dba_password -
</code></pre>

<p>List the Secrets:</p>

<pre><code class="bash">$ docker secret ls
ID                          NAME                CREATED             UPDATED
jzhrwyxkiqt8v81ow0xjktqnw   db_root_password    12 seconds ago      12 seconds ago
plr6rbrqkqy7oplrd21pja3ol   db_dba_password     4 seconds ago       4 seconds ago
</code></pre>

<p>Inspect the secret, so that we can see that theres not value exposed:</p>

<pre><code class="bash">$ docker secret inspect db_root_password
[
    {
        "ID": "jzhrwyxkiqt8v81ow0xjktqnw",
        "Version": {
            "Index": 982811
        },
        "CreatedAt": "2017-11-23T14:33:17.005968748Z",
        "UpdatedAt": "2017-11-23T14:33:17.005968748Z",
        "Spec": {
            "Name": "db_root_password",
            "Labels": {}
        }
    }
]
</code></pre>

<p>Create the Directory for MySQL:</p>

<pre><code class="bash">$ mkdir -p /opt/docker/volumes/mysql
</code></pre>

<h2>Deployment Time!</h2>

<p>Deploy the stack:</p>

<pre><code>$ docker stack deploy -c docker-compose.yml apps
Creating service apps_adminer
Creating service apps_db
</code></pre>

<p>As you can see the data of our MySQL container resides on our host, which makes the data persistent for the container:</p>

<pre><code class="bash">$ ls /opt/docker/volumes/mysql/
auto.cnf  ca-key.pem  ca.pem  client-cert.pem  client-key.pem  ib_buffer_pool  ibdata1  ib_logfile0  ib_logfile1  ibtmp1  mydb  mysql  performance_schema  private_key.pem  public_key.pem  server-cert.pem  server-key.pem  sys
</code></pre>

<h2>Connect to MySQL</h2>

<p>The value of our secrets will reside under <code>/run/secrets/</code> in our container, as we have mapped it to our mysql container, lets have a look at them:</p>

<pre><code class="bash">$ docker exec -it $(docker ps -f name=apps_db -q) ls /run/secrets/
db_dba_password  db_root_password
</code></pre>

<p>View the actual value of the <code>db_root_password</code>:</p>

<pre><code class="bash">$ docker exec -it $(docker ps -f name=apps_db -q) cat /run/secrets/db_root_password
mRpcY1eY2+wimf10
</code></pre>

<p>Connecting to MySQL:</p>

<pre><code class="bash">$ docker exec -it $(docker ps -f name=apps_db -q) mysql -u root -p
Enter password:
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 8
Server version: 5.7.20 MySQL Community Server (GPL)

Copyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql&gt; show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mydb               |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
5 rows in set (0.00 sec)
</code></pre>

<p>As we have deployed adminer, you can access the Adminer WebUI on the Host&rsquo;s IP and the Defined Port.</p>

<h2>Testing Data Persistance:</h2>

<pre><code class="bash">$ docker exec -it $(docker ps -f name=apps_db -q) mysql -u root -p
Enter password:
Welcome to the MySQL monitor.  Commands end with ; or \g.

mysql&gt; create database ruan;
Query OK, 1 row affected (0.00 sec)

mysql&gt; show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mydb               |
| mysql              |
| performance_schema |
| ruan               |
| sys                |
+--------------------+
6 rows in set (0.00 sec)

mysql&gt; exit;
Bye
</code></pre>

<p>Verify the hostname of our container, before we kill the container:</p>

<pre><code class="bash">$ docker exec -it $(docker ps -f name=apps_db -q) hostname
bdedb54bbc2b
</code></pre>

<p>Kill the container:</p>

<pre><code>$ docker kill $(docker ps -f name=apps_db -q)
bdedb54bbc2b
</code></pre>

<p>Verify the status of the MySQL Service, as we can see the service count is 0, so the container was succesfully killed.</p>

<pre><code class="bash">$ docker service ls -f name=apps_db
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
nzf96q05fktm        apps_db             replicated          0/1                 mysql:latest        *:3306-&gt;3306/tcp
</code></pre>

<p>After waiting for a couple of seconds, we can see the service is in service again, then check the hostname so that we can confirm that its a new container:</p>

<pre><code>$ docker service ls -f name=apps_db
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
nzf96q05fktm        apps_db             replicated          1/1                 mysql:latest        *:3306-&gt;3306/tcp

$ docker exec -it $(docker ps -f name=apps_db -q) hostname
95c15c89f891
</code></pre>

<p>Logong to MySQL again and verify if our perviously created database is still there:</p>

<pre><code class="bash">$ docker exec -it $(docker ps -f name=apps_db -q) mysql -u root -p
Enter password:
Welcome to the MySQL monitor.  Commands end with ; or \g.

mysql&gt; show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mydb               |
| mysql              |
| performance_schema |
| ruan               |
| sys                |
+--------------------+
6 rows in set (0.01 sec)
</code></pre>

<p>By design docker is stateless, but as we mapped the host&rsquo;s path to the container our data is persistent. As we have set a constraint so that the container must only spin up on this node, the container will always have access to the data path.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a 3 Node Galera MariaDB Cluster on Ubuntu 16]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/11/22/setup-a-3-node-galera-mariadb-cluster-on-ubuntu-16/"/>
    <updated>2017-11-22T18:17:14-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/11/22/setup-a-3-node-galera-mariadb-cluster-on-ubuntu-16</id>
    <content type="html"><![CDATA[<p><img src="https://i.snag.gy/lpT6Du.jpg" alt="" /></p>

<p>Today we will setup a 3-Node Galera MariaDB Cluster which is a Multi Master MySQL/MariaDB Cluster on Ubuntu 16.04</p>

<h2>Our Server Details:</h2>

<pre><code class="bash">172.31.11.174     mysql-1
172.31.13.206     mysql-2
172.31.6.93       mysql-3
</code></pre>

<h2>Update Repo Index and Upgrade:</h2>

<p>Update the repository indexes and install the needed packages:</p>

<pre><code class="bash">$ sudo apt update &amp;&amp; sudo apt upgrade -y
</code></pre>

<p>Install the needed repository and packages:</p>

<pre><code class="bash">$ apt install software-properties-common -y
$ apt-key adv --recv-keys --keyserver hkp://keyserver.ubuntu.com:80 0xF1656F24C74CD1D8
$ add-apt-repository 'deb [arch=amd64,i386,ppc64el] http://mirror.lstn.net/mariadb/repo/10.1/ubuntu xenial main'
$ apt update
$ apt install mariadb-server rsync -y
</code></pre>

<h2>Configuration:</h2>

<pre><code class="bash">cat &gt; /etc/mysql/conf.d/galera.cnf &lt;&lt; EOF
[mysqld]
binlog_format=ROW
default-storage-engine=innodb
innodb_autoinc_lock_mode=2
bind-address=0.0.0.0

# Galera Provider Configuration
wsrep_on=ON
wsrep_provider=/usr/lib/galera/libgalera_smm.so

# Galera Cluster Configuration
wsrep_cluster_name="my-galera-cluster"
wsrep_cluster_address="gcomm://172.31.11.174,172.31.13.206,172.31.6.93"
# Galera Synchronization Configuration
wsrep_sst_method=rsync

# Galera Node Configuration
wsrep_node_address="172.31.11.174"
wsrep_node_name="mysql-1"
EOF
</code></pre>

<p>Comment out bind-address, so that MariaDB process is reachable from other nodes, by default it wont be in the config, but just to make sure, if it is uncommented, comment the config:</p>

<pre><code class="bash /etc/mysql/my.cnf"># bind-address = 127.0.0.1
</code></pre>

<p>Stop the MariaDB Process:</p>

<pre><code class="bash">$ systemctl stop mariadb
</code></pre>

<p>Note: Repeat the above steps on all 3 nodes.</p>

<h2>Initialize the Cluster:</h2>

<p>On the First Node, Initialize the Galera Cluster:</p>

<pre><code class="bash">$ /usr/bin/galera_new_cluster
$ systemctl enable mariadb
</code></pre>

<p>Check how many nodes are active in the Cluster:</p>

<pre><code class="mysql">$ mysql -u root -p -e "SHOW STATUS LIKE 'wsrep_cluster_size';"
Enter password:
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| wsrep_cluster_size | 1     |
+--------------------+-------+
</code></pre>

<h2>Node-2: Start and Enable MariaDB</h2>

<pre><code class="bash">$ systemctl start mariadb
$ systemctl enable mariadb
</code></pre>

<p>Verify that the Node has checked in with the Cluster:</p>

<pre><code class="mysql">$ mysql -u root -p -e "SHOW STATUS LIKE 'wsrep_cluster_size';"
Enter password:
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| wsrep_cluster_size | 2     |
+--------------------+-------+
</code></pre>

<h2>Node-3: Start and Enable MariaDB</h2>

<pre><code class="bash">$ systemctl start mariadb
$ systemctl enable mariadb
</code></pre>

<p>Verify that the Node has checked in with the Cluster:</p>

<pre><code class="mysql">$ mysql -u root -p -e "SHOW STATUS LIKE 'wsrep_cluster_size';"
Enter password:
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| wsrep_cluster_size | 3     |
+--------------------+-------+
</code></pre>

<h2>Create a Database, Table and Record:</h2>

<p>Write some data to the table, then reboot the node, in this example on node-1, then logon to node-2 check the number of nodes that&rsquo;s active in the cluster, which should be 2, then at the same time, look if the data is replicated:</p>

<h2>Node-1: Writing the Data to Our Galera Cluster</h2>

<pre><code class="mysql">MariaDB [(none)]&gt; create database test;
MariaDB [(none)]&gt; use test;
MariaDB [test]&gt;   create database test;
MariaDB [test]&gt;   create table foo (name VARCHAR(20));
MariaDB [test]&gt;   insert into foo values('ruan');
MariaDB [test]&gt;   select * from foo;
+------+
| name |
+------+
| ruan |
+------+
</code></pre>

<p>Now that our data is in our database, reboot the node, logon to node-2 and check if the data is replicated:</p>

<pre><code class="mysql">$ mysql -u root -p
MariaDB [(none)]&gt; use test;
MariaDB [test]&gt;   select * from foo;
+------+
| name |
+------+
| ruan |
+------+
</code></pre>

<p>While the one node is rebooting, check how many nodes are checked into our cluster:</p>

<pre><code class="mysql">$ mysql -u root -p -e "SHOW STATUS LIKE 'wsrep_cluster_size';"
Enter password:
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| wsrep_cluster_size | 2     |
+--------------------+-------+
</code></pre>

<p>Our data is replicated, and after waiting for a couple of seconds, we retry our command to see if the rebooted node checked into the cluster:</p>

<pre><code class="mysql">$ mysql -u root -p -e "SHOW STATUS LIKE 'wsrep_cluster_size';"
Enter password:
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| wsrep_cluster_size | 3     |
+--------------------+-------+
</code></pre>

<p>We can confirm that the node that was rebooted, has checked in with the cluster again.</p>

<h2>Firewall Rules opened while testing:</h2>

<p>TCP: <code>3306, 4567, 4568, 4444</code>
UDP: <code>4567</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Local Dev Environment With Docker MySQL and Adminer WebUI With Docker Compose]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/11/13/local-dev-environment-with-docker-mysql-and-adminer-webui-with-docker-compose/"/>
    <updated>2017-11-13T16:15:34-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/11/13/local-dev-environment-with-docker-mysql-and-adminer-webui-with-docker-compose</id>
    <content type="html"><![CDATA[<p>Let&rsquo;s setup a local development environment with Docker, MySQL and Adminer WebUI using Docker Compose</p>

<h2>Docker Compose File:</h2>

<p>Let&rsquo;s look at our docker-compose file:</p>

<pre><code class="yml">version: '3.2'

services:
  mysql-client:
    image: alpine:edge
    volumes:
      - type: bind
        source: ./workspace
        target: /root/workspace
    networks:
      - docknet
    command: ping 127.0.0.1

  db:
    image: mysql
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: example
    networks:
      - docknet
    volumes:
      - type: volume
        source: dbdata
        target: /var/lib/mysql

  adminer:
    image: adminer
    restart: always
    ports:
      - 8080:8080
    networks:
      - docknet

networks:
    docknet:
        external: true

volumes:
  dbdata:
    external: true
</code></pre>

<h2>Pre-Requirements:</h2>

<p>Let&rsquo;s create our pre-requirement:</p>

<ol>
<li>Networks:</li>
</ol>


<pre><code class="bash">$ docker network create docknet
</code></pre>

<ol>
<li>Volumes:</li>
</ol>


<p>Our Volume for MySQL so that we have persistent data:</p>

<pre><code class="bash">$ docker volume create dbdata
</code></pre>

<p>Our <code>workspace</code> directory that will be persistent in our <code>debug-client</code> alpine container:</p>

<pre><code class="bash">$ mkdir -p workspace/python
</code></pre>

<h2>Launching our Services:</h2>

<p>Let&rsquo;s launch our services:</p>

<pre><code class="bash ">$ docker-compose -f mysql-compose.yml up -d
Creating mysql_db_1 ...
Creating mysql_adminer_1
Creating mysql_debug-client_1
</code></pre>

<p>Listing our Containers:</p>

<pre><code class="bash">$ docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                      NAMES
e05804ab6d64        alpine:edge         "ping 127.0.0.1"         21 seconds ago      Up 4 seconds                                   mysql_debug-client_1
c052ceeb6d3b        mysql               "docker-entrypoint..."   21 seconds ago      Up 5 seconds        3306/tcp                   mysql_db_1
2b0446daab4c        adminer             "entrypoint.sh doc..."   26 seconds ago      Up 5 seconds        0.0.0.0:8080-&gt;8080/tcp     mysql_adminer_1
</code></pre>

<h2>Using the Debug Container:</h2>

<p>I will use the debug container as the client to connect to the internal services, for example, the mysql-client:</p>

<pre><code class="bash">$ apk update
$ apk add mysql-client
$ mysql -h db -u root -ppassword
MySQL [(none)]&gt;
</code></pre>

<p>Also, you will find the persistent data directory for our workspace:</p>

<pre><code class="bash">$ ls /root/workspace/
python
</code></pre>

<h2>Accessing the MySQL WebUI: Adminer</h2>

<p>Access the service via the exposed endpoint:</p>

<pre><code class="bash">+ http://localhost:8080/
</code></pre>

<p>The login view:</p>

<p><img src="https://i.snag.gy/m8dUxe.jpg" alt="" /></p>

<p>Creating the Table:</p>

<p><img src="https://i.snag.gy/tPVbg6.jpg" alt="" /></p>

<h2>Deleting the Environment:</h2>

<p>The External Resources will not be deleted:</p>

<pre><code class="bash">$ docker-compose -f mysql-compose.yml down
Removing mysql_debug-client_1 ... done
Removing mysql_db_1           ... done
Removing mysql_adminer_1      ... done
Network docknet is external, skipping
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Python to Query MySQL Database With MySQLdb Library]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/10/26/using-python-to-query-mysql-database-with-mysqldb-library/"/>
    <updated>2017-10-26T03:40:11-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/10/26/using-python-to-query-mysql-database-with-mysqldb-library</id>
    <content type="html"><![CDATA[<p>a Quick post to demostrate how to use Python to Query data from MySQL. We will use the MySQL Docker Image for the demonstration.</p>

<h2>Provision MySQL</h2>

<p>We will use the latest mysql image, and use the environment variable to pass the root password, and also expose the mysql port:</p>

<pre><code class="bash">$ docker run -itd -p 3306:3306 -e MYSQL_ROOT_PASSWORD=password mysql
</code></pre>

<h2>Populate some data in MySQL</h2>

<p>Connect to MySQL:</p>

<pre><code class="bash">$ mysql -h 127.0.0.1 -u root -ppasword
</code></pre>

<p>Create some test data:</p>

<pre><code class="sql">mysql&gt; create database foo;
mysql&gt; use foo;
mysql&gt; create table bar (name VARCHAR(20), surname VARCHAR(20));
mysql&gt; insert into bar values('ruan', 'bekker');
mysql&gt; insert into bar values('stefan', 'bester');
mysql&gt; insert into bar values('peter', 'williams');
</code></pre>

<h2>Python with MySQL: Setup the Environment</h2>

<p>We will use virtualenv to create a virtual environment to keep our installation isolated from the rest of our system. Install virtualenv:</p>

<pre><code class="bash">$ pip install virtualenv
</code></pre>

<p>Create a virtual environment and install the required dependency:</p>

<pre><code class="bash">$ virtualenv venv-mysql
$ source venv-mysql/bin/activate
(venv-mysql) pip install MySQL-python
</code></pre>

<h2>Python with MySQL: Develop the Client</h2>

<pre><code class="python">&gt;&gt;&gt; import MySQLdb
&gt;&gt;&gt; db = MySQLdb.connect('127.0.0.1', 'root', 'password', 'foo')
&gt;&gt;&gt; con = db.cursor()
&gt;&gt;&gt; con.execute("SELECT * from bar")
4L
&gt;&gt;&gt; rows = con.fetchall()
&gt;&gt;&gt; for row in rows:
...     print(row[0], row[1])
... 
('ruan', 'bekker')
('stefan', 'bester')
('peter', 'williams')
&gt;&gt;&gt; exit()
</code></pre>
]]></content>
  </entry>
  
</feed>
