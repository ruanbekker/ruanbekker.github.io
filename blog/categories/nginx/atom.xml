<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Nginx | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/nginx/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2023-01-24T16:56:25-05:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Run Docker Containers With Terraform]]></title>
    <link href="https://blog.ruanbekker.com/blog/2021/11/23/run-docker-containers-with-terraform/"/>
    <updated>2021-11-23T11:06:03-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2021/11/23/run-docker-containers-with-terraform</id>
    <content type="html"><![CDATA[<p>In this post I will demonstrate how to use the terraform <a href="https://registry.terraform.io/providers/kreuzwerker/docker/latest/docs/resources/container">docker_container</a> resource from the <a href="https://github.com/kreuzwerker/terraform-provider-docker">docker provider</a> to run two docker containers, traefik and nginx and use the random provider to generate a random url for us.</p>

<h2>Pre-Requisites</h2>

<p>You will require <a href="https://www.terraform.io/downloads.html">terraform</a> and <a href="https://docs.docker.com/get-docker/">docker</a> to be installed.</p>

<h2>Project Structure</h2>

<p>The source code for this post is available on my github repository, but the project structure will look like the following:</p>

<p><img src="https://user-images.githubusercontent.com/567298/143061769-c619e7eb-c5b1-42bc-9fa4-ed59c15448fa.png" alt="image" /></p>

<p>Our <code>providers.tf</code>:</p>

<pre><code>terraform {
  required_providers {
    docker = {
      source  = "kreuzwerker/docker"
      version = "2.15.0"
    }
    random = {
      version = "~&gt; 3.0"
    }
  }
}

provider "docker" {
  host = "unix:///var/run/docker.sock"
}

provider "random" {}
</code></pre>

<p>Our <code>variables.tf</code>:</p>

<pre><code>variable "domain" {
  type    = string
  default = "localdns.xyz"
}
</code></pre>

<p>Our <code>outputs.tf</code>:</p>

<pre><code>output "nginx_container_name" {
  value = docker_container.nginx.name
}

output "traefik_container_name" {
  value = docker_container.traefik.name
}

output "traefik_url" {
  value = "http://traefik.${var.domain}/"
}

output "nginx_url" {
  value = "http://www.${random_string.nginx.result}.${var.domain}/"
}
</code></pre>

<p>Our <code>main.tf</code>:</p>

<pre><code>resource "random_string" "nginx" {
  length  = 8
  upper   = false
  special = false
}

resource "docker_image" "nginx" {
  name = "nginx:stable-alpine"
}

resource "docker_image" "traefik" {
  name = "traefik:1.7.14"
}

resource "docker_network" "nginx" {
  name   = "docknet"
  driver = "bridge"
}

resource "docker_container" "traefik" {
  name  = "traefik"
  image = docker_image.traefik.name

  networks_advanced {
    name    = docker_network.nginx.name
    aliases = ["docknet"]
  }

  restart = "unless-stopped"
  destroy_grace_seconds = 30
  must_run = true
  memory = 256

  volumes {
    host_path      = "/var/run/docker.sock"
    container_path = "/var/run/docker.sock"
  }

  command = [
    "--api",
    "--docker",
    "--docker.watch",
    "--entrypoints=Name:http Address::80",
    "--logLevel=INFO"
  ]

  ports {
    internal = 80
    external = 80
    ip       = "0.0.0.0"
  }

  labels {
    label = "traefik.enable"
    value = true
  }

  labels {
    label = "traefik.docker.network"
    value = "docknet"
  }

  labels {
    label = "traefik.frontend.rule"
    value = "Host:traefik.${var.domain}"
  }

  labels {
    label = "traefik.port"
    value = 8080
  }

}

resource "docker_container" "nginx" {
  name  = "nginx"
  image = docker_image.nginx.name

  networks_advanced {
    name    = docker_network.nginx.name
    aliases = ["docknet"]
  }

  restart = "unless-stopped"
  destroy_grace_seconds = 30
  must_run = true
  memory = 256

  volumes {
    host_path      = "/Users/ruan/personal/terraform-playground/docker-containers/html"
    container_path = "/usr/share/nginx/html"
  }

  volumes {
    host_path      = "/Users/ruan/personal/terraform-playground/docker-containers/configs/nginx.conf"
    container_path = "/etc/nginx/nginx.conf"
  }

  volumes {
    host_path      = "/Users/ruan/personal/terraform-playground/docker-containers/configs/app.conf"
    container_path = "/etc/nginx/conf.d/app.conf"
  }

  env = [
    "PUID=501",
    "PGID=20"
  ]

  labels {
    label = "traefik.enable"
    value = true
  }

  labels {
    label = "traefik.docker.network"
    value = "docknet"
  }

  labels {
    label = "traefik.frontend.rule"
    value = "Host:www.${random_string.nginx.result}.${var.domain}"
  }

  labels {
    label = "traefik.port"
    value = 80
  }

  depends_on = [
    docker_container.traefik,
    random_string.nginx
  ]

}
</code></pre>

<p>Our <code>html/index.html</code>:</p>

<pre><code class="html">&lt;!doctype html&gt;
&lt;html lang="en"&gt;
    &lt;head&gt;
        &lt;meta charset="utf-8"&gt;
        &lt;meta name="viewport" content="width=device-width, initial-scale=1"&gt;

        &lt;title&gt;Welcome&lt;/title&gt;

        &lt;!-- Fonts --&gt;
        &lt;link href="https://fonts.googleapis.com/css?family=Nunito:200,600" rel="stylesheet"&gt;

        &lt;!-- Styles --&gt;
        &lt;style&gt;
            html, body {
                background-color: #fff;
                color: #636b6f;
                font-family: 'Nunito', sans-serif;
                font-weight: 200;
                height: 100vh;
                margin: 0;
            }

            .full-height {
                height: 100vh;
            }

            .flex-center {
                align-items: center;
                display: flex;
                justify-content: center;
            }

            .position-ref {
                position: relative;
            }

            .top-right {
                position: absolute;
                right: 10px;
                top: 18px;
            }

            .content {
                text-align: center;
            }

            .title {
                font-size: 84px;
            }

            .links &gt; a {
                color: #636b6f;
                padding: 0 25px;
                font-size: 13px;
                font-weight: 600;
                letter-spacing: .1rem;
                text-decoration: none;
                text-transform: uppercase;
            }

            .m-b-md {
                margin-bottom: 30px;
            }
        &lt;/style&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;div class="flex-center position-ref full-height"&gt;
            &lt;div class="content"&gt;
                &lt;div class="title m-b-md"&gt;
                    Welcome
                &lt;/div&gt;

                &lt;div class="links"&gt;
                    &lt;a href="https://ruan.dev" target="_blank"&gt;About Me&lt;/a&gt;
                &lt;/div&gt;
            &lt;/div&gt;
        &lt;/div&gt;
    &lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>Our <code>configs/nginx.conf</code>:</p>

<pre><code>user  nginx;
worker_processes  auto;
error_log  /var/log/nginx/error.log notice;
pid        /var/run/nginx.pid;

events {
    worker_connections  1024;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

    access_log  /var/log/nginx/access.log  main;
    sendfile        on;
    keepalive_timeout  65;

    include /etc/nginx/conf.d/app.conf;
}
</code></pre>

<p>And lastly, our <code>configs/app.conf</code>:</p>

<pre><code>server {
  listen 80;
  server_name _;

  location / {
    root   /usr/share/nginx/html;
    index  index.html;
  }

  location /healthz {
    return 200 'up';
  }
}
</code></pre>

<h2>Deployment</h2>

<p>Once everything is in place, or if you want to clone my repository, you can do that by:</p>

<pre><code>git clone https://github.com/ruanbekker/terraform-docker-container-example
cd terraform-docker-container-example
</code></pre>

<p>Then we can initialize terraform by fetching the required plugins:</p>

<pre><code>terraform init
</code></pre>

<p>Once that completes we can run a plan:</p>

<pre><code>terraform plan
</code></pre>

<p>And that should output something more or less like:</p>

<pre><code>Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with
the following symbols:
  + create

Terraform will perform the following actions:

  # docker_container.nginx will be created
  + resource "docker_container" "nginx" {
      + attach                = false
      + bridge                = (known after apply)
      + command               = (known after apply)
      + container_logs        = (known after apply)
      + destroy_grace_seconds = 30
      + entrypoint            = (known after apply)
      + env                   = [
          + "PGID=20",
          + "PUID=501",
        ]
      + exit_code             = (known after apply)
      + gateway               = (known after apply)
      + hostname              = (known after apply)
      + id                    = (known after apply)
      + image                 = "nginx:stable-alpine"
      + init                  = (known after apply)
      + ip_address            = (known after apply)
      + ip_prefix_length      = (known after apply)
      + ipc_mode              = (known after apply)
      + log_driver            = "json-file"
      + logs                  = false
      + memory                = 256
      + must_run              = true
      + name                  = "nginx"
      + network_data          = (known after apply)
      + read_only             = false
      + remove_volumes        = true
      + restart               = "unless-stopped"
      + rm                    = false
      + security_opts         = (known after apply)
      + shm_size              = (known after apply)
      + start                 = true
      + stdin_open            = false
      + tty                   = false

      + healthcheck {
          + interval     = (known after apply)
          + retries      = (known after apply)
          + start_period = (known after apply)
          + test         = (known after apply)
          + timeout      = (known after apply)
        }

      + labels {
          + label = "traefik.docker.network"
          + value = "docknet"
        }
      + labels {
          + label = "traefik.enable"
          + value = "true"
        }
      + labels {
          + label = "traefik.frontend.rule"
          + value = (known after apply)
        }
      + labels {
          + label = "traefik.port"
          + value = "80"
        }

      + networks_advanced {
          + aliases = [
              + "docknet",
            ]
          + name    = "docknet"
        }

      + volumes {
          + container_path = "/etc/nginx/conf.d/app.conf"
          + host_path      = "/Users/ruan/personal/terraform-playground/docker-containers/configs/app.conf"
        }
      + volumes {
          + container_path = "/etc/nginx/nginx.conf"
          + host_path      = "/Users/ruan/personal/terraform-playground/docker-containers/configs/nginx.conf"
        }
      + volumes {
          + container_path = "/usr/share/nginx/html"
          + host_path      = "/Users/ruan/personal/terraform-playground/docker-containers/html"
        }
    }

  # docker_container.traefik will be created
  + resource "docker_container" "traefik" {
      + attach                = false
      + bridge                = (known after apply)
      + command               = [
          + "--api",
          + "--docker",
          + "--docker.watch",
          + "--entrypoints=Name:http Address::80",
          + "--logLevel=INFO",
        ]
      + container_logs        = (known after apply)
      + destroy_grace_seconds = 30
      + entrypoint            = (known after apply)
      + env                   = (known after apply)
      + exit_code             = (known after apply)
      + gateway               = (known after apply)
      + hostname              = (known after apply)
      + id                    = (known after apply)
      + image                 = "traefik:1.7.14"
      + init                  = (known after apply)
      + ip_address            = (known after apply)
      + ip_prefix_length      = (known after apply)
      + ipc_mode              = (known after apply)
      + log_driver            = "json-file"
      + logs                  = false
      + memory                = 256
      + must_run              = true
      + name                  = "traefik"
      + network_data          = (known after apply)
      + read_only             = false
      + remove_volumes        = true
      + restart               = "unless-stopped"
      + rm                    = false
      + security_opts         = (known after apply)
      + shm_size              = (known after apply)
      + start                 = true
      + stdin_open            = false
      + tty                   = false

      + healthcheck {
          + interval     = (known after apply)
          + retries      = (known after apply)
          + start_period = (known after apply)
          + test         = (known after apply)
          + timeout      = (known after apply)
        }

      + labels {
          + label = "traefik.docker.network"
          + value = "docknet"
        }
      + labels {
          + label = "traefik.enable"
          + value = "true"
        }
      + labels {
          + label = "traefik.frontend.rule"
          + value = "Host:traefik.localdns.xyz"
        }
      + labels {
          + label = "traefik.port"
          + value = "8080"
        }

      + networks_advanced {
          + aliases = [
              + "docknet",
            ]
          + name    = "docknet"
        }

      + ports {
          + external = 80
          + internal = 80
          + ip       = "0.0.0.0"
          + protocol = "tcp"
        }

      + volumes {
          + container_path = "/var/run/docker.sock"
          + host_path      = "/var/run/docker.sock"
        }
    }

  # docker_image.nginx will be created
  + resource "docker_image" "nginx" {
      + id          = (known after apply)
      + latest      = (known after apply)
      + name        = "nginx:stable-alpine"
      + output      = (known after apply)
      + repo_digest = (known after apply)
    }

  # docker_image.traefik will be created
  + resource "docker_image" "traefik" {
      + id          = (known after apply)
      + latest      = (known after apply)
      + name        = "traefik:1.7.14"
      + output      = (known after apply)
      + repo_digest = (known after apply)
    }

  # docker_network.nginx will be created
  + resource "docker_network" "nginx" {
      + driver      = "bridge"
      + id          = (known after apply)
      + internal    = (known after apply)
      + ipam_driver = "default"
      + name        = "docknet"
      + options     = (known after apply)
      + scope       = (known after apply)

      + ipam_config {
          + aux_address = (known after apply)
          + gateway     = (known after apply)
          + ip_range    = (known after apply)
          + subnet      = (known after apply)
        }
    }

  # random_string.nginx will be created
  + resource "random_string" "nginx" {
      + id          = (known after apply)
      + length      = 8
      + lower       = true
      + min_lower   = 0
      + min_numeric = 0
      + min_special = 0
      + min_upper   = 0
      + number      = true
      + result      = (known after apply)
      + special     = false
      + upper       = false
    }

Plan: 6 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + nginx_container_name   = "nginx"
  + nginx_url              = (known after apply)
  + traefik_container_name = "traefik"
  + traefik_url            = "http://traefik.localdns.xyz/"
</code></pre>

<p>Which we can see will create 2 containers, traefik and then nginx, map the configs and html in place and also sets the traefik hostname in the labels for our respective containers so that we can reach them via the specific host headers.</p>

<p>The we can deploy our containers:</p>

<pre><code>terraform apply -auto-approve
</code></pre>

<p>Which will provide us the output detail defined from our <code>outputs.tf</code>:</p>

<pre><code>Apply complete! Resources: 6 added, 0 changed, 0 destroyed.

Outputs:

nginx_container_name = "nginx"
nginx_url = "http://www.5igjdfq9.localdns.xyz/"
traefik_container_name = "traefik"
traefik_url = "http://traefik.localdns.xyz/"
</code></pre>

<h2>Access our Containers</h2>

<p>We can access our Traefik Dashboard on <a href="http://traefik.localdns.xyz">http://traefik.localdns.xyz</a> and should look something like this:</p>

<p><img src="https://user-images.githubusercontent.com/567298/143064031-23e9dbe4-522b-4f11-96f9-f30a2104ee44.png" alt="image" /></p>

<p>And when we access our Nginx container on <a href="http://www.5igjdfq9.localdns.xyz">http://www.5igjdfq9.localdns.xyz</a> it should look more or less like this:</p>

<p><img src="https://user-images.githubusercontent.com/567298/143064228-88107b75-31fc-41eb-aee0-f26ff976c42a.png" alt="image" /></p>

<p>Running a <code>docker ps</code> will show our running containers:</p>

<pre><code>docker ps
CONTAINER ID   IMAGE                  COMMAND                  CREATED         STATUS                PORTS                NAMES
e45158ae8cba   nginx:stable-alpine    "/docker-entrypoint   3 minutes ago   Up 3 minutes          80/tcp               nginx
ebdbe42a0fcb   traefik:1.7.14         "/traefik --api       3 minutes ago   Up 3 minutes          0.0.0.0:80-&gt;80/tcp   traefik
</code></pre>

<h2>Cleanup</h2>

<p>We can delete our containers by running:</p>

<pre><code>terraform destroy -auto-approve
</code></pre>

<h2>Thank You</h2>

<p>Thanks for reading, if you like my content, check out my <strong><a href="https://ruan.dev">website</a></strong> or follow me at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running Loki Behind Nginx Reverse Proxy]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/10/29/running-loki-behind-nginx-reverse-proxy/"/>
    <updated>2020-10-29T08:29:13+00:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/10/29/running-loki-behind-nginx-reverse-proxy</id>
    <content type="html"><![CDATA[<p>In this tutorial I will demonstrate how to run <strong>Loki v2.0.0</strong> behind a <strong>Nginx Reverse Proxy</strong> with basic http authentication enabled on Nginx and what to do to configure Nginx for <strong>websockets</strong>, which is required when you want to use <strong>tail in logcli</strong> via Nginx.</p>

<h2>Assumptions</h2>

<p>My environment consists of a AWS Application LoadBalancer with a Host entry and a Target Group associated to port 80 of my Nginx/Loki EC2 instance.</p>

<p>Health checks to my EC2 instance are being performed to <code>instance:80/ready</code></p>

<p>I have a S3 bucket and a DynamoDB table already running in my account which Loki will use. But <strong>NOTE</strong> that boltdb-shipper is now production ready since <a href="https://github.com/grafana/loki/blob/v2.0.0/CHANGELOG.md#20">v2.0.0</a>, which is awesome, because now you only require a object store such as S3, so you don&rsquo;t need DynamoDB.</p>

<p>More information on this topic can be found under their <a href="https://github.com/grafana/loki/blob/v2.0.0/CHANGELOG.md#20">changelog</a></p>

<h2>What can you expect from this blogpost</h2>

<p>We will go through the following topics:</p>

<ul>
<li>Install Loki v2.0.0 and Nginx</li>
<li>Configure HTTP Basic Authentication to Loki&rsquo;s API Endpoints</li>
<li>Bypass HTTP Basic Authentication to the <code>/ready</code> endpoint for our Load Balancer to perform healthchecks</li>
<li>Enable Nginx to upgrade websocket connections so that we can use <code>logcli --tail</code></li>
<li>Test out access to Loki via our Nginx Reverse Proxy</li>
<li>Install and use LogCLI</li>
</ul>


<h2>Install Software</h2>

<p>First we will install <code>nginx</code> and <code>apache2-utils</code>. In my use-case I will be using Ubuntu 20 as my operating system:</p>

<pre><code>$ sudo apt update &amp;&amp; sudo apt install nginx apache2-utils -y
</code></pre>

<p>Next we will install Loki v2.0.0, if you are upgrading from a previous version of Loki, I would recommend checking out the <a href="https://github.com/grafana/loki/releases/tag/v2.0.0">upgrade guide</a> mentioned on their releases page.</p>

<p>Download the package:</p>

<pre><code>$ curl -O -L "https://github.com/grafana/loki/releases/download/v2.0.0/loki-linux-amd64.zip"
</code></pre>

<p>Unzip the archive:</p>

<pre><code>$ unzip loki-linux-amd64.zip
</code></pre>

<p>Move the binary to your $PATH:</p>

<pre><code>$ sudo mv loki-linux-amd64 /usr/local/bin/loki
</code></pre>

<p>And ensure that the binary is executable:</p>

<pre><code>$ sudo chmod a+x /usr/local/bin/loki
</code></pre>

<h2>Configuration</h2>

<p>Create the user that will be responsible for running loki:</p>

<pre><code>$ useradd --no-create-home --shell /bin/false loki
</code></pre>

<p>Create the directory where we will place the loki configuration:</p>

<pre><code>$ mkdir /etc/loki
</code></pre>

<p>Create the loki configuration file:</p>

<pre><code>$ cat /etc/loki/loki-config.yml
auth_enabled: false

server:
  http_listen_port: 3100
  http_listen_address: 127.0.0.1
  http_server_read_timeout: 1000s
  http_server_write_timeout: 1000s
  http_server_idle_timeout: 1000s
  log_level: info

ingester:
  lifecycler:
    address: 127.0.0.1
    ring:
      kvstore:
        store: inmemory
      replication_factor: 1
    final_sleep: 0s
  chunk_encoding: snappy
  chunk_idle_period: 1h
  chunk_target_size: 1048576
  chunk_retain_period: 30s
  max_transfer_retries: 0

# https://grafana.com/docs/loki/latest/configuration/#schema_config
schema_config:
  configs:
    - from: 2020-05-15
      store: aws
      object_store: s3
      schema: v11
      index:
        prefix: loki-logging-index

storage_config:
  aws:
    http_config:
      idle_conn_timeout: 90s
      response_header_timeout: 0s
    s3: s3://myak:mysk@eu-west-1/loki-logs-datastore

    dynamodb:
      dynamodb_url: dynamodb://myak:mysk@eu-west-1

limits_config:
  enforce_metric_name: false
  reject_old_samples: true
  reject_old_samples_max_age: 168h
  ingestion_rate_mb: 30
  ingestion_burst_size_mb: 60

# https://grafana.com/docs/loki/latest/operations/storage/retention/
# To avoid querying of data beyond the retention period, max_look_back_period config in chunk_store_config
# must be set to a value less than or equal to what is set in table_manager.retention_period
chunk_store_config:
  max_look_back_period: 720h

# https://grafana.com/docs/loki/latest/operations/storage/retention/
table_manager:
  retention_deletes_enabled: true
  retention_period: 720h
  chunk_tables_provisioning:
    inactive_read_throughput: 10
    inactive_write_throughput: 10
    provisioned_read_throughput: 50
    provisioned_write_throughput: 20
  index_tables_provisioning:
    inactive_read_throughput: 10
    inactive_write_throughput: 10
    provisioned_read_throughput: 50
    provisioned_write_throughput: 20
</code></pre>

<p>Apply permissions so that the loki user has access to it&rsquo;s configuration:</p>

<pre><code>$ chown -R loki:loki /etc/loki
</code></pre>

<p>Create a systemd unit file:</p>

<pre><code>$ cat /etc/systemd/system/loki.service
[Unit]
Description=Loki
Wants=network-online.target
After=network-online.target

[Service]
User=loki
Group=loki
Type=simple
Restart=on-failure
ExecStart=/usr/local/bin/loki -config.file /etc/loki/loki-config.yml

[Install]
WantedBy=multi-user.target
</code></pre>

<p>Create the main nginx config:</p>

<pre><code>$ cat /etc/nginx/nginx.conf
user www-data;
worker_processes auto;
pid /run/nginx.pid;
include /etc/nginx/modules-enabled/*.conf;
worker_rlimit_nofile 100000;

events {
        worker_connections 4000;
        use epoll;
        multi_accept on;
}

http {

    # basic settings
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
        open_file_cache_valid 30s;
        open_file_cache_min_uses 2;
        open_file_cache_errors on;

    include /etc/nginx/mime.types;
    default_type application/octet-stream;

        # ssl settings
    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;
    ssl_prefer_server_ciphers on;

    # websockets config
    map $http_upgrade $connection_upgrade {
            default upgrade;
            '' close;
        }

    # logging settings
    access_log off;
    access_log /var/log/nginx/access.log;
    error_log /var/log/nginx/error.log;

    # gzip settings
    gzip on;
        gzip_min_length 10240;
        gzip_comp_level 1;
        gzip_vary on;
        gzip_disable msie6;
        gzip_proxied expired no-cache no-store private auth;
        gzip_types
        text/css
        text/javascript
        text/xml
        text/plain
        text/x-component
        application/javascript
        application/x-javascript
        application/json
    application/xml
        application/rss+xml
    application/atom+xml
        font/truetype
        font/opentype
        application/vnd.ms-fontobject
        image/svg+xml;
        reset_timedout_connection on;
        client_body_timeout 10;
        send_timeout 2;
        keepalive_requests 100000;

        # virtual host configs
    include /etc/nginx/conf.d/loki.conf;
}
</code></pre>

<p>Create the virtual host config:</p>

<pre><code>$ cat /etc/nginx/conf.d/loki.conf
upstream loki {
  server 127.0.0.1:3100;
  keepalive 15;
}

server {
  listen 80;
  server_name loki.localdns.xyz;

  auth_basic "loki auth";
  auth_basic_user_file /etc/nginx/passwords;

  location / {
    proxy_read_timeout 1800s;
    proxy_connect_timeout 1600s;
    proxy_pass http://loki;
    proxy_http_version 1.1;
    proxy_set_header Upgrade $http_upgrade;
    proxy_set_header Connection $connection_upgrade;
    proxy_set_header Connection "Keep-Alive";
    proxy_set_header Proxy-Connection "Keep-Alive";
    proxy_redirect off;
  }

  location /ready {
    proxy_pass http://loki;
    proxy_http_version 1.1;
    proxy_set_header Connection "Keep-Alive";
    proxy_set_header Proxy-Connection "Keep-Alive";
    proxy_redirect off;
    auth_basic "off";
  }
}
</code></pre>

<p>As you&rsquo;ve noticed, we are providing a <code>auth_basic_user_file</code> to <code>/etc/nginx/passwords</code>, so let&rsquo;s create a user that we will be using to authenticate against loki:</p>

<pre><code>$ htpasswd -c /etc/nginx/passwords lokiisamazing
</code></pre>

<h2>Enable and Start Services</h2>

<p>Because we created a systemd unit file, we need to reload the systemd daemon:</p>

<pre><code>$ sudo systemctl daemon-reload
</code></pre>

<p>Then enable nginx and loki on boot:</p>

<pre><code>$ sudo systemctl enable nginx
$ sudo systemctl enable loki
</code></pre>

<p>Then start or restart both services:</p>

<pre><code>$ sudo systemctl restart nginx
$ sudo systemctl restart loki
</code></pre>

<p>You should see both ports, 80 and 3100 are listening:</p>

<pre><code>$ sudo netstat -tulpn | grep -E '(3100|80)'
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      8949/nginx: master
tcp        0      0 127.0.0.1:3100          0.0.0.0:*               LISTEN      23498/loki
</code></pre>

<h2>Test Access</h2>

<p>You will notice that I have a <code>/ready</code> endpoint that I am proxy passing to loki, which bypasses authentication, this has been setup for my AWS Application Load Balancer&rsquo;s Target Group to perform health checks against.</p>

<p>We can verify if we are getting a 200 response code without passing authentication:</p>

<pre><code>$ curl -i http://loki.localdns.xyz/ready
HTTP/1.1 200 OK
Server: nginx/1.14.0 (Ubuntu)
Date: Thu, 29 Oct 2020 09:15:52 GMT
Content-Type: text/plain; charset=utf-8
Content-Length: 6
Connection: keep-alive
X-Content-Type-Options: nosniff

ready
</code></pre>

<p>If we try to make a request to Loki&rsquo;s labels API endpoint, you will notice that we are returned with a 401 unauthorized response:</p>

<pre><code>$ curl -i http://loki.localdns.xyz/loki/api/v1/labels
HTTP/1.1 401 Unauthorized
Server: nginx/1.14.0 (Ubuntu)
Date: Thu, 29 Oct 2020 09:16:52 GMT
Content-Type: text/html
Content-Length: 204
Connection: keep-alive
WWW-Authenticate: Basic realm="loki auth"
</code></pre>

<p>So let&rsquo;s access the labels API endpoint by passing our basic auth credentials. To leave no leaking passwords behind, create a file and save your password content in that file:</p>

<pre><code>$ vim /tmp/.pass
-&gt; then enter your password and save the file &lt;-
</code></pre>

<p>Expose the content as an environment variable:</p>

<pre><code>$ pass=$(cat /tmp/.pass)
</code></pre>

<p>Now make a request to Loki&rsquo;s labels endpoint by passing authentication:</p>

<pre><code>$ curl -i -u lokiisawesome:$pass http://loki.localdns.xyz/loki/api/v1/labels
HTTP/1.1 200 OK
Server: nginx/1.14.0 (Ubuntu)
Date: Thu, 29 Oct 2020 09:20:20 GMT
Content-Type: application/json; charset=UTF-8
Content-Length: 277
Connection: keep-alive

{"status":"success","data":["__name__","aws_account","cluster_name","container_name","environment","filename","job","service","team"]}
</code></pre>

<p>Then ensure that your remove the password file:</p>

<pre><code>$ rm -rf /tmp/.pass
</code></pre>

<p>And unset your pass environment variable, to clean up your tracks:</p>

<pre><code>$ unset pass
</code></pre>

<h2>LogCLI</h2>

<p>Now for my favorite part, using logcli to interact with Loki, but more specifically using <code>--tail</code> as it requires websockets, nginx will now be able to upgrade those connections:</p>

<p>Install logcli, in my case I am using a mac, so I will be using darwin:</p>

<pre><code>$ wget https://github.com/grafana/loki/releases/download/v2.0.0/logcli-darwin-amd64.zip
$ unzip logcli-darwin-amd64.zip
$ mv logcli-darwin-amd64 /usr/local/bin/logcli
</code></pre>

<p>Set your environment variables for logcli:</p>

<pre><code>$ export LOKI_ADDR=https://loki.yourdomain.com # im doing ssl termination on the aws alb
$ export LOKI_USERNAME=lokiisawesome
$ export LOKI_PASSWORD=$pass 
</code></pre>

<p>Now for that sweetness of tailing ALL THE LOGS!! :-D . Let&rsquo;s first discover the label that we want to select:</p>

<pre><code>$ logcli labels --quiet container_name | grep deadman
ecs-deadmanswitch-4-deadmanswitch-01234567890abcdefghi
</code></pre>

<p>Then tail for the win!</p>

<pre><code>$ logcli query --quiet --output raw --tail '{job="prod/dockerlogs", container_name=~"ecs-deadmanswitch.*"}'
time="2020-10-29T09:03:36Z" level=info msg="timerID: xxxxxxxxxxxxxxxxxxxx"
time="2020-10-29T09:03:36Z" level=info msg="POST - /ping/xxxxxxxxxxxxxxxxxxx"
</code></pre>

<p>Awesome right?</p>

<p><img src="https://media.giphy.com/media/3ohzdIuqJoo8QdKlnW/giphy.gif" alt="" /></p>

<h2>Thank You</h2>

<p>Hope that you found this useful, make sure to follow Grafana&rsquo;s blog for more awesome content:</p>

<ul>
<li><a href="https://grafana.com/blog/">https://grafana.com/blog/</a></li>
</ul>


<p>If you liked this content, please make sure to share or come say hi on my website or twitter:</p>

<ul>
<li><a href="https://ruan.dev">w: ruan.dev</a></li>
<li><a href="https://ruan.dev">t: @ruanbekker</a></li>
</ul>


<p>For other content of mine on Loki:</p>

<ul>
<li><a href="https://blog.ruanbekker.com/blog/categories/loki/">https://blog.ruanbekker.com/blog/categories/loki/</a></li>
<li><a href="https://github.com/ruanbekker/docker-loki-distributed-minio">https://github.com/ruanbekker/docker-loki-distributed-minio</a></li>
<li><a href="https://github.com/ruanbekker/loki-docker-nginx-example">https://github.com/ruanbekker/loki-docker-nginx-example</a></li>
<li><a href="https://github.com/ruanbekker/loki-minio-docker">https://github.com/ruanbekker/loki-minio-docker</a></li>
<li><a href="https://github.com/ruanbekker/cheatsheets/tree/master/loki">https://github.com/ruanbekker/cheatsheets/tree/master/loki</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Increase Performance With Your Ghost Blog on Docker]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/06/08/build-a-ghost-blog-with-nginx-cache-on-docker/"/>
    <updated>2020-06-08T23:28:07+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/06/08/build-a-ghost-blog-with-nginx-cache-on-docker</id>
    <content type="html"><![CDATA[<p><img src="https://img.sysadmins.co.za/wngib2.png" alt="nginx-blog-ghost-caching" /></p>

<p>Nginx Caching + Ghost == Great Performance.</p>

<p>In this post we will build a nginx reverse proxy with caching enabled for our static content such as images, which will be our frontend and therefore we will have port 80 exposed, and run our ghost blog as our backend, which we will proxy traffic through from our nginx container.</p>

<h2>But why would you want caching?</h2>

<p>Returning data from memory is a lot faster than returning data from disk, and in this case where a request is being made against nginx, then it proxy passes the request to ghost, gets the data that you requested and returns the data to the client.</p>

<p>So for items that rarely changes like images, we can benefit from caching, so the images can be returned from the nginx service, where the first request will be made to ghost and then it will be loaded into nginx cache, so then the next time when you request the same image it will be returned from cache instead of making that same request to ghost again.</p>

<h2>Caching Info</h2>

<p>For this demonstration once we define the size of our chache which will be 500MB and we specify that if an object has not been accessed for 24 hours, we can expire the object from the cache.</p>

<h2>Nginx</h2>

<p>We will build our nginx container by adding our custom nginx config to our dockerfile.</p>

<p>Our <code>Dockerfile</code> will look like the following:</p>

<pre><code>ROM nginx:stable
ADD nginx.conf /etc/nginx/nginx.conf
</code></pre>

<p>Our <code>nginx.conf</code> configuration file:</p>

<pre><code>events {
  worker_connections  1024;
}

http {
  default_type       text/html;
  access_log         /dev/stdout;
  sendfile           on;
  keepalive_timeout  65;

  #proxy_cache_path /tmp/ghostcache levels=1:2 keys_zone=ghostcache:500m max_size=2g inactive=30d;
  proxy_cache_path /tmp/ghostcache levels=1:2 keys_zone=ghostcache:60m max_size=500m inactive=24h;
  proxy_cache_key "$scheme$request_method$host$request_uri";
  proxy_cache_methods GET HEAD;

  server {
    listen 80;

    location / {
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header Host $http_host;
        proxy_pass http://ghost:2368;
    }

    location ~* \.(?:css|js|ico)$ {
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header Host $http_host;
        proxy_pass http://ghost:2368;
        access_log off;
    }

    location ^~ /content/images/ {
        proxy_cache ghostcache;
        proxy_cache_valid 60m;
        proxy_cache_valid 404 1m;
        proxy_ignore_headers Set-Cookie;
        proxy_hide_header Set-Cookie;
        proxy_cache_use_stale error timeout invalid_header updating http_500 http_502 http_503 http_504;
        proxy_ignore_headers Cache-Control;
        add_header X-Cache-Status $upstream_cache_status;

        proxy_set_header Host $http_host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_pass http://ghost:2368;
        access_log off;
    }
  }
}
</code></pre>

<p>Then our <code>docker-compose.yml</code> where we will add our nginx and ghost container to run together:</p>

<pre><code>version: '3.4'

services:
  ghost:
    image: ghost:3.15.1
    container_name: 'ghost'
    environment:
      - NODE_ENV=production
      - url=http://localhost:80
    networks:
      - ghost
    volumes:
      - ghost_content:/var/lib/ghost/content/data

  proxy:
    build: .
    container_name: 'proxy'
    depends_on:
      - ghost
    ports:
      - 80:80
    networks:
      - ghost

networks:
  ghost: {}

volumes:
  ghost_content: {}
</code></pre>

<p>To boot our stack:</p>

<pre><code>$ docker-compose up
</code></pre>

<h2>Test Caching</h2>

<p>Once your containers are in a running state, open your browsers devloper tools and look at the networking tab, then access your ghost blog on <code>http://localhost:80/</code>, the first time a image is opened you should see the cache shows <code>MISS</code> when you refresh again you should see a <code>HIT</code>, which means that the object is being returned from your cache.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nginx Analysis Dashboard Using Grafana and Elasticsearch]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/04/28/nginx-analysis-dashboard-using-grafana-and-elasticsearch/"/>
    <updated>2020-04-28T20:07:22+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/04/28/nginx-analysis-dashboard-using-grafana-and-elasticsearch</id>
    <content type="html"><![CDATA[<p>In this post we will be setting up a <strong>analytical dashboard</strong> using <strong>grafana</strong> to visualize our <strong>nginx access logs</strong>.</p>

<p><img width="1212" alt="grafana-nginx-elasticsearch-prometheus" src="https://user-images.githubusercontent.com/567298/80539136-48ac0c00-89a7-11ea-869d-597da4fa4d92.png"></p>

<p>In this tutorial I will be using my other blog <code>sysadmins.co.za</code> which is being served on nginx. We will also be setting up the other components such as filebeat, logstash, elasticsearch and redis, which require if you would like to follow along.</p>

<h2>The End Result</h2>

<p>We will be able to analyze our Nginx Access logs to answer questions such as:</p>

<ul>
<li>Whats the Top 10 Countries accessing your website in the last 24 hours</li>
<li>Who&rsquo;s the Top 10 Referers?</li>
<li>Whats the most popular page for the past 24 hours?</li>
<li>How does the percentage of 200&rsquo;s vs 404&rsquo;s look like?</li>
<li>Ability to view results based on status code</li>
<li>Everyone loves a World Map to view hotspots</li>
</ul>


<p>At the end of the tutorial, your dashboard will look similar to this:</p>

<p><img width="1123" alt="grafana-elasticsearch-nginx-dashboard" src="https://user-images.githubusercontent.com/567298/80523974-32925180-898f-11ea-96b6-e8e559655745.png"></p>

<h2>High Level Overview</h2>

<p>Our infrastructure will require Nginx with Filebeat, Redis, Logstash, Elasticsearch and Grafana and will look like this:</p>

<p><img width="871" alt="grafana-elasticsearch-logs-setup" src="https://user-images.githubusercontent.com/567298/80526020-6d49b900-8992-11ea-9a39-67331ccc3808.png"></p>

<p>I will drill down how everything is connected:</p>

<ol>
<li>Nginx has a custom <code>log_format</code> that we define, that will write to <code>/var/log/nginx/access_json.log</code>, which will be picked up by <strong>Filebeat</strong> as a input.</li>
<li>and <strong>Filebeat</strong> has an output that pushes the data to <strong>Redis</strong></li>
<li><strong>Logstash</strong> is configured with <strong>Redis</strong> as an input with configured filter section to transform the data and outputs to <strong>Elasticsearch</strong></li>
<li>From <strong>Grafana</strong> we have a configured <strong>Elasticsearch</strong> datasource</li>
<li>Use the grafana template to build this awesome dashboard on Grafana</li>
</ol>


<p>But first, a massive thank you to <a href="https://www.akiraka.net">akiraka</a> for templatizing this dashboard and made it available on <a href="https://grafana.com/orgs/akiraka">grafana</a></p>

<h2>Let&rsquo;s build all the things</h2>

<p>I will be using LXD to run my system/server containers (running ubuntu 18), but you can use a vps, cloud instance, multipass, virtualbox, or anything to host your servers that we will be deploying redis, logstash, etc.</p>

<p>Servers provisioned for this setup:</p>

<ul>
<li>Nginx</li>
<li>Redis</li>
<li>Logstash</li>
<li>Elasticsearch</li>
<li>Grafana</li>
<li>Prometheus</li>
</ul>


<h2>Elasticsearch</h2>

<p>If you don&rsquo;t have a cluster running already, you can follow <strong><a href="https://blog.ruanbekker.com/blog/2019/04/02/setup-a-5-node-highly-available-elasticsearch-cluster/">this tutorial</a></strong> which will help you deploy a HA Elasticsearch Cluster, or if you prefer docker, you can follow <strong><a href="https://blog.ruanbekker.com/blog/2018/04/29/running-a-3-node-elasticsearch-cluster-with-docker-compose-on-your-laptop-for-testing/">this tutorial</a></strong></p>

<h2>Redis</h2>

<p>For our in-memory data store, I will be securing my redis installation with a password as well.</p>

<p>Install redis:</p>

<pre><code>$ apt update &amp;&amp; apt install redis-server -y
</code></pre>

<p>Generate a password:</p>

<pre><code>$ openssl rand -base64 36
9V5YlWvm8WuC4n1KZLYUEbLruLJLNJEnDzhu4WnAIfgxMmlv
</code></pre>

<p>In your redis config <code>/etc/redis/redis.conf</code>, you need to change the following:</p>

<pre><code>...
bind 0.0.0.0
port 6379
daemonize yes
supervised systemd
requirepass 9V5YlWvm8WuC4n1KZLYUEbLruLJLNJEnDzhu4WnAIfgxMmlv
...
</code></pre>

<p>Restart redis to activate your changes:</p>

<pre><code>$ systemctl restart redis.service
</code></pre>

<p>and then set and get a key using your password:</p>

<pre><code>$ redis-cli -a "9V5YlWvm8WuC4n1KZLYUEbLruLJLNJEnDzhu4WnAIfgxMmlv" set test ok
$ redis-cli -a "9V5YlWvm8WuC4n1KZLYUEbLruLJLNJEnDzhu4WnAIfgxMmlv" get test
ok
</code></pre>

<h2>Logstash</h2>

<p>On the logstash server, install the requirements:</p>

<pre><code>$ apt update &amp;&amp; apt install wget apt-transport-https default-jre -y
$ wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | apt-key add -
$ echo "deb https://artifacts.elastic.co/packages/7.x/apt stable main" | tee -a /etc/apt/sources.list.d/elastic-7.x.list
</code></pre>

<p>Now the repository for elastic is setup now we need to update and install logstash:</p>

<pre><code>$ apt update &amp;&amp; apt install logstash -y
</code></pre>

<p>Once logstash is installed, we need to provide logstash with a configuration, in our scenario we will have a input for redis, a filter section to transform and output as elasticsearch.</p>

<p>Just make sure of the following:</p>

<ul>
<li>Populate the connection details of redis (we will define the key in filebeat later)</li>
<li>Ensure that <code>GeoLite2-City.mmdb</code> is in the path that I have under filter</li>
<li>Populate the connectiond details of Elasticsearch and choose a suitable index name, we will need to provide that index name in Grafana later</li>
</ul>


<p>Create the config: <code>/etc/logstash/conf.d/logs.conf</code> and my config will look like the following. (<a href="https://grafana.com/grafana/dashboards/11190">config source</a>)</p>

<pre><code>input {
  redis {
    data_type =&gt;"list"
    key =&gt;"nginx_logs"
    host =&gt;"10.47.127.37"
    port =&gt; 6379
    password =&gt; "9V5YlWvm8WuC4n1KZLYUEbLruLJLNJEnDzhu4WnAIfgxMmlv"
    db =&gt; 0
  }
}

filter {
  geoip {
    target =&gt; "geoip"
    source =&gt; "client_ip"
    database =&gt; "/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-filter-geoip-6.0.3-java/vendor/GeoLite2-City.mmdb"
    add_field =&gt; [ "[geoip][coordinates]", "%{[geoip][longitude]}" ]
    add_field =&gt; [ "[geoip][coordinates]", "%{[geoip][latitude]}" ]
    remove_field =&gt; ["[geoip][latitude]", "[geoip][longitude]", "[geoip][country_code]", "[geoip][country_code2]", "[geoip][country_code3]", "[geoip][timezone]", "[geoip][continent_code]", "[geoip][region_code]"]
  }
  mutate {
    convert =&gt; [ "size", "integer" ]
    convert =&gt; [ "status", "integer" ]
    convert =&gt; [ "responsetime", "float" ]
    convert =&gt; [ "upstreamtime", "float" ]
    convert =&gt; [ "[geoip][coordinates]", "float" ]
    remove_field =&gt; [ "ecs","agent","host","cloud","@version","input","logs_type" ]
  }
  useragent {
    source =&gt; "http_user_agent"
    target =&gt; "ua"
    remove_field =&gt; [ "[ua][minor]","[ua][major]","[ua][build]","[ua][patch]","[ua][os_minor]","[ua][os_major]" ]
  }
}
output {
  elasticsearch {
    hosts =&gt; ["10.47.127.132", "10.47.127.199", "10.47.127.130"]
    #user =&gt; "myusername"
    #password =&gt; "mypassword"
    index =&gt; "logstash-nginx-sysadmins-%{+YYYY.MM.dd}"
  }
}
</code></pre>

<h2>Nginx</h2>

<p>On our nginx server we will install nginx and filebeat, then configure nginx to log to a custom log format, and configure filebeat to read the logs and push it to redis.</p>

<p>Installing nginx:</p>

<pre><code>$ apt update &amp;&amp; apt install nginx -y
</code></pre>

<p>Installing <a href="https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-installation.html">filebeat</a>:</p>

<pre><code>$ curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.6.2-amd64.deb
$ dpkg -i filebeat-7.6.2-amd64.deb
</code></pre>

<p>Next we will configure nginx to log to a seperate file with a custom log format to include data such as the, request method, upstream response time, hostname, remote address, etc.</p>

<p>Under the <code>http</code> directive in your <code>/etc/nginx/nginx.conf</code>, configure the <code>log_format</code> and <code>access_log</code>:</p>

<pre><code>http {
...
        log_format json_logs '{"@timestamp":"$time_iso8601","host":"$hostname",'
                            '"server_ip":"$server_addr","client_ip":"$remote_addr",'
                            '"xff":"$http_x_forwarded_for","domain":"$host",'
                            '"url":"$uri","referer":"$http_referer",'
                            '"args":"$args","upstreamtime":"$upstream_response_time",'
                            '"responsetime":"$request_time","request_method":"$request_method",'
                            '"status":"$status","size":"$body_bytes_sent",'
                            '"request_body":"$request_body","request_length":"$request_length",'
                            '"protocol":"$server_protocol","upstreamhost":"$upstream_addr",'
                            '"file_dir":"$request_filename","http_user_agent":"$http_user_agent"'
                            '}';

        access_log  /var/log/nginx/access_json.log  json_logs;
...
}
</code></pre>

<p>Restart nginx to activate the changes:</p>

<pre><code>$ systemctl restart nginx
</code></pre>

<p>Next we need to configure filebeat to read from our nginx access logs and configure the output to redis. Edit the filebeat config:</p>

<pre><code>$ vim /etc/filebeat/filebeat.yml
</code></pre>

<p>And configure filebeat with the following and make sure to change the values where you need to:</p>

<pre><code># config source: akiraka.net
# filebeat input 
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/nginx/access_json.log
  json.keys_under_root: true
  json.overwrite_keys: true
  json.add_error_key: true

# filebeat modules 
filebeat.config.modules:
  # remove the escape character before the wildcard below
  path: ${path.config}/modules.d/\*.yml
  reload.enabled: false

# elasticsearch template settings
setup.template.settings:
  index.number_of_shards: 3

# redis output
output.redis:
  hosts: ["10.47.127.140:6379"]
  password: "9V5YlWvm8WuC4n1KZLYUEbLruLJLNJEnDzhu4WnAIfgxMmlv"
  key: "nginx_logs"
  # ^ this key needs to be the same as the configured key on logstash 
  db: 0
  timeout: 5
</code></pre>

<p>Restart filebeat:</p>

<pre><code>$ systemctl restart filebeat
</code></pre>

<p>When you make a request to your nginx server, you should see a similar logline like below:</p>

<pre><code>$ tail -n1 /var/log/nginx/access_elg.log
{"@timestamp":"2020-04-28T20:05:03+00:00","host":"sysadmins-blog","server_ip":"10.68.100.89","client_ip":"x.x.x.x","xff":"x.x.x.x","domain":"sysadmins.co.za","url":"/","referer":"-","args":"-","upstreamtime":"0.310","responsetime":"0.312","request_method":"GET","status":"200","size":"4453","request_body":"-","request_length":"519","protocol":"HTTP/1.1","upstreamhost":"127.0.0.1:2369","file_dir":"/var/www/web/root/","http_user_agent":"Mozilla/5.0"}
</code></pre>

<h2>Grafana</h2>

<p>On the grafana server, install grafana:</p>

<pre><code>$ apt update &amp;&amp; apt install apt-transport-https software-properties-common wget -y
$ wget -q -O - https://packages.grafana.com/gpg.key | apt-key add -
$ add-apt-repository "deb https://packages.grafana.com/oss/deb stable main"
$ apt update &amp;&amp; apt install grafana -y
</code></pre>

<p>Now we need to install a couple of grafana plugins that we require for our dashboards:</p>

<pre><code>$ grafana-cli plugins install grafana-worldmap-panel
$ grafana-cli plugins install grafana-clock-panel
$ grafana-cli plugins install grafana-piechart-panel
</code></pre>

<p>Now reload systemd and restart grafana:</p>

<pre><code>$ systemctl daemon-reload
$ systemctl restart grafana-server
</code></pre>

<p>If you would like to setup nginx as a reverse proxy to grafana, you can have a look at <strong><a href="https://blog.ruanbekker.com/blog/2019/05/17/install-grafana-to-visualize-your-metrics-from-datasources-such-as-prometheus-on-linux/">this blogpost</a></strong> on how to do that.</p>

<h2>Prometheus</h2>

<p>If you don&rsquo;t have Prometheus installed already, you can view my <a href="https://blog.ruanbekker.com/blog/2019/05/07/setup-prometheus-and-node-exporter-on-ubuntu-for-epic-monitoring/">blogpost</a> on setting up Prometheus.</p>

<h2>Verifying</h2>

<p>To verify if everything works as expected, make a request to your nginx server, then have a look if your index count on elasticsearch increases:</p>

<pre><code>$ curl http://elasticsearch-endpoint-address:9200/_cat/indices/logstash-*?v
health status index                               uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   logstash-nginx-x-2020.04.28 SWbHCer-TeOcw6bi_695Xw   5   1      58279            0     32.6mb         16.3mb
</code></pre>

<p>If you dont, make sure that all the processes are running on the servers, and that each server is able to reach each other on the targeted ports.</p>

<h2>The Fun Part: Dashboarding</h2>

<p>Now that we have everything in place, the fun part is to build the dashboards, first we need to configure elasticsearch as our datasource and specify the index we want to read from. Open grafana on <code>http://ip.of.grafana.server:3000</code>, default user and password is admin.</p>

<p>Select config on the left and select datasources, add a datasource, select elasticsearch and specify your datasource name, mine is <strong>es-nginx</strong> in this example, the <strong>url</strong> of your elasticsearch endpoint, if you have secured your elasticsearch cluster with authentication, provide the auth, then provide your index name as as provided in logstash.</p>

<p>My configured index will look like <code>logstash-nginx-sysadmins-YYYY-MM-dd</code>, therefore I specified index name as <code>logstash-nginx-sysadmins-*</code> and my timefield as <code>@timestamp</code>, the version, and select save and test, which would look like this:</p>

<p><img width="569" alt="AC025E20-38D0-4676-B576-9F5932913BA1" src="https://user-images.githubusercontent.com/567298/80538019-48ab0c80-89a5-11ea-8f4f-a30384991ab9.png"></p>

<p>Now we will import our dashboard template (Once again a massive thank you to <a href="https://grafana.com/grafana/dashboards/11190">Shenxiang, Qingkong and Ruixi</a> which made this template available!), head over to dashboards and select import, then provide the ID: <code>11190</code>, after that it will prompt what your dashboard needs to be named and you need to select your Elasticsearch and Prometheus datasource.</p>

<p>The description of the panels is in Chinese, if you would like it in english, I have translated mine to english and made the dashboard json available in <a href="https://gist.githubusercontent.com/ruanbekker/699fca31ebd7223b675d0acd25ea84bc/raw/316a015a0464989117cd72a1e8e056854d582178/nginx_grafana_dashboard_11190_eng.json">this gist</a></p>

<h2>Tour of our Dashboard Panels</h2>

<p>Looking at our hotspot map:</p>

<p><img width="1212" alt="grafana" src="https://user-images.githubusercontent.com/567298/80539136-48ac0c00-89a7-11ea-869d-597da4fa4d92.png"></p>

<p>The summary and top 10 pages:</p>

<p><img width="1243" alt="76E8CBE1-4B03-4226-8041-B98879BAD66A" src="https://user-images.githubusercontent.com/567298/80540596-e86a9980-89a9-11ea-924d-29f777a7c15a.png"></p>

<p>Page views, historical trends:</p>

<p><img width="1239" alt="grafana-page-views" src="https://user-images.githubusercontent.com/567298/80539728-4eeeb800-89a8-11ea-959e-5e2915387b7b.png"></p>

<p>Top 10 referers and table data of our logs:</p>

<p><img width="1235" alt="B17C4F55-DF91-4EA0-9669-C237FF560459" src="https://user-images.githubusercontent.com/567298/80540381-772ae680-89a9-11ea-9067-61cd519c9d8a.png"></p>

<h2>Thank You</h2>

<p>I hope this was useful, if you have any issues with this feel free to reach out to me. If you like my work, please feel free to share this post, follow me on Twitter at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong> or visit me on my <strong><a href="https://ruan.dev">website</a></strong></p>

<p><a href="https://ko-fi.com/A6423ZIQ"><img src="https://www.ko-fi.com/img/githubbutton_sm.svg" alt="ko-fi" /></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nginx Metrics on Prometheus With the Nginx Log Exporter]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/04/25/nginx-metrics-on-prometheus-with-the-nginx-log-exporter/"/>
    <updated>2020-04-25T01:42:35+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/04/25/nginx-metrics-on-prometheus-with-the-nginx-log-exporter</id>
    <content type="html"><![CDATA[<p>In this post we will setup a nginx log exporter for prometeus to get metrics of our nginx web server, such as number of requests per method, status code, processed bytes etc. Then we will configure prometheus to scrape our nginx metric endpoint and also create a basic dashbaord to visualize our data.</p>

<p>If you follow along on this tutorial, it assumes that you have <a href="https://blog.ruanbekker.com/blog/2019/05/07/setup-prometheus-and-node-exporter-on-ubuntu-for-epic-monitoring/">Prometheus</a> and <a href="https://blog.ruanbekker.com/blog/2019/05/17/install-grafana-to-visualize-your-metrics-from-datasources-such-as-prometheus-on-linux/">Grafana</a> up and running. But if not the embedded links will take you to the blog posts to set it up.</p>

<h2>Nginx Webserver</h2>

<p>Install nginx:</p>

<pre><code>$ apt update
$ apt install nginx -y
</code></pre>

<p>Configure your nginx server&rsquo;s log format to match the nginx log exporter&rsquo;s expected format, we will name it custom:</p>

<pre><code>  log_format custom   '$remote_addr - $remote_user [$time_local] '
                      '"$request" $status $body_bytes_sent '
                      '"$http_referer" "$http_user_agent" "$http_x_forwarded_for"';
</code></pre>

<p>Edit your main nginx config:</p>

<pre><code>$ vim /etc/nginx/nginx.conf
</code></pre>

<p>This is how my complete config looks like:</p>

<pre><code>user www-data;
worker_processes auto;
pid /run/nginx.pid;
# remote the escape char if you are going to use this config
include /etc/nginx/modules-enabled/\*.conf;

events {
  worker_connections 768;
}

http {

  # basic config
  sendfile on;
  tcp_nopush on;
  tcp_nodelay on;
  keepalive_timeout 65;
  types_hash_max_size 2048;
  include /etc/nginx/mime.types;
  default_type application/octet-stream;

  # ssl config
  ssl_protocols TLSv1 TLSv1.1 TLSv1.2; 
  ssl_prefer_server_ciphers on;

  # logging config
  log_format custom   '$remote_addr - $remote_user [$time_local] '
                      '"$request" $status $body_bytes_sent '
                      '"$http_referer" "$http_user_agent" "$http_x_forwarded_for"';

  access_log /var/log/nginx/access.log custom;
  error_log /var/log/nginx/error.log;

  # gzip
  gzip on;

  # virtual host config
  include /etc/nginx/conf.d/myapp.conf;

}
</code></pre>

<p>I will delete the default host config:</p>

<pre><code>$ rm -rf /etc/nginx/sites-enabled/default
</code></pre>

<p>And then create my <code>/etc/nginx/conf.d/myapp.conf</code> as referenced in my main config, with the following:</p>

<pre><code>server {

  listen 80 default_server;
  # remove the escape char if you are going to use this config
  server_name \_;

  root /var/www/html;
  index index.html index.htm index.nginx-debian.html;

  location / {
    try_files $uri $uri/ =404;
  }

}
</code></pre>

<p>When you make a GET request to your server, you should see something like this in your access log:</p>

<pre><code>10x.1x.2x.1x - - [25/Apr/2020:00:31:11 +0000] "GET / HTTP/1.1" 200 396 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15" "-"
</code></pre>

<h2>Nginx Log Exporter</h2>

<p>Head over to the <a href="https://github.com/martin-helmich/prometheus-nginxlog-exporter/releases">prometheus-nginxlog-exporter releases</a> page and get the latest version, in the time of writing it is v1.4.0:</p>

<pre><code>$ wget https://github.com/martin-helmich/prometheus-nginxlog-exporter/releases/download/v1.4.0/prometheus-nginxlog-exporter
</code></pre>

<p>Make it executable and move it to your path:</p>

<pre><code>$ chmod +x prometheus-nginxlog-exporter
$ mv prometheus-nginxlog-exporter /usr/bin/prometheus-nginxlog-exporter
</code></pre>

<p>Create the directory where we will place our config for our exporter:</p>

<pre><code>$ mkdir /etc/prometheus
</code></pre>

<p>Create the config file:</p>

<pre><code>$ vim /etc/prometheus/nginxlog_exporter.yml
</code></pre>

<p>You can follow the instructions from <a href="https://github.com/martin-helmich/prometheus-nginxlog-exporter">github.com/prometheus-nginxlog-exporter</a> for more information on configuration, but I will be using the following config:</p>

<pre><code>listen:
  port: 4040
  address: "0.0.0.0"

consul:
  enable: false

namespaces:
  - name: myapp
    format: "$remote_addr - $remote_user [$time_local] \"$request\" $status $body_bytes_sent \"$http_referer\" \"$http_user_agent\" \"$http_x_forwarded_for\""
    source:
      files:
        - /var/log/nginx/access.log
    labels:
      service: "myapp"
      environment: "production"
      hostname: "myapp.example.com"
    histogram_buckets: [.005, .01, .025, .05, .1, .25, .5, 1, 2.5, 5, 10]
</code></pre>

<p>Create the systemd unit file:</p>

<pre><code>$ vim /etc/systemd/system/nginxlog_exporter.service
</code></pre>

<p>And my configuration that I will be using:</p>

<pre><code>[Unit]
Description=Prometheus Log Exporter
Wants=network-online.target
After=network-online.target

[Service]
User=root
Group=root
Type=simple
ExecStart=/usr/bin/prometheus-nginxlog-exporter -config-file /etc/prometheus/nginxlog_exporter.yml

[Install]
WantedBy=multi-user.target
</code></pre>

<p>Reload systemd and enable the service on boot:</p>

<pre><code>$ systemctl daemon-reload
$ systemctl enable nginxlog_exporter
</code></pre>

<p>Restart the service:</p>

<pre><code>$ systemctl restart nginxlog_exporter
</code></pre>

<p>Ensure that the service is running:</p>

<pre><code>$ systemctl status nginxlog_exporter

● nginxlog_exporter.service - Prometheus Log Exporter
   Loaded: loaded (/etc/systemd/system/nginxlog_exporter.service; disabled; vendor preset: enabled)
   Active: active (running) since Sat 2020-04-25 00:50:06 UTC; 5s ago
 Main PID: 4561 (prometheus-ngin)
    Tasks: 7 (limit: 2317)
   CGroup: /system.slice/nginxlog_exporter.service
           └─4561 /usr/bin/prometheus-nginxlog-exporter -config-file /etc/prometheus/nginxlog_exporter.yml

Apr 25 00:50:06 nginx-log-exporter systemd[1]: Started Prometheus Log Exporter.
Apr 25 00:50:06 nginx-log-exporter prometheus-nginxlog-exporter[4561]: loading configuration file /etc/prometheus/nginxlog_exporter.yml
Apr 25 00:50:06 nginx-log-exporter prometheus-nginxlog-exporter[4561]: using configuration {Listen:{Port:4040 Address:0.0.0.0} Consul:{Enable:false Address: Datacenter: Scheme: Toke
Apr 25 00:50:06 nginx-log-exporter prometheus-nginxlog-exporter[4561]: starting listener for namespace myapp
Apr 25 00:50:06 nginx-log-exporter prometheus-nginxlog-exporter[4561]: running HTTP server on address 0.0.0.0:4040
Apr 25 00:50:06 nginx-log-exporter prometheus-nginxlog-exporter[4561]: 2020/04/25 00:50:06 Seeked /var/log/nginx/access.log - &amp;{Offset:0 Whence:2}
</code></pre>

<h2>Test the exporter</h2>

<p>Make a couple of requests against your webserver:</p>

<pre><code>$ for each in {1..10}; do curl http://78.141.211.49 ; done
</code></pre>

<p>So prometheus will now scrape the exporter http endpoint (<code>:4040/metrics</code>) and push the returned values into prometheus. But to get a feel on how the metrics look like, make a request to the metrics endpoint:</p>

<pre><code>$ curl http://localhost:4040/metrics
...
# HELP myapp_http_response_count_total Amount of processed HTTP requests
# TYPE myapp_http_response_count_total counter
myapp_http_response_count_total{environment="production",hostname="myapp.example.com",method="GET",service="myapp",status="200"} 10
myapp_http_response_count_total{environment="production",hostname="myapp.example.com",method="POST",service="myapp",status="404"} 1
# HELP myapp_http_response_size_bytes Total amount of transferred bytes
# TYPE myapp_http_response_size_bytes counter
myapp_http_response_size_bytes{environment="production",hostname="myapp.example.com",method="GET",service="myapp",status="200"} 6120
myapp_http_response_size_bytes{environment="production",hostname="myapp.example.com",method="POST",service="myapp",status="404"} 152
# HELP myapp_parse_errors_total Total number of log file lines that could not be parsed
# TYPE myapp_parse_errors_total counter
myapp_parse_errors_total 0
...
</code></pre>

<p>As you can see we are getting metrics such as response count total, response size, errors, etc.</p>

<h2>Configure Prometheus</h2>

<p>Let&rsquo;s configure prometheus to scrape this endpoint. Head over to your prometheus instance, and edit your prometheus config:</p>

<pre><code>$ vim /etc/prometheus/prometheus.yml
</code></pre>

<p>Note that in my config I have 2 endpoints that I am scraping, the prometheus endpoint which exists and I will be adding the nginx endpoint, so in full, this is how my config will look like:</p>

<pre><code>global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'prometheus'
    scrape_interval: 5s
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'nginx'
    scrape_interval: 15s
    static_configs:
      - targets: ['ip.of.nginx.exporter:4040']
</code></pre>

<p>Restart prometheus:</p>

<pre><code>$ systemctl restart prometheus
</code></pre>

<p>To verify that the exporter is working as expected, head over to your prometheus ui on port 9090, and query <code>up{}</code> to see if your exporters are returning 1:</p>

<p><img width="1280" alt="image" src="https://user-images.githubusercontent.com/567298/80267654-7b51be00-86a2-11ea-98e2-a48a5c2a1e4f.png"></p>

<p>We can then query prometheus with <code>myapp_http_response_count_total{service="myapp"}</code> to see the response counts:</p>

<p><img width="1273" alt="image" src="https://user-images.githubusercontent.com/567298/80267823-590c7000-86a3-11ea-9098-28e37e7941d7.png"></p>

<h2>Dashboarding in Grafana</h2>

<p>If you don&rsquo;t have Grafana installed, you can look at my <a href="https://blog.ruanbekker.com/blog/2019/05/17/install-grafana-to-visualize-your-metrics-from-datasources-such-as-prometheus-on-linux/">Grafana Installation</a> post to get that up and running.</p>

<p>If you have not created the Prometheus datasource, on Grafana, head over to the configuration section on your left, select Datasources, add a Prometheus datasource and add the following (this is assuming grafana runs on the prometheus node - which is fine for testing):</p>

<p><img width="592" alt="image" src="https://user-images.githubusercontent.com/567298/80267986-48a8c500-86a4-11ea-9046-3fba601d41cf.png"></p>

<p>Create a new dashboard and add a new panel:</p>

<p><img width="605" alt="image" src="https://user-images.githubusercontent.com/567298/80267884-b3a5cc00-86a3-11ea-8624-797e5310de80.png"></p>

<p>Let&rsquo;s query our data to show us HTTP Method and Status code per 30s: <code>rate(myapp_http_response_count_total{service="myapp"}[$__interval])</code></p>

<p><img width="1271" alt="image" src="https://user-images.githubusercontent.com/567298/80269073-e607f700-86ac-11ea-8d42-4814084dfb4a.png"></p>

<h2>Thank You</h2>

<p>Hope you found this helpful, if you haven&rsquo;t seen my other posts on Prometheus, have a look at the following:</p>

<ul>
<li><a href="https://blog.ruanbekker.com/blog/2019/05/07/setup-prometheus-and-node-exporter-on-ubuntu-for-epic-monitoring/">Setup Prometheus</a></li>
<li><a href="https://blog.ruanbekker.com/blog/2019/05/17/install-grafana-to-visualize-your-metrics-from-datasources-such-as-prometheus-on-linux/">Setup Grafana</a></li>
<li><a href="https://blog.ruanbekker.com/blog/2019/05/07/setup-prometheus-and-node-exporter-on-ubuntu-for-epic-monitoring/">Setup Node Exporter</a></li>
<li><a href="https://blog.ruanbekker.com/blog/2019/05/17/install-blackbox-exporter-to-monitor-websites-with-prometheus/">Setup Blackbox Exporter</a></li>
<li><a href="https://blog.ruanbekker.com/blog/2019/05/17/install-alertmanager-to-alert-based-on-metrics-from-prometheus/">Setup Alertmanager</a></li>
<li><a href="https://blog.ruanbekker.com/blog/2019/05/17/install-pushgateway-to-expose-metrics-to-prometheus/">Setup Pushgateway</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
