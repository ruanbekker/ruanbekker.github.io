<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Python | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/python/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2018-10-23T17:26:06-04:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Splitting Query String Parameters From a URL in Python]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/10/04/splitting-query-string-parameters-from-a-url-in-python/"/>
    <updated>2018-10-04T09:58:46-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/10/04/splitting-query-string-parameters-from-a-url-in-python</id>
    <content type="html"><![CDATA[<p>I&rsquo;m working on capturing some data that I want to use for analytics, and a big part of that is capturing the query string parameters that is in the request URL.</p>

<p>So essentially I would like to break the data up into key value pairs, using Python and the urllib module, which will then pushed into a database like MongoDB or DynamoDB.</p>

<h2>Our URL:</h2>

<p>So the URL&rsquo;s that we will have, will more or less look like the following:</p>

<pre><code class="bash">https://surveys.mydomain.com/one/abc123?companyId=178231&amp;group_name=abc_12&amp;utm_source=survey&amp;utm_medium=email&amp;utm_campaign=survey-top-1
</code></pre>

<p>So we have a couple of utm parameters, company id, group name etc, which will be use for analysis</p>

<h2>Python to Capture the Parameters:</h2>

<p>Using Python, it&rsquo;s quite easy:</p>

<pre><code class="python">&gt;&gt;&gt; from urllib import parse
&gt;&gt;&gt; url = 'https://surveys.mydomain.com/one/abc123?companyId=178231&amp;group_name=abc_12&amp;utm_source=survey&amp;utm_medium=email&amp;utm_campaign=survey-top-1'

&gt;&gt;&gt; parse.urlsplit(url)
SplitResult(scheme='https', netloc='surveys.mydomain.com', path='/one/abc123', query='companyId=178231&amp;group_name=abc_12&amp;utm_source=survey&amp;utm_medium=email&amp;utm_campaign=survey-top-1', fragment='')
&gt;&gt;&gt; parse.parse_qsl(parse.urlsplit(url).query)
[('companyId', '178231'), ('group_name', 'abc_12'), ('utm_source', 'survey'), ('utm_medium', 'email'), ('utm_campaign', 'survey-top-1')]
</code></pre>

<p>Now to get our data in a dictionary, we can just convert it using the <code>dict()</code> function:</p>

<pre><code class="python">&gt;&gt;&gt; dict(parse.parse_qsl(parse.urlsplit(url).query))
{'companyId': '178231', 'group_name': 'abc_12', 'utm_source': 'survey', 'utm_medium': 'email', 'utm_campaign': 'survey-top-1'}
</code></pre>

<p>This data can then be used to write to a database, which can then be used for analysis.</p>

<h2>Resources:</h2>

<ul>
<li><a href="http://blog.rafflecopter.com/2014/04/utm-parameters-best-practices/">http://blog.rafflecopter.com/2014/04/utm-parameters-best-practices/</a></li>
<li><a href="https://stackoverflow.com/questions/21584545/url-query-parameters-to-dict-python">https://stackoverflow.com/questions/21584545/url-query-parameters-to-dict-python</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Give Your Database a Break and Use Memcached to Return Frequently Accessed Data]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/09/01/give-your-database-a-break-and-use-memcached-to-return-frequently-accessed-data/"/>
    <updated>2018-09-01T17:05:10-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/09/01/give-your-database-a-break-and-use-memcached-to-return-frequently-accessed-data</id>
    <content type="html"><![CDATA[<p><img src="https://objects.ruanbekker.com/assets/images/memcached-logo.png" alt="" /></p>

<p>So let&rsquo;s take this scenario:</p>

<p>Your database is getting hammered with requests and building up some load over time and we would like to place a caching layer in front of our database that will return data from the caching layer, to reduce some traffic to our database and also improve our performance for our application.</p>

<h2>The Scenario:</h2>

<p>Our scenario will be very simple for this demonstration:</p>

<ul>
<li>Database will be using SQLite with product information (product_name, product_description)</li>
<li>Caching Layer will be Memcached</li>
<li>Our Client will be written in Python, which checks if the product name is in cache, if not a <code>GET_MISS</code> will be returned, then the data will be fetched from the database, returns it to the client and save it to the cache</li>
<li>Next time the item will be read, a <code>GET_HIT</code> will be received, then the item will be delivered to the client directly from the cache</li>
</ul>


<h2>SQL Database:</h2>

<p>As mentioned we will be using sqlite for demonstration.</p>

<p>Create the table, populate some very basic data:</p>

<pre><code class="sql">$ sqlite3 db.sql -header -column
import sqlite3 as sql
SQLite version 3.16.0 2016-11-04 19:09:39
Enter ".help" for usage hints.

sqlite&gt; create table products (product_name STRING(32), product_description STRING(32));
sqlite&gt; insert into products values('apple', 'fruit called apple');
sqlite&gt; insert into products values('guitar', 'musical instrument');
</code></pre>

<p>Read all the data from the table:</p>

<pre><code class="sql">sqlite&gt; select * from products;
product_name  product_description
------------  -------------------
apple         fruit called apple
guitar        musical instrument
sqlite&gt; .exit
</code></pre>

<h2>Run a Memcached Container:</h2>

<p>We will use docker to run a memcached container on our workstation:</p>

<pre><code class="bash">$ docker run -itd --name memcached -p 11211:11211 rbekker87/memcached:alpine
</code></pre>

<h2>Our Application Code:</h2>

<p>I will use <a href="https://pymemcache.readthedocs.io/en/latest/getting_started.html">pymemcache</a> as our client library. Install:</p>

<pre><code class="bash">$ virtualenv .venv &amp;&amp; source .venv/bin/activate
$ pip install pymemcache
</code></pre>

<p>Our Application Code which will be in Python</p>

<pre><code class="python">import sqlite3 as sql
from pymemcache.client import base

product_name = 'guitar'

client = base.Client(('localhost', 11211))
result = client.get(product_name)

def query_db(product_name):
    db_connection = sql.connect('db.sql')
    c = db_connection.cursor()
    try:
        c.execute('select product_description from products where product_name = "{k}"'.format(k=product_name))
        data = c.fetchone()[0]
        db_connection.close()
    except:
        data = 'invalid'
    return data

if result is None:
    print("got a miss, need to get the data from db")
    result = query_db(product_name)
    if result == 'invalid':
        print("requested data does not exist in db")
    else:
        print("returning data to client from db")
        print("=&gt; Product: {p}, Description: {d}".format(p=product_name, d=result))
        print("setting the data to memcache")
        client.set(product_name, result)

else:
    print("got the data directly from memcache")
    print("=&gt; Product: {p}, Description: {d}".format(p=product_name, d=result))
</code></pre>

<p>Explanation:</p>

<ul>
<li>We have a function that takes a argument of the product name, that makes the call to the database and returns the description of that product</li>
<li>We will make a get operation to memcached, if nothing is returned, then we know the item does not exists in our cache,</li>
<li>Then we will call our function to get the data from the database and return it directly to our client, and</li>
<li>Save it to the cache in memcached so the next time the same product is queried, it will be delivered directly from the cache</li>
</ul>


<h2>The Demo:</h2>

<p>Our Product Name is <code>guitar</code>, lets call the product, which will be the first time so memcached wont have the item in its cache:</p>

<pre><code class="bash">$ python app.py
got a miss, need to get the data from db
returning data to client from db
=&gt; Product: guitar, Description: musical instrument
setting the data to memcache
</code></pre>

<p>Now from the output, we can see that the item was delivered from the database and saved to the cache, lets call that same product and observe the behavior:</p>

<pre><code class="bash">$ python app.py
got the data directly from memcache
=&gt; Product: guitar, Description: musical instrument
</code></pre>

<p>When our cache instance gets rebooted we will lose our data that is in the cache, but since the source of truth will be in our database, data will be re-added to the cache as they are requested. That is one good reason not to rely on a cache service to be your primary data source.</p>

<p>What if the product we request is not in our cache or database, let&rsquo;s say the product <code>tree</code></p>

<pre><code class="bash">$ python app.py
got a miss, need to get the data from db
requested data does not exist in db
</code></pre>

<p>This was a really simple scenario, but when working with masses amount of data, you can benefit from a lot of performance using caching.</p>

<h2>Resources:</h2>

<ul>
<li><a href="https://realpython.com/python-memcache-efficient-caching/">https://realpython.com/python-memcache-efficient-caching/</a></li>
<li><a href="https://github.com/ruanbekker/dockerhub-sources/tree/master/memcached/alpine">https://github.com/ruanbekker/dockerhub-sources/tree/master/memcached/alpine</a></li>
<li><a href="https://pymemcache.readthedocs.io/en/latest/getting_started.html#basic-usage">https://pymemcache.readthedocs.io/en/latest/getting_started.html#basic-usage</a></li>
<li><a href="https://sebastianraschka.com/Articles/2014_sqlite_in_python_tutorial.html">https://sebastianraschka.com/Articles/2014_sqlite_in_python_tutorial.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Improving Performance From Your Lambda Function From the Use of Global Variables]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/08/27/improving-performance-from-your-lambda-function-from-the-use-of-global-variables/"/>
    <updated>2018-08-27T08:30:30-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/08/27/improving-performance-from-your-lambda-function-from-the-use-of-global-variables</id>
    <content type="html"><![CDATA[<p><img src="https://objects.ruanbekker.com/assets/images/aws-logo.png" alt="" /></p>

<p>When using Lambda and DynamoDB, you can use global variables to gain performance when your data from DynamoDB does not get updated that often, and you would like to use caching to prevent a API call to DynamoDB everytime your Lambda Function gets invoked.</p>

<p>You can use external services like Redis or Memcached when you would like to verify that each invocation is as true as your source of truth which will be DynamoDB. Then your application logic can work with caching.</p>

<p>But in this case we just want a simple piece of code that can keep the state for the remaining time that the function is running on that underlying container. I am not 100% sure, but I have seen that the data can be cached for up to 60 minutes. This can be a total mess when your data gets updated regularly, then I would set all my calls in functions, as the global variables keeps their state for some time.</p>

<h2>Example Function:</h2>

<p>This function gets data from DynamoDB, iterates through a small dataset (10 Items), and appends each group name to my list which is the value of my <code>groups</code> key inside my dictionary.</p>

<p>Due to my global variable <code>mydata</code>, you will see that the first invocation will result in a API call to DynamoDB as the length of my <code>mydata["groups"]</code> being 0, the second invocation, the data will exist inside my global variable, therefore I am returning the data directly from my variable.</p>

<pre><code class="python">import boto3, json

client = boto3.resource('dynamodb', region_name='eu-west-1')
tbl = client.Table('my-dynamo-table')

mydata = {}
mydata["groups"] = []

def lambda_handler(event, context):
    if len(mydata["groups"]) == 0:
        # data is not cached, make call to dynamo
        data = tbl.scan()
        group_data = data['Items']

        for group in group_data:
            mydata["groups"].append(group['name'])
        return mydata

    else:
        # return cached content
        return mydata
</code></pre>

<h2>Results of my Invocations:</h2>

<p>The first call that I made:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/lambda-caching-miss.png" alt="" /></p>

<p>The second call that I made:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/lambda-caching-hit.png" alt="" /></p>

<p>If you need a small layer of caching that can improve your latency, this can be used. But if you need your data to be accurate from every call, rather looking into a different approach and external caching services.</p>

<h2>Resources:</h2>

<p><em>Take advantage of Execution Context reuse to improve the performance of your function.</em>:</p>

<p>&ldquo;Make sure any externalized configuration or dependencies that your code retrieves are stored and referenced locally after initial execution. Limit the re-initialization of variables/objects on every invocation. Instead use static initialization/constructor, global/static variables and singletons. Keep alive and reuse connections (HTTP, database, etc.) that were established during a previous invocation.&rdquo;</p>

<ul>
<li><a href="https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html">https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html</a></li>
<li><a href="https://aws.amazon.com/blogs/compute/container-reuse-in-lambda/">https://aws.amazon.com/blogs/compute/container-reuse-in-lambda/</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Send Emails Using Python and Sendgrid Using SMTPlib]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/08/21/send-emails-using-python-and-sendgrid-using-smtplib/"/>
    <updated>2018-08-21T11:30:08-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/08/21/send-emails-using-python-and-sendgrid-using-smtplib</id>
    <content type="html"><![CDATA[<p><img src="https://objects.ruanbekker.com/assets/images/sendgrid-logo.png" alt="" /></p>

<p>Quick tutorial on how to send emails using Python and smtplib.</p>

<h2>Sendgrid</h2>

<p>Sendgrid offers 100 free outbound emails per day, sign up with them via <a href="https://sendgrid.com/free/">sendgrid.com/free</a>, create a API Key and save your credentials in a safe place.</p>

<p>You first need to verify your account by sending a mail using their API, but it&rsquo;s step by step so won&rsquo;t take more than 2 minutes to complete.</p>

<h2>Python Code</h2>

<p>Once the verification is completed, our Python Code:</p>

<pre><code class="python">import smtplib
from email.MIMEMultipart import MIMEMultipart
from email.MIMEText import MIMEText

mail_from = 'Ruan Bekker &lt;ruan@ruanbekker.com&gt;'
mail_to = 'Ruan Bekker &lt;xxxx@gmail.com&gt;'

msg = MIMEMultipart()
msg['From'] = mail_from
msg['To'] = mail_to
msg['Subject'] = 'Sending mails with Python'
mail_body = """
Hey,

This is a test.

Regards,\nRuan

"""
msg.attach(MIMEText(mail_body))

try:
    server = smtplib.SMTP_SSL('smtp.sendgrid.net', 465)
    server.ehlo()
    server.login('apikey', 'your-api-key')
    server.sendmail(mail_from, mail_to, msg.as_string())
    server.close()
    print("mail sent")
except:
    print("issue")
</code></pre>

<p>When I ran the code, I received the mail, and when you inspect the headers you can see that the mail came via sendgrid:</p>

<pre><code>Received: from xx.xx.s2shared.sendgrid.net (xx.xx.s2shared.sendgrid.net. [xx.xx.xx.xx])
</code></pre>

<h2>Resources:</h2>

<p>Great post on SSL / TLS:
- <a href="https://stackabuse.com/how-to-send-emails-with-gmail-using-python/">https://stackabuse.com/how-to-send-emails-with-gmail-using-python/</a></p>

<p>Enjoy :D</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using IAM Authentication With Amazon Elasticsearch Service]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/08/20/using-iam-authentication-with-amazon-elasticsearch-service/"/>
    <updated>2018-08-20T04:12:21-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/08/20/using-iam-authentication-with-amazon-elasticsearch-service</id>
    <content type="html"><![CDATA[<p><img src="https://objects.ruanbekker.com/assets/images/aws-logo.png" alt="" /></p>

<p>Today I will demonstrate how to allow access to Amazons Elasticsearch Service using IAM Authenticationi using AWS Signature Version4.</p>

<h2>Elasticsearch Service Authentication Support:</h2>

<p>When it comes to security, Amazons Elasticsearch Service supports three types of access policies:</p>

<ul>
<li>Resource Based</li>
<li>Identity Based</li>
<li>IP Access Based</li>
</ul>


<p>More information on this can be found below:
- <a href="https://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/es-ac.html">https://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/es-ac.html</a></p>

<h2>Securing your Amazon Elasticsearch Search Domain:</h2>

<p>To secure your domain with IAM Based Authentication, the following steps will be neeed:</p>

<ul>
<li>Create IAM Policy to be associated with a IAM User or Role</li>
<li>On Elasticsearch Access Policy, associate the ARN to the Resource</li>
<li>Use the AWS4Auth package to sign the requests as AWS supports Signature Version 4</li>
</ul>


<pre><code class="json">{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "es:*"
            ],
            "Resource": "arn:aws:es:eu-west-1:&lt;ACCOUNT-ID&gt;:domain/&lt;ES-DOMAIN&gt;"
        }
    ]
}
</code></pre>

<p>Create the IAM Role with EC2 Identity Provider as a Trusted Relationship eg. es-role and associate the IAM Policy es-policy to the role.</p>

<p>Create/Moodify the Elasticsearch Access Policy, in this example we will be using a combination of IAM Role, IAM User and IP Based access:</p>

<ul>
<li>IAM Role for EC2 Role Based Services</li>
<li>IAM User for User/System Account</li>
<li>IP Based for cients that needs to be whitelisted via IP (ip-based just for demonstration, as the tests will be used only for IAM)</li>
</ul>


<pre><code class="json">{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "arn:aws:iam::&lt;ACCOUNT-ID&gt;:role/&lt;IAM-ROLE-NAME&gt;",
          "arn:aws:iam::&lt;ACCOUNT-ID&gt;:user/&lt;IAM-USER-NAME&gt;"
        ]
      },
      "Action": "es:*",
      "Resource": "arn:aws:es:eu-west-1:&lt;ACCOUNT-ID&gt;:domain/&lt;ES-DOMAIN&gt;/*"
    },
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "*"
      },
      "Action": "es:*",
      "Resource": "arn:aws:es:eu-west-1:&lt;ACCOUNT-ID&gt;:domain/&lt;ES-DOMAIN&gt;/*",
      "Condition": {
        "IpAddress": {
          "aws:SourceIp": [
            "x.x.x.x",
            "x.x.x.x"
          ]
        }
      }
    }
  ]
}
</code></pre>

<p>After the Access Policy has been updated, the Elasticsearch Domain Status will show <code>Active</code></p>

<h2>Testing from EC2 using IAM Instance Profile:</h2>

<p>Launch a EC2 Instance with the IAM Role eg. es-role, then using Python, we will make a request to our Elasticsearch Domain using boto3, aws4auth and the native elasticsearch client for python via our IAM Role, which we will get the temporary credentials from boto3.Session.</p>

<p>Installing the dependencies:</p>

<pre><code class="bash">$ pip install virtualenv
$ virtualenv .venv
$ source .venv/bin/activate
$ pip install boto3 elasticsearch requests_aws4auth
</code></pre>

<p>Our code:</p>

<pre><code class="python">import boto3, json
from elasticsearch import Elasticsearch, RequestsHttpConnection
from requests_aws4auth import AWS4Auth

my_region = 'eu-west-1'
my_service = 'es'
my_eshost = 'search-replaceme.eu-west-1.es.amazonaws.com'

session = boto3.Session(region_name=my_region) # thanks Leon
credentials = session.get_credentials()
credentials = credentials.get_frozen_credentials()
access_key = credentials.access_key
secret_key = credentials.secret_key
token = credentials.token

aws_auth = AWS4Auth(
    access_key,
    secret_key,
    my_region,
    my_service,
    session_token=token
)

es = Elasticsearch(
    hosts = [{'host': my_eshost, 'port': 443}],
    http_auth=aws_auth,
    use_ssl=True,
    verify_certs=True,
    connection_class=RequestsHttpConnection
)

print(json.dumps(es.info(), indent=2))
</code></pre>

<p>Running our piece of code, will result in this:</p>

<pre><code class="bash">$ python get-info-from-role.py
{
  "cluster_name": "&lt;ACCOUNT-ID&gt;:&lt;ES-DOMAIN&gt;",
  "cluster_uuid": "sLUnqFSsQdCMlBLrn7BTUA",
  "version": {
    "lucene_version": "6.6.0",
    "build_hash": "Unknown",
    "build_snapshot": false,
    "number": "5.5.2",
    "build_date": "2017-10-18T04:35:01.381Z"
  },
  "name": "KXSwBvT",
  "tagline": "You Know, for Search"
}
</code></pre>

<h2>Testing using IAM Credentials from Credentials Provider:</h2>

<p>Configure your credentials provider:</p>

<pre><code class="bash">$ pip install awscli
$ aws configure --profile ruan
AWS Access Key ID [None]: xxxxxxxxx
AWS Secret Access Key [None]: xxxxxx
Default region name [None]: eu-west-1
Default output format [None]: json
</code></pre>

<p>Using Python, we will get the credentials from the Credential Provider, using our profile name:</p>

<pre><code class="python">import boto3, json
from elasticsearch import Elasticsearch, RequestsHttpConnection
from requests_aws4auth import AWS4Auth

my_service = 'es'
my_region = 'eu-west-1'
my_eshost = 'search-replaceme.eu-west-1.es.amazonaws.com'

session = boto3.Session(
    region_name=my_region,
    profile_name='ruan'
)

credentials = session.get_credentials()
access_key = credentials.access_key
secret_key = credentials.secret_key

aws_auth = AWS4Auth(
    access_key,
    secret_key,
    my_region,
    my_service
)

es = Elasticsearch(
    hosts = [{'host': my_eshost, 'port': 443}],
    http_auth=aws_auth,
    use_ssl=True,
    verify_certs=True,
    connection_class=RequestsHttpConnection
)

print(json.dumps(es.info(), indent=2))
</code></pre>

<p>Running it will result in:</p>

<pre><code class="bash">$ python get-info-from-user.py
{
  "cluster_name": "&lt;ACCOUNT-ID&gt;:&lt;ES-DOMAIN&gt;",
  "cluster_uuid": "sLUnqFSsQdCMlBLrn7BTUA",
  "version": {
    "lucene_version": "6.6.0",
    "build_hash": "Unknown",
    "build_snapshot": false,
    "number": "5.5.2",
    "build_date": "2017-10-18T04:37:21.381Z"
  },
  "name": "KXSwBvT",
  "tagline": "You Know, for Search"
}
</code></pre>

<p>For more blog posts on Elasticsearch have a look at:
- <a href="http://blog.ruanbekker.com/blog/categories/elasticsearch">blog.ruanbekker.com:elasticsearch</a>
- <a href="https://sysadmins.co.za/tags/elasticsearch">sysadmins.co.za:elasticsearch</a></p>
]]></content>
  </entry>
  
</feed>
