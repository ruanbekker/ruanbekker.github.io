<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Python | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/python/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2018-11-24T17:28:29-05:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[IAM Policy to Allow Team Wide and User Level Permissions on AWS Secrets Manager]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/11/12/iam-policy-to-allow-team-wide-and-user-level-permissions-on-aws-secrets-manager/"/>
    <updated>2018-11-12T16:32:24-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/11/12/iam-policy-to-allow-team-wide-and-user-level-permissions-on-aws-secrets-manager</id>
    <content type="html"><![CDATA[<p>In this post we will simulate a scenario where a team would like to have access to create secrets under a team path name like <code>/security-team/prod/*</code> and <code>/security-team/dev/*</code> and allow all the users from that team to be able to write and read secrets from that path. Then have individual users create and read secrets from their own isolated path: <code>/security-team/personal/aws-username/*</code> so they can create their personal secrets.</p>

<script id="mNCC" language="javascript">
    medianet_width = "728";
    medianet_height = "90";
    medianet_crid = "218284798";
    medianet_versionId = "3111299";
  </script>


<script src="//contextual.media.net/nmedianet.js?cid=8CUD78FSV"></script>


<h2>Our Scenario:</h2>

<ul>
<li>Create IAM Policy</li>
<li>Create 2 IAM Users: <code>jack.smith</code> and <code>steve.adams</code></li>
<li>Create IAM Group, Associate IAM Policy to the Group</li>
<li>Attach 2 Users to the Group</li>
</ul>


<p>The IAM Policy:</p>

<pre><code>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "Stmt1541597166491",
            "Action": [
                "secretsmanager:CreateSecret",
                "secretsmanager:DeleteSecret",
                "secretsmanager:DescribeSecret",
                "secretsmanager:GetRandomPassword",
                "secretsmanager:GetSecretValue",
                "secretsmanager:ListSecretVersionIds",
                "secretsmanager:ListSecrets",
                "secretsmanager:PutSecretValue",
                "secretsmanager:TagResource",
                "secretsmanager:UpdateSecret"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:secretsmanager:eu-west-1:123456789012:secret:/security-team/prod/*",
                "arn:aws:secretsmanager:eu-west-1:123456789012:secret:/security-team/dev/*",
                "arn:aws:secretsmanager:eu-west-1:123456789012:secret:/security-team/personal/${aws:username}/*"
            ]
        }
    ]
}
</code></pre>

<p>Either configure the access keys and secret keys into the credential provider using aws cli, or for this demonstration I will use them inside the code. But never hardcode your credentials.</p>

<h2>Create Secrets with Secrets Manager in AWS using Python Boto3</h2>

<p>Instantiate user1 and user2:</p>

<pre><code class="python">&gt;&gt;&gt; import boto3
&gt;&gt;&gt; jack = boto3.Session(aws_access_key_id='ya', aws_secret_access_key='xx', region_name='eu-west-1').client('secretsmanager')
&gt;&gt;&gt; steve = boto3.Session(aws_access_key_id='yb', aws_secret_access_key='xx', region_name='eu-west-1').client('secretsmanager')
</code></pre>

<p>Create a team wide secret with jack:</p>

<pre><code class="python">&gt;&gt;&gt; jack.create_secret(Name='/security-team/prod/app1/username', SecretString='appreader')
{'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': 'x', 'HTTPHeaders': {'date': 'Thu, 08 Nov 2018 07:50:35 GMT', 'x-amzn-requestid': 'x', 'content-length': '193', 'content-type': 'application/x-amz-json-1.1', 'connection': 'keep-alive'}}, u'VersionId': u'x', u'Name': u'/security-team/prod/app1/username', u'ARN': u'arn:aws:secretsmanager:eu-west-1:123456789012:secret:/security-team/prod/app1/username-12ABC00'}
</code></pre>

<p>Let jack and steve try to read the secret:</p>

<pre><code class="python">&gt;&gt;&gt; jack.get_secret_value(SecretId='/security-team/prod/app1/username')['SecretString']
'appreader'
&gt;&gt;&gt; steve.get_secret_value(SecretId='/security-team/prod/app1/username')['SecretString']
'appreader'
</code></pre>

<p>Now let jack create a personal secret, let him read it:</p>

<pre><code class="python">&gt;&gt;&gt; jack.create_secret(Name='/security-team/personal/jack.smith/svc1/password', SecretString='secret')
&gt;&gt;&gt; jack.get_secret_value(SecretId='/security-team/personal/jack.smith/svc1/password')['SecretString']
'secret'
</code></pre>

<p>Now let steve try to read the secret and you will see that access is denied:</p>

<pre><code class="python">&gt;&gt;&gt; steve.get_secret_value(SecretId='/security-team/personal/jack.smith/username')['SecretString']
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
...
    raise error_class(parsed_response, operation_name)
botocore.exceptions.ClientError: An error occurred (AccessDeniedException) when calling the GetSecretValue operation: User: arn:aws:iam::123456789012:user/steve.adams is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:eu-west-1:123456789012:secret:/security-team/personal/jack.smith/svc1/password-a1234b
</code></pre>

<p>Thats it for this post</p>

<script type="text/javascript">
  ( function() {
    if (window.CHITIKA === undefined) { window.CHITIKA = { 'units' : [] }; };
    var unit = {"calltype":"async[2]","publisher":"rbekker87","width":728,"height":90,"sid":"Chitika Default"};
    var placement_id = window.CHITIKA.units.length;
    window.CHITIKA.units.push(unit);
    document.write('<div id="chitikaAdBlock-' + placement_id + '"></div>');
}());
</script>


<script type="text/javascript" src="//cdn.chitika.net/getads.js" async></script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Query 24 Hours Worth of Data Using BatchGet on Amazon DynamoDB Using Scan and Filter Without a GSI]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/10/30/query-24-hours-worth-of-data-using-batchget-on-amazon-dynamodb-using-scan-and-filter-without-a-gsi/"/>
    <updated>2018-10-30T14:53:43-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/10/30/query-24-hours-worth-of-data-using-batchget-on-amazon-dynamodb-using-scan-and-filter-without-a-gsi</id>
    <content type="html"><![CDATA[<p>I&rsquo;m testing how to query data in DynamoDB which will always be the retrieval of yesterdays data, without using a Global Secondary Index.</p>

<p>This is done just to see what other ways you can use to query data based on a specific timeframe.</p>

<script id="mNCC" language="javascript">
    medianet_width = "728";
    medianet_height = "90";
    medianet_crid = "218284798";
    medianet_versionId = "3111299"; 
  </script>


<script src="//contextual.media.net/nmedianet.js?cid=8CUD78FSV"></script>


<h2>Use-Case:</h2>

<p>Data from DynamoDB needs to be batch processed (daily for the last 24-hours), into a external datasource. Data will be written into DynamoDB, the HK (uuid) and RK (timestamp) will be duplicated to the daily table. But only uuid and timestamp will be duplicated to the daily table, and only data for that day will be written into that datestamp formatted table name.</p>

<p>Let&rsquo;s say data for 2018-10-30 needs to be written into our external data source, we will do a scan on table <code>tbl-test_20181030</code>, then from our response we will have a list of HashKeys (uuid) which we will use to do a BatchGet Item on our base table: <code>tbl-test_base</code>, which essentially grabs all the data for that day.</p>

<p>If deeper filtering needs to be done on that day, the FilterExpression can be used to do a deeper filtering which leads to grabbing only the filtered down data from the base table.</p>

<p><em>Note:</em> The base table might have millions of items, so a Scan operation on the Base table would be really expensive, as it reads all the items in the table.</p>

<p>Once the data has been processed, the daily or metadata table can be removed.</p>

<h2>DynamoDB Table Design</h2>

<p>The base table: <code>tbl-test_base</code> will have:</p>

<ul>
<li>HashKey: uuid (string)</li>
<li>RangeKey: timestamp (number)</li>
<li>Attributes: city, stream, transaction_date, name, metric_uri</li>
<li>Item will look like:</li>
</ul>


<pre><code class="python">{
  u'uuid': u'fb4ddeb9-3b5e-47b3-bbab-1aa1d8e8f47b', 
  u'timestamp': 1540891276, 
  u'city': u'sydney', 
  u'stream': u'NONE', 
  u'transaction_date': u'2018-10-30 11:21:16', 
  u'metric_uri': u'some-dummy-metric-uri', 
  u'name': u'frank'
}
</code></pre>

<p>he Daily Table: <code>tbl-test_20181030</code> will look like:</p>

<ul>
<li>HashKey: <code>uuid</code></li>
<li>Attributes: <code>timestamp</code></li>
<li>Item will look like:</li>
</ul>


<pre><code class="python">{
  u'uuid': u'fb4ddeb9-3b5e-47b3-bbab-1aa1d8e8f47b', 
  u'timestamp': 1540891276
}
</code></pre>

<h2>Demonstration using Python</h2>

<p>Creating the Metadata table:</p>

<pre><code class="python">import boto3, time, uuid, random

session = boto3.Session(region_name='eu-west-1', profile_name='dev')
resource = session.resource('dynamodb')
client = session.client('dynamodb')

def create_table():
    table_name = "tbl-test_{0}".format(time.strftime("%Y%m%d"))
    response = resource.create_table(
        TableName=table_name,
        KeySchema=[{
            'AttributeName': 'uuid',
            'KeyType': 'HASH'
        }],
        AttributeDefinitions=[{
            'AttributeName': 'uuid',
            'AttributeType': 'S'
        }],
        ProvisionedThroughput={
            'ReadCapacityUnits': 1,
            'WriteCapacityUnits': 1
        }
    )

    resource.Table(table_name).wait_until_exists()

    arn = client.describe_table(TableName=table_name)['Table']['TableArn']
    client.tag_resource(
        ResourceArn=arn,
        Tags=[
            {'Key': 'Name','Value': 'dynamo_table'},
            {'Key': 'Environment','Value': 'Dev'},
            {'Key': 'CreatedBy','Value': 'Ruan'}
        ]
    )

    return resource.Table(table_name).table_status

print(create_table())
</code></pre>

<p>Write 400 Items to DynamoDB:</p>

<pre><code class="python">import boto3, time, uuid, random

session = boto3.Session(region_name='eu-west-1', profile_name='dev')
resource = session.resource('dynamodb')
client = session.client('dynamodb')

base_table = 'tbl-test_base'
meta_table = 'tbl-test_{0}'.format(time.strftime("%Y%m%d"))

people = ['james', 'john', 'frank', 'paul', 'nathan', 'kevin']
cities = ['ireland', 'cape town', 'pretoria', 'paris', 'amsterdam', 'auckland', 'sydney']

def write_dynamo(uuid, timestamp):
    resource.Table(base_table).put_item(
        Item={
            'uuid': uuid, 
            'timestamp': timestamp, 
            'metric_uri': 'some-dummy-metric-uri', 
            'transaction_date': time.strftime("%Y-%m-%d %H:%M:%S"), 
            'name': random.choice(people), 
            'stream': 'NONE', 
            'city': random.choice(cities)
        }
    )

    resource.Table(meta_table).put_item(
        Item={
            'uuid': uuid, 
            'timestamp': timestamp
        }
    )

    return 'Written'

for x in xrange(400):
    time.sleep(1)
    write_dynamo(str(uuid.uuid4()), int(time.time()))
    print(x)
</code></pre>

<p>Getting Data for 20181030 but also filter data greater than the timestamp attribute, greater than <code>1540841144</code> in epoch time (which will give us about 254 items).</p>

<p>The BatchGet Item supports up to 100 items per call, we will limit the scans on 100 items per call, then paginate using the ExlusiveStartKey with the value of our LastEvaluatedKey that we will get from our response:</p>

<pre><code class="python">import boto3,time
from boto3.dynamodb.conditions import Key

base_table = 'tbl-test_base'
meta_table = 'tbl-test_20181030'

session = boto3.Session(region_name='eu-west-1', profile_name='dev')
resource = session.resource('dynamodb')
table = resource.Table(meta_table)
filtering_expression = Key('timestamp').gt(1540841144)

response = table.scan(FilterExpression=filtering_expression, Limit=100)

finished=False
while finished != True:
    if 'LastEvaluatedKey' in response.keys():
        print("Getting {} Items".format(response['Count']))
        items = resource.batch_get_item(RequestItems={base_table: {'Keys': response['Items']}})
        print(items['Responses'][base_table])
        time.sleep(2)
        response = table.scan(FilterExpression=filtering_expression, Limit=100, ExclusiveStartKey=response['LastEvaluatedKey'])
    else:
        print("Getting {} Items".format(response['Count']))
        items = resource.batch_get_item(RequestItems={base_table: {'Keys': response['Items']}})
        print(items['Responses'][base_table])
        finished=True
</code></pre>

<p>Running it:</p>

<pre><code class="bash">$ python dynamodb-batch-get.py
Getting 100 Items
[{u'city': u'pretoria', u'uuid': u'e8bc0d1c-2b57-4de2-b0e1-35ef1fe0edf1', u'stream': u'NONE', u'timestamp': Decimal('1540846990'), u'transaction_date': u'2018-10-29 23:03:10', u'metric_uri': u'some-dummy-metric-uri', u'name': u'frank'}, {u'city': u'amsterdam', u'uuid':
...
Getting 100 Items
[{u'city': u'sydney', u'uuid': u'5bc51ce9-2809-46c9-a3f2-ff8180086d92', u'stream': u'NONE', u'timestamp': Decimal('1540848599'), u'transaction_date': u'2018-10-29 23:29:59', u'metric_uri': u'some-dummy-metric-uri', u'name': u'frank'}
...
Getting 54 Items
[{u'city': u'cape town', u'uuid': u'5e069f34-0e97-4a49-9ca9-da2213edb689'...
</code></pre>

<p>Verifying that each call only scans 100 at a time:</p>

<pre><code class="python">&gt;&gt;&gt; response = table.scan(FilterExpression=filtering_expression, Limit=100)
&gt;&gt;&gt; response.keys()
[u'Count', u'Items', u'LastEvaluatedKey', u'ScannedCount', 'ResponseMetadata']
&gt;&gt;&gt; response.get('LastEvaluatedKey')
{u'uuid': u'e8c52a55-ca9e-4718-83d2-1b44a90f43e6'}
&gt;&gt;&gt; response.get('Count')
100
&gt;&gt;&gt; response.get('ScannedCount')
100
</code></pre>

<h2>Other Thoughts:</h2>

<p>Querying data is a lot easier using a Global Secondary Index where you could similarly have the metric_uri as the HashKey and transaction_date as the RangeKey:</p>

<pre><code class="python">&gt;&gt;&gt; response = table.query(
    IndexName='metric_uri-transaction_date-index', 
    KeyConditionExpression=Key('metric_uri').eq('some-dummy-metric-uri') &amp; Key('transaction_date').begins_with('2018-10-30')
)
&gt;&gt;&gt; response['Count']
400
</code></pre>

<p>Also note that depending on how you setup your GSI, in most cases its a exact duplicate in storage from your base table, so could potentially be double the costs.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Python Flask and JavaScript for Client Side Filtering Through Returned Data]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/10/24/using-python-flask-and-javascript-for-client-side-filtering-through-returned-data/"/>
    <updated>2018-10-24T05:39:33-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/10/24/using-python-flask-and-javascript-for-client-side-filtering-through-returned-data</id>
    <content type="html"><![CDATA[<p><img src="https://objects.ruanbekker.com/assets/images/python-logo.png" alt="" /></p>

<p>This post will cover 2 sections, using Python Flask and Javascript to filter returned data, where you could have a table that represents 100 items, and you want to have a search box to filter down your results as you type.</p>

<p>The other section will be used as a demo, with solving a problem with Amazon CloudWatch Logs. I&rsquo;m a Massive AWS Fanatic, but when it comes to CloudWatch Logs, I&rsquo;m not so big of a fan of that specific service. Especially when you use Docker Swarm for AWS and have your logdriver set to CloudWatch Logs.</p>

<script id="mNCC" language="javascript">
    medianet_width = "728";
    medianet_height = "90";
    medianet_crid = "218284798";
    medianet_versionId = "3111299"; 
  </script>


<script src="//contextual.media.net/nmedianet.js?cid=8CUD78FSV"></script>


<h2>The Problem I have with CloudWatch Logs</h2>

<p>When you point to your CloudWatch LogGroups, you can search for your streams, and in my case searching for a specific swarm service, but you can&rsquo;t sort by date, like this:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/cloudwatch-logs-date-issue.png" alt="" /></p>

<p>This makes it really tedious when trying to search find your logs in a quick way.</p>

<h2>Python Flask to the Resque</h2>

<p>We will create a Python Flask application that retrieves your data about all your Docker Swarm Services and Container Id&rsquo;s running on each node. For this demonstration, I have hard coded the services and container id&rsquo;s, but using it in a real environment, you can utilise the Docker API or some logic that retrieves it from a datastore where a process populates it to.</p>

<p>The Application Code will do the following:</p>

<ul>
<li>returns a list of your swarm services (mock data in the code)</li>
<li>when you select a service, it will get a list of the container ids and run through a for loop unsing jinja templates and display them in table format</li>
<li>when you select the containerId, it will populate the containerId to the cloudwatch logs filter, giving you the exact logstream which you are looking for</li>
<li><p>this will do a redirect to the AWS Console, and you will see the data in the sorted time of interest</p></li>
<li><p><code>app.py</code></p></li>
</ul>


<pre><code class="python">from flask import Flask, render_template

app = Flask(__name__)

# faking datasets that can be returned from a api or database
swarm_services = ['my-web-service', 'my-api-service']
swarm_tasks = {
    "my-web-service": {
        "container_names": [
            "my-web-service.1.alfjshoehfosfn",
            "my-web-service.2.fuebchduehakjdu"
        ]
    },
    "my-api-service": {
        "container_names": [
            "my-api-service.1.oprudhyuythvbzx",
            "my-api-service.2.sjduebansifotuf"
        ]
    }
}

def get_container_name(app_name):
    data = []
    response = swarm_tasks[app_name]
    for container in response['container_names']:
        data.append(container)
    return render_template('index.html', app_name=app_name, number=len(data), data=data)

@app.route('/')
def list():
    return render_template('list.html', number=len(swarm_services), apps=swarm_services, aws_region='eu-west-1', cloudwatch_log_stream='docker-swarm-lg')

@app.route('/describe/&lt;string:app_name&gt;')
def get_app(app_name):
    app = get_container_name(app_name)
    return app

if __name__ == '__main__':
    app.run()
</code></pre>

<p>The <code>index.html</code>:</p>

<script src="https://gist.github.com/ruanbekker/08b02a3ef30367ea7306a31eb5f33cb1.js"></script>


<p>The <code>list.html</code> :</p>

<script src="https://gist.github.com/ruanbekker/98eab090e218bbbf0e46d5efc1595e04.js"></script>


<h2>Filtering the Data</h2>

<p>So at this moment all your data will be returned when a list is done, if you are in a case where you have lots of information, it can be overwelming and you will need to search for the service of interest. Using HTML and JavaScript, you can filter through the results:</p>

<p>The JavaScript Function: <code>assets/js/filter.js</code></p>

<pre><code class="javascript">function SearchAndFilterThingy() {
  var input, filter, table, tr, td, x;
  input = document.getElementById("UserInput");
  filter = input.value.toUpperCase();
  table = document.getElementById("ServicesTable");
  tr = table.getElementsByTagName("tr");

  for (x = 0; x &lt; tr.length; x++) {
    td = tr[x].getElementsByTagName("td")[0];
    if (td) {
      if (td.innerHTML.toUpperCase().indexOf(filter) &gt; -1) {
        tr[x].style.display = "";
      }
      else {
        tr[x].style.display = "none";
      }
    }
  }
}
</code></pre>

<h2>Screenshot</h2>

<p>Once you search for a specific keyword on the service you are looking for the output should more or less look like the following:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/docker-flask-running-services.png" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Splitting Query String Parameters From a URL in Python]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/10/04/splitting-query-string-parameters-from-a-url-in-python/"/>
    <updated>2018-10-04T09:58:46-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/10/04/splitting-query-string-parameters-from-a-url-in-python</id>
    <content type="html"><![CDATA[<p>I&rsquo;m working on capturing some data that I want to use for analytics, and a big part of that is capturing the query string parameters that is in the request URL.</p>

<p>So essentially I would like to break the data up into key value pairs, using Python and the urllib module, which will then pushed into a database like MongoDB or DynamoDB.</p>

<h2>Our URL:</h2>

<p>So the URL&rsquo;s that we will have, will more or less look like the following:</p>

<pre><code class="bash">https://surveys.mydomain.com/one/abc123?companyId=178231&amp;group_name=abc_12&amp;utm_source=survey&amp;utm_medium=email&amp;utm_campaign=survey-top-1
</code></pre>

<p>So we have a couple of utm parameters, company id, group name etc, which will be use for analysis</p>

<h2>Python to Capture the Parameters:</h2>

<p>Using Python, it&rsquo;s quite easy:</p>

<pre><code class="python">&gt;&gt;&gt; from urllib import parse
&gt;&gt;&gt; url = 'https://surveys.mydomain.com/one/abc123?companyId=178231&amp;group_name=abc_12&amp;utm_source=survey&amp;utm_medium=email&amp;utm_campaign=survey-top-1'

&gt;&gt;&gt; parse.urlsplit(url)
SplitResult(scheme='https', netloc='surveys.mydomain.com', path='/one/abc123', query='companyId=178231&amp;group_name=abc_12&amp;utm_source=survey&amp;utm_medium=email&amp;utm_campaign=survey-top-1', fragment='')
&gt;&gt;&gt; parse.parse_qsl(parse.urlsplit(url).query)
[('companyId', '178231'), ('group_name', 'abc_12'), ('utm_source', 'survey'), ('utm_medium', 'email'), ('utm_campaign', 'survey-top-1')]
</code></pre>

<p>Now to get our data in a dictionary, we can just convert it using the <code>dict()</code> function:</p>

<pre><code class="python">&gt;&gt;&gt; dict(parse.parse_qsl(parse.urlsplit(url).query))
{'companyId': '178231', 'group_name': 'abc_12', 'utm_source': 'survey', 'utm_medium': 'email', 'utm_campaign': 'survey-top-1'}
</code></pre>

<p>This data can then be used to write to a database, which can then be used for analysis.</p>

<h2>Resources:</h2>

<ul>
<li><a href="http://blog.rafflecopter.com/2014/04/utm-parameters-best-practices/">http://blog.rafflecopter.com/2014/04/utm-parameters-best-practices/</a></li>
<li><a href="https://stackoverflow.com/questions/21584545/url-query-parameters-to-dict-python">https://stackoverflow.com/questions/21584545/url-query-parameters-to-dict-python</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Give Your Database a Break and Use Memcached to Return Frequently Accessed Data]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/09/01/give-your-database-a-break-and-use-memcached-to-return-frequently-accessed-data/"/>
    <updated>2018-09-01T17:05:10-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/09/01/give-your-database-a-break-and-use-memcached-to-return-frequently-accessed-data</id>
    <content type="html"><![CDATA[<p><img src="https://objects.ruanbekker.com/assets/images/memcached-logo.png" alt="" /></p>

<p>So let&rsquo;s take this scenario:</p>

<p>Your database is getting hammered with requests and building up some load over time and we would like to place a caching layer in front of our database that will return data from the caching layer, to reduce some traffic to our database and also improve our performance for our application.</p>

<h2>The Scenario:</h2>

<p>Our scenario will be very simple for this demonstration:</p>

<ul>
<li>Database will be using SQLite with product information (product_name, product_description)</li>
<li>Caching Layer will be Memcached</li>
<li>Our Client will be written in Python, which checks if the product name is in cache, if not a <code>GET_MISS</code> will be returned, then the data will be fetched from the database, returns it to the client and save it to the cache</li>
<li>Next time the item will be read, a <code>GET_HIT</code> will be received, then the item will be delivered to the client directly from the cache</li>
</ul>


<h2>SQL Database:</h2>

<p>As mentioned we will be using sqlite for demonstration.</p>

<p>Create the table, populate some very basic data:</p>

<pre><code class="sql">$ sqlite3 db.sql -header -column
import sqlite3 as sql
SQLite version 3.16.0 2016-11-04 19:09:39
Enter ".help" for usage hints.

sqlite&gt; create table products (product_name STRING(32), product_description STRING(32));
sqlite&gt; insert into products values('apple', 'fruit called apple');
sqlite&gt; insert into products values('guitar', 'musical instrument');
</code></pre>

<p>Read all the data from the table:</p>

<pre><code class="sql">sqlite&gt; select * from products;
product_name  product_description
------------  -------------------
apple         fruit called apple
guitar        musical instrument
sqlite&gt; .exit
</code></pre>

<h2>Run a Memcached Container:</h2>

<p>We will use docker to run a memcached container on our workstation:</p>

<pre><code class="bash">$ docker run -itd --name memcached -p 11211:11211 rbekker87/memcached:alpine
</code></pre>

<h2>Our Application Code:</h2>

<p>I will use <a href="https://pymemcache.readthedocs.io/en/latest/getting_started.html">pymemcache</a> as our client library. Install:</p>

<pre><code class="bash">$ virtualenv .venv &amp;&amp; source .venv/bin/activate
$ pip install pymemcache
</code></pre>

<p>Our Application Code which will be in Python</p>

<pre><code class="python">import sqlite3 as sql
from pymemcache.client import base

product_name = 'guitar'

client = base.Client(('localhost', 11211))
result = client.get(product_name)

def query_db(product_name):
    db_connection = sql.connect('db.sql')
    c = db_connection.cursor()
    try:
        c.execute('select product_description from products where product_name = "{k}"'.format(k=product_name))
        data = c.fetchone()[0]
        db_connection.close()
    except:
        data = 'invalid'
    return data

if result is None:
    print("got a miss, need to get the data from db")
    result = query_db(product_name)
    if result == 'invalid':
        print("requested data does not exist in db")
    else:
        print("returning data to client from db")
        print("=&gt; Product: {p}, Description: {d}".format(p=product_name, d=result))
        print("setting the data to memcache")
        client.set(product_name, result)

else:
    print("got the data directly from memcache")
    print("=&gt; Product: {p}, Description: {d}".format(p=product_name, d=result))
</code></pre>

<p>Explanation:</p>

<ul>
<li>We have a function that takes a argument of the product name, that makes the call to the database and returns the description of that product</li>
<li>We will make a get operation to memcached, if nothing is returned, then we know the item does not exists in our cache,</li>
<li>Then we will call our function to get the data from the database and return it directly to our client, and</li>
<li>Save it to the cache in memcached so the next time the same product is queried, it will be delivered directly from the cache</li>
</ul>


<h2>The Demo:</h2>

<p>Our Product Name is <code>guitar</code>, lets call the product, which will be the first time so memcached wont have the item in its cache:</p>

<pre><code class="bash">$ python app.py
got a miss, need to get the data from db
returning data to client from db
=&gt; Product: guitar, Description: musical instrument
setting the data to memcache
</code></pre>

<p>Now from the output, we can see that the item was delivered from the database and saved to the cache, lets call that same product and observe the behavior:</p>

<pre><code class="bash">$ python app.py
got the data directly from memcache
=&gt; Product: guitar, Description: musical instrument
</code></pre>

<p>When our cache instance gets rebooted we will lose our data that is in the cache, but since the source of truth will be in our database, data will be re-added to the cache as they are requested. That is one good reason not to rely on a cache service to be your primary data source.</p>

<p>What if the product we request is not in our cache or database, let&rsquo;s say the product <code>tree</code></p>

<pre><code class="bash">$ python app.py
got a miss, need to get the data from db
requested data does not exist in db
</code></pre>

<p>This was a really simple scenario, but when working with masses amount of data, you can benefit from a lot of performance using caching.</p>

<h2>Resources:</h2>

<ul>
<li><a href="https://realpython.com/python-memcache-efficient-caching/">https://realpython.com/python-memcache-efficient-caching/</a></li>
<li><a href="https://github.com/ruanbekker/dockerhub-sources/tree/master/memcached/alpine">https://github.com/ruanbekker/dockerhub-sources/tree/master/memcached/alpine</a></li>
<li><a href="https://pymemcache.readthedocs.io/en/latest/getting_started.html#basic-usage">https://pymemcache.readthedocs.io/en/latest/getting_started.html#basic-usage</a></li>
<li><a href="https://sebastianraschka.com/Articles/2014_sqlite_in_python_tutorial.html">https://sebastianraschka.com/Articles/2014_sqlite_in_python_tutorial.html</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
