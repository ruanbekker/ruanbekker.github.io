<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Python | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/python/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2020-05-07T13:55:41+02:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Graphing Covid-19 Stats With Grafana and Elasticsearch Using Python]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/04/26/graphing-covid-19-stats-with-grafana-and-elasticsearch-using-python/"/>
    <updated>2020-04-26T02:24:27+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/04/26/graphing-covid-19-stats-with-grafana-and-elasticsearch-using-python</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/80421197-62345180-88dc-11ea-9e0a-557199aaf613.png" alt="coronavirus-covid19-grafana-metrics" /></p>

<p>I stumbled upon a <a href="https://github.com/pomber/covid19/">github repository</a> that stores time-series data in json format of corona virus / covid19 statistics, which get updated daily.</p>

<p>I was curious to see data about my country and want to see how metrics will look like after our lockdown started, so I decided to consume that data with <strong>Python</strong> and the requests library, then ingest data about covid19 into <strong>Elasticsearch</strong> and the visualize the data with <strong>Grafana</strong>.</p>

<h2>Sample of the Data</h2>

<p>Let&rsquo;s have a peek at the data to determine how we will use it to write to Elasticsearch. Let&rsquo;s consume the data with python:</p>

<pre><code>&gt;&gt;&gt; import requests
&gt;&gt;&gt; import json
&gt;&gt;&gt; response = requests.get('https://pomber.github.io/covid19/timeseries.json').json()
</code></pre>

<p>Now let&rsquo;s determine the data type:</p>

<pre><code>&gt;&gt;&gt; type(response)
&lt;type 'dict'&gt;
</code></pre>

<p>Now as it&rsquo;s a dictionary, let&rsquo;s look at they keys:</p>

<pre><code>&gt;&gt;&gt; response.keys()
[u'Canada', u'Sao Tome and Principe', u'Lithuania', u'Cambodia', u'Ethiopia',....
</code></pre>

<p>So let&rsquo;s take a look how the data looks like if we do a lookup for Canada:</p>

<pre><code>&gt;&gt;&gt; type(response['Canada'])
&lt;type 'list'&gt;
</code></pre>

<p>As we can see it&rsquo;s a list, we can count how many items is in our list:</p>

<pre><code>&gt;&gt;&gt; len(response['Canada'])
94
</code></pre>

<p>Now let&rsquo;s peek at the data by accessing our first index of our list:</p>

<pre><code>&gt;&gt;&gt; response['Canada'][0]
{u'date': u'2020-1-22', u'confirmed': 0, u'recovered': 0, u'deaths': 0}
</code></pre>

<p>So our data will look like this:</p>

<pre><code>{
  [
    'Country Name': [
      {
        'date': '&lt;string&gt;', 
        'confirmed': '&lt;int&gt;', 
        'recovered': '&lt;int&gt;', 
        'deaths': '&lt;int&gt;'
      },
      {
        'date': '&lt;string&gt;',
        'confirmed': '&lt;int&gt;',
        'recovered': '&lt;int&gt;',
        'deaths': '&lt;int&gt;'
      },
    ],
    'Country Name': [
      ...
    ]
  ]
}
</code></pre>

<h2>Some issues we need to fix</h2>

<p>As you can see the date is displayed as <code>2020-1-22</code> instead of <code>2020-01-22</code>, I want to make it consistent as I will be ingesting the data with a <code>@timestamp</code> key which we will use the date from the returned data. So first we will need to convert that before we ingest the data.</p>

<p>The other thing I was thinking of is that, if for some reason we need to ingest this data again, we dont want to sit with duplicates (same document with different _id&rsquo;s), so for that I decided to generate a hash value that consist of the date and the country, so if the script run to ingest the data, it will use the same id for the specific document, which would just overwrite it, therefore we won&rsquo;t sit with duplicates.</p>

<p>So the idea is to ingest a document to elasticsearch like this:</p>

<pre><code>doc = {
    "_id": "sha_hash_value",
    "day": "2020-01-22",
    "timestamp": "@2020-01-22 00:00:00",
    "country": "CountryName",
    "confirmed": 0,
    "recovered": 0,
    "deaths": 0
}
</code></pre>

<h2>How we will ingest the data</h2>

<p>The first run will load all the data and ingest all the data up to the current day to elasticsearch. Once that is done, we will add code to our script to only ingest the most recent day&rsquo;s data into elasticsearch, which we will control with a cronjob.</p>

<p>Create a index with a mapping to let Elasticsearch know <code>timestamp</code> will be a date field:</p>

<pre><code>$ curl -XPUT -H 'Content-Type: application/json' \
  -u username:pass 'https://es.domain.com/coronastats' -d \
  '{"mappings": {"foo1": {"properties": {"timestamp" : {"type" : "date","format" : "yyyy-MM-dd HH:mm:ss"}}}}}'
</code></pre>

<p>Once our index is created, create the python script that will load the data, loop through each country&rsquo;s daily data and ingest it into elasticsearch:</p>

<pre><code class="python">#!/usr/bin/python
import requests
import datetime as dt
import json
import hashlib

url = 'https://pomber.github.io/covid19/timeseries.json'
elasticsearch_url = "https://es.domain.com"
elasticsearch_username = ""
elasticsearch_password = ""

api_response = requests.get(url).json()

def convert_datestamp(day):
    return str(dt.datetime.strptime(day, '%Y-%m-%d'))

def hash_function(country, date):
    string_to_hash = country + date
    hash_obj  = hashlib.sha1(string_to_hash.encode('utf-8'))
    hash_value = hash_obj.hexdigest()
    return hash_value

def map_es_doc(payload, country):
    doc = {
        "day": payload['date'],
        "timestamp": convert_datestamp(payload['date']),
        "country": country,
        "confirmed": payload['confirmed'],
        "recovered": payload['recovered'],
        "deaths": payload['deaths']
    }
    return doc

def ingest(doc_id, payload):
    response = requests.put(
        elasticsearch_url + '/coronastats/coronastats/' + doc_id,
        auth=(elasticsearch_username, elasticsearch_password),
        headers={'content-type': 'application/json'},
        json=payload
    )
    return response.status_code

for country in api_response.keys():
    try:
        for each_payload in api_response[country]:
            doc_id = hash_function(country, each_payload['date'])
            doc = map_es_doc(each_payload, country)
            response = ingest(doc_id, doc)
            print(response)
    except Exception as e:
        print(e)
</code></pre>

<p>Run the script to ingest all the data into elasticsearch. Now we will create the script that will run daily to only ingest the previous day&rsquo;s data, so that we only ingest the latest data and not all the data from scratch again.</p>

<p>I will create this file in <code>/opt/scripts/corona_covid19_ingest.py</code>:</p>

<pre><code>#!/usr/bin/python
import requests
import datetime as dt
import json
import hashlib

url = 'https://pomber.github.io/covid19/timeseries.json'
elasticsearch_url = "https://es.domain.com"
elasticsearch_username = ""
elasticsearch_password = ""

api_response = requests.get(url).json()

yesterdays_date = dt.date.today() - dt.timedelta(days=1)

def convert_datestamp(day):
    return str(dt.datetime.strptime(day, '%Y-%m-%d'))

def hash_function(country, date):
    string_to_hash = country + date
    hash_obj  = hashlib.sha1(string_to_hash.encode('utf-8'))
    hash_value = hash_obj.hexdigest()
    return hash_value

def map_es_doc(payload, country):
    doc = {
        "day": payload['date'],
        "timestamp": convert_datestamp(payload['date']),
        "country": country,
        "confirmed": payload['confirmed'],
        "recovered": payload['recovered'],
        "deaths": payload['deaths']
    }
    return doc

def ingest(doc_id, payload):
    response = requests.put(
        elasticsearch_url + '/coronastats/coronastats/' + doc_id,
        auth=(elasticsearch_username, elasticsearch_password),
        headers={'content-type': 'application/json'},
        json=payload
    )
    return response.status_code

for country in api_response.keys():
    try:
        for each_payload in api_response[country]:
            if convert_datestamp(each_payload['date']).split()[0] == str(yesterdays_date):
                print("ingesting latest data for {country}".format(country=country))
                doc_id = hash_function(country, each_payload['date'])
                doc = map_es_doc(each_payload, country)
                response = ingest(doc_id, doc)
                print(response)
    except Exception as e:
        print(e)
</code></pre>

<p>The only difference with this script is that it checks if the date is equals to yesterday&rsquo;s date, and if so the document will be prepared and ingested into elasticsearch. We will create a cronjob that runs this script every morning at 08:45.</p>

<p>First make the file executable:</p>

<pre><code>$ chmod +x /opt/scripts/corona_covid19_ingest.py
</code></pre>

<p>Run <code>crontab -e</code> and add the following</p>

<pre><code>45 8 * * * /opt/scripts/corona_covid19_ingest.py
</code></pre>

<h2>Visualize the Data with Grafana</h2>

<p>We will create this dashboard:</p>

<p><img src="https://user-images.githubusercontent.com/567298/80418135-35ca0680-88d7-11ea-83f6-3432a903333d.png" alt="corona-covid-19-dashboard" /></p>

<p>We need a elasticsearch datasource that points to the index that we ingest our data into. Head over to datasources, add a elasticsearch datasource and set the index to <code>coronastats</code> and add the timefield as <code>timestamp</code>.</p>

<p>We want to make the dashboard dynamic to have a <strong>&ldquo;country&rdquo;</strong> dropdown selector, for that go to the dashboard settings, select variable and add a country variable:</p>

<p><img src="https://user-images.githubusercontent.com/567298/80419463-7cb8fb80-88d9-11ea-959f-8f37ae3f6dc7.png" alt="covid19-dashboard-variables" /></p>

<p>First panel: &ldquo;Reported Cases per Day&rdquo;:</p>

<p><img src="https://user-images.githubusercontent.com/567298/80419572-af62f400-88d9-11ea-802e-7eeacb61ee19.png" alt="covid19-reported-cases" /></p>

<p>Second panel: &ldquo;Confirmed Cases&rdquo;:</p>

<p><img src="https://user-images.githubusercontent.com/567298/80419675-db7e7500-88d9-11ea-98a5-3aae4d9a6c87.png" alt="covid19-confirmed-cases" /></p>

<p>Third panel: &ldquo;Recovered Cases&rdquo;:</p>

<p><img src="https://user-images.githubusercontent.com/567298/80419750-fa7d0700-88d9-11ea-82a3-f26ff8c807ef.png" alt="covid19-recovered-cases" /></p>

<p>Now, if we select Italy, Spain and France as an example, we will see something like this:</p>

<p><img src="https://user-images.githubusercontent.com/567298/80419966-56479000-88da-11ea-8f30-39ac3da27007.png" alt="covid19-country-stats" /></p>

<h2>Thank You</h2>

<p>Although its pretty cool visualizing data, the issue that we are in at the moment with coronavirus / covid19 is really scary and we should all do our part to try and stay home, sanitize and try not to spread the virus. Together we can all do great things by reducing the spread of this virus.</p>

<p>Stay safe everyone.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Asynchronous Function With OpenFaas]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/02/17/python-asynchronous-function-with-openfaas/"/>
    <updated>2020-02-17T23:51:22+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/02/17/python-asynchronous-function-with-openfaas</id>
    <content type="html"><![CDATA[<p>In this post we will explore how to use asynchronous functions in OpenFaas.</p>

<h2>What are we doing</h2>

<p>A synchronous request blocks the client until operation completes, where a asynchronous request doesn’t block the client, which is nice to use for long-running tasks or function invocations to run in the background through the use of NATS Streaming.</p>

<p>We will be building a Python Flask API Server which will act as our webhook service. When we invoke our function by making a http request, we also include a callback url as a header which will be the address where the queue worker will post it&rsquo;s results.</p>

<p>Then we will make a http request to the synchronous function where we will get the response from the function and a http request to the asynchronous function, where we will get the response from the webhook service&rsquo;s logs</p>

<h2>Deploy OpenFaas</h2>

<p>Deploy OpenFaas on a k3d Kubernetes Cluster if you want to follow along on your laptop. You can follow this post to deploy a kubernetes cluster and deploying openfaas:</p>

<ul>
<li><a href="https://blog.ruanbekker.com/blog/2020/02/17/traefik-ingress-for-openfaas-on-kubernetes-k3d/">https://blog.ruanbekker.com/blog/2020/02/17/traefik-ingress-for-openfaas-on-kubernetes-k3d/</a></li>
</ul>


<h2>Webhook Service</h2>

<p>Lets build the Python Flask Webhook Service, our application code:</p>

<pre><code class="python">from flask import Flask, request
from logging.config import dictConfig

dictConfig({
    'version': 1,
    'formatters': {'default': {
        'format': '[%(asctime)s] %(levelname)s in %(module)s: %(message)s',
    }},
    'handlers': {'wsgi': {
        'class': 'logging.StreamHandler',
        'stream': 'ext://flask.logging.wsgi_errors_stream',
        'formatter': 'default'
    }},
    'root': {
        'level': 'INFO',
        'handlers': ['wsgi']
    }
})

app = Flask(__name__)

@app.route("/", methods=["POST", "GET"])
def main():
    response = {}

    if request.method == "GET":
        response["event"] = "GET"
        app.logger.info("Received Event: GET")

    if request.method == "POST":
        response["event"] = request.get_data()
        app.logger.info("Receveid Event: {}".format(response))

    else:
        response["event"] == "OTHER"

    print("Received Event:")
    print(response)
    return "event: {} \n".format(response)

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
</code></pre>

<p>Our <code>Dockerfile</code>:</p>

<pre><code>FROM python:3.7-alpine
RUN pip install flask
ADD app.py /app.py
EXPOSE 5000
CMD ["python", "/app.py"]
</code></pre>

<p>Building and Pushing to Docker Hub (or you can use my docker image):</p>

<pre><code>$ docker build -t yourusername/python-flask-webhook:openfaas .
$ docker push yourusername/python-flask-webhook:openfaas
</code></pre>

<p>Create the deployment manifest <code>webhook.yml</code> for our webhook service:</p>

<pre><code>$ cat &gt; webhook.yml &lt;&lt; EOF
apiVersion: v1
kind: Service
metadata:
  name: webhook-service
spec:
  selector:
    app: webhook
  ports:
    - protocol: TCP
      port: 5000
      targetPort: 5000
      name: web
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: webhook-ingress
  annotations:
    kubernetes.io/ingress.class: traefik
spec:
  rules:
  - host: webhook.localdns.xyz
    http:
      paths:
      - backend:
          serviceName: webhook-service
          servicePort: 5000
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: webhook
  name: webhook
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webhook
  template:
    metadata:
      labels:
        app: webhook
    spec:
      containers:
      - name: webhook
        image: ruanbekker/python-flask-webhook:openfaas
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 5000
          name: http
          protocol: TCP
EOF
</code></pre>

<p>Now deploy to kubernetes:</p>

<pre><code>$ kubectl apply -f webhook.yml
</code></pre>

<p>After a minute or so, verify that you get a response when making a http request:</p>

<pre><code>$ curl http://webhook.localdns.xyz
event: {'event': 'GET'}
</code></pre>

<h2>Deploy the OpenFaas Function</h2>

<p>We will deploy a dockerfile type function which will return the data that we feed it:</p>

<pre><code>$ faas-cli new --lang dockerfile function-async-task
$ faas-cli up -f function-async-task.yml

Deploying: function-async-task.

Deployed. 202 Accepted.
URL: http://openfaas.localdns.xyz/function/function-async-task
</code></pre>

<p>List the functions:</p>

<pre><code>$ faas-cli list
Function                        Invocations     Replicas
function-async-task             0               1
</code></pre>

<p>Describe the function:</p>

<pre><code>$ faas-cli describe function-async-task
Name:                function-async-task
Status:              Ready
Replicas:            1
Available replicas:  1
Invocations:         0
Image:               ruanbekker/function-async-task:latest
Function process:
URL:                 http://openfaas.localdns.xyz/function/function-async-task
Async URL:           http://openfaas.localdns.xyz/async-function/function-async-task
Labels:              faas_function : function-async-task
Annotations:         prometheus.io.scrape : false
</code></pre>

<h2>Testing</h2>

<p>Test synchronous function:</p>

<pre><code>$ curl http://openfaas.localdns.xyz/function/function-async-task -d "test"
test
</code></pre>

<p>Test asynchronous function, remember, here we need to provide the callback url which the queue worker will inform, which will be our webhook service:</p>

<pre><code>$ curl -i -H "X-Callback-Url: http://webhook-service.default.svc.cluster.local:5000" http://openfaas.localdns.xyz/async-async-function/function-async-task -d "asyyyyync"
HTTP/1.1 202 Accepted
Content-Length: 0
Date: Mon, 17 Feb 2020 13:57:26 GMT
Vary: Accept-Encoding
X-Call-Id: d757c10f-4293-4daa-bf52-bbdc17b7dea3
X-Start-Time: 1581947846737501600
</code></pre>

<p>Check the logs of the webhook pod:</p>

<pre><code>$ kubectl logs -f pod/$(kubectl get pods --selector=app=webhook --output=jsonpath="{.items..metadata.name}")
[2020-02-17 13:57:26,774] INFO in app: Receveid Event: {'event': b'asyyyyync'}
[2020-02-17 13:57:26,775] INFO in internal: 10.42.0.6 - - [17/Feb/2020 13:57:26] "POST / HTTP/1.1" 200 -
</code></pre>

<p>Check the logs of the queue worker:</p>

<pre><code>$ kubectl logs -f deployment/queue-worker -n openfaas
[45] Received on [faas-request]: 'sequence:45 subject:"faas-request" data:"{\"Header\":{\"Accept\":[\"*/*\"],\"Accept-Encoding\":[\"gzip\"],\"Content-Length\":[\"9\"],\"Content-Type\":[\"application/x-www-form-urlencoded\"],\"User-Agent\":[\"curl/7.54.0\"],\"X-Call-Id\":[\"d757c10f-4293-4daa-bf52-bbdc17b7dea3\"],\"X-Callback-Url\":[\"http://webhook-service.default.svc.cluster.local:5000\"],\"X-Forwarded-For\":[\"10.42.0.0\"],\"X-Forwarded-Host\":[\"openfaas.localdns.xyz\"],\"X-Forwarded-Port\":[\"80\"],\"X-Forwarded-Proto\":[\"http\"],\"X-Forwarded-Server\":[\"traefik-6787cddb4b-87zss\"],\"X-Real-Ip\":[\"10.42.0.0\"],\"X-Start-Time\":[\"1581947846737501600\"]},\"Host\":\"openfaas.localdns.xyz\",\"Body\":\"YXN5eXl5eW5j\",\"Method\":\"POST\",\"Path\":\"\",\"QueryString\":\"\",\"Function\":\"openfaas-function-cat\",\"CallbackUrl\":{\"Scheme\":\"http\",\"Opaque\":\"\",\"User\":null,\"Host\":\"webhook-service.default.svc.cluster.local:5000\",\"Path\":\"\",\"RawPath\":\"\",\"ForceQuery\":false,\"RawQuery\":\"\",\"Fragment\":\"\"}}" timestamp:1581947846738308800 '
Invoking: openfaas-function-cat with 9 bytes, via: http://gateway.openfaas.svc.cluster.local:8080/function/openfaas-function-cat/
Invoked: openfaas-function-cat [200] in 0.029029s
Callback to: http://webhook-service.default.svc.cluster.local:5000
openfaas-function-cat returned 9 bytes
Posted result for openfaas-function-cat to callback-url: http://webhook-service.default.svc.cluster.local:5000, status: 200
</code></pre>

<p>Make 1000 Requests:</p>

<pre><code>$ date &gt; time.date 
  for x in {1..1000}
    do 
      curl -i -H "X-Callback-Url: http://webhook-service.default.svc.cluster.local:5000" http://openfaas.localdns.xyz/async-function/openfaas-function-cat -d "asyyyyync"
    done
  date &gt;&gt; time.date
</code></pre>

<p>View the log file that we wrote before we started and finished our requests:</p>

<pre><code>$ cat time.date
Mon Feb 17 16:03:16 SAST 2020
Mon Feb 17 16:03:48 SAST 2020
</code></pre>

<p>The last request was actioned at:</p>

<pre><code>[2020-02-17 14:03:52,421] INFO in internal: 10.42.0.6 - - [17/Feb/2020 14:03:52] "POST / HTTP/1.1" 200 -
</code></pre>

<h2>Thank You</h2>

<p>This was a basic example to demonstrate async functions using OpenFaas</p>

<h2>OpenFaas Documentation:</h2>

<ul>
<li><a href="https://docs.openfaas.com">https://docs.openfaas.com</a></li>
<li><a href="https://docs.openfaas.com/reference/async/">https://docs.openfaas.com/reference/async/</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AWS S3 KMS and Python for Secrets Management]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/09/04/aws-s3-kms-and-python-for-secrets-management/"/>
    <updated>2019-09-04T19:58:45+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/09/04/aws-s3-kms-and-python-for-secrets-management</id>
    <content type="html"><![CDATA[<p><img src="https://miro.medium.com/max/2400/1*9PSzVZDHjr321CpxJHcxPQ.png" alt="" /></p>

<p>So your application need to store secrets and you are looking for a home for them. In this tutorial we will see how we can use Python, S3 and KMS to build our own solution for managing secrets.</p>

<p>There is SSM and Secrets Manager that probably does a better job, but my mind got curious :D</p>

<h2>High Level Goal</h2>

<p>From a High-Level we want to store secrets encrypted on S3 with KMS, namespaced with <strong>team/application/environment/value</strong> in json format so that our application receives the json dictionary of configured key/value pairs.</p>

<p>We can leverage <strong>IAM</strong> to delegate permissions on the namespacing that we decide on, for my example the namespace will look like this on S3:</p>

<pre><code>s3://s3bucket/secrets/engineering/app1/production/appconfig.json
</code></pre>

<p>We will apply <strong>IAM</strong> permissions for our user to only <strong>Put</strong> and <strong>Get</strong> on <code>secrets/engineering*</code>. So with this idea we can apply IAM permissions on groups for different departments, or even let users manage their own secrets such as:</p>

<pre><code>s3://s3bucket/secrets/personal/user.name/app/appconfig.json
</code></pre>

<p>After the object has been downloaded from S3 and decrypted using KMS, the value of the object will look like this:</p>

<pre><code>{u'surname': u'bekker', u'name': u'ruan', u'job_title': u'systems-development-engineer'}
</code></pre>

<h2>Requirements</h2>

<p>We will create the following resources on AWS:</p>

<ul>
<li>KMS Key</li>
<li>S3 Bucket</li>
<li>IAM User</li>
<li>IAM Policy</li>
<li>Python Dependencies: Boto3</li>
</ul>


<h2>Provision AWS Resources</h2>

<p><img src="https://miro.medium.com/max/2728/1*Lq9xaUXuNo2Nb8kQakYdsg.png" alt="" /></p>

<p>First we will create our <strong>S3 Bucket</strong>,  head over to <a href="https://s3.console.aws.amazon.com/s3/home?region=eu-west-1">Amazon S3</a> create a new s3 bucket, make sure that the bucket is <strong>NOT</strong> public, by using the default configuration, you should be good.</p>

<p>Once your S3 Bucket is provisioned, head over to <a href="https://console.aws.amazon.com/iam/home#/users">Amazon IAM</a> and create a IAM User, enable programmatic access, and keep your access key and secret key safe. For now we will not apply any permissions as we will come back to this step.</p>

<p>Head over to <a href="https://eu-west-1.console.aws.amazon.com/kms/home?region=eu-west-1#/kms/home">Amazon KMS</a> and create a KMS Key, we will define the <strong>key administrator</strong>, which will be my user (ruan.bekker in this case) with more privileged permissions:</p>

<p><img src="https://miro.medium.com/max/5120/1*EUPWbCQ8nsfbBWHQI6srYw.png" alt="" /></p>

<p>and then we will define the <strong>key usage permissions</strong> (app.user in this case), which will be the user that we provisioned from the previous step, this will be the user that will encrypt and decrypt the data:</p>

<p><img src="https://miro.medium.com/max/5120/1*5xA5H0qpJ1FYTG1hUjy_Tw.png" alt="" /></p>

<p>Next, review the policy generated from the previous selected sections:</p>

<p><img src="https://miro.medium.com/max/5120/1*bLDVPFaZUDQ4EyWjACYRUw.png" alt="" /></p>

<p>Once you select finish, you will be returned to the section where your KMS Key information will be displayed, keep note of your <strong>KMS Key Alias</strong>, as we will need it later:</p>

<p><img src="https://miro.medium.com/max/5120/1*aooUMS0OyEd5hopnOUrcmA.png" alt="" /></p>

<h2>Create a IAM Policy for our App User</h2>

<p>Next we will create the IAM Policy for the user that will encrypt/decrypt and store data in S3</p>

<pre><code class="json">{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "S3PutAndGetAccess",
            "Effect": "Allow",
            "Action": [
                "s3:PutObject",
                "s3:GetObject"
            ],
            "Resource": "arn:aws:s3:::arn:aws:s3:::s3-bucket-name/secrets/engineering*"
        },
        {
            "Sid": "KMSDecryptAndEncryptAccess",
            "Effect": "Allow",
            "Action": [
                "kms:Decrypt",
                "kms:Encrypt"
            ],
            "Resource": "arn:aws:kms:eu-west-1:123456789012:key/xxxx-xxxx-xxxx-xxxx-xxxx"
        }
    ]
}
</code></pre>

<p>After the policy has been saved, associate the policy to the IAM User</p>

<h2>Encrypt and Put to S3</h2>

<p>Now we will use Python to define the data that we want to <strong>store in S3</strong>, we will then <strong>encrypt</strong> the data with <strong>KMS</strong>, use base64 to <strong>encode</strong> the ciphertext and push the encrypted value to <strong>S3</strong>, with Server Side Encryption enabled, which we will also use our KMS key.</p>

<p>Install boto3 in Python:</p>

<pre><code class="bash">$ pip install boto3
</code></pre>

<p>Enter the Python REPL and import the required packages, we will also save the access key and secret key as variables so that we can use it with boto3. You can also save it to the <a href="https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html">credential provider</a> and utilise the profile name:</p>

<pre><code class="python">&gt;&gt;&gt; import boto3
&gt;&gt;&gt; import json
&gt;&gt;&gt; import base64
&gt;&gt;&gt; aws_access_key_id='redacted'
&gt;&gt;&gt; aws_secret_access_key='redacted'
</code></pre>

<p>Next define the data that we want to <strong>encrypt and store</strong> in S3:</p>

<pre><code class="python">&gt;&gt;&gt; mydata = {
    "name": "ruan",
    "surname": "bekker",
    "job_title": "systems-development-engineer"
}
</code></pre>

<p>Next we will use KMS to encrypt the data and use base64 to encode the ciphertext:</p>

<pre><code class="python">&gt;&gt;&gt; kms = boto3.Session(
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key
).client('kms')
&gt;&gt;&gt; ciphertext = kms.encrypt(
    KeyId='alias/secrets-key',
    Plaintext=json.dumps(mydata)
)
&gt;&gt;&gt; encoded_ciphertext = base64.b64encode(ciphertext["CiphertextBlob"])
# preview the data
&gt;&gt;&gt; encoded_ciphertext
'AQICAHiKOz...42720nCleoI26UW7P89lPdwvV8Q=='
</code></pre>

<p>Next we will use S3 to push the encrypted data onto S3 in our name spaced key: <strong>secrets/engineering/app1/production/appconfig.json</strong></p>

<pre><code class="python">&gt;&gt;&gt; s3 = boto3.Session(
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key,
    region_name='eu-west-1'
).client('s3')
&gt;&gt;&gt; response = s3.put_object(
    Body=encoded_ciphertext,
    Bucket='ruan-secret-store',
    Key='secrets/engineering/app1/production/appconfig.json',
    ServerSideEncryption='aws:kms',
    SSEKMSKeyId='alias/secrets-key'
)
</code></pre>

<p>Now our object is stored in S3, encrypted with KMS and ServerSideEncryption Enabled.</p>

<p>You can try to download the object and decode the base64 encoded file and you will find that its complete garbage as its encrypted.</p>

<p>Next we will use S3 to Get the object and use KMS to decrypt and use base64 to decode after the object has been decrypted:</p>

<pre><code class="python">&gt;&gt;&gt; response = s3.get_object(
    Bucket='ruan-secret-store',
    Key='secrets/engineering/app1/production/appconfig.json'
)
&gt;&gt;&gt; encoded_ciphertext = response['Body'].read()
&gt;&gt;&gt; encoded_ciphertext
'AQICAHiKOz...42720nCleoI26UW7P89lPdwvV8Q=='
</code></pre>

<p>Now let’s decode the result with base64:</p>

<pre><code class="python">&gt;&gt;&gt; decoded_ciphertext = base64.b64decode(encoded_ciphertext)
&gt;&gt;&gt; plaintext = kms.decrypt(CiphertextBlob=bytes(decoded_ciphertext))
</code></pre>

<p>Now we need to deserialize the JSON as it’s in string format:</p>

<pre><code>&gt;&gt;&gt; json.loads(plaintext["Plaintext"])
{u'surname': u'bekker', u'name': u'ruan', u'job_title': u'systems-development-engineer'}
</code></pre>

<h2>Using it in a Application</h2>

<p>Let’s say you are using <strong>Docker</strong> and you want to bootstrap your application configs to your environment that you are retrieving from S3.</p>

<p>We will use a <code>get_secrets.py</code> python script that will read the data into memory, decrypt and write the values in plaintext to disk, then we will use the <code>boot.sh</code> script to read the values into the environment and remove the temp file that was written to disk, then start the application since we have the values stored in our environment.</p>

<p>Our <strong>&ldquo;application&rdquo;</strong> in this example will just be a line of echo to return the values for demonstration.</p>

<p>The <code>get_secrets.py</code> file:</p>

<pre><code class="python">import boto3
import json
import base64

aws_access_key_id='redacted'
aws_secret_access_key='redacted'

kms = boto3.Session(aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key).client('kms')
s3 = boto3.Session(aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, region_name='eu-west-1').client('s3')

response = s3.get_object(Bucket='ruan-secret-store', Key='secrets/engineering/app1/production/appconfig.json')
encoded_ciphertext = response['Body'].read()

decoded_ciphertext = base64.b64decode(encoded_ciphertext)
plaintext = kms.decrypt(CiphertextBlob=bytes(decoded_ciphertext))
values = json.loads(plaintext["Plaintext"])

with open('envs.tmp', 'w') as f:
    for key in values.keys():
        f.write("{}={}".format(key.upper(), values[key]) + "\n")
</code></pre>

<p>And our <code>boot.sh</code> script:</p>

<pre><code class="bash">#!/usr/bin/env bash
source ./envs.tmp
rm -rf ./envs.tmp
echo "Hello, my name is ${NAME} ${SURNAME}, and I am a ${JOB_TITLE}"
</code></pre>

<p>Running that will produce:</p>

<pre><code>$ bash boot.sh
Hello, my name is ruan bekker, and I am a systems-development-engineer
</code></pre>

<h2>Thank You</h2>

<p>And there we have a simple and effective way of encrypting/decrypting data using S3, KMS and Python at a ridiculously cheap cost, its almost free.</p>

<p>If you liked my content, feel free to checkout my content on <strong><a href="https://ruan.dev/">ruan.dev</a></strong> or follow me on twitter at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Making Deploying Functions Even Easier With Faas-cli Up Using OpenFaaS]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/07/07/making-deploying-functions-even-easier-with-faas-cli-up-using-openfaas/"/>
    <updated>2019-07-07T09:53:59+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/07/07/making-deploying-functions-even-easier-with-faas-cli-up-using-openfaas</id>
    <content type="html"><![CDATA[<p><img src="https://camo.githubusercontent.com/cf01eefb5b6905f3774376d6d1ed55b8f052d211/68747470733a2f2f626c6f672e616c6578656c6c69732e696f2f636f6e74656e742f696d616765732f323031372f30382f666161735f736964652e706e67" alt="" /></p>

<p><a href="https://saythanks.io/to/ruanbekker"><img src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" alt="Say Thanks!" /></a> <a href="https://linux-hackers-slack.herokuapp.com/"><img src="https://linux-hackers-slack.herokuapp.com/badge.svg" alt="Slack Status" /></a> <a href="https://linux-hackers.slack.com/"><img src="https://img.shields.io/badge/chat-on_slack-orange.svg" alt="Chat on Slack" /></a> <img src="https://img.shields.io/github/followers/ruanbekker.svg?label=Follow&amp;style=social" alt="GitHub followers" /> <img src="https://img.shields.io/twitter/follow/ruanbekker.svg?style=social" alt="Twitter Follow" /></p>

<p>I recently discovered that the <code>faas-cli</code> allows you to append your function&rsquo;s yaml to an existing file when generating a new function. And that <code>faas-cli up</code> does the build, push and deploy for you.</p>

<h2>The way I always did it:</h2>

<p>Usually, I will go through this flow: create, build, push, deploy, when creating 2 functions that will be in the same stack:</p>

<pre><code>$ faas-cli new --lang python3 fn-old-foo \
--prefix=ruanbekker \
--gateway https://openfaas.domain.com

$ faas-cli build -f fn-old-foo.yml &amp;&amp; \
faas-cli push -f fn-old-foo.yml &amp;&amp; \
faas-cli deploy -f fn-old-foo.yml
</code></pre>

<p>And for my other function:</p>

<pre><code>$ faas-cli new --lang python3 fn-old-bar \
--prefix=ruanbekker \
--gateway https://openfaas.domain.com

$ faas-cli build -f fn-old-bar.yml &amp;&amp; \
faas-cli push -f fn-old-bar.yml &amp;&amp; \
faas-cli deploy -f fn-old-bar.yml
</code></pre>

<p>And then you are ready to invoke those functions.</p>

<h2>The new discovered way</h2>

<p>So recently I discovered that you can append the yaml definition of your function to an existing yaml file, and use <code>faas-cli up</code> to build, push and deploy your functions:</p>

<p>Generating the first function:</p>

<pre><code>$ faas-cli new --lang python3 fn-foo \
--prefix=ruanbekker \
--gateway https://openfaas.domain.com

Stack file written: fn-foo.yml
</code></pre>

<p>Now that we have <code>fn-foo.yml</code> in our current work directory, we will append the second function the that file:</p>

<pre><code>$ faas-cli new --lang python3 fn-bar \
--prefix=ruanbekker \
--gateway https://openfaas.domain.com \
--append fn-foo.yml

Stack file updated: fn-foo.yml
</code></pre>

<p>Now, when using <code>faas-cli up</code> it expects by default that the filename is <code>stack.yml</code> which we can change with <code>-f</code> but to keep this as easy as possible, we will change the filename to <code>stack.yml</code>:</p>

<pre><code>$ mv fn-foo.yml stack.yml
</code></pre>

<p>At the moment, our <code>stack.yml</code> will look like this:</p>

<pre><code>$ cat stack.yml
provider:
  name: openfaas
  gateway: https://openfaas.domain.com
functions:
  fn-foo:
    lang: python3
    handler: ./fn-foo
    image: ruanbekker/fn-foo:latest
  fn-bar:
    lang: python3
    handler: ./fn-bar
    image: ruanbekker/fn-bar:latest
</code></pre>

<p>Deploying our functions is as easy as:</p>

<pre><code>$ faas-cli up
...
Deploying: fn-foo.

Deployed. 202 Accepted.
URL: https://openfaas.domain.com/function/fn-foo

Deploying: fn-bar.

Deployed. 202 Accepted.
URL: https://openfaas.domain.com/function/fn-bar
</code></pre>

<p>Simply amazing. OpenFaaS done a great job in making it as simple and easy as possible to get your functions from zero to deployed in seconds.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Play With Kinesis Data Streams for Free]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/06/22/play-with-kinesis-data-streams-for-free/"/>
    <updated>2019-06-22T23:35:19+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/06/22/play-with-kinesis-data-streams-for-free</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/59969559-3f187300-9550-11e9-9e6d-7fa4dbc30198.png" alt="image" /></p>

<p>Misleading title?? Perhaps, depends on how you look at it. Amazon Kinesis is a fully managed, cloud-based service for real-time processing of distributed data streams. So if you&rsquo;re a curious mad person like me, you want to test out stuff and when you can test stuff out for free, why not.</p>

<p>So before paying for that, why not spin something up locally, such as <a href="https://github.com/mhart/kinesalite">Kinesisalite</a> which is an implementation of Amazon Kinesis built on top of LevelDB.</p>

<p>Kinesis overview:</p>

<p><img src="https://user-images.githubusercontent.com/567298/59969540-caddcf80-954f-11e9-8e3d-23c932d35ef1.png" alt="image" /></p>

<h2>What will we be doing?</h2>

<p>In this tutorial we will setup a local kinesis instance using docker then do the following:</p>

<ul>
<li>Create a Kinesis Stream, List, Describe, PutRecord, GetRecords using Python&rsquo;s Boto3 Interface</li>
<li>Write a Python Producer and Consumer</li>
<li>Write and Read Records from our Local Kinesis Stream</li>
</ul>


<h2>Building Kinesis Local on Docker</h2>

<p>If you would like to skip this step, you can use my docker image: <a href="https://hub.docker.com/r/ruanbekker/kinesis-local">ruanbekker/kinesis-local:latest</a></p>

<p>Our Dockerfile:</p>

<pre><code>FROM node:8.16.0-stretch-slim

RUN apt update &amp;&amp; apt install build-essential python-minimal -y
RUN npm install --unsafe-perm -g kinesalite
RUN apt-get clean

CMD ["kinesalite", "--port", "4567", "--createStreaMs", "5"]
</code></pre>

<p>Build:</p>

<pre><code>$ docker build -t kinesis-local .
</code></pre>

<p>Run and expose port 4567:</p>

<pre><code>$ docker run -it -p 4567:4567 kinesis-local:latest
</code></pre>

<h2>Interact with Kinesis Local:</h2>

<p>In this next steps we will setup our environment, which will only require <code>python</code> and <code>boto3</code>. To keep things isolated, I will do this with a docker container:</p>

<pre><code>$ docker run -it python:3.7-alpine sh
</code></pre>

<p>Now we need to install boto3 and enter the python repl:</p>

<pre><code>$ pip3 install boto3
$ python3
Python 3.7.3 (default, May 11 2019, 02:00:41)
[GCC 8.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt;
</code></pre>

<p>Import boto and create the connection to our kinesis local instance:</p>

<pre><code>&gt;&gt;&gt; import boto3
&gt;&gt;&gt; client = boto3.Session(
    region_name='eu-west-1').client('kinesis', aws_access_key_id='', aws_secret_access_key='', endpoint_url='http://localhost:4567'
)
</code></pre>

<p>Let&rsquo;s list our streams and as expected, we should have zero streams available:</p>

<pre><code>&gt;&gt;&gt; client.list_streams()
{u'StreamNames': [], u'HasMoreStreams': False, 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': '637xx', 'HTTPHeaders': {'x-amzn-requestid': '6xx', 'content-length': '41', 'x-amz-id-2': 'xx', 'connection': 'keep-alive', 'date': 'Sat, 22 Jun 2019 19:17:34 GMT', 'content-type': 'application/x-amz-json-1.1'}}}
</code></pre>

<p>Let&rsquo;s create a stream named <code>mystream</code> with 1 primary shard:</p>

<pre><code>&gt;&gt;&gt; client.create_stream(StreamName='mystream', ShardCount=1)
</code></pre>

<p>Let&rsquo;s list our streams again:</p>

<pre><code>&gt;&gt;&gt; client.list_streams()
{u'StreamNames': [u'mystream'], u'HasMoreStreams': False, 'ResponseMetadata': ...
</code></pre>

<p>Let&rsquo;s put some data in our kinesis stream, we will push a payload with the body: <code>{"name": "ruan"}</code> to our kinesis stream with partition key: <code>a01</code> which is used for sharding:</p>

<pre><code>&gt;&gt;&gt; response = client.put_record(StreamName='mystream', Data=json.dumps({"name": "ruan"}), PartitionKey='a01')
&gt;&gt;&gt; response
{u'ShardId': u'shardId-000000000000', 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': 'cb0xx', 'HTTPHeaders': {'x-amzn-requestid': 'xx', 'content-length': '110', 'x-amz-id-2': 'xx', 'connection': 'keep-alive', 'date': 'Sat, 22 Jun 2019 19:20:27 GMT', 'content-type': 'application/x-amz-json-1.1'}}, u'SequenceNumber': u'490xx'}
</code></pre>

<p>Now that we have data in our stream we need to read data from our kinesis stream. Before data can be read from the stream we need to obtain the shard iterator for the shard we are interested in. A shard iterator represents the position of the stream and shard from which the consumer will read, in this case we will call the get_shard_operator method and passing the stream name, shard id and shard iterator type.</p>

<p>There are 2 comman iterator types:</p>

<ul>
<li>TRIM_HORIZON: Points to the last untrimmed record in the shard</li>
<li>LATEST: Reads the most recent data in the shard</li>
</ul>


<p>We will use TRIM_HORIZON in this case, get the shard iterator id:</p>

<pre><code>&gt;&gt;&gt; shard_id = response['ShardId']
&gt;&gt;&gt; response = client.get_shard_iterator(StreamName='mystream', ShardId=shard_id, ShardIteratorType='TRIM_HORIZON')
&gt;&gt;&gt; response
{u'ShardIterator': u'AAAxx=', 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': '22dxx', 'HTTPHeaders': {'x-amzn-requestid': '22dxx', 'content-length': '224', 'x-amz-id-2': 'xx', 'connection': 'keep-alive', 'date': 'Sat, 22 Jun 2019 19:22:55 GMT', 'content-type': 'application/x-amz-json-1.1'}}}
</code></pre>

<p>Now that we have the shard iterator id, we can call the get_records method with the shard iterator id, to read the data from the stream:</p>

<pre><code>&gt;&gt;&gt; shard_iterator = response['ShardIterator']
&gt;&gt;&gt; response = client.get_records(ShardIterator=shard_iterator)
&gt;&gt;&gt; response
{u'Records': [{u'Data': '{"name": "ruan"}', u'PartitionKey': u'a01', u'ApproximateArrivalTimestamp': datetime.datetime(2019, 6, 22, 21, 20, 27, 937000, tzinfo=tzlocal()), u'SequenceNumber': u'495xx'}], 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': '2b6xx', 'HTTPHeaders': {'x-amzn-requestid': '2b6xx', 'content-length': '441', 'x-amz-id-2': 'xx', 'connection': 'keep-alive', 'date': 'Sat, 22 Jun 2019 19:30:19 GMT', 'content-type': 'application/x-amz-json-1.1'}}, u'NextShardIterator': u'AAAxx=', u'MillisBehindLatest': 0}
</code></pre>

<p>To loop and parse through the response to make it more readable:</p>

<pre><code>&gt;&gt;&gt; for record in response['Records']:
...     if 'Data' in record:
...         json.loads(record['Data'])
...
{u'name': u'ruan'}
</code></pre>

<p>Once we are done, we can delete our stream:</p>

<pre><code>&gt;&gt;&gt; client.delete_stream(StreamName='mystream')
</code></pre>

<p>Now that we have the basics, lets create our producer and consumer for a demonstration on pushing data to a kinesis stream from one process and consuming it from another process. As this demonstration we will be producing and consuming data from the same laptop, in real use-cases, you will do them from seperate servers and using Amazon Kinesis.</p>

<h2>Our Kinesis Producer</h2>

<p>The following will create a Kinesis Local Stream and Write 25 JSON Documents to our stream:</p>

<pre><code class="python">import boto3
import random
import json
import time

names = ['james', 'stefan', 'pete', 'tom', 'frank', 'peter', 'ruan']

session = boto3.Session(region_name='eu-west-1')
client = session.client(
    'kinesis', 
    aws_access_key_id='', 
    aws_secret_access_key='', 
    endpoint_url='http://localhost:4567'
)

list_streams = client.list_streams()

if 'mystream' not in list_streams['StreamNames']:
    client.create_stream(StreamName='mystream', ShardCount=1)
    time.sleep(1)

count = 0
print("Starting at {}".format(time.strftime("%H:%m:%S")))

while count != 25:
    count += 1
    response = client.put_record(
        StreamName='mystream', 
        Data=json.dumps({
            "number": count, 
            "name": random.choice(names), 
            "age": random.randint(20,50)}
        ), 
        PartitionKey='a01'
    )
    time.sleep(1)

print("Finished at {}".format(time.strftime("%H:%m:%S")))
</code></pre>

<h2>Our Kinesis Local Consumer:</h2>

<p>This will read 5 records at a time from our stream, you will notice if you run them on the same time it will only read one at a time as the producer only writes one per second.</p>

<pre><code class="python">import boto3
import json
import time
import os

session = boto3.Session(region_name='eu-west-1')
client = session.client(
    'kinesis', 
    aws_access_key_id='', 
    aws_secret_access_key='', 
    endpoint_url='http://localhost:4567'
)

stream_details = client.describe_stream(StreamName='mystream')
shard_id = stream_details['StreamDescription']['Shards'][0]['ShardId']

response = client.get_shard_iterator(
    StreamName='mystream', 
    ShardId=shard_id, 
    ShardIteratorType='TRIM_HORIZON'
)

shard_iterator = response['ShardIterator']

while True:
    response = client.get_records(ShardIterator=shard_iterator, Limit=5)
    shard_iterator = response['NextShardIterator']
    for record in response['Records']:
        if 'Data' in record and len(record['Data']) &gt; 0:
            print(json.loads(record['Data']))
    time.sleep(0.75)
</code></pre>

<h2>Demo Time!</h2>

<p>Now that we have our <code>producer.py</code> and <code>consumer.py</code>, lets test this out.</p>

<p>Start the server:</p>

<pre><code>$ docker run -it -p 4567:4567 ruanbekker/kinesis-local:latest
Listening at http://:::4567
</code></pre>

<p>Run the Producer from your Python Environment:</p>

<pre><code>$ python producer.py
Starting at 00:06:16
Finished at 00:06:42
</code></pre>

<p>Run the Consumer from your Python Environment:</p>

<pre><code>$ python consumer.py
Starting Consuming at 00:06:31
{u'age': 30, u'number': 1, u'name': u'pete'}
{u'age': 23, u'number': 2, u'name': u'ruan'}
{u'age': 22, u'number': 3, u'name': u'peter'}
{u'age': 45, u'number': 4, u'name': u'stefan'}
{u'age': 49, u'number': 5, u'name': u'tom'}
{u'age': 47, u'number': 6, u'name': u'pete'}
{u'age': 35, u'number': 7, u'name': u'stefan'}
{u'age': 45, u'number': 8, u'name': u'ruan'}
{u'age': 38, u'number': 9, u'name': u'frank'}
{u'age': 20, u'number': 10, u'name': u'tom'}
{u'age': 38, u'number': 11, u'name': u'james'}
{u'age': 20, u'number': 12, u'name': u'james'}
{u'age': 38, u'number': 13, u'name': u'tom'}
{u'age': 25, u'number': 14, u'name': u'tom'}
{u'age': 20, u'number': 15, u'name': u'peter'}
{u'age': 50, u'number': 16, u'name': u'james'}
{u'age': 29, u'number': 17, u'name': u'james'}
{u'age': 42, u'number': 18, u'name': u'pete'}
{u'age': 25, u'number': 19, u'name': u'pete'}
{u'age': 36, u'number': 20, u'name': u'tom'}
{u'age': 45, u'number': 21, u'name': u'peter'}
{u'age': 39, u'number': 22, u'name': u'ruan'}
{u'age': 43, u'number': 23, u'name': u'tom'}
{u'age': 38, u'number': 24, u'name': u'pete'}
{u'age': 40, u'number': 25, u'name': u'frank'}
Finshed Consuming at 00:06:35
</code></pre>

<h2>Thanks</h2>

<p><a href="https://saythanks.io/to/ruanbekker"><img src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" alt="Say Thanks!" /></a></p>

<p>Hope that was useful, feel free to check out <a href="https://aws.amazon.com/kinesis/">Amazon&rsquo;s Kinesis</a> out if you are planning to run this in any non-testing environment</p>

<center>
        <script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Buy Me a Coffee', '#46b798', 'A6423ZIQ');kofiwidget2.draw();</script>
</center>

]]></content>
  </entry>
  
</feed>
