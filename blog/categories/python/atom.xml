<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Python | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/python/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2020-09-27T17:21:05+00:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Get the Top 10 Items on Hackernews in Python]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/06/13/get-the-top-10-items-on-hackernews-in-python/"/>
    <updated>2020-06-13T19:53:20+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/06/13/get-the-top-10-items-on-hackernews-in-python</id>
    <content type="html"><![CDATA[<p>This is a quick post on how to use python to get the 10 latest items from hacker<a href="news:">news:</a></p>

<pre><code>import requests
import json

def get_top_ten():
    ids = requests.get('https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty').json()[0:10]
    for id in ids:
        postresponse = requests.get('https://hacker-news.firebaseio.com/v0/item/{postid}.json?print=pretty'.format(postid=id)).json()
        formatted = {"title": postresponse["title"], "type": postresponse["type"], "url": postresponse["url"], "by": postresponse["by"]}
        print(json.dumps(formatted, indent=2))
</code></pre>

<p>When running it:</p>

<pre><code>&gt;&gt;&gt; get_top_ten()
..
{
  "title": "Play Counter-Strike 1.6 in your browser",
  "type": "story",
  "url": "http://cs-online.club",
  "by": "m0ck"
}
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Improve MySQL Write Performance Using Batch Writes]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/06/13/improve-mysql-write-performance-using-batch-writes/"/>
    <updated>2020-06-13T19:31:32+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/06/13/improve-mysql-write-performance-using-batch-writes</id>
    <content type="html"><![CDATA[<p><img src="https://img.sysadmins.co.za/wngib2.png" alt="mysql-python-performance" /></p>

<p>I am no DBA, but I got curious when I noticed sluggish write performance on a mysql database, and I remembered somewhere that you should always use batch writes over sequential writes. So I decided to test it out, using a python script and a mysql server.</p>

<h2>What will we be doing</h2>

<p>I wrote a python script that writes 100,000 records to a database and keeps time of how long the writes took, 2 examples which I will compare:</p>

<ul>
<li>One script writing each record to the database</li>
<li>One script writing all the records as batch</li>
</ul>


<h2>Sequential Writes</h2>

<p>It took 48 seconds to write 100,000 records into a database using sequential writes:</p>

<pre><code class="python">...
for user in userids:
    userid = user["uid"]
    name = user["uid"].split('_')[0]
    job = random.choice(job)
    age = random.randint(24,49)
    credit_card_num = user["ccnum"]
    status = random.choice(["active", "inactive", "disabled"])

    cur.execute(
        """INSERT INTO customers(userid, name, job, age, credit_card_num, status) VALUES(%s, %s, %s, %s, %s, %s)""",
        (userid, name, job, age, credit_card_num, status)
    )
...
</code></pre>

<p>Running that shows us this:</p>

<pre><code>$ python3 mysql_seq_writes.py
start
writing customers to database
finish
inserted 100000 records in 48s
</code></pre>

<h2>Batch Writes</h2>

<p>It took 3 seconds to write to write 100,000 records using batch writes:</p>

<pre><code class="python">...
for user in userids:
    userid = user["uid"]
    name = user["uid"].split('_')[0]
    job = random.choice(job)
    age = random.randint(24,49)
    credit_card_num = user["ccnum"]
    status = random.choice(["active", "inactive", "disabled"])

    bunch_users.append((userid, name, job, age, credit_card_num, status))

cur.executemany(
    """INSERT INTO customers(userid, name, job, age, credit_card_num, status) VALUES(%s, %s, %s, %s, %s, %s)""",
    bunch_users
)
...
</code></pre>

<p>Running that shows us this:</p>

<pre><code>$ python3 mysql_batch_writes.py
start
writing customers to database
finish
inserted 100000 records in 3s
</code></pre>

<h2>Looking at the Scripts</h2>

<p>The script used for sequential writes:</p>

<pre><code class="python">import datetime
import random
import MySQLdb
from datetime import datetime as dt

host="172.18.0.1"
user="root"
password="password"
dbname="shopdb"
records=100000

db = MySQLdb.connect(host, user, password, dbname)

names = ['ruan', 'donovan', 'james', 'warren', 'angie', 'nicole', 'jenny', 'penny', 'amber']
job = ['doctor', 'scientist', 'teacher', 'police officer', 'waiter', 'banker', 'it']

cur = db.cursor()
cur.execute("DROP TABLE IF EXISTS customers")
cur.execute("CREATE TABLE customers(userid VARCHAR(50), name VARCHAR(50), surname VARCHAR(50), job VARCHAR(50), age INT(2), credit_card_num VARCHAR(50), status VARCHAR(10))")

bunch_users = []
userids = []

print("start")

def gen_id():
    return str(random.randint(0,9999)).zfill(4)

def gen_user(username):
    ccnum = '{0}-{1}-{2}-{3}'.format(gen_id(), gen_id(), gen_id(), gen_id())
    userid = username + '_' + ccnum.split('-')[0] + ccnum.split('-')[2]
    return {"uid": userid, "ccnum": ccnum}

for name in range(records):
    userids.append(gen_user(random.choice(names)))

print("writing customers to database")

timestart = int(dt.now().strftime("%s"))

for user in userids:
    userid = user["uid"]
    name = user["uid"].split('_')[0]
    job = random.choice(job)
    age = random.randint(24,49)
    credit_card_num = user["ccnum"]
    status = random.choice(["active", "inactive", "disabled"])

    #bunch_users.append((userid, name, job, age, credit_card_num, status))

    cur.execute(
        """INSERT INTO customers(userid, name, job, age, credit_card_num, status) VALUES(%s, %s, %s, %s, %s, %s)""",
        (userid, name, job, age, credit_card_num, status)
    )

db.commit()
db.close()
timefinish = int(dt.now().strftime("%s"))
print("finish")
print("inserted {} records in {}s".format(records, timefinish-timestart))
</code></pre>

<p>The script used for the batch writes:</p>

<pre><code class="python">import datetime
import random
import MySQLdb
from datetime import datetime as dt

host="172.18.0.1"
user="root"
password="password"
dbname="shopdb"
records=100000

db = MySQLdb.connect(host, user, password, dbname)

names = ['ruan', 'donovan', 'james', 'warren', 'angie', 'nicole', 'jenny', 'penny', 'amber']
job = ['doctor', 'scientist', 'teacher', 'police officer', 'waiter', 'banker', 'it']

cur = db.cursor()
cur.execute("DROP TABLE IF EXISTS customers")
cur.execute("CREATE TABLE customers(userid VARCHAR(50), name VARCHAR(50), surname VARCHAR(50), job VARCHAR(50), age INT(2), credit_card_num VARCHAR(50), status VARCHAR(10))")

bunch_users = []
userids = []

print("start")

def gen_id():
    return str(random.randint(0,9999)).zfill(4)

def gen_user(username):
    ccnum = '{0}-{1}-{2}-{3}'.format(gen_id(), gen_id(), gen_id(), gen_id())
    userid = username + '_' + ccnum.split('-')[0] + ccnum.split('-')[2]
    return {"uid": userid, "ccnum": ccnum}

for name in range(records):
    userids.append(gen_user(random.choice(names)))

for user in userids:
    userid = user["uid"]
    name = user["uid"].split('_')[0]
    job = random.choice(job)
    age = random.randint(24,49)
    credit_card_num = user["ccnum"]
    status = random.choice(["active", "inactive", "disabled"])

    bunch_users.append((userid, name, job, age, credit_card_num, status))

timestart = int(dt.now().strftime("%s"))

print("writing customers to database")
cur.executemany(
    """INSERT INTO customers(userid, name, job, age, credit_card_num, status) VALUES(%s, %s, %s, %s, %s, %s)""",
    bunch_users
)

db.commit()
db.close()
timefinish = int(dt.now().strftime("%s"))
print("finish")
print("inserted {} records in {}s".format(records, timefinish-timestart))
</code></pre>

<h2>Thanks</h2>

<p>Thanks for reading, so this was kind of interesting to see to never do sequential writes but write them in bulk when you have large amount of writes.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ingesting Pocket.com Links Into Elasticsearch]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/06/08/ingesting-pocket-items-into-elasticsearch/"/>
    <updated>2020-06-08T23:06:23+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/06/08/ingesting-pocket-items-into-elasticsearch</id>
    <content type="html"><![CDATA[<p><img src="https://img.sysadmins.co.za/wngib2.png" alt="python-elasticsearch-pocket" /></p>

<p>Links that I stumble upon, I always save to <a href="https://getpocket.com">getpocket.com</a> and tag them with the relevant info. So the one day I had this random idea to list my links per category on a web service and I was wondering how to approach that scenario, which lead me to this.</p>

<p>In this post we will consume all our saved bookmarks from pocket.com and ingest them into elasticsearch. But we dont want to read all the items from pocket&rsquo;s api every single time when the consumer run, therefore I have a method of checkpointing the last save run with a timestamp, so the next time it runs, we have context where to start from</p>

<h2>What will we be doing</h2>

<p>We will authenticate with pocket, then write the code how we will read the data from pocket and ingest them into elasticsearch.</p>

<h2>Authentication</h2>

<p>Head over to the <a href="https://getpocket.com/developer/apps/new">developer console</a> on pocket and create a new application then save your config in <code>config.py</code> which we will have as:</p>

<pre><code>application_name = "Awesome Links"
application_link = "https://getpocket.com/developer/app/x/x"
application_url = "https://awesome-links.domain"
consumer_key = "x"
access_token = "x"
es_host = ""
es_user = ""
es_pass = ""
</code></pre>

<p>Ensure that you have the requests library installed (<code>pip install requests</code>), the code that I used to get a access token:</p>

<pre><code>import config
import requests
import webbrowser
import time

CONSUMER_KEY = config.consumer_key
BASE_URL = "https://getpocket.com"
REDIRECT_URL = "localhost" # &lt;-- you can run python -m SimpleHTTPServer 80 to have a local server listening on port 80
HEADERS = {"Content-Type": "application/json; charset=UTF-8", "X-Accept": "application/json"}

def request_code():
    payload = {
        "consumer_key": CONSUMER_KEY,
        "redirect_uri": REDIRECT_URL,
    }
    response = requests.post("https://getpocket.com/v3/oauth/request", headers=HEADERS, json=payload)
    print("request_code")
    print(response.json())
    return response.json()["code"]

def request_access_token(code):
    payload = {
        "consumer_key": CONSUMER_KEY,
        "code": code,
    }
    response = requests.post("https://getpocket.com/v3/oauth/authorize", headers=HEADERS, json=payload)
    print("request_access_token")
    print(response.json())
    time.sleep(10)
    return response.json()["access_token"]

def request_authorization(code):
    url = "https://getpocket.com/auth/authorize?request_token={code}&amp;redirect_uri={redirect_url}".format(code=code, redirect_url=REDIRECT_URL)
    print("request_authorization")
    print(url)
    webbrowser.open(url, new=2)

def authenticate_pocket():
    code = request_code()
    request_authorization(code)
    return request_access_token(code)

authenticate_pocket()
# access_token will be returned
</code></pre>

<h2>Main App</h2>

<p>Once we have our access_token we can save that to our <code>config.py</code>, we will also be working with elasticsearch so we can add our elasticsearch info there as well:</p>

<pre><code>#!/usr/bin/env python

import config
import requests
import time

CONSUMER_KEY = config.consumer_key
ACCESS_TOKEN = config.access_token
HEADERS = {"Content-Type": "application/json; charset=UTF-8", "X-Accept": "application/json"}
ES_HOST = config.es_host
ES_USER = config.es_user
ES_PASS = config.es_pass

def write_checkpoint(timestamp):
    response = requests.put(
        'https://{eshost}/pocket-data/_doc/checkpoint'.format(eshost=ES_HOST),
        auth=(ES_USER, ES_PASS),
        json={
            "checkpoint_timestamp": timestamp
        }
    )
    return {"checkpoint_timestamp": timestamp}

def get_checkpoint():
    response = requests.get(
        'https://{eshost}/pocket-data/_doc/checkpoint'.format(ES_HOST),
        auth=(ES_USER, ES_PASS)
    )
    checkpoint_timestamp = response.json()['_source']['checkpoint_timestamp']
    return checkpoint_timestamp

def ingest_to_es(payload):
    response = requests.put(
        'https://{eshost}/pocket-data/_doc/{item_id}'.format(eshost=ES_HOST, item_id=payload['item_id']),
        auth=(ES_USER, ES_PASS),
        json=payload
    )
    return response.json()

def convert_timestamp(epoch):
    return time.strftime('%Y-%m-%d', time.localtime(int(epoch)))

def mapper(pocket_item):
    try:
        payload = {
            "item_id": pocket_item['item_id'],
            "time_added": convert_timestamp(pocket_item['time_added']),
            "url": pocket_item['resolved_url'],
            "title": pocket_item['resolved_title'],
            #"excerpt": pocket_item['excerpt'],
            "tags": list(pocket_item['tags'].keys())
        }
    except:
        print("error, item has been skipped:")
        print(pocket_item)
        payload = "skip"
    return payload

def ingest_pocket_items(payload):
    pocket_items = list()
    pocket_items.extend(payload['list'].keys())
    last_scraped_time = payload['since']
    number_of_items = len(pocket_items)
    print('got {} items from pocket'.format(len(pocket_items)))
    time.sleep(5)
    if len(pocket_items) &gt; 0:
        for pocket_item in pocket_items:
            mapped_payload = mapper(payload['list'][pocket_item])
            #print(mapped_payload)
            if mapped_payload != "skip":
                ingest_to_es(mapped_payload)
            print("Number of items left to ingest: {}".format(number_of_items))
            number_of_items-=1
    else:
        print('nothing new')
    print('writing checkpoint to es: {}'.format(last_scraped_time))
    write_checkpoint(last_scraped_time)
    return 'done'

def fetch_pocket_items(timestamp):
    response = requests.post(
        "https://getpocket.com/v3/get",
        headers=HEADERS,
        json={
            "consumer_key": CONSUMER_KEY,
            "access_token": ACCESS_TOKEN,
            "state": "all",
            "contentType": "article",
            "sort": "newest",
            "detailType": "complete",
            "since": int(timestamp)
        }
    )
    return response.json()

# get checkpoint
print('getting checkpoint id')
checkpoint_timestamp = get_checkpoint()
print('got checkpoint id: {}'.format(checkpoint_timestamp))
time.sleep(5)

# fetch items from pocket
print('fetch items from pocket')
pocket_response = fetch_pocket_items(checkpoint_timestamp)

# write
print('ingesting pocket items into es')
ingest_pocket_items(pocket_response)
</code></pre>

<p>So what we are doing here is that we are reading from the pocket api all the data that you saved in your account, and save the current time in epoch format, which we will need to tell our run when was the last time we consumed and keep that value in memory.</p>

<p>Then from the data we received, we will map the data that we are interested in, into key/value pairs and then ingest the data into elasticsearch.</p>

<p>After the initial ingestion has been done, which can take some time depending on how many items you have on pocket, as soon as it&rsquo;s done it will write the checkpoint time to elasticsearch so that the client know the next time from what time to search from again.</p>

<p>This way we dont ingest all the items again, testing it:</p>

<pre><code>$ python server.py
getting checkpoint id
got checkpoint id: 1591045652
fetch items from pocket
ingesting pocket items into es
got 2 items from pocket
Number of items left to ingest: 2
Number of items left to ingest: 1
writing checkpoint to es: 1591392580
</code></pre>

<p>Add one more item to pocket, then run our ingester again:</p>

<pre><code>$ python server.py
getting checkpoint id
got checkpoint id: 1591392580
fetch items from pocket
ingesting pocket items into es
got 1 items from pocket
Number of items left to ingest: 1
writing checkpoint to es: 1591650259
</code></pre>

<p>Search for one document on elasticsearch:</p>

<pre><code>$ curl -u user:pass 'https://es.domain/pocket-data/_search?pretty=true&amp;size=1'
{
  "took" : 194,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 766,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "pocket-data",
        "_type" : "_doc",
        "_id" : "2676106577",
        "_score" : 1.0,
        "_source" : {
          "item_id" : "2676106577",
          "time_added" : "2020-05-03",
          "url" : "https://programmaticponderings.com/2019/07/30/managing-aws-infrastructure-as-code-using-ansible-cloudformation-and-codebuild/",
          "title" : "Managing AWS Infrastructure as Code using Ansible, CloudFormation, and CodeBuild",
          "tags" : [
            "ansible",
            "aws",
            "cicd",
            "cloudformation",
            "devops"
          ]
        }
      }
    ]
  }
}
</code></pre>

<p>Search for aws tags:</p>

<pre><code>$ curl -u x:x 'https://es.domain/pocket-data/_search?q=tags:aws&amp;pretty=true&amp;size=1'
{
  "took" : 101,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 112,
    "max_score" : 2.6346242,
    "hits" : [
      {
        "_index" : "pocket-data",
        "_type" : "_doc",
        "_id" : "2673747670",
        "_score" : 2.6346242,
        "_source" : {
          "item_id" : "2673747670",
          "time_added" : "2019-07-28",
          "url" : "https://github.com/lgoodridge/serverless-chat",
          "title" : "lgoodridge/serverless-chat",
          "tags" : [
            "aws"
          ]
        }
      }
    ]
  }
}
</code></pre>

<h2>Now what</h2>

<p>Now that our data is in elasticsearch, we can build a search engine or a web application that can list our favorite links per category. I wil write up a post on the search engine in the future.</p>

<h2>Thank You</h2>

<p>If you liked this please send me a shout out on Twitter: <a href="https://twitter.com/ruanbekker">@ruanbekker</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Python RQ for Task Queues in Python]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/05/16/using-python-rq-for-task-queues-in-python/"/>
    <updated>2020-05-16T21:12:36+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/05/16/using-python-rq-for-task-queues-in-python</id>
    <content type="html"><![CDATA[<p>This is a getting started on python-rq tutorial and I will demonstrate how to work with asynchronous tasks using python redis queue (python-rq).</p>

<h2>What will we be doing</h2>

<p>We want a client to submit 1000&rsquo;s of jobs in a non-blocking asynchronous fashion, and then we will have workers which will consume these jobs from our redis queue, and process those tasks at the rate of what our consumer can handle.</p>

<p>The nice thing about this is that, if our consumer is unavailable for processing the tasks will remain in the queue and once the consumer is ready to consume, the tasks will be executed. It&rsquo;s also nice that its asynchronous, so the client don&rsquo;t have to wait until the task has finished.</p>

<p>We will run a redis server using docker, which will be used to queue all our jobs, then we will go through the basics in python and python-rq such as:</p>

<ul>
<li>Writing a Task</li>
<li>Enqueueing a Job</li>
<li>Getting information from our queue, listing jobs, job statuses</li>
<li>Running our workers to consume from the queue and action our tasks</li>
<li>Basic application which queues jobs to the queue, consumes and action them and monitors the queue</li>
</ul>


<h2>Redis Server</h2>

<p>You will require docker for this next step, to start the redis server:</p>

<pre><code>$ docker run --rm -itd --name redis -p 6379:6379 redis:alpine
</code></pre>

<h2>Python RQ</h2>

<p>Install python-rq:</p>

<pre><code>$ pip install rq
</code></pre>

<p>Create the task which will be actioned by our workers, in our case it will just be a simple function that adds all the numbers from a given string to a list, then adds them up and return the total value.</p>

<p>This is however a very basic task, but its just for demonstration.</p>

<p>Our <code>tasks.py</code>:</p>

<pre><code>def sum_numbers_from_string(string):
    numbers = []
    for each_character in string:
        if each_character.isdigit():
            numbers.append(int(each_character))
    total = 0
    for each_number in numbers:
        total=total+each_number

    return total
</code></pre>

<p>To test this locally:</p>

<pre><code>&gt;&gt;&gt; from tasks import sum_numbers_from_string
&gt;&gt;&gt; sum_numbers_from_string('adje-fje5-sjfdu1s-gdj9-asd1fg')
16
</code></pre>

<p>Now, lets import redis and redis-queue, with our tasks and instantiate a queue object:</p>

<pre><code>&gt;&gt;&gt; from redis import Redis
&gt;&gt;&gt; from rq import Connection, Queue, Worker
&gt;&gt;&gt; from tasks import sum_numbers_from_string
&gt;&gt;&gt; redis_connection = Redis(host='localhost', port=6379, db=0)
&gt;&gt;&gt; q = Queue(connection=redis_connection)
</code></pre>

<h2>Submit a Task to the Queue</h2>

<p>Let&rsquo;s submit a task to the queue:</p>

<pre><code>&gt;&gt;&gt; result = q.enqueue(sum_numbers_from_string, 'hbj2-plg5-2xf4r1s-f2lf-9sx4ff')
</code></pre>

<p>We have a couple of properties from <code>result</code> which we can inspect, first let&rsquo;s have a look at the id that we got back when we submitted our task to the queue:</p>

<pre><code>&gt;&gt;&gt; result.get_id()
'5a607474-cf1b-4fa5-9adb-f8437555a7e7'
</code></pre>

<p>We can also get the status from our task:</p>

<pre><code>&gt;&gt;&gt; result.get_status()
'queued'
</code></pre>

<p>We can also view our results in json format:</p>

<pre><code>&gt;&gt;&gt; import json
&gt;&gt;&gt; print(json.dumps(result.to_dict(), indent=2, default=str))
{
  "created_at": "2020-05-16T11:56:49.892713Z",
  "data": "b'..\\x00\\x99\\xa0\\x16\\xfe..'",
  "origin": "default",
  "description": "tasks.sum_numbers_from_string('hbj2-plg5-2xf4r1s-f2lf-9sx4ff')",
  "enqueued_at": "2020-05-16T11:56:49.893252Z",
  "started_at": "",
  "ended_at": "",
  "timeout": 180,
  "status": "queued"
}
</code></pre>

<p>If we dont have context of the job id, we can use <code>get_jobs</code> to get all the jobs which is queued:</p>

<pre><code>&gt;&gt;&gt; list_jobs = q.get_jobs
&gt;&gt;&gt; list_jobs()
[Job('5a607474-cf1b-4fa5-9adb-f8437555a7e7', enqueued_at=datetime.datetime(2020, 5, 16, 12, 30, 22, 699609))]
</code></pre>

<p>Then we can loop through the results and get the id like below:</p>

<pre><code>&gt;&gt;&gt; for j in list_jobs():
...     j.id
...
'5a607474-cf1b-4fa5-9adb-f8437555a7e7'
</code></pre>

<p>Or to get the job id&rsquo;s in a list:</p>

<pre><code>&gt;&gt;&gt; list_job_ids = q.get_job_ids()
&gt;&gt;&gt; list_job_ids
['5a607474-cf1b-4fa5-9adb-f8437555a7e7']
</code></pre>

<p>Since we received the job id, we can use <code>fetch_job</code> to get more info about the job:</p>

<pre><code>&gt;&gt;&gt; fetched_job = q.fetch_job('5a607474-cf1b-4fa5-9adb-f8437555a7e7')
&gt;&gt;&gt; fetched_job
Job('5a607474-cf1b-4fa5-9adb-f8437555a7e7', enqueued_at=datetime.datetime(2020, 5, 16, 12, 30, 22, 699609))
</code></pre>

<p>And as before we can view it in json format:</p>

<pre><code>&gt;&gt;&gt; fetched_job.to_dict()
{'created_at': '2020-05-16T12:30:22.698728Z', 'data': b'..x\x9c6\xfe..', 'origin': 'queue1', 'description': "tasks.sum_numbers_from_string('hbj2-plg5-2xf4r1s-f2lf-9sx4ff')", 'enqueued_at': '2020-05-16T12:30:22.699609Z', 'started_at': '', 'ended_at': '', 'timeout': 180, 'status': 'queued'}
</code></pre>

<p>We can also view the key in redis by passing the job_id:</p>

<pre><code>&gt;&gt;&gt; result.key_for(job_id='5a607474-cf1b-4fa5-9adb-f8437555a7e7')
b'rq:job:5a607474-cf1b-4fa5-9adb-f8437555a7e7'
</code></pre>

<p>To view how many jobs are in our queue, we can either do:</p>

<pre><code>&gt;&gt;&gt; len(q)
1
</code></pre>

<p>or:</p>

<pre><code>&gt;&gt;&gt; q.get_job_ids()
['5a607474-cf1b-4fa5-9adb-f8437555a7e7']
</code></pre>

<h2>Consuming from the Queue</h2>

<p>Now that our task is queued, let&rsquo;s fire of our worker to consume the job from the queue and action the task:</p>

<pre><code>&gt;&gt;&gt; w = Worker([q], connection=redis_connection)
&gt;&gt;&gt; w.work()
14:05:35 Worker rq:worker:49658973741d4085961e34e9641227dd: started, version 1.4.1
14:05:35 Listening on default...
14:05:35 Cleaning registries for queue: default
14:05:35 default: tasks.sum_numbers_from_string('hbj2-plg5-2xf4r1s-f2lf-9sx4ff') (5a607474-cf1b-4fa5-9adb-f8437555a7e7)
14:05:40 default: Job OK (5a607474-cf1b-4fa5-9adb-f8437555a7e7)
14:05:40 Result is kept for 500 seconds
14:05:59 Warm shut down requested
True
</code></pre>

<p>Now, when we get the status of our job, you will see that it finished:</p>

<pre><code>&gt;&gt;&gt; result.get_status()
'finished'
</code></pre>

<p>And to get the result from our worker:</p>

<pre><code>&gt;&gt;&gt; result.result
29
</code></pre>

<p>And like before, if you dont have context of your job id, you can get the job id, then return the result:</p>

<pre><code>&gt;&gt;&gt; result = fetched_job = q.fetch_job('5a607474-cf1b-4fa5-9adb-f8437555a7e7')
&gt;&gt;&gt; result.result
29
</code></pre>

<h2>Naming Queues</h2>

<p>We can namespace our tasks into specific queues, for example if we want to create <code>queue1</code>:</p>

<pre><code>&gt;&gt;&gt; q1 = Queue('queue1', connection=redis_connection)
</code></pre>

<p>To verify the queue name:</p>

<pre><code>&gt;&gt;&gt; q1
Queue('queue1')
</code></pre>

<p>As we can see our queue is empty:</p>

<pre><code>&gt;&gt;&gt; q1.get_job_ids()
[]
</code></pre>

<p>Let&rsquo;s submit 10 jobs to our queue:</p>

<pre><code>&gt;&gt;&gt; from uuid import uuid4
&gt;&gt;&gt; for attempt in range(0,10):
...     random_string = uuid4().hex
...     q1.enqueue(sum_numbers_from_string, random_string)
...
Job('c3f2369d-5b27-40e0-97be-8fe26989a78e', enqueued_at=datetime.datetime(2020, 5, 16, 13, 1, 14, 472508))
Job('06b93517-5dae-4133-8131-e8d35b8dd780', enqueued_at=datetime.datetime(2020, 5, 16, 13, 1, 14, 475604))
Job('81f05aef-4bd6-421b-912d-78b5d419b10a', enqueued_at=datetime.datetime(2020, 5, 16, 13, 1, 14, 478071))
Job('8f14e81f-74fa-44d9-9fc7-e8e7b8c7b76f', enqueued_at=datetime.datetime(2020, 5, 16, 13, 1, 14, 480438))
Job('e8552750-89d2-4538-8c3e-a48c4c3e9a51', enqueued_at=datetime.datetime(2020, 5, 16, 13, 1, 14, 483106))
Job('bf19a0a3-eb0c-4692-b452-67c5ad954094', enqueued_at=datetime.datetime(2020, 5, 16, 13, 1, 14, 486193))
Job('0da3688a-cffa-4ba6-a272-b6cc90942ef6', enqueued_at=datetime.datetime(2020, 5, 16, 13, 1, 14, 488545))
Job('717bd147-615c-458d-8386-9ea6a198e137', enqueued_at=datetime.datetime(2020, 5, 16, 13, 1, 14, 491074))
Job('7cdac5aa-8dc3-40be-a8fc-b273ce61b03b', enqueued_at=datetime.datetime(2020, 5, 16, 13, 1, 14, 493618))
Job('4f7ea527-0695-4e2b-bc8b-3d8807a86390', enqueued_at=datetime.datetime(2020, 5, 16, 13, 1, 14, 496930))
</code></pre>

<p>To verify the number of jobs in our queue:</p>

<pre><code>&gt;&gt;&gt; q1.get_job_ids()
['c3f2369d-5b27-40e0-97be-8fe26989a78e', '06b93517-5dae-4133-8131-e8d35b8dd780', '81f05aef-4bd6-421b-912d-78b5d419b10a', '8f14e81f-74fa-44d9-9fc7-e8e7b8c7b76f', 'e8552750-89d2-4538-8c3e-a48c4c3e9a51', 'bf19a0a3-eb0c-4692-b452-67c5ad954094', '0da3688a-cffa-4ba6-a272-b6cc90942ef6', '717bd147-615c-458d-8386-9ea6a198e137', '7cdac5aa-8dc3-40be-a8fc-b273ce61b03b', '4f7ea527-0695-4e2b-bc8b-3d8807a86390']
</code></pre>

<p>And to count them:</p>

<pre><code>&gt;&gt;&gt; len(q1)
10
</code></pre>

<h2>Cleaning the Queue</h2>

<p>Cleaning the queue can either be done with:</p>

<pre><code>&gt;&gt;&gt; q.empty()
10
</code></pre>

<p>or</p>

<pre><code>&gt;&gt;&gt; q.delete(delete_jobs=True)
</code></pre>

<p>Then to verify that our queue is clean:</p>

<pre><code>&gt;&gt;&gt; q.get_job_ids()
[]
</code></pre>

<h2>Naming Workers</h2>

<p>The same way that we defined a name for our queue, we can define a name for our workers:</p>

<pre><code>&gt;&gt;&gt; worker = Worker([q1], connection=redis_connection, name='worker1')
&gt;&gt;&gt; worker.work()
</code></pre>

<p>Which means you can have different workers consuming jobs from specific queues.</p>

<h2>Resources</h2>

<p>Documentation:</p>

<ul>
<li><a href="https://python-rq.org/docs/">https://python-rq.org/docs/</a></li>
<li><a href="https://python-rq.org/docs/workers/">https://python-rq.org/docs/workers/</a></li>
<li><a href="https://python-rq.org/docs/monitoring/">https://python-rq.org/docs/monitoring/</a></li>
</ul>


<h2>Thank You</h2>

<p>I hope this was usful, if you enjoyed this come say hi on Twitter <a href="https://twitter.com/ruanbekker">@ruanbekker</a> or visit my website at <a href="https://ruan.dev">ruan.dev</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Graphing Covid-19 Stats With Grafana and Elasticsearch Using Python]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/04/26/graphing-covid-19-stats-with-grafana-and-elasticsearch-using-python/"/>
    <updated>2020-04-26T02:24:27+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/04/26/graphing-covid-19-stats-with-grafana-and-elasticsearch-using-python</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/80421197-62345180-88dc-11ea-9e0a-557199aaf613.png" alt="coronavirus-covid19-grafana-metrics" /></p>

<p>I stumbled upon a <a href="https://github.com/pomber/covid19/">github repository</a> that stores time-series data in json format of corona virus / covid19 statistics, which get updated daily.</p>

<p>I was curious to see data about my country and want to see how metrics will look like after our lockdown started, so I decided to consume that data with <strong>Python</strong> and the requests library, then ingest data about covid19 into <strong>Elasticsearch</strong> and the visualize the data with <strong>Grafana</strong>.</p>

<h2>Sample of the Data</h2>

<p>Let&rsquo;s have a peek at the data to determine how we will use it to write to Elasticsearch. Let&rsquo;s consume the data with python:</p>

<pre><code>&gt;&gt;&gt; import requests
&gt;&gt;&gt; import json
&gt;&gt;&gt; response = requests.get('https://pomber.github.io/covid19/timeseries.json').json()
</code></pre>

<p>Now let&rsquo;s determine the data type:</p>

<pre><code>&gt;&gt;&gt; type(response)
&lt;type 'dict'&gt;
</code></pre>

<p>Now as it&rsquo;s a dictionary, let&rsquo;s look at they keys:</p>

<pre><code>&gt;&gt;&gt; response.keys()
[u'Canada', u'Sao Tome and Principe', u'Lithuania', u'Cambodia', u'Ethiopia',....
</code></pre>

<p>So let&rsquo;s take a look how the data looks like if we do a lookup for Canada:</p>

<pre><code>&gt;&gt;&gt; type(response['Canada'])
&lt;type 'list'&gt;
</code></pre>

<p>As we can see it&rsquo;s a list, we can count how many items is in our list:</p>

<pre><code>&gt;&gt;&gt; len(response['Canada'])
94
</code></pre>

<p>Now let&rsquo;s peek at the data by accessing our first index of our list:</p>

<pre><code>&gt;&gt;&gt; response['Canada'][0]
{u'date': u'2020-1-22', u'confirmed': 0, u'recovered': 0, u'deaths': 0}
</code></pre>

<p>So our data will look like this:</p>

<pre><code>{
  [
    'Country Name': [
      {
        'date': '&lt;string&gt;', 
        'confirmed': '&lt;int&gt;', 
        'recovered': '&lt;int&gt;', 
        'deaths': '&lt;int&gt;'
      },
      {
        'date': '&lt;string&gt;',
        'confirmed': '&lt;int&gt;',
        'recovered': '&lt;int&gt;',
        'deaths': '&lt;int&gt;'
      },
    ],
    'Country Name': [
      ...
    ]
  ]
}
</code></pre>

<h2>Some issues we need to fix</h2>

<p>As you can see the date is displayed as <code>2020-1-22</code> instead of <code>2020-01-22</code>, I want to make it consistent as I will be ingesting the data with a <code>@timestamp</code> key which we will use the date from the returned data. So first we will need to convert that before we ingest the data.</p>

<p>The other thing I was thinking of is that, if for some reason we need to ingest this data again, we dont want to sit with duplicates (same document with different _id&rsquo;s), so for that I decided to generate a hash value that consist of the date and the country, so if the script run to ingest the data, it will use the same id for the specific document, which would just overwrite it, therefore we won&rsquo;t sit with duplicates.</p>

<p>So the idea is to ingest a document to elasticsearch like this:</p>

<pre><code>doc = {
    "_id": "sha_hash_value",
    "day": "2020-01-22",
    "timestamp": "@2020-01-22 00:00:00",
    "country": "CountryName",
    "confirmed": 0,
    "recovered": 0,
    "deaths": 0
}
</code></pre>

<h2>How we will ingest the data</h2>

<p>The first run will load all the data and ingest all the data up to the current day to elasticsearch. Once that is done, we will add code to our script to only ingest the most recent day&rsquo;s data into elasticsearch, which we will control with a cronjob.</p>

<p>Create a index with a mapping to let Elasticsearch know <code>timestamp</code> will be a date field:</p>

<pre><code>$ curl -XPUT -H 'Content-Type: application/json' \
  -u username:pass 'https://es.domain.com/coronastats' -d \
  '{"mappings": {"foo1": {"properties": {"timestamp" : {"type" : "date","format" : "yyyy-MM-dd HH:mm:ss"}}}}}'
</code></pre>

<p>Once our index is created, create the python script that will load the data, loop through each country&rsquo;s daily data and ingest it into elasticsearch:</p>

<pre><code class="python">#!/usr/bin/python
import requests
import datetime as dt
import json
import hashlib

url = 'https://pomber.github.io/covid19/timeseries.json'
elasticsearch_url = "https://es.domain.com"
elasticsearch_username = ""
elasticsearch_password = ""

api_response = requests.get(url).json()

def convert_datestamp(day):
    return str(dt.datetime.strptime(day, '%Y-%m-%d'))

def hash_function(country, date):
    string_to_hash = country + date
    hash_obj  = hashlib.sha1(string_to_hash.encode('utf-8'))
    hash_value = hash_obj.hexdigest()
    return hash_value

def map_es_doc(payload, country):
    doc = {
        "day": payload['date'],
        "timestamp": convert_datestamp(payload['date']),
        "country": country,
        "confirmed": payload['confirmed'],
        "recovered": payload['recovered'],
        "deaths": payload['deaths']
    }
    return doc

def ingest(doc_id, payload):
    response = requests.put(
        elasticsearch_url + '/coronastats/coronastats/' + doc_id,
        auth=(elasticsearch_username, elasticsearch_password),
        headers={'content-type': 'application/json'},
        json=payload
    )
    return response.status_code

for country in api_response.keys():
    try:
        for each_payload in api_response[country]:
            doc_id = hash_function(country, each_payload['date'])
            doc = map_es_doc(each_payload, country)
            response = ingest(doc_id, doc)
            print(response)
    except Exception as e:
        print(e)
</code></pre>

<p>Run the script to ingest all the data into elasticsearch. Now we will create the script that will run daily to only ingest the previous day&rsquo;s data, so that we only ingest the latest data and not all the data from scratch again.</p>

<p>I will create this file in <code>/opt/scripts/corona_covid19_ingest.py</code>:</p>

<pre><code>#!/usr/bin/python
import requests
import datetime as dt
import json
import hashlib

url = 'https://pomber.github.io/covid19/timeseries.json'
elasticsearch_url = "https://es.domain.com"
elasticsearch_username = ""
elasticsearch_password = ""

api_response = requests.get(url).json()

yesterdays_date = dt.date.today() - dt.timedelta(days=1)

def convert_datestamp(day):
    return str(dt.datetime.strptime(day, '%Y-%m-%d'))

def hash_function(country, date):
    string_to_hash = country + date
    hash_obj  = hashlib.sha1(string_to_hash.encode('utf-8'))
    hash_value = hash_obj.hexdigest()
    return hash_value

def map_es_doc(payload, country):
    doc = {
        "day": payload['date'],
        "timestamp": convert_datestamp(payload['date']),
        "country": country,
        "confirmed": payload['confirmed'],
        "recovered": payload['recovered'],
        "deaths": payload['deaths']
    }
    return doc

def ingest(doc_id, payload):
    response = requests.put(
        elasticsearch_url + '/coronastats/coronastats/' + doc_id,
        auth=(elasticsearch_username, elasticsearch_password),
        headers={'content-type': 'application/json'},
        json=payload
    )
    return response.status_code

for country in api_response.keys():
    try:
        for each_payload in api_response[country]:
            if convert_datestamp(each_payload['date']).split()[0] == str(yesterdays_date):
                print("ingesting latest data for {country}".format(country=country))
                doc_id = hash_function(country, each_payload['date'])
                doc = map_es_doc(each_payload, country)
                response = ingest(doc_id, doc)
                print(response)
    except Exception as e:
        print(e)
</code></pre>

<p>The only difference with this script is that it checks if the date is equals to yesterday&rsquo;s date, and if so the document will be prepared and ingested into elasticsearch. We will create a cronjob that runs this script every morning at 08:45.</p>

<p>First make the file executable:</p>

<pre><code>$ chmod +x /opt/scripts/corona_covid19_ingest.py
</code></pre>

<p>Run <code>crontab -e</code> and add the following</p>

<pre><code>45 8 * * * /opt/scripts/corona_covid19_ingest.py
</code></pre>

<h2>Visualize the Data with Grafana</h2>

<p>We will create this dashboard:</p>

<p><img src="https://user-images.githubusercontent.com/567298/80418135-35ca0680-88d7-11ea-83f6-3432a903333d.png" alt="corona-covid-19-dashboard" /></p>

<p>We need a elasticsearch datasource that points to the index that we ingest our data into. Head over to datasources, add a elasticsearch datasource and set the index to <code>coronastats</code> and add the timefield as <code>timestamp</code>.</p>

<p>We want to make the dashboard dynamic to have a <strong>&ldquo;country&rdquo;</strong> dropdown selector, for that go to the dashboard settings, select variable and add a country variable:</p>

<p><img src="https://user-images.githubusercontent.com/567298/80419463-7cb8fb80-88d9-11ea-959f-8f37ae3f6dc7.png" alt="covid19-dashboard-variables" /></p>

<p>First panel: &ldquo;Reported Cases per Day&rdquo;:</p>

<p><img src="https://user-images.githubusercontent.com/567298/80419572-af62f400-88d9-11ea-802e-7eeacb61ee19.png" alt="covid19-reported-cases" /></p>

<p>Second panel: &ldquo;Confirmed Cases&rdquo;:</p>

<p><img src="https://user-images.githubusercontent.com/567298/80419675-db7e7500-88d9-11ea-98a5-3aae4d9a6c87.png" alt="covid19-confirmed-cases" /></p>

<p>Third panel: &ldquo;Recovered Cases&rdquo;:</p>

<p><img src="https://user-images.githubusercontent.com/567298/80419750-fa7d0700-88d9-11ea-82a3-f26ff8c807ef.png" alt="covid19-recovered-cases" /></p>

<p>Now, if we select Italy, Spain and France as an example, we will see something like this:</p>

<p><img src="https://user-images.githubusercontent.com/567298/80419966-56479000-88da-11ea-8f30-39ac3da27007.png" alt="covid19-country-stats" /></p>

<h2>Thank You</h2>

<p>Although its pretty cool visualizing data, the issue that we are in at the moment with coronavirus / covid19 is really scary and we should all do our part to try and stay home, sanitize and try not to spread the virus. Together we can all do great things by reducing the spread of this virus.</p>

<p>Stay safe everyone.</p>
]]></content>
  </entry>
  
</feed>
