<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: S3 | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/s3/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2022-05-22T18:55:35-04:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Setup AWS S3 Cross Account Access]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/11/26/setup-aws-s3-cross-account-access/"/>
    <updated>2019-11-26T22:40:12+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/11/26/setup-aws-s3-cross-account-access</id>
    <content type="html"><![CDATA[<p><a href="https://saythanks.io/to/ruanbekker"><img src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" alt="Say Thanks!" /></a> <a href="https://linux-hackers-slack.herokuapp.com/"><img src="https://linux-hackers-slack.herokuapp.com/badge.svg" alt="Slack Status" /></a> <a href="https://linux-hackers.slack.com/"><img src="https://img.shields.io/badge/chat-on_slack-orange.svg" alt="Chat on Slack" /></a> <a href="https://github.com/ruanbekker"><img src="https://img.shields.io/github/followers/ruanbekker.svg?label=Follow&amp;style=social" alt="GitHub followers" /></a></p>

<p><a href="https://twitter.com/ruanbekker?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @ruanbekker</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>

<p>In this tutorial I will demonstrate how to setup cross account access from S3.</p>

<p><a href="https://github.com/ruanbekker/cheatsheets" target="_blank"><img alt="ruanbekker-cheatsheets" src="https://user-images.githubusercontent.com/567298/169162832-ef3019de-bc49-4d6c-b2a6-8ac17c457d24.png"></a></p>

<h2>Scenario</h2>

<p>We will have 2 AWS Accounts:</p>

<ol>
<li><p>a Green AWS Account which will host the IAM Users, this account will only be used for our IAM Accounts.</p></li>
<li><p>a Blue AWS Account which will be the account that hosts our AWS Resources, S3 in this scenario.</p></li>
</ol>


<p>We will the allow the Green Account to access the Blue Account&rsquo;s S3 Bucket.</p>

<h2>Setup the Blue Account</h2>

<p>In the Blue Account, we will setup the S3 Bucket, as well as the Trust Relationship with the Policy, which is where we will define what we want to allow for the Green Account.</p>

<p><img width="1280" alt="9488F107-A5B0-4A9E-A7A4-5A91B9805DE3" src="https://user-images.githubusercontent.com/567298/69668149-fe40ff00-1097-11ea-896a-5f3106fe5dfa.png"></p>

<p>Now we need to setup the IAM Role which will allow the Green Account and also define what needs to be allowed.</p>

<p>Go ahead to your IAM Console and create a IAM Policy (just remember to replace the bucket name if you are following along)</p>

<pre><code>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "PutGetListAccessOnS3",
            "Effect": "Allow",
            "Action": [
                "s3:PutObject",
                "s3:GetObject",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::ruanbekker-prod-s3-bucket",
                "arn:aws:s3:::ruanbekker-prod-s3-bucket/*"
            ]
        }
    ]
}
</code></pre>

<p>In my case I have named my IAM Policy <code>CrossAccountS3Access</code>. After you have created your IAM Policy, go ahead and create a IAM Role. Here we need the source account that we want to allow as a trusted entity, which will be the AWS AccountId of the Green Account:</p>

<p><img width="1277" alt="E73FC957-EBFA-4E41-AFDB-D994D6D3110E" src="https://user-images.githubusercontent.com/567298/69668615-ee75ea80-1098-11ea-8536-b32c6c034f7a.png"></p>

<p>Associate the IAM Policy that you created earlier:</p>

<p><img width="1280" alt="610814A8-E8CB-45F7-A038-FE4274FD425C" src="https://user-images.githubusercontent.com/567298/69668712-19603e80-1099-11ea-8ba0-2d0bc84e21cf.png"></p>

<p>After you have done that, you should see a summary screen:</p>

<p><img width="1278" alt="ABAADD0E-9140-4EB1-855A-0B0E46F429FF" src="https://user-images.githubusercontent.com/567298/69668817-50ceeb00-1099-11ea-8bb2-98537a742857.png"></p>

<p>Make note of your IAM Role ARN, it will look something like this: <code>arn:aws:iam::xxxxxxxxxxxx:role/CrossAccountS3Access-Role</code></p>

<h2>Setup the Green Account</h2>

<p>In the Green Account is where we will create the IAM User and the credentials will be provided to the user which requires to access the S3 Bucket.</p>

<p>Let&rsquo;s create a IAM Group, I will name mine <code>prod-s3-users</code>. I will just create the group, as I will attach the policy later:</p>

<p><img width="1280" alt="459D98BF-7A5D-49B4-BBD9-11717655188D" src="https://user-images.githubusercontent.com/567298/69669190-07cb6680-109a-11ea-8193-db476f5fa1db.png"></p>

<p>From the IAM Group, select the Permissions tab and create a New Inline Policy:</p>

<p><img width="1280" alt="E55E521D-A3C1-4669-B0AB-C23A5BA51E21" src="https://user-images.githubusercontent.com/567298/69669427-81635480-109a-11ea-8b4b-7bd79f2a12cd.png"></p>

<p>Select the &ldquo;STS&rdquo; service, select the &ldquo;AssumeRole&rdquo; action, and provide the Role ARN of the Blue Account that we created earlier:</p>

<p><img width="1280" alt="FDECEF7C-14F1-41DC-94F5-B6E63FE46A7D" src="https://user-images.githubusercontent.com/567298/69669597-d8692980-109a-11ea-804c-914c9a8cb608.png"></p>

<p>This will allow the Blue account to assume the credentials from the Green account. And the Blue account will only obtain permissions to access the resources that we have defined in the policy document of the Blue Account. In summary, it should look like this:</p>

<p><img width="1280" alt="0133A1AF-D2B0-4A61-B179-B4B40B81953C" src="https://user-images.githubusercontent.com/567298/69669773-30079500-109b-11ea-83bd-69c8301c4f21.png"></p>

<p>Select the Users tab on the left hand side, create a New IAM User (I will name mine s3-prod-user) and select the &ldquo;Programmatic Access&rdquo; check box as we need API keys as we will be using the CLI to access S3:</p>

<p><img width="1278" alt="ACE1F066-4400-4000-A9D8-0FD438DB7028" src="https://user-images.githubusercontent.com/567298/69669927-82e14c80-109b-11ea-9adf-de5c01cec41c.png"></p>

<p>Then from the next window, add the user to the group that we have created earlier:</p>

<p><img width="1279" alt="0AEC8E84-091F-44CB-966D-BDA93970C881" src="https://user-images.githubusercontent.com/567298/69669976-9987a380-109b-11ea-9c16-ea63cebe2e82.png"></p>

<h2>Test Cross Account Access</h2>

<p>Let&rsquo;s configure our AWS CLI with the API Keys that we received. Our credential provider will consist with 2 profiles, the Green Profile which holds the API Keys of the Green Account:</p>

<pre><code>$ aws configure --profile green
AWS Access Key ID [None]: AKIATPRT2G4SAHA7ZQU2
AWS Secret Access Key [None]: x
Default region name [None]: eu-west-1
Default output format [None]: json
</code></pre>

<p>And configure the Blue profile that will reference the Green account as a source profile and also specify the IAM Role ARN of the Blue Account:</p>

<pre><code>$ vim ~/.aws/credentials
</code></pre>

<pre><code>[blue]
role_arn=arn:aws:iam::xxxxxxxxxxxx:role/CrossAccountS3Access-Role
source_profile=green
region=eu-west-1
</code></pre>

<p>Now we can test if we can authenticate with our Green AWS Account:</p>

<pre><code>$ aws --profile green sts get-caller-identity
{
    "UserId": "AKIATPRT2G4SAHA7ZQU2",
    "Account": "xxxxxxxxxxxx",
    "Arn": "arn:aws:iam:: xxxxxxxxxxxx:user/s3-prod-user"
}
</code></pre>

<p>Now let&rsquo;s upload an object to S3 using our blue profile:</p>

<pre><code>$ aws --profile blue s3 cp foo s3://ruanbekker-prod-s3-bucket/
upload: ./foo to s3://ruanbekker-prod-s3-bucket/foo
</code></pre>

<p>Let&rsquo;s verify if we can see the object:</p>

<pre><code>$ aws --profile blue s3 ls s3://ruanbekker-prod-s3-bucket/
2019-10-03 22:13:30      14582 foo
</code></pre>

<h2>Thank You</h2>

<p>Let me know what you think. If you liked my content, feel free to checkout my content on <strong><a href="https://ruan.dev/">ruan.dev</a></strong> or follow me on twitter at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong></p>

<center><script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Buy Me a Coffee', '#46b798', 'A6423ZIQ');kofiwidget2.draw();</script></center>


<p><br></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Expire Objects in AWS S3 Automatically After 30 Days]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/09/12/expire-objects-in-aws-s3-automatically-after-30-days/"/>
    <updated>2019-09-12T22:37:11+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/09/12/expire-objects-in-aws-s3-automatically-after-30-days</id>
    <content type="html"><![CDATA[<p><img src="https://objects.ruanbekker.com/assets/images/aws-logo.png" alt="" /></p>

<p>In AWS S3 you can make use of lifecycle policies to manage the lifetime of your objects stored in S3.</p>

<p>In this tutorial, I will show you how to delete objects automatically from S3 after 30 days.</p>

<h2>Navigate to your Bucket</h2>

<p>Head over to your AWS S3 bucket where you want to delete objects after they have been stored for 30 days:</p>

<p><img width="1039" alt="0400F9CB-9223-4FDF-8FA5-D0BC1FA8EB71" src="https://user-images.githubusercontent.com/567298/64819546-c3f2b600-d5ae-11e9-93ba-13777e9b02b0.png"></p>

<h2>Lifecycle Policies</h2>

<p>Select &ldquo;Management&rdquo; and click on &ldquo;Add lifecycle rule&rdquo;:</p>

<p><img width="701" alt="9BB26C7C-F251-45C4-AE44-A34459BD0F4B" src="https://user-images.githubusercontent.com/567298/64819628-f00e3700-d5ae-11e9-9740-8aa3608163a7.png"></p>

<p>Set a rule name of choice and you have the option to provide a prefix if you want to delete objects based on a specific prefix. I will leave this blank as I want to delete objects in the root level of the bucket. Head to next on the following section:</p>

<p><img width="700" alt="AEF8B151-3FA8-454F-AC71-778A531BD1EE" src="https://user-images.githubusercontent.com/567298/64819785-58f5af00-d5af-11e9-8485-fb0dca3a02ac.png"></p>

<p>From the &ldquo;Transitions&rdquo; section, configure the transition section, by selecting to expire the current version of the object after 30 days:</p>

<p><img width="701" alt="2B395671-A4C0-4E5A-82E7-00EE6579DB5A" src="https://user-images.githubusercontent.com/567298/64819851-7c205e80-d5af-11e9-98d7-7e1dd09bcfef.png"></p>

<p>Review the configuration:</p>

<p><img width="705" alt="F7F8E800-62FF-4156-B506-5FB9BCC148E0" src="https://user-images.githubusercontent.com/567298/64819869-893d4d80-d5af-11e9-8034-8a2e3a8939f8.png"></p>

<p>When you select &ldquo;Save&rdquo;, you should be returned to the following section:</p>

<p><img width="1041" alt="8421EBCE-9503-4259-92AA-DB66C6F532AF" src="https://user-images.githubusercontent.com/567298/64819895-99edc380-d5af-11e9-84b4-7f4cc69cfd2e.png"></p>

<h2>Housecleaning on your S3 Bucket</h2>

<p>Now 30 days after you created objects on AWS S3, they will be deleted.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AWS S3 KMS and Python for Secrets Management]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/09/04/aws-s3-kms-and-python-for-secrets-management/"/>
    <updated>2019-09-04T19:58:45+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/09/04/aws-s3-kms-and-python-for-secrets-management</id>
    <content type="html"><![CDATA[<p><img src="https://miro.medium.com/max/2400/1*9PSzVZDHjr321CpxJHcxPQ.png" alt="" /></p>

<p>So your application need to store secrets and you are looking for a home for them. In this tutorial we will see how we can use Python, S3 and KMS to build our own solution for managing secrets.</p>

<p>There is SSM and Secrets Manager that probably does a better job, but my mind got curious :D</p>

<h2>High Level Goal</h2>

<p>From a High-Level we want to store secrets encrypted on S3 with KMS, namespaced with <strong>team/application/environment/value</strong> in json format so that our application receives the json dictionary of configured key/value pairs.</p>

<p>We can leverage <strong>IAM</strong> to delegate permissions on the namespacing that we decide on, for my example the namespace will look like this on S3:</p>

<pre><code>s3://s3bucket/secrets/engineering/app1/production/appconfig.json
</code></pre>

<p>We will apply <strong>IAM</strong> permissions for our user to only <strong>Put</strong> and <strong>Get</strong> on <code>secrets/engineering*</code>. So with this idea we can apply IAM permissions on groups for different departments, or even let users manage their own secrets such as:</p>

<pre><code>s3://s3bucket/secrets/personal/user.name/app/appconfig.json
</code></pre>

<p>After the object has been downloaded from S3 and decrypted using KMS, the value of the object will look like this:</p>

<pre><code>{u'surname': u'bekker', u'name': u'ruan', u'job_title': u'systems-development-engineer'}
</code></pre>

<h2>Requirements</h2>

<p>We will create the following resources on AWS:</p>

<ul>
<li>KMS Key</li>
<li>S3 Bucket</li>
<li>IAM User</li>
<li>IAM Policy</li>
<li>Python Dependencies: Boto3</li>
</ul>


<h2>Provision AWS Resources</h2>

<p><img src="https://miro.medium.com/max/2728/1*Lq9xaUXuNo2Nb8kQakYdsg.png" alt="" /></p>

<p>First we will create our <strong>S3 Bucket</strong>,  head over to <a href="https://s3.console.aws.amazon.com/s3/home?region=eu-west-1">Amazon S3</a> create a new s3 bucket, make sure that the bucket is <strong>NOT</strong> public, by using the default configuration, you should be good.</p>

<p>Once your S3 Bucket is provisioned, head over to <a href="https://console.aws.amazon.com/iam/home#/users">Amazon IAM</a> and create a IAM User, enable programmatic access, and keep your access key and secret key safe. For now we will not apply any permissions as we will come back to this step.</p>

<p>Head over to <a href="https://eu-west-1.console.aws.amazon.com/kms/home?region=eu-west-1#/kms/home">Amazon KMS</a> and create a KMS Key, we will define the <strong>key administrator</strong>, which will be my user (ruan.bekker in this case) with more privileged permissions:</p>

<p><img src="https://miro.medium.com/max/5120/1*EUPWbCQ8nsfbBWHQI6srYw.png" alt="" /></p>

<p>and then we will define the <strong>key usage permissions</strong> (app.user in this case), which will be the user that we provisioned from the previous step, this will be the user that will encrypt and decrypt the data:</p>

<p><img src="https://miro.medium.com/max/5120/1*5xA5H0qpJ1FYTG1hUjy_Tw.png" alt="" /></p>

<p>Next, review the policy generated from the previous selected sections:</p>

<p><img src="https://miro.medium.com/max/5120/1*bLDVPFaZUDQ4EyWjACYRUw.png" alt="" /></p>

<p>Once you select finish, you will be returned to the section where your KMS Key information will be displayed, keep note of your <strong>KMS Key Alias</strong>, as we will need it later:</p>

<p><img src="https://miro.medium.com/max/5120/1*aooUMS0OyEd5hopnOUrcmA.png" alt="" /></p>

<h2>Create a IAM Policy for our App User</h2>

<p>Next we will create the IAM Policy for the user that will encrypt/decrypt and store data in S3</p>

<pre><code class="json">{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "S3PutAndGetAccess",
            "Effect": "Allow",
            "Action": [
                "s3:PutObject",
                "s3:GetObject"
            ],
            "Resource": "arn:aws:s3:::arn:aws:s3:::s3-bucket-name/secrets/engineering*"
        },
        {
            "Sid": "KMSDecryptAndEncryptAccess",
            "Effect": "Allow",
            "Action": [
                "kms:Decrypt",
                "kms:Encrypt"
            ],
            "Resource": "arn:aws:kms:eu-west-1:123456789012:key/xxxx-xxxx-xxxx-xxxx-xxxx"
        }
    ]
}
</code></pre>

<p>After the policy has been saved, associate the policy to the IAM User</p>

<h2>Encrypt and Put to S3</h2>

<p>Now we will use Python to define the data that we want to <strong>store in S3</strong>, we will then <strong>encrypt</strong> the data with <strong>KMS</strong>, use base64 to <strong>encode</strong> the ciphertext and push the encrypted value to <strong>S3</strong>, with Server Side Encryption enabled, which we will also use our KMS key.</p>

<p>Install boto3 in Python:</p>

<pre><code class="bash">$ pip install boto3
</code></pre>

<p>Enter the Python REPL and import the required packages, we will also save the access key and secret key as variables so that we can use it with boto3. You can also save it to the <a href="https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html">credential provider</a> and utilise the profile name:</p>

<pre><code class="python">&gt;&gt;&gt; import boto3
&gt;&gt;&gt; import json
&gt;&gt;&gt; import base64
&gt;&gt;&gt; aws_access_key_id='redacted'
&gt;&gt;&gt; aws_secret_access_key='redacted'
</code></pre>

<p>Next define the data that we want to <strong>encrypt and store</strong> in S3:</p>

<pre><code class="python">&gt;&gt;&gt; mydata = {
    "name": "ruan",
    "surname": "bekker",
    "job_title": "systems-development-engineer"
}
</code></pre>

<p>Next we will use KMS to encrypt the data and use base64 to encode the ciphertext:</p>

<pre><code class="python">&gt;&gt;&gt; kms = boto3.Session(
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key
).client('kms')
&gt;&gt;&gt; ciphertext = kms.encrypt(
    KeyId='alias/secrets-key',
    Plaintext=json.dumps(mydata)
)
&gt;&gt;&gt; encoded_ciphertext = base64.b64encode(ciphertext["CiphertextBlob"])
# preview the data
&gt;&gt;&gt; encoded_ciphertext
'AQICAHiKOz...42720nCleoI26UW7P89lPdwvV8Q=='
</code></pre>

<p>Next we will use S3 to push the encrypted data onto S3 in our name spaced key: <strong>secrets/engineering/app1/production/appconfig.json</strong></p>

<pre><code class="python">&gt;&gt;&gt; s3 = boto3.Session(
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key,
    region_name='eu-west-1'
).client('s3')
&gt;&gt;&gt; response = s3.put_object(
    Body=encoded_ciphertext,
    Bucket='ruan-secret-store',
    Key='secrets/engineering/app1/production/appconfig.json',
    ServerSideEncryption='aws:kms',
    SSEKMSKeyId='alias/secrets-key'
)
</code></pre>

<p>Now our object is stored in S3, encrypted with KMS and ServerSideEncryption Enabled.</p>

<p>You can try to download the object and decode the base64 encoded file and you will find that its complete garbage as its encrypted.</p>

<p>Next we will use S3 to Get the object and use KMS to decrypt and use base64 to decode after the object has been decrypted:</p>

<pre><code class="python">&gt;&gt;&gt; response = s3.get_object(
    Bucket='ruan-secret-store',
    Key='secrets/engineering/app1/production/appconfig.json'
)
&gt;&gt;&gt; encoded_ciphertext = response['Body'].read()
&gt;&gt;&gt; encoded_ciphertext
'AQICAHiKOz...42720nCleoI26UW7P89lPdwvV8Q=='
</code></pre>

<p>Now let’s decode the result with base64:</p>

<pre><code class="python">&gt;&gt;&gt; decoded_ciphertext = base64.b64decode(encoded_ciphertext)
&gt;&gt;&gt; plaintext = kms.decrypt(CiphertextBlob=bytes(decoded_ciphertext))
</code></pre>

<p>Now we need to deserialize the JSON as it’s in string format:</p>

<pre><code>&gt;&gt;&gt; json.loads(plaintext["Plaintext"])
{u'surname': u'bekker', u'name': u'ruan', u'job_title': u'systems-development-engineer'}
</code></pre>

<h2>Using it in a Application</h2>

<p>Let’s say you are using <strong>Docker</strong> and you want to bootstrap your application configs to your environment that you are retrieving from S3.</p>

<p>We will use a <code>get_secrets.py</code> python script that will read the data into memory, decrypt and write the values in plaintext to disk, then we will use the <code>boot.sh</code> script to read the values into the environment and remove the temp file that was written to disk, then start the application since we have the values stored in our environment.</p>

<p>Our <strong>&ldquo;application&rdquo;</strong> in this example will just be a line of echo to return the values for demonstration.</p>

<p>The <code>get_secrets.py</code> file:</p>

<pre><code class="python">import boto3
import json
import base64

aws_access_key_id='redacted'
aws_secret_access_key='redacted'

kms = boto3.Session(aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key).client('kms')
s3 = boto3.Session(aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, region_name='eu-west-1').client('s3')

response = s3.get_object(Bucket='ruan-secret-store', Key='secrets/engineering/app1/production/appconfig.json')
encoded_ciphertext = response['Body'].read()

decoded_ciphertext = base64.b64decode(encoded_ciphertext)
plaintext = kms.decrypt(CiphertextBlob=bytes(decoded_ciphertext))
values = json.loads(plaintext["Plaintext"])

with open('envs.tmp', 'w') as f:
    for key in values.keys():
        f.write("{}={}".format(key.upper(), values[key]) + "\n")
</code></pre>

<p>And our <code>boot.sh</code> script:</p>

<pre><code class="bash">#!/usr/bin/env bash
source ./envs.tmp
rm -rf ./envs.tmp
echo "Hello, my name is ${NAME} ${SURNAME}, and I am a ${JOB_TITLE}"
</code></pre>

<p>Running that will produce:</p>

<pre><code>$ bash boot.sh
Hello, my name is ruan bekker, and I am a systems-development-engineer
</code></pre>

<h2>Thank You</h2>

<p>And there we have a simple and effective way of encrypting/decrypting data using S3, KMS and Python at a ridiculously cheap cost, its almost free.</p>

<p>If you liked my content, feel free to checkout my content on <strong><a href="https://ruan.dev/">ruan.dev</a></strong> or follow me on twitter at <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Persist Vault Data With Amazon S3 as a Storage Backend]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/05/07/persist-vault-data-with-amazon-s3-as-a-storage-backend/"/>
    <updated>2019-05-07T16:01:45-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/05/07/persist-vault-data-with-amazon-s3-as-a-storage-backend</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/57256060-f1a27e00-7055-11e9-9a05-77d3fdd6c76f.png" alt="" /></p>

<p>In a previous post we have set up <a href="https://blog.ruanbekker.com/blog/2019/05/06/setup-hashicorp-vault-server-on-docker-and-cli-guide/">the vault server on docker</a>, but using a file backend to persist our data.</p>

<p>In this tutorial we will configure vault to use <a href="https://www.vaultproject.io/docs/configuration/storage/s3.html">amazon s3 as a storage backend</a> to persist our data for vault.</p>

<h2>Provision S3 Bucket</h2>

<p>Create the S3 Bucket where our data will reside:</p>

<pre><code>$ aws s3 mb --region=eu-west-1 s3://somename-vault-backend
</code></pre>

<h2>Vault Config</h2>

<p>Create the vault config, where we will provide details about our storage backend and configuration for the vault server:</p>

<pre><code>$ vim volumes/config/s3vault.json
</code></pre>

<p>Populate the config file with the following details, you will just need to provide your own credentials:</p>

<pre><code class="json">{
  "backend": {
    "s3": {
      "region": "eu-west-1",
      "access_key": "ACCESS_KEY",
      "secret_key": "SECRET_KEY",
      "bucket": "somename-vault-backend"
    }
  },
  "listener": {
    "tcp":{
      "address": "0.0.0.0:8200",
      "tls_disable": 1
    }
  },
  "ui": true
}
</code></pre>

<h2>Docker Compose</h2>

<p>As we are using docker to deploy our vault server, our docker-compose.yml:</p>

<pre><code>$ cat &gt; docker-compose.yml &lt;&lt; EOF
version: '2'
services:
  vault:
    image: vault
    container_name: vault
    ports:
      - "8200:8200"
    restart: always
    volumes:
      - ./volumes/logs:/vault/logs
      - ./volumes/file:/vault/file
      - ./volumes/config:/vault/config
    cap_add:
      - IPC_LOCK
    entrypoint: vault server -config=/vault/config/s3vault.json
EOF
</code></pre>

<p>Deploy the vault server:</p>

<pre><code>$ docker-compose up
</code></pre>

<p>Go ahead and create some secrets, then deploy the docker container on another host to test out the data persistence.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Python Boto3 and DreamHosts DreamObjects to Interact With Their Object Storage Offering]]></title>
    <link href="https://blog.ruanbekker.com/blog/2018/04/03/using-python-boto3-and-dreamhosts-dreamobjects-to-interact-with-their-object-storage-offering/"/>
    <updated>2018-04-03T07:19:27-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2018/04/03/using-python-boto3-and-dreamhosts-dreamobjects-to-interact-with-their-object-storage-offering</id>
    <content type="html"><![CDATA[<p><img src="https://i.snag.gy/uxK5qy.jpg" alt="" /></p>

<p>In this post I will demonstrate how to interact with Dreamhost&rsquo;s Object Storage Service Offering called DreamObjects using Python Boto3 library. Dreamhost offers Object Storage at great pricing, for more information have a look at their <a href="https://goo.gl/N7Xws8">Documentation</a></p>

<h2>Whats on the Menu:</h2>

<p>We will do the following:</p>

<ul>
<li>List Buckets</li>
<li>List Objects</li>
<li>Put Object</li>
<li>Get Object</li>
<li>Upload Object</li>
<li>Download Object</li>
<li>Delete Object(s)</li>
</ul>


<h2>Configuration</h2>

<p>First we need to configure credentials, by providing the access key and access secret key, that is provided by DreamHost:</p>

<pre><code class="bash">$ pip install awscli
$ aws configure --profile dreamhost
</code></pre>

<p>After your credentials is set to your profile, we will need to import boto3 and instantiate the s3 client with our profile name, region name and endpoint url:</p>

<pre><code class="python">&gt;&gt;&gt; import boto3
&gt;&gt;&gt; session = boto3.Session(region_name='us-west-2', profile_name='dreamhost')
&gt;&gt;&gt; s3 = session.client('s3', endpoint_url='https://objects-us-west-1.dream.io')
</code></pre>

<h2>List Buckets:</h2>

<p>To list our Buckets:</p>

<pre><code>&gt;&gt;&gt; response = s3.list_buckets()
&gt;&gt;&gt; print(response)
{u'Owner': {u'DisplayName': 'foobar', u'ID': 'foobar'}, u'Buckets': [{u'CreationDate': datetime.datetime(2017, 4, 15, 21, 51, 3, 921000, tzinfo=tzutc()), u'Name': 'ruanbucket'}], 'ResponseMetadata': {'HTTPStatusCode': 200, 'RetryAttempts': 0, 'HostId': '', 'RequestId': 'tx00000000000000003cd88-005ac361f5-foobar-default', 'HTTPHeaders': {'date': 'Tue, 03 Apr 2018 11:13:57 GMT', 'content-length': '306', 'x-amz-request-id': 'tx00000000000000003cd88-005ac361f5-foobar-default', 'content-type': 'application/xml'}}}

&gt;&gt;&gt; for bucket in response['Buckets']:
...     print(bucket['Name'])
...
ruanbucket
</code></pre>

<h2>List Objects:</h2>

<p>List all the Objects, after the given prefix:</p>

<pre><code class="python">&gt;&gt;&gt; response = s3.list_objects(Bucket='ruanbucket', Prefix='logs/sysadmins.co.za/access/')
&gt;&gt;&gt; for obj in response['Contents']:
...     print obj['Key']
...
logs/sysadmins.co.za/access/access.log-2017-10-10.gz
logs/sysadmins.co.za/access/access.log-2017-10-11.gz
logs/sysadmins.co.za/access/access.log-2017-10-12.gz
</code></pre>

<h2>Put Object:</h2>

<p>Write text as the body to the destination key on the Bucket:</p>

<pre><code class="python">&gt;&gt;&gt; response = s3.put_object(Bucket='ruanbucket', Body='My Name is Ruan\n', Key='uploads/docs/file.txt')
&gt;&gt;&gt; print(response)
{u'Body': &lt;botocore.response.StreamingBody object at 0x13cde10&gt;, u'AcceptRanges': 'bytes', u'ContentType': 'binary/octet-stream', 'ResponseMetadata': {'HTTPStatusCode': 200, 'RetryAttempts': 0, 'HostId': '', 'RequestId': 'tx0000000000000000053f2-005ac3e0db-foobar-default', 'HTTPHeaders': {'content-length': '16', 'accept-ranges': 'bytes', 'last-modified': 'Tue, 03 Apr 2018 20:14:54 GMT', 'etag': '"292edceea84d1234465f725c3921fc2a"', 'x-amz-request-id': 'tx0000000000000000053f2-005ac3e0db-foobar-default', 'date': 'Tue, 03 Apr 2018 20:15:23 GMT', 'content-type': 'binary/octet-stream'}}, u'LastModified': datetime.datetime(2018, 4, 3, 20, 14, 54, tzinfo=tzutc()), u'ContentLength': 16, u'ETag': '"292edceea84d1234465f725c3921fc2a"', u'Metadata': {}}
</code></pre>

<p>List the Object that we have created in the Bucket::</p>

<pre><code class="python">&gt;&gt;&gt; response = s3.list_objects(Bucket='ruanbucket', Prefix='uploads/')
&gt;&gt;&gt; for obj in response['Contents']:
...     print obj['Key']
...
uploads/docs/file.txt
</code></pre>

<h2>Get Object:</h2>

<p>Read the value from the key that was uploaded:</p>

<pre><code class="python">&gt;&gt;&gt; response = s3.get_object(Bucket='ruanbucket', Key='uploads/docs/file.txt')
&gt;&gt;&gt; print(response['Body'].read())
My Name is Ruan
</code></pre>

<h2>Upload Files:</h2>

<p>Upload the file from disk to the Bucket:</p>

<pre><code class="python">&gt;&gt;&gt; with open('myfile.txt', 'rb') as data:
...     s3.upload_fileobj(Fileobj=data, Bucket='ruanbucket', Key='uploads/docs/uploadobj.txt')
...
</code></pre>

<p>Read the contents from the uploaded file:</p>

<pre><code class="python">&gt;&gt;&gt; response = s3.get_object(Bucket='ruanbucket', Key='uploads/docs/uploadobj.txt')
&gt;&gt;&gt; print(response['Body'].read())
This is some text
</code></pre>

<h2>Download File:</h2>

<p>Download the file from the Bucket to the local disk:</p>

<pre><code class="python">&gt;&gt;&gt; with open('downloaded.txt', 'wb') as data:
...     s3.download_fileobj(Bucket='ruanbucket', Key='uploads/docs/uploadobj.txt', Fileobj=data)
...
</code></pre>

<p>Read the file&rsquo;s content from disk:</p>

<pre><code class="python">&gt;&gt;&gt; print(open('downloaded.txt').read())
This is some text
</code></pre>

<h2>Delete Object:</h2>

<p>Delete one object:</p>

<pre><code class="python">&gt;&gt;&gt; response = s3.delete_object(Bucket='ruanbucket', Key='uploads/docs/uploadobj.txt')
&gt;&gt;&gt; print(response)
{'ResponseMetadata': {'HTTPStatusCode': 204, 'RetryAttempts': 0, 'HostId': '', 'RequestId': 'tx00000000000000000be5a-005ac3e61a-foobar-default', 'HTTPHeaders': {'date': 'Tue, 03 Apr 2018 20:37:46 GMT', 'x-amz-request-id': 'tx00000000000000000be5a-005ac3e61a-foobar-default'}}}
</code></pre>

<h2>Delete Objects:</h2>

<p>Delete more than one object with a single API call:</p>

<pre><code class="python">&gt;&gt;&gt; response = s3.delete_objects(Bucket='ruanbucket', Delete={'Objects': [{'Key': 'uploads/docs/file.txt'}, {'Key': 'uploads/docs/file2.txt'}, {'Key': 'uploads/docs/file3.txt'}]})
&gt;&gt;&gt; print(response)
{u'Deleted': [{u'Key': 'uploads/docs/file.txt'}, {u'Key': 'uploads/docs/file2.txt'}, {u'Key': 'uploads/docs/file3.txt'}], 'ResponseMetadata': {'HTTPStatusCode': 200, 'RetryAttempts': 0, 'HostId': '', 'RequestId': 'tx000000000000000011008-005ac3e951-foobar-default', 'HTTPHeaders': {'date': 'Tue, 03 Apr 2018 20:51:29 GMT', 'content-length': '270', 'x-amz-request-id': 'tx000000000000000011008-005ac3e951-217c0ac5-default', 'content-type': 'application/xml'}}}
</code></pre>

<p>For more information on the above, have a look at <a href="http://boto3.readthedocs.io/en/latest/guide/quickstart.html">Boto&rsquo;s Documentation</a> and <a href="https://www.dreamhost.com/">DreamHost&rsquo;s Website</a></p>
]]></content>
  </entry>
  
</feed>
