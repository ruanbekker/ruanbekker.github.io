<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: S3 | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/s3/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2017-12-09T02:14:13-05:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[AWS: IAM S3 Policy for Cyberduck to Allow Listing Buckets and Access to One Bucket]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/09/15/aws-iam-s3-policy-for-cyberduck-to-allow-listing-buckets-and-access-to-one-bucket/"/>
    <updated>2017-09-15T11:18:17-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/09/15/aws-iam-s3-policy-for-cyberduck-to-allow-listing-buckets-and-access-to-one-bucket</id>
    <content type="html"><![CDATA[<p>When using Cyberduck to access S3, and a account has restrictive policies, you may find error <code>Listing Directory: /</code> failed.</p>

<p>If you have restrictive IAM Policies in your account, this may be due to the fact that <code>S3:ListMyBuckets</code> is not allowed.</p>

<p>In this post we want to allow a user to list all buckets, so that Cyberduck can do the initial list after configuration / launch, and we would like to give the user access to their designated bucket.</p>

<h2>Creating the IAM Policy:</h2>

<p>We will create this IAM Policy and associate the policy to the user&rsquo;s account:</p>

<pre><code class="json">{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "Stmt1480515305000",
            "Effect": "Allow",
            "Action": [
                "s3:ListAllMyBuckets",
                "s3:GetBucketLocation"
            ],
            "Resource": [
                "arn:aws:s3:::*"
            ]
        },
        {
            "Sid": "Stmt1480515305002",
            "Effect": "Allow",
            "Action": [
                "s3:List*",
                "s3:GetObject",
                "s3:PutObject",
                "s3:DeleteObject"
            ],
            "Resource": [
                "arn:aws:s3:::allowed-bucket",
                "arn:aws:s3:::allowed-bucket/*"
            ]
        }
    ]
}
</code></pre>

<p>So here we should be able to list the buckets:</p>

<pre><code class="bash">$ aws --profile cyberduck s3 ls /
2017-06-08 08:27:01 allowed-bucket
2017-05-21 13:39:21 private-bucket
2016-12-21 08:23:45 confidential-bucket
2017-08-10 14:18:19 test-bucket
2016-08-03 12:38:29 datalake-bucket
</code></pre>

<p>Able to list inside the bucket, as well as Get, Put etc.</p>

<pre><code class="bash">$ aws --profile cyberduck s3 ls allowed-bucket/
                           PRE data/
</code></pre>

<p>Unable to list the buckets content which is expected, as we did not mention in the policy:</p>

<pre><code class="bash">$ aws --profile cyberduck s3 ls confidential-bucket/

An error occurred (AccessDenied) when calling the ListObjects operation: Access Denied
</code></pre>

<h2>Resources:</h2>

<ul>
<li><a href="https://aws.amazon.com/blogs/security/writing-iam-policies-how-to-grant-access-to-an-amazon-s3-bucket/">https://aws.amazon.com/blogs/security/writing-iam-policies-how-to-grant-access-to-an-amazon-s3-bucket/</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Minios Python SDK to Interact With a Minio S3 Bucket]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/09/08/using-minios-python-sdk-to-interact-with-a-minio-s3-bucket/"/>
    <updated>2017-09-08T16:15:52-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/09/08/using-minios-python-sdk-to-interact-with-a-minio-s3-bucket</id>
    <content type="html"><![CDATA[<p>In our previous post, we have <a href="http://blog.ruanbekker.com/blog/2017/09/08/run-your-self-hosted-s3-service-with-minio-on-docker-swarm/">Setup Minio Server</a> which is a self-hosted alternative to Amazon&rsquo;s S3 Service.</p>

<p>We will go through some basic examples on working with the Python SDK, to interact with Minio.</p>

<h2>Installing the Minio Python Library:</h2>

<p>Ensure that Python and Pip is installed, the install the Python Minio Library:</p>

<pre><code class="bash">$ virtualenv -p /usr/local/bin/python2.7 .venv
$ source .venv/bin/activate
(.venv)$ pip install minio
</code></pre>

<h2>Create a Bucket:</h2>

<p>Enter the Python Interpreter and Create a S3 Bucket on your Minio Server:</p>

<pre><code class="python">&gt;&gt;&gt; from minio import Minio
&gt;&gt;&gt; client = Minio('10.0.0.2:9000', access_key='ASDASDASD', secret_key='ASDASDASD', secure=False)
&gt;&gt;&gt; client.make_bucket('pythonbucket', location='us-west-1')
</code></pre>

<h2>List Buckets:</h2>

<p>I have also created a bucket from my previous post, so we should have 2 buckets:</p>

<pre><code class="python">&gt;&gt;&gt; buckets = client.list_buckets()
&gt;&gt;&gt; for bucket in buckets:
...     print(bucket).name
...
news3bucket
pythonbucket
</code></pre>

<h2>Put Objects to your Bucket:</h2>

<p>Write a string to a file, then upload the file to 2 different destination objects. The arguments is: BucketName, Destination, Source.</p>

<pre><code class="python">&gt;&gt;&gt; data = open('file.txt', 'w')
&gt;&gt;&gt; data.write('This is some text' + '\n')
&gt;&gt;&gt; data.close()

&gt;&gt;&gt; client.fput_object('pythonbucket', 'bucket/contents/file.txt', 'file.txt')
'6b8c327f0fc6f470c030a5b6c71154c5'
&gt;&gt;&gt; client.fput_object('pythonbucket', 'bucket/contents/file2.txt', 'file.txt')
'6b8c327f0fc6f470c030a5b6c71154c5'
</code></pre>

<h2>List Objects in your Bucket:</h2>

<p>List the objects in your bucket:</p>

<pre><code class="python">&gt;&gt;&gt; objects = client.list_objects('pythonbucket', prefix='bucket/contents/', recursive=True)
&gt;&gt;&gt; for obj in objects:
&gt;&gt;&gt; for obj in objects:
...     print(obj.object_name, obj.size)
...
('bucket/contents/file.txt', 18)
('bucket/contents/file2.txt', 18)
</code></pre>

<h2>Remove Objects from your Bucket:</h2>

<p>Remove the objects from your Bucket, the list your bucket to verify that they are removed:</p>

<pre><code class="python">&gt;&gt;&gt; client.remove_object('pythonbucket', 'bucket/contents/file.txt')
&gt;&gt;&gt; client.remove_object('pythonbucket', 'bucket/contents/file2.txt')

&gt;&gt;&gt; for obj in objects:
...     print(obj.object_name, obj.size)
...
&gt;&gt;&gt;
</code></pre>

<h2>Remove the Bucket:</h2>

<p>Remove the Bucket that we created:</p>

<pre><code class="python">&gt;&gt;&gt; client.remove_bucket('pythonbucket')
&gt;&gt;&gt; exit()
</code></pre>

<h2>Resources:</h2>

<p>Minio has some great documentation, for more information on their SDK:</p>

<ul>
<li><a href="https://docs.minio.io/docs/python-client-api-reference">https://docs.minio.io/docs/python-client-api-reference</a></li>
</ul>


<center>
    <script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Buy Me a Coffee', '#46b798', 'A6423ZIQ');kofiwidget2.draw();</script> 
</center>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Run Your Self-Hosted S3 Service With Minio on Docker Swarm]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/09/08/run-your-self-hosted-s3-service-with-minio-on-docker-swarm/"/>
    <updated>2017-09-08T15:29:29-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/09/08/run-your-self-hosted-s3-service-with-minio-on-docker-swarm</id>
    <content type="html"><![CDATA[<p>Minio is a distributed object storage server built for cloud applications, which is similar to Amazon&rsquo;s S3 Service.</p>

<p>Today, we will create the server on docker swarm, as I don&rsquo;t currently have a external data store like GlusterFS / NFS etc, I will host the data on the manager node, and set a constraint for the service so that the service can only run on the manager node.</p>

<h2>Prepare the Data Directory:</h2>

<p>I will only rely on the manager node for my data, so on my manager node:</p>

<pre><code>$ mkdir -p /mnt/data
</code></pre>

<h2>Create the Service:</h2>

<p>If you have a Replicated Gluster Volume or NFS which is mounted throughout your docker swarm, you can create the directory path for it, and the update your <code>--mount</code> source path to your external data store. In my case, I will just point it to my manager node&rsquo;s <code>/mnt/data</code> path as I have setup the service to only run on the one manager node in my swarm:</p>

<pre><code>$ docker service create \
--name minio \
--network appnet \
--replicas 1 \
--publish 9000:9000 \
--constraint 'node.role==manager' \
-e "MINIO_ACCESS_KEY=AKIAASDKJASDL" \
-e "MINIO_SECRET_KEY=AKIAASDKJASDL" \
--mount "type=bind,source=/mnt/data,target=/data" \
minio/minio server /data
</code></pre>

<h2>Install the AWS CLI Tools:</h2>

<p>We will use the awscli tools to interact with our Minio Server:</p>

<pre><code>$ pip install awscli
</code></pre>

<h2>Configure the Client:</h2>

<p>Configure the awscli client with the access details that we passed in our docker service:</p>

<pre><code>$ aws configure --profile minio
AWS Access Key ID []: AKIAASDKJASDL
AWS Secret Access Key []: ASLDKJASDLKJASDLKJ
Default region name []: us-west-1
Default output format []: json
</code></pre>

<h2>Create the Bucket:</h2>

<p>Create a New Bucket, in this case <code>news3bucket</code></p>

<pre><code>aws --profile minio --endpoint-url http://MINIO-IP:9000 s3 mb s3://news3bucket
make_bucket: news3bucket
</code></pre>

<h2>List Buckets:</h2>

<p>List our endpoint, to see the buckets on our server:</p>

<pre><code>$ aws --profile minio --endpoint-url http://MINIO-IP:9000 s3 ls /
2017-09-08 15:01:40 news3bucket
</code></pre>

<h2>Upload an Object to your Bucket:</h2>

<p>We will upload an image <code>awsddb-1.png</code> to our new bucket:</p>

<pre><code>$ aws --profile minio --endpoint-url http://MINIO-IP:9000 s3 cp awsddb-1.png s3://news3bucket/
upload: ./awsddb-1.png to s3://news3bucket/awsddb-1.png
</code></pre>

<h2>List Bucket:</h2>

<p>List your bucket, to see the uploaded object:</p>

<pre><code>$ aws --profile minio --endpoint-url http://MINIO-IP:9000 s3 ls s3://news3bucket
2017-09-08 15:03:11      19851 awsddb-1.png
</code></pre>

<h2>Download Object:</h2>

<p>Download the image from your Bucket, and set the local file to <code>file.png</code>:</p>

<pre><code>$ aws --profile minio --endpoint-url http://MINIO-IP:9000 s3 cp s3://news3bucket/awsddb-1.png file.png
download: s3://news3bucket/awsddb-1.png to ./file.png
</code></pre>

<h2>Web Access:</h2>

<p>You can also access Minio&rsquo;s Web Interface on the port that you have exposed, in my case: <code>http://MINIO-IP:9000/minio/</code></p>

<h2>Resources:</h2>

<ul>
<li><a href="https://www.minio.io/">https://www.minio.io/</a></li>
<li><a href="https://docs.minio.io/docs/minio-docker-quickstart-guide">https://docs.minio.io/docs/minio-docker-quickstart-guide</a></li>
<li><a href="https://github.com/minio/minio/blob/master/README.md">https://github.com/minio/minio/blob/master/README.md</a></li>
<li><a href="https://github.com/arschles/minio-howto/blob/master/aws-cli-with-minio-server.md">https://github.com/arschles/minio-howto/blob/master/aws-cli-with-minio-server.md</a></li>
</ul>


<center>
    <script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Buy Me a Coffee', '#46b798', 'A6423ZIQ');kofiwidget2.draw();</script> 
</center>

]]></content>
  </entry>
  
</feed>
