<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Scaleway | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/scaleway/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2020-08-13T15:46:30+00:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Setup Traefik as an Ingress Controller on Kubernetes]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/06/10/setup-traefik-as-an-ingress-controller-on-kubernetes/"/>
    <updated>2019-06-10T16:21:36-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/06/10/setup-traefik-as-an-ingress-controller-on-kubernetes</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/59225379-db498e00-8bd0-11e9-9f20-62aecf915431.png" alt="image" /></p>

<p>If you have not provisioned a Kubernetes Cluster, you can <a href="https://blog.ruanbekker.com/blog/2019/06/10/testing-out-scaleways-kapsule-their-kubernetes-as-a-service-offering/">see this tutorial</a> on how to provision a Kubernetes Cluster on Scaleway</p>

<h2>What will we be doing</h2>

<p>In this tutorial we will setup Traefik as an Ingress Controller on Kubernetes and deploy a logos web app to our Kubernetes Cluster, using frontend rules to map subdomains to specific services.</p>

<p>We will have 3 subdomains, being mapped to containers from the below docker images:</p>

<pre><code>FQDN                     Image Name
- python.domain.com   -&gt; ruanbekker/logos:python
- openfaas.domain.com -&gt; ruanbekker/logos:openfaas
- rancher.domain.com  -&gt; ruanbekker/logos:rancher
</code></pre>

<h2>Get the sources</h2>

<p>If you would like to get the source code for this demonstration you can checkout this repository: <a href="https://github.com/ruanbekker/traefik-kubernetes-scaleway-demo">https://github.com/ruanbekker/traefik-kubernetes-scaleway-demo</a></p>

<pre><code>$ git clone https://github.com/ruanbekker/traefik-kubernetes-scaleway-demo
$ cd traefik-kubernetes-scaleway-demo
</code></pre>

<h2>Provision Traefik as an Ingress Controller</h2>

<p>Apply role based access control to authorize Traefik to use the Kubernetes API:</p>

<pre><code>$ kubectl apply -f traefik/01-traefik-rbac.yaml
clusterrole.rbac.authorization.k8s.io/traefik-ingress-controller created
clusterrolebinding.rbac.authorization.k8s.io/traefik-ingress-controller created
</code></pre>

<p>Consulting <a href="https://docs.traefik.io/user-guide/kubernetes/#deploy-traefik-using-a-deployment-or-daemonset">Traefik&rsquo;s</a> documentation, when deploying Traefik, it&rsquo;s possible to use a deployment or a demonset, not both. <a href="https://docs.traefik.io/user-guide/kubernetes/#deploy-traefik-using-a-deployment-or-daemonset">More details on why</a></p>

<p>I will go ahead and apply the Daemon Set:</p>

<pre><code>$ kubectl apply -f traefik/03-traefik-ds.yaml
serviceaccount/traefik-ingress-controller created
daemonset.extensions/traefik-ingress-controller created
service/traefik-ingress-service created
</code></pre>

<p>The Traefik UI Service will be associated with a FQDN, remember to set the FQDN for the endpoint, as example:</p>

<pre><code>$ cat traefik/04-traefik-ui.yaml
...
spec:
  rules:
  - host: traefik-ui.x-x-x-x-x.nodes.k8s.fr-par.scw.cloud
    http:
      paths:
      - path: /
...
</code></pre>

<p>Create the Traefik UI Service:</p>

<pre><code>$ kubectl apply -f traefik/04-traefik-ui.yaml
service/traefik-web-ui created
</code></pre>

<p>Traefik UI Ingress:</p>

<pre><code>$ kubectl apply -f traefik/05-traefik-ui-ingress.yaml
ingress.extensions/traefik-web-ui created
</code></pre>

<p>View the services:</p>

<pre><code>$ kubectl get services --namespace=kube-system
NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
coredns                   ClusterIP   x.x.x.x         &lt;none&gt;        53/UDP,53/TCP,9153/TCP   11h
heapster                  ClusterIP   x.x.x.x         &lt;none&gt;        80/TCP                   11h
kubernetes-dashboard      ClusterIP   x.x.x.x         &lt;none&gt;        443/TCP                  11h
metrics-server            ClusterIP   x.x.x.x         &lt;none&gt;        443/TCP                  11h
monitoring-influxdb       ClusterIP   x.x.x.x         &lt;none&gt;        8086/TCP                 11h
traefik-ingress-service   ClusterIP   x.x.x.x         &lt;none&gt;        80/TCP,8080/TCP          24m
traefik-web-ui            ClusterIP   x.x.x.x         &lt;none&gt;        80/TCP                   24m
</code></pre>

<h2>Deploy the Logo App to the Cluster</h2>

<p>We will deploy the logo app to our cluster:</p>

<pre><code>$ kubectl apply -f logos-app/logos-services.yaml
service/openfaas created
service/rancher created
service/python created
</code></pre>

<p>Create the deployment:</p>

<pre><code>$ kubectl apply -f logos-app/logos-deployments.yaml
deployment.extensions/openfaas created
deployment.extensions/rancher created
deployment.extensions/python created
</code></pre>

<p>Before creating the ingress for the logo&rsquo;s applications, we need to set the fqdn endpoints that we want to route traffic to as below as an example:</p>

<pre><code>$ cat logos-app/logos-ingress.yaml
...
spec:
  rules:
  - host: openfaas.x-x-x-x-x.nodes.k8s.fr-par.scw.cloud
    http:
      paths:
      - path: /
        backend:
          serviceName: openfaas
          servicePort: http
...
</code></pre>

<p>Create the ingress:</p>

<pre><code>$ kubectl apply -f logos-app/logos-ingress.yaml
ingress.extensions/logo created
</code></pre>

<p>After some time, have a look at the pods to get the status:</p>

<pre><code>$ kubectl get pods
NAME                                     READY   STATUS    RESTARTS   AGE
openfaas-cffdddc4-lvn5w                  1/1     Running   0          4m6s
openfaas-cffdddc4-wbcl6                  1/1     Running   0          4m6s
python-65ccf9c74b-8kmgp                  1/1     Running   0          4m6s
python-65ccf9c74b-dgnqb                  1/1     Running   0          4m6s
rancher-597b6b8554-mgcjr                 1/1     Running   0          4m6s
rancher-597b6b8554-mpk62                 1/1     Running   0          4m6s
</code></pre>

<h2>Navigating with Kubectl</h2>

<p>Show nodes:</p>

<pre><code>$ kubectl get nodes
NAME                                             STATUS   ROLES    AGE   VERSION
scw-k8s-mystifying-torvald-jovial-mclar-25a942   Ready    node     20h   v1.14.1
scw-k8s-mystifying-torvald-jovial-mclar-eaf1a2   Ready    node     20h   v1.14.1
scw-k8s-mystifying-torvalds-default-7f263aabab   Ready    master   20h   v1.14.1
</code></pre>

<p>Show services:</p>

<pre><code>$ kubectl get services
NAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)           AGE
kubernetes              ClusterIP   10.32.0.1      &lt;none&gt;        443/TCP           20h
openfaas                ClusterIP   10.41.47.185   &lt;none&gt;        80/TCP            9h
python                  ClusterIP   10.42.56.141   &lt;none&gt;        80/TCP            9h
rancher                 ClusterIP   10.32.41.218   &lt;none&gt;        80/TCP            9h
</code></pre>

<p>Show Pods:</p>

<p><em>To see pods from the kube-system namespace add -n kube-system</em></p>

<pre><code>$ kubectl get pods
NAME                                     READY   STATUS    RESTARTS   AGE
openfaas-cffdddc4-lvn5w                  1/1     Running   0          9h
openfaas-cffdddc4-wbcl6                  1/1     Running   0          9h
python-65ccf9c74b-8kmgp                  1/1     Running   0          9h
python-65ccf9c74b-dgnqb                  1/1     Running   0          9h
rancher-597b6b8554-mgcjr                 1/1     Running   0          9h
rancher-597b6b8554-mpk62                 1/1     Running   0          9h
</code></pre>

<p>Show deployments:</p>

<pre><code>$ kubectl get deployments -o wide
NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS  IMAGES                      SELECTOR
openfaas                2/2     2            2           9h    logo        ruanbekker/logos:openfaas   app=logo,task=openfaas
python                  2/2     2            2           9h    logo        ruanbekker/logos:python     app=logo,task=python
rancher                 2/2     2            2           9h    logo        ruanbekker/logos:rancher    app=logo,task=rancher
</code></pre>

<p>Show ingress:</p>

<pre><code>$ kubectl get ingress -o wide
NAME      HOSTS                                                          ADDRESS   PORTS   AGE
logo      openfaas.domain.com,rancher.domain.com,python.domain.com       80      9h
</code></pre>

<p>Show system ingress:</p>

<pre><code>$ kubectl get ingress -o wide -n kube-system
NAME             HOSTS                     ADDRESS   PORTS   AGE
traefik-web-ui   traefik-ui.domain.com               80      9h
</code></pre>

<h2>Access your Applications</h2>

<p>Access the Traefik-UI, and filter for one of the applications. Let&rsquo;s take OpenFaaS for an example:</p>

<p><img src="https://user-images.githubusercontent.com/567298/59177432-63418080-8b5c-11e9-8e54-20600508e510.png" alt="image" /></p>

<p>Access the OpenFaaS Page via the URL:</p>

<p><img src="https://user-images.githubusercontent.com/567298/59177206-a4856080-8b5b-11e9-8954-238590f18e5c.png" alt="image" /></p>

<h2>Resources</h2>

<ul>
<li><a href="https://docs.traefik.io/user-guide/kubernetes/">https://docs.traefik.io/user-guide/kubernetes/</a></li>
</ul>


<center>
        <script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Buy Me a Coffee', '#46b798', 'A6423ZIQ');kofiwidget2.draw();</script>
</center>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Testing Out Scaleways Kapsule Their Kubernetes as a Service Offering]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/06/10/testing-out-scaleways-kapsule-their-kubernetes-as-a-service-offering/"/>
    <updated>2019-06-10T12:28:45-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/06/10/testing-out-scaleways-kapsule-their-kubernetes-as-a-service-offering</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/59177864-a5b78d00-8b5d-11e9-931c-5b5dd4e81805.png" alt="" /></p>

<p>At this time of writing (2019.06.10) Scaleway&rsquo;s Kubernetes as a Service, named Kapsule is in Private Beta and got access and pretty stoked on how easy it is to provision a Kubernetes cluster.</p>

<h2>What are we doing today?</h2>

<p>In this tutorial I will show you how easy it is to provision a 3 node Kubernetes Cluster on Scaleway. In the upcoming tutorial, I will create traefik as an ingress controller and deploy applications to our cluster. <a href="https://github.com/ruanbekker/traefik-kubernetes-scaleway-demo">Github Repo Version available for now</a></p>

<h2>Provision a Kapsule Cluster</h2>

<p>Head over to Kapsule and provision a Kubernetes Cluster:</p>

<p><img src="https://user-images.githubusercontent.com/567298/59164353-e71f4c80-8b0b-11e9-8f5c-7c65db1af7b2.png" alt="" /></p>

<p>At this point in time, I will only create a one node &ldquo;cluster&rdquo;, as I want to show how to add pools after the intial creation.</p>

<p>After the cluster has been provisioned, you will get information about your endpoints from the Cluster Infromation Section, which we will need for our ingresses:</p>

<p><img src="https://user-images.githubusercontent.com/567298/59180685-df8c9180-8b65-11e9-82aa-05ee3cd42c78.png" alt="" /></p>

<p>Scroll down to download your config:</p>

<p><img src="https://user-images.githubusercontent.com/567298/59164356-f56d6880-8b0b-11e9-8c00-34dff0ba61fb.png" alt="" /></p>

<p>Move your config in place:</p>

<pre><code>$ mv ~/Downloads/kubeconfig-k8s-mystifying-torvalds.yaml ~/.kube/config
</code></pre>

<h2>Interact with your Cluster</h2>

<p>Test the connection by getting the info of your nodes in your kubernetes cluster:</p>

<pre><code>$ kubectl get node
NAME                                             STATUS    ROLES     AGE       VERSION
scw-k8s-mystifying-torvalds-default-7f263aabab   Ready     &lt;none&gt;    4m        v1.14.1
</code></pre>

<h2>Add more nodes:</h2>

<p>Provision another pool with 2 more nodes in our cluster:</p>

<p><img src="https://user-images.githubusercontent.com/567298/59164387-4e3d0100-8b0c-11e9-8633-b3fc680ac4cd.png" alt="" /></p>

<p>After the pool has been provisioned, verified that they have joined the cluster:</p>

<pre><code>$ kubectl get nodes
NAME                                             STATUS    ROLES     AGE       VERSION
scw-k8s-mystifying-torvald-jovial-mclar-25a942   Ready     &lt;none&gt;    2m        v1.14.1
scw-k8s-mystifying-torvald-jovial-mclar-eaf1a2   Ready     &lt;none&gt;    2m        v1.14.1
scw-k8s-mystifying-torvalds-default-7f263aabab   Ready     &lt;none&gt;    15m       v1.14.1
</code></pre>

<h2>Master / Node Capabilities</h2>

<p>Usually, I will label master nodes as master: <code>node-role.kubernetes.io/master</code> and worker nodes as nodes: <code>node-role.kubernetes.io/node</code> to allow container scheduling only on the worker nodes. But Scaleway manages this on their end and when you list your nodes, the nodes that you see are your &ldquo;worker&rdquo; nodes.</p>

<p>The master nodes are managed by Scaleway.</p>

<h2>Well Done Scaleway</h2>

<p>Just one more reason I really love Kapsule. Simplicity at its best, well done to <a href="https://scaleway.com">Scaleway</a>. I hope most of the people got access to private beta, but if not, im pretty sure they will keep the public informed on public release dates.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Deploy a Docker Swarm Cluster on Scaleway With Terraform]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/03/21/how-to-deploy-a-docker-swarm-cluster-on-scaleway-with-terraform/"/>
    <updated>2019-03-21T02:15:07-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/03/21/how-to-deploy-a-docker-swarm-cluster-on-scaleway-with-terraform</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/54737111-09fa2e80-4bb7-11e9-97f4-a94a31fc9a3a.png" alt="" /></p>

<p>We will deploy a 3 node docker swarm cluster with terraform on scaleway. I have used the base source code from <a href="https://github.com/stefanprodan/scaleway-swarm-terraform">this</a> repository but tweaked the configuration to my needs.</p>

<h2>Pre-Requisites</h2>

<p>Ensure terraform and jq is instaled:</p>

<pre><code class="bash">$ brew install terraform
$ brew install jq
</code></pre>

<h2>Terraform</h2>

<p>You can have a look at the linked source at the top for the source code, but below I will provide each file that will make up our terraform deployment.</p>

<p>Ource <code>main.tf</code></p>

<pre><code>provider "scaleway" {
  region = "${var.region}"
}

data "scaleway_bootscript" "debian" {
  architecture = "x86_64"
  name = "x86_64 mainline 4.15.11 rev1"
}

data "scaleway_image" "debian_stretch" {
  architecture = "x86_64"
  name         = "Debian Stretch"
}

data "template_file" "docker_conf" {
  template = "${file("conf/docker.tpl")}"

  vars {
    ip = "${var.docker_api_ip}"
  }
}
</code></pre>

<p>The <code>outputs.tf</code></p>

<pre><code>output "swarm_manager_public_ip" {
  value = "${scaleway_ip.swarm_manager_ip.0.ip}"
}

output "swarm_manager_private_ip" {
  value = "${scaleway_server.swarm_manager.0.private_ip}"
}

output "swarm_workers_public_ip" {
  value = "${concat(scaleway_server.swarm_worker.*.name, scaleway_server.swarm_worker.*.public_ip)}"
}

output "swarm_workers_private_ip" {
  value = "${concat(scaleway_server.swarm_worker.*.name, scaleway_server.swarm_worker.*.private_ip)}"
}

output "workspace" {
  value = "${terraform.workspace}"
}
</code></pre>

<p>Our <code>security-groups.tf</code></p>

<pre><code>resource "scaleway_security_group" "swarm_managers" {
  name        = "swarm_managers"
  description = "Allow HTTP/S and SSH traffic"
}

resource "scaleway_security_group_rule" "ssh_accept" {
  security_group = "${scaleway_security_group.swarm_managers.id}"

  action    = "accept"
  direction = "inbound"
  ip_range  = "0.0.0.0/0"
  protocol  = "TCP"
  port      = 22
}

resource "scaleway_security_group_rule" "http_accept" {
  security_group = "${scaleway_security_group.swarm_managers.id}"

  action    = "accept"
  direction = "inbound"
  ip_range  = "0.0.0.0/0"
  protocol  = "TCP"
  port      = 80
}

resource "scaleway_security_group_rule" "https_accept" {
  security_group = "${scaleway_security_group.swarm_managers.id}"

  action    = "accept"
  direction = "inbound"
  ip_range  = "0.0.0.0/0"
  protocol  = "TCP"
  port      = 443
}

resource "scaleway_security_group" "swarm_workers" {
  name        = "swarm_workers"
  description = "Allow SSH traffic"
}

resource "scaleway_security_group_rule" "ssh_accept_workers" {
  security_group = "${scaleway_security_group.swarm_workers.id}"

  action    = "accept"
  direction = "inbound"
  ip_range  = "0.0.0.0/0"
  protocol  = "TCP"
  port      = 22
}
</code></pre>

<p>Our <code>variables.tf</code></p>

<pre><code>variable "docker_version" {
  default = "18.06.3~ce~3-0~debian"
}

variable "region" {
  default = "ams1"
}

variable "manager_instance_type" {
  default = "START1-M"
}

variable "worker_instance_type" {
  default = "START1-M"
}

variable "worker_instance_count" {
  default = 2
}

variable "docker_api_ip" {
  default = "127.0.0.1"
}
</code></pre>

<p>Our <code>managers.tf</code></p>

<pre><code>resource "scaleway_ip" "swarm_manager_ip" {
  count = 1
}

resource "scaleway_server" "swarm_manager" {
  count          = 1
  name           = "${terraform.workspace}-manager-${count.index + 1}"
  image          = "${data.scaleway_image.debian_stretch.id}"
  type           = "${var.manager_instance_type}"
  bootscript     = "${data.scaleway_bootscript.debian.id}"
  security_group = "${scaleway_security_group.swarm_managers.id}"
  public_ip      = "${element(scaleway_ip.swarm_manager_ip.*.ip, count.index)}"

  volume {
    size_in_gb = 50
    type       = "l_ssd"
  }

  provisioner "remote-exec" {
    script = "scripts/mount-disk.sh"
  }

  connection {
    type = "ssh"
    user = "root"
    private_key = "${file("~/.ssh/id_rsa")}"
  }

  provisioner "remote-exec" {
    inline = [
      "mkdir -p /etc/systemd/system/docker.service.d",
    ]
  }

  provisioner "file" {
    content     = "${data.template_file.docker_conf.rendered}"
    destination = "/etc/systemd/system/docker.service.d/docker.conf"
  }

  provisioner "file" {
    source      = "scripts/install-docker-ce.sh"
    destination = "/tmp/install-docker-ce.sh"
  }

  provisioner "file" {
    source      = "scripts/local-persist-plugin.sh"
    destination = "/tmp/local-persist-plugin.sh"
  }

  provisioner "remote-exec" {
    inline = [
      "chmod +x /tmp/install-docker-ce.sh",
      "/tmp/install-docker-ce.sh ${var.docker_version}",
      "docker swarm init --advertise-addr ${self.private_ip}",
      "chmod +x /tmp/local-persist-plugin.sh",
      "/tmp/local-persist-plugin.sh"
    ]
  }
}
</code></pre>

<p>Our <code>workers.tf</code></p>

<pre><code>resource "scaleway_ip" "swarm_worker_ip" {
  count = "${var.worker_instance_count}"
}

resource "scaleway_server" "swarm_worker" {
  count          = "${var.worker_instance_count}"
  name           = "${terraform.workspace}-worker-${count.index + 1}"
  image          = "${data.scaleway_image.debian_stretch.id}"
  type           = "${var.worker_instance_type}"
  bootscript     = "${data.scaleway_bootscript.debian.id}"
  security_group = "${scaleway_security_group.swarm_workers.id}"
  public_ip      = "${element(scaleway_ip.swarm_worker_ip.*.ip, count.index)}"

  volume {
    size_in_gb = 50
    type       = "l_ssd"
  }

  provisioner "remote-exec" {
    script = "scripts/mount-disk.sh"
  }

  connection {
    type = "ssh"
    user = "root"
    private_key = "${file("~/.ssh/id_rsa")}"
  }

  provisioner "remote-exec" {
    inline = [
      "mkdir -p /etc/systemd/system/docker.service.d",
    ]
  }

  provisioner "file" {
    content     = "${data.template_file.docker_conf.rendered}"
    destination = "/etc/systemd/system/docker.service.d/docker.conf"
  }

  provisioner "file" {
    source      = "scripts/install-docker-ce.sh"
    destination = "/tmp/install-docker-ce.sh"
  }

  provisioner "file" {
    source      = "scripts/local-persist-plugin.sh"
    destination = "/tmp/local-persist-plugin.sh"
  }

  provisioner "remote-exec" {
    inline = [
      "chmod +x /tmp/install-docker-ce.sh",
      "/tmp/install-docker-ce.sh ${var.docker_version}",
      "docker swarm join --token ${data.external.swarm_tokens.result.worker} ${scaleway_server.swarm_manager.0.private_ip}:2377",
      "chmod +x /tmp/local-persist-plugin.sh",
      "/tmp/local-persist-plugin.sh",
    ]
  }

  provisioner "remote-exec" {
    when = "destroy"

    inline = [
      "docker node update --availability drain ${self.name}",
    ]

    on_failure = "continue"

    connection {
      type = "ssh"
      user = "root"
      host = "${scaleway_ip.swarm_manager_ip.0.ip}"
    }
  }

  provisioner "remote-exec" {
    when = "destroy"

    inline = [
      "docker swarm leave",
    ]

    on_failure = "continue"
  }

  provisioner "remote-exec" {
    when = "destroy"

    inline = [
      "docker node rm --force ${self.name}",
    ]

    on_failure = "continue"

    connection {
      type = "ssh"
      user = "root"
      host = "${scaleway_ip.swarm_manager_ip.0.ip}"
    }
  }
}

data "external" "swarm_tokens" {
  program = ["./scripts/fetch-tokens.sh"]

  query = {
    host = "${scaleway_ip.swarm_manager_ip.0.ip}"
  }

  depends_on = ["scaleway_server.swarm_manager"]
}
</code></pre>

<p>Our config for the docker daemon: <code>conf/docker.tpl</code></p>

<pre><code>[Service]
ExecStart=
ExecStart=/usr/bin/dockerd -H fd:// \
  -H tcp://${ip}:2375 \
  --storage-driver=overlay2 \
  --dns 8.8.4.4 --dns 8.8.8.8 \
  --log-driver json-file \
  --log-opt max-size=50m --log-opt max-file=10 \
  --experimental=true \
  --metrics-addr 172.17.0.1:9323
</code></pre>

<p>Our script to mount our additional disk: <code>scripts/mount-disk.sh</code></p>

<pre><code class="bash">#!/bin/bash
apt update
apt install xfsprogs attr -y
mkfs -t xfs /dev/vdb
echo "/dev/vdb /mnt xfs defaults 0 0" &gt;&gt; /etc/fstab
mount -a
</code></pre>

<p>Our script to install docker: <code>scripts/install-docker-ce.sh</code></p>

<pre><code class="bash">#!/usr/bin/env bash

DOCKER_VERSION=$1
DEBIAN_FRONTEND=noninteractive apt-get -qq update
apt-get -qq install apt-transport-https ca-certificates curl software-properties-common
curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -
add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable"

apt-get -q update -y
apt-get -q install -y docker-ce=$DOCKER_VERSION containerd.io
</code></pre>

<p>Our script that retrieves the swarm tokens: <code>scripts/fetch-tokens.sh</code></p>

<pre><code class="bash">#!/usr/bin/env bash

# Processing JSON in shell scripts
# https://www.terraform.io/docs/providers/external/data_source.html#processing-json-in-shell-scripts

set -e

# Extract "host" argument from the input into HOST shell variable
eval "$(jq -r '@sh "HOST=\(.host)"')"

MANAGER=$(ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null root@$HOST docker swarm join-token manager -q)
WORKER=$(ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null root@$HOST docker swarm join-token worker -q)

# produce a json object containing the tokens
jq -n --arg manager "$MANAGER" --arg worker "$WORKER" '{"manager":$manager,"worker":$worker}'
</code></pre>

<p>Our script to install the <a href="https://github.com/CWSpear/local-persist">local-persist docker volume</a> plugin: <code>scripts/local-persist-plugin.sh</code></p>

<pre><code>#!/usr/bin/env bash
set -e
curl -fsSL https://raw.githubusercontent.com/CWSpear/local-persist/master/scripts/install.sh | bash
</code></pre>

<h2>Deploy your Swarm</h2>

<p>Note that we will be deploying 3x SMART1-M servers with Debian Stretch. At this moment the image id is the one of debian stretch but may change in the future. If you want to change the distro, update the install script, and the terraform files.</p>

<p><a href="https://www.scaleway.com/docs/generate-an-api-token/">Generate API Token on Scaleway</a> then export it to your current shell:</p>

<pre><code class="bash">export SCALEWAY_ORGANIZATION="&lt;organization-id&gt;"
export SCALEWAY_TOKEN="&lt;secret&gt;"
</code></pre>

<p>Make sure that your ssh private key is the intended one as in the config, in my example: <code>~/.ssh/id_rsa</code> and that they are allowed in your servers <code>authorized_keys</code> file</p>

<p>Create a new workspace:</p>

<pre><code class="bash">$ terraform new workspace swarm
</code></pre>

<p>Pull down the providers and initialize:</p>

<pre><code class="bash">$ terraform init
</code></pre>

<p>Deploy!</p>

<pre><code class="bash">$ terraform apply
...
...
scaleway_server.swarm_worker[0]: Creation complete after 4m55s (ID: xx-xx-xx-xx-xx)

Apply complete! Resources: 14 added, 0 changed, 0 destroyed.
Outputs:

swarm_manager_private_ip = 10.21.x.x
swarm_manager_public_ip = 51.xx.xx.xx
swarm_workers_private_ip = [
    swarm-worker-1,
    swarm-worker-2,
    10.20.xx.xx,
    10.20.xx.xx,
]
swarm_workers_public_ip = [
    swarm-worker-1,
    swarm-worker-2,
    51.xx.xx.xx,
    51.xx.xx.xx,
]
workspace = swarm
</code></pre>

<p>Once your deployment is done you will be prompted with the public/private ip addresses of your nodes as seen above, you can also manually retrieve them:</p>

<pre><code>$ terraform terraform output
</code></pre>

<p>Or for a specific node, such as the manager:</p>

<pre><code>$ terraform terraform output swarm-manager
51.xx.xx.xx
</code></pre>

<p>Go ahead and ssh to your manager nodes and list the swarm nodes, boom, easy right.</p>

<pre><code>$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
2696o0vrt93x8qf2gblbfc8pf *   swarm-manager       Ready               Active              Leader              18.09.3
72ava7rrp2acnyadisg52n7ym     swarm-worker-1      Ready               Active                                  18.09.3
sy2otqn20qe9jc2v9io3a21jm     swarm-worker-2      Ready               Active                                  18.09.3
</code></pre>

<p>When you want to destroy the environment:</p>

<pre><code>$ terraform destroy -force
</code></pre>

<h2>References:</h2>

<p>Big thanks goes to <a href="https://github.com/stefanprodan">@stefanprodan</a></p>

<ul>
<li><a href="https://www.terraform.io/docs/index.html">https://www.terraform.io/docs/index.html</a></li>
<li><a href="https://docs.docker.com/engine/swarm/">https://docs.docker.com/engine/swarm/</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Manage Scaleway Instances via Their API Like a Boss With Their Command Line Tool Scw]]></title>
    <link href="https://blog.ruanbekker.com/blog/2018/05/09/manage-scaleway-instances-via-their-api-like-a-boss-with-their-command-line-tool-scw/"/>
    <updated>2018-05-09T12:31:11-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2018/05/09/manage-scaleway-instances-via-their-api-like-a-boss-with-their-command-line-tool-scw</id>
    <content type="html"><![CDATA[<p><img src="https://preview.ibb.co/bBRhn7/scw.png" alt="" /></p>

<p>Let&rsquo;s set things straight: I am a command line fan boy, If I can do the things I have to do with a command line interface, i&rsquo;m happy! And that means automation ftw! :D</p>

<h2>Scaleway Command Line Interface:</h2>

<p>I have been using Scaleway for about 2 years now, and absolutely loving their services! So I recently found their <a href="https://github.com/scaleway/scaleway-cli">command line interface utility: scw</a>, which is written in golang and has a very similar feel to docker.</p>

<h2>Install the SCW CLI Tool:</h2>

<p>A golang environment is needed and I will be using docker to drop myself into a golang environment and then install the scw utility:</p>

<pre><code class="bash">$ docker run -it golang:alpine sh
$ apk update
$ apk add openssl git openssh curl
$ go get -u github.com/scaleway/scaleway-cli/cmd/scw
</code></pre>

<p>Verify that it was installed:</p>

<pre><code class="bash">$ scw --version
scw version v1.16+dev, build
</code></pre>

<p>Awesome sauce!</p>

<h2>Authentication:</h2>

<p>When we authenticate to Scaleway, it will prompt you to upload your public ssh key, as I am doing this in a container I have no ssh keys, so therefore will generate one before I authenticate.</p>

<p>Generate the SSH Key:</p>

<pre><code class="bash">$ ssh-keygen -t rsa
Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa):
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /root/.ssh/id_rsa.
Your public key has been saved in /root/.ssh/id_rsa.pub.
</code></pre>

<p>Now loging to Scaleway using the cli tools:</p>

<pre><code class="bash">$ scw login
Login (cloud.scaleway.com): &lt;youremail@domain.com&gt;
Password:
Do you want to upload an SSH key ?
[0] I don't want to upload a key !
[1] id_rsa.pub
Which [id]: 1

You are now authenticated on Scaleway.com as Ruan.
You can list your existing servers using `scw ps` or create a new one using `scw run ubuntu-xenial`.
You can get a list of all available commands using `scw -h` and get more usage examples on github.com/scaleway/scaleway-cli.
Happy cloud riding.
</code></pre>

<p>Sweeet!</p>

<p><img src="https://pics.me.me/hacker-voice-im-in-24303160.png" alt="" /></p>

<h2>Getting Info from Scaleway</h2>

<p>List Instance Types:</p>

<pre><code class="bash">$ scw products servers
COMMERCIAL TYPE     ARCH     CPUs      RAM  BAREMETAL
ARM64-128GB        arm64       64   137 GB      false
ARM64-16GB         arm64       16    17 GB      false
ARM64-2GB          arm64        4   2.1 GB      false
ARM64-32GB         arm64       32    34 GB      false
ARM64-4GB          arm64        6   4.3 GB      false
ARM64-64GB         arm64       48    69 GB      false
ARM64-8GB          arm64        8   8.6 GB      false
C1                   arm        4   2.1 GB       true
C2L               x86_64        8    34 GB       true
C2M               x86_64        8    17 GB       true
C2S               x86_64        4   8.6 GB       true
START1-L          x86_64        8   8.6 GB      false
START1-M          x86_64        4   4.3 GB      false
START1-S          x86_64        2   2.1 GB      false
START1-XS         x86_64        1   1.1 GB      false
VC1L              x86_64        6   8.6 GB      false
VC1M              x86_64        4   4.3 GB      false
VC1S              x86_64        2   2.1 GB      false
X64-120GB         x86_64       12   129 GB      false
X64-15GB          x86_64        6    16 GB      false
X64-30GB          x86_64        8    32 GB      false
X64-60GB          x86_64       10    64 GB      false
</code></pre>

<p>Get a list of available Images, in my case I am just looking for Ubuntu:</p>

<pre><code class="bash">$ scw images | grep -i ubuntu
Ubuntu_Bionic               latest              a21bb700            11 days             [ams1 par1]         [x86_64]
Ubuntu_Mini_Xenial_25G      latest              bc75c00b            13 days             [ams1 par1]         [x86_64]
</code></pre>

<p>List Running Instances:</p>

<pre><code class="bash">$ scw ps
SERVER ID           IMAGE                       ZONE                CREATED             STATUS              PORTS               NAME                  COMMERCIAL TYPE
abc123de            Ubuntu_Xenial_16_04_lates   ams1                5 weeks             running             xx.xx.xx.xx         scw-elasticsearch-01  ARM64-4GB
abc456de            ruan-docker-swarm-17_03     par1                10 months           running             xx.xx.xxx.xxx       scw-swarm-manager-01  VC1M
...
</code></pre>

<p>List All Instances (Running, Stopped, Started, etc):</p>

<pre><code class="bash">$ scw ps -a
SERVER ID           IMAGE                       ZONE                CREATED             STATUS              PORTS               NAME                  COMMERCIAL TYPE
abc123df            Ubuntu_Xenial_16_04_lates   ams1                5 weeks             stopped             xx.xx.xx.xx         scw-elasticsearch-02  ARM64-4GB
...
</code></pre>

<p>List Instances with a filter based on its name:</p>

<pre><code class="bash">$ scw ps -f name=scw-swarm-worker-02
SERVER ID           IMAGE               ZONE                CREATED             STATUS              PORTS               NAME                COMMERCIAL TYPE
1234abcd            Ubuntu_Xenial       par1                8 minutes           running             xx.xx.xxx.xxx       scw-swarm-worker-2  START1-XS
</code></pre>

<p>List the Latest Instance that was created:</p>

<pre><code class="bash">$ scw ps -l
SERVER ID           IMAGE               ZONE                CREATED             STATUS              PORTS               NAME                COMMERCIAL TYPE
1234abce            Ubuntu_Xenial       par1                6 minutes           running             xx.xx.xxx.xxx       scw-swarm-worker-3  START1-XS
</code></pre>

<h2>Create Instances:</h2>

<p>In my scenario, I would like to create a instance named <code>docker-swarm-worker-4</code> with the instance type <code>START1-XS</code> in the Paris datacenter, and I will be using my key that I have uploaded, also the image id that I passed, was retrieved when listing for images:</p>

<pre><code class="bash">$ scw --region=par1 create --commercial-type=START1-XS --ip-address=dynamic --ipv6=false --name="docker-swarm-worker-4" --tmp-ssh-key=false  bc75c00b
&lt;response: random uuid string&gt;
</code></pre>

<p>Now that the instance is created, we can start it by calling either the name or the id:</p>

<pre><code class="bash">$ scw start docker-swarm-worker-4
</code></pre>

<p>To verify the status of the instance, we can do:</p>

<pre><code class="bash">$ scw ps -l
SERVER ID           IMAGE               ZONE                CREATED             STATUS              PORTS               NAME                   COMMERCIAL TYPE
102abc34            Ubuntu_Xenial                           28 seconds          starting                                docker-swarm-worker-4  START1-XS
</code></pre>

<p>At this moment it is still starting, after waiting a minute or so, run it again:</p>

<pre><code class="bash">$ scw ps -l
SERVER ID           IMAGE               ZONE                CREATED             STATUS              PORTS               NAME                   COMMERCIAL TYPE
102abc34            Ubuntu_Xenial       par1                About a minute      running             xx.xx.xx.xx         docker-swarm-worker-4  START1-XS
</code></pre>

<p>As we can see its in a running state, so we are good to access our instance. You have 2 options to access your server, via exec and ssh.</p>

<pre><code>$ scw exec docker-swarm-worker-4 /bin/bash
root@docker-swarm-worker-4:~
</code></pre>

<p>or via SSH:</p>

<pre><code>$ ssh root@xx.xx.xx.xx
root@docker-swarm-worker-4:~
</code></pre>

<p>If you would like to access your server without uploading your SSH key to your account, you can pass <code>--tmp-ssh-key=true</code> as in:</p>

<pre><code class="bash">$ scw --region=par1 create --commercial-type=START1-XS --ip-address=dynamic --ipv6=false --name="scw-temp-instance" --tmp-ssh-key=true  bc75c00b
</code></pre>

<h2>Terminating Resources:</h2>

<p>This wil stop, terminate the instance with the associated volumes and reserved ip</p>

<pre><code class="bash">$ scw stop --terminate=true scw-temp-instance 
scw-temp-instance
</code></pre>

<p>If you had to remove a volume that is not needed, or unused:</p>

<pre><code>$ scw rmi test-1-snapshot-&lt;long-string&gt;--2018-04-26_12:42
</code></pre>

<p>To logout:</p>

<pre><code class="bash">$ scw logout
</code></pre>

<h2>Resources:</h2>

<p>Have a look at <a href="https://github.com/scaleway/scaleway-cli">Scaleway-CLI Documentation</a> and their <a href="https://www.scaleway.com/">Website</a> for more info, and have a look at their new <code>START1-XS</code> instance types, that is only 1.99 Euro&rsquo;s, that is insane!</p>

<p>Personally love what they are doing, feel free to head over to their <a href="https://www.scaleway.com/pricing/">pricing page</a> to see some sweet deals!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup MongoDB Server on ARM64 Using Scaleway]]></title>
    <link href="https://blog.ruanbekker.com/blog/2018/04/01/setup-mongodb-server-on-arm64-using-scaleway/"/>
    <updated>2018-04-01T18:46:27-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2018/04/01/setup-mongodb-server-on-arm64-using-scaleway</id>
    <content type="html"><![CDATA[<p><img src="https://preview.ibb.co/bBRhn7/scw.png" alt="" /></p>

<p>I&rsquo;ve been using Scaleway for the past 18 months and I must admit, I love hosting my Applications on their Infrastructure. They have expanded rapidly recently, and currently deploying more infrstructure due to the high demand.</p>

<p>Scaleway is a Cloud Division of Online.net. They provide Baremetal and Cloud SSD Virtual Servers. Im currently hosting a Docker Swarm Cluster, Blogs, Payara Java Application Servers, Elasticsearch and MongoDB Clusters with them and really happy with the performance and stability of their services.</p>

<h2>What will we be doing today:</h2>

<p>Today I will be deploying MongoDB Server on a ARM64-2GB Instance, which costs you 2.99 Euros per month, absolutely awesome pricing! After we install MongoDB we will setup authentication, and then just a few basic examples on writing and reading from MongoDB.</p>

<h2>Getting Started:</h2>

<p>Logon to <a href="cloud.scaleway.com">cloud.scaleway.com</a> then launch an instance, which will look like the following:</p>

<p><img src="https://image.ibb.co/e7T9jn/scw_launch.png" alt="" /></p>

<p>After you deployed your instance, SSH to your instance, and it should look like this:</p>

<p><img src="https://preview.ibb.co/k16C4n/scw_ssh.png" alt="" /></p>

<h2>Dependencies:</h2>

<p>Get the repository and install MongoDB:</p>

<pre><code class="bash">$ apt update
$ apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 2930ADAE8CAF5059EE73BB4B58712A2291FA4AD5
$ echo "deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 multiverse" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.6.list
$ apt update &amp;&amp; apt upgrade -y
$ apt install mongodb-org -y
</code></pre>

<p>Enable MongoDB on Boot:</p>

<pre><code class="bash">$ systemctl enable mongod
</code></pre>

<h2>Configuration:</h2>

<p>Your configuration might look different from mine, so I recommend to backup your config first, as the following command will overwrite the config to the configuration that I will be using for this demonstration:</p>

<pre><code class="bash">$ cat &gt; /etc/mongod.conf &lt;&lt; EOF
storage:
  dbPath: /var/lib/mongodb
  journal:
    enabled: false

storage:
  mmapv1:
    smallFiles: true

systemLog:
  destination: file
  logAppend: true
  path: /var/log/mongodb/mongod.log

net:
  port: 27017
  bindIp: 0.0.0.0

processManagement:
  timeZoneInfo: /usr/share/zoneinfo

security:
  authorization: enabled
EOF
</code></pre>

<p>Restart MongoDB for the config changes to take affect:</p>

<pre><code class="bash">$ systemctl restart mongod
</code></pre>

<h2>Authentication:</h2>

<p>Create the Authentication:</p>

<pre><code class="bash">$ mongo
MongoDB shell version v3.6.3
connecting to: mongodb://127.0.0.1:27017
MongoDB server version: 3.6.3
Welcome to the MongoDB shell.

&gt; use admin
&gt; db.createUser({user: "ruan", pwd: "pass123", roles:[{role: "root", db: "admin"}]})
Successfully added user: {
        "user" : "ruan",
        "roles" : [
                {
                        "role" : "root",
                        "db" : "admin"
                }
        ]
}

&gt; exit
</code></pre>

<p>Restart MongoDB and logon with your credentials:</p>

<pre><code class="bash">$ systemctl restart mongod

$ mongo --authenticationDatabase admin --host localhost --port 27017 -u ruan -p
MongoDB shell version v3.6.3
Enter password:
connecting to: mongodb://localhost:27017/
MongoDB server version: 3.6.3
&gt;
</code></pre>

<h2>Write and Read from MongoDB</h2>

<p>While you are on the MongoDB Shell, we will insert a couple of documents, first drop in to the database that you would like to write to:</p>

<pre><code class="bash">&gt; use testdb
switched to db testdb
</code></pre>

<p>Now we will write to the collection: <code>collection1</code>:</p>

<pre><code class="bash">&gt; db.collection1.insert({"name": "ruan", "surname": "bekker", "age": 31, "country": "south africa"})
WriteResult({ "nInserted" : 1 })

&gt; db.collection1.insert({"name": "stefan", "surname": "bester", "age": 30, "country": "south africa"})
WriteResult({ "nInserted" : 1 })
</code></pre>

<p>To find all the documents in our collection:</p>

<pre><code class="bash">&gt; db.collection1.find()
{ "_id" : ObjectId("5ac15ff0f4a5500484defd23"), "name" : "ruan", "surname" : "bekker", "age" : 31, "country" : "south africa" }
{ "_id" : ObjectId("5ac16003f4a5500484defd24"), "name" : "stefan", "surname" : "bester", "age" : 30, "country" : "south africa" }
</code></pre>

<p>To prettify the output:</p>

<pre><code class="bash">&gt; db.collection1.find().pretty()
{
        "_id" : ObjectId("5ac15ff0f4a5500484defd23"),
        "name" : "ruan",
        "surname" : "bekker",
        "age" : 31,
        "country" : "south africa"
}
{
        "_id" : ObjectId("5ac16003f4a5500484defd24"),
        "name" : "stefan",
        "surname" : "bester",
        "age" : 30,
        "country" : "south africa"
}
</code></pre>

<p>To find a document with the key/value of <code>name: ruan</code>:</p>

<pre><code class="bash">&gt; db.collection1.find({"name": "ruan"}).pretty()
{
        "_id" : ObjectId("5ac15ff0f4a5500484defd23"),
        "name" : "ruan",
        "surname" : "bekker",
        "age" : 31,
        "country" : "south africa"
}
</code></pre>

<p>To view the database that you are currently switched to:</p>

<pre><code class="bash">&gt; db
testdb
</code></pre>

<p>To view all the databases:</p>

<pre><code class="bash">&gt; show dbs
admin   0.000GB
config  0.000GB
local   0.000GB
testdb  0.000GB
</code></pre>

<p>To view the collections in the database:</p>

<pre><code class="bash">&gt; show collections
collection1

&gt; exit
</code></pre>

<p>That was just a quick post on installing MongoDB on ARM64 using Scaleway. Try them out, and they are also hiring: <a href="https://careers.scaleway.com/">careers.scaleway.com</a></p>
]]></content>
  </entry>
  
</feed>
