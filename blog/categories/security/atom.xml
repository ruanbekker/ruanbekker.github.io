<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Security | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/security/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2017-12-11T09:35:13-05:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Python Script to Decrypt Encrypted Data With AWS KMS]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/10/20/python-script-to-decrypt-encrypted-data-with-aws-kms/"/>
    <updated>2017-10-20T04:54:51-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/10/20/python-script-to-decrypt-encrypted-data-with-aws-kms</id>
    <content type="html"><![CDATA[<p>Quick script to decrypt data that was encrypted with your KMS key:</p>

<h2>The Script:</h2>

<p>The script requires the encrypted scring as an argument:</p>

<pre><code class="python">#!/usr/bin/env python

import boto3
import sys
from base64 import b64decode

try:
    encrypted_value = sys.argv[1]
except IndexError:
    print("Usage: {} {}".format(sys.argv[0], 'the-encrypted-string'))
    exit(1)

session = boto3.Session(
        region_name='eu-west-1',
        profile_name='default'
    )

kms = session.client('kms')

response = kms.decrypt(CiphertextBlob=b64decode(encrypted_value))['Plaintext']
print("Decrypted Value: {}".format(response))
</code></pre>

<p>Change the permissions so that the file is executable:</p>

<pre><code class="bash">$ chmod +x decrypt.py
</code></pre>

<h2>Usage:</h2>

<pre><code class="bash">$ ./decrypt.py asdlaskjdasidausd09q3uoijad09ujd38u309
Decrypted Value: thisIsMyDecryptedValue
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nginx Reverse Proxy for Elasticsearch and Kibana 5 on AWS]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/09/16/nginx-reverse-proxy-for-elasticsearch-and-kibana-5-on-aws/"/>
    <updated>2017-09-16T17:24:32-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/09/16/nginx-reverse-proxy-for-elasticsearch-and-kibana-5-on-aws</id>
    <content type="html"><![CDATA[<p>As up untill today, there&rsquo;s currently no VPC Support for Amazon&rsquo;s Elasticsearch Service.</p>

<p>So for scenarios where you would like to allow private network traffic to Elasticsearch is impossible straight out of the box as Amazon&rsquo;s Elasticsearch Services, only sees Public Internet Traffic.</p>

<p>We will setup 2 configs, one for Kibana and one for Elasticsearch, each one having its own FQDN:</p>

<ul>
<li>Kibana: <code>http://kibana.domain.com</code></li>
<li>Elasticsearch: <code>http://elasticsearch.domain.com</code></li>
</ul>


<h2>Workaround:</h2>

<p>There&rsquo;s a couple of workarounds, which includes:</p>

<ul>
<li>Nginx Reverse Proxy</li>
<li>NAT Gateway</li>
<li>Allow IAM Users/Roles</li>
</ul>


<p>Today we will tackle the Nginx Reverse Proxy Route.</p>

<p>The benefit of this, would be to associate an EIP to the Nginx EC2 Instnace, then whitelist your EIP with Elasticsearch, so the only traffic that will be accepted will be the traffic that is coming from the Nginx Instance. We will also apply an additional layer of security, in this case we will use HTTP Basic Authentication, then also authorize network sources on a Security Group level.</p>

<h2>Installing Nginx:</h2>

<p>In this case I am using Ubuntu 16.04, so we will need to install <code>nginx</code> and <code>apache2-utils</code> for creating the Basic HTTP Auth accounts.</p>

<pre><code class="bash">$ apt update &amp;&amp; apt upgrade -y
$ apt install nginx apache2-utils -y
</code></pre>

<h2>Configure Nginx:</h2>

<p>Our main config: <code>/etc/nginx/nginx.conf</code>:</p>

<pre><code class="bash /etc/nginx/nginx.conf">user www-data;
worker_processes auto;
pid /run/nginx.pid;
error_log /var/log/nginx/error.log;

events {
    worker_connections 1024;
}

http {

    # Basic Settings
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    server_names_hash_bucket_size 128;

    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # Logging Settings
        log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

    access_log /var/log/nginx/access.log main;

    # Gzip Settings
    gzip on;
    gzip_disable "msie6";

    # Elasticsearch and Kibana Configs
    include /etc/nginx/conf.d/elasticsearch.conf;
    include /etc/nginx/conf.d/kibana.conf;
}
</code></pre>

<p>Our <code>/etc/nginx/conf.d/elasticsearch.conf</code> configuration:</p>

<pre><code class="bash /etc/nginx/conf.d/elasticsearch.conf">server {

  listen 80;
  server_name elasticsearch.domain.com;

  # error logging
  error_log /var/log/nginx/elasticsearch_error.log;

  # authentication: elasticsearch
  auth_basic "Elasticsearch Auth";
  auth_basic_user_file /etc/nginx/.secrets_elasticsearch;

  location / {

    proxy_http_version 1.1;
    proxy_set_header Host https://search-elasticsearch-name.eu-west-1.es.amazonaws.com;
    proxy_set_header X-Real-IP &lt;ELASTIC-IP&gt;;
    proxy_set_header Connection "Keep-Alive";
    proxy_set_header Proxy-Connection "Keep-Alive";
    proxy_set_header Authorization "";

    proxy_pass https://search-elasticsearch-name.eu-west-1.es.amazonaws.com/;
    proxy_redirect https://search-elasticsearch-name.eu-west-1.es.amazonaws.com/ http://&lt;ELASTIC-IP&gt;/;

  }

  # ELB Health Checks
  location /status {
    root /usr/share/nginx/html/;
  }

}
</code></pre>

<p>Our <code>/etc/nginx/conf.d/kibana.conf</code> configuration:</p>

<pre><code class="bash /etc/nginx/conf.d/kibana.conf">server {

  listen 80;
  server_name kibana.domain.com;

  # error logging
  error_log /var/log/nginx/kibana_error.log;

  # authentication: kibana
  auth_basic "Kibana Auth";
  auth_basic_user_file /etc/nginx/.secrets_kibana;

  location / {

    proxy_http_version 1.1;
    proxy_set_header Host https://search.elasticsearch-name.eu-west-1.es.amazonaws.com;
    proxy_set_header X-Real-IP &lt;ELASTIC-IP&gt;;
    proxy_set_header Connection "Keep-Alive";
    proxy_set_header Proxy-Connection "Keep-Alive";
    proxy_set_header Authorization "";

    proxy_pass https://search.elasticsearch-name.eu-west-1.es.amazonaws.com/_plugin/kibana/;
    proxy_redirect https://search.elasticsearch-name.eu-west-1.es.amazonaws.com/_plugin/kibana/ http://&lt;ELASTIC-IP&gt;/kibana/;

  }

      location ~ (/app/kibana|/app/timelion|/bundles|/es_admin|/plugins|/api|/ui|/elasticsearch) {
         proxy_pass              https://search.elasticsearch-name.eu-west-1.es.amazonaws.com;
         proxy_set_header        Host $host;
         proxy_set_header        X-Real-IP $remote_addr;
         proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;
         proxy_set_header        X-Forwarded-Proto $scheme;
         proxy_set_header        X-Forwarded-Host $http_host;
         proxy_set_header    Authorization  "";
    }
}
</code></pre>

<p>Once you have replaced the elasticsearch endpoint and your EPI values, we can go ahead and create the auth accounts.</p>

<h2>Create User Accounts for HTTP Basic Auth</h2>

<p>Create the 2 accounts for authentication on kibana and elasticsearch:</p>

<pre><code class="bash">$ htpasswd -c /etc/nginx/.secrets_elasticsearch elasticsearch-admin
$ htpasswd -c /etc/nginx/.secrets_kibana kibana-admin
</code></pre>

<h2>Restart Nginx:</h2>

<p>Restart and enable Nginx on boot:</p>

<pre><code class="bash">$ systemctl enable nginx
$ systemctl restart nginx
</code></pre>

<p>Once your Nginx Service is running, you should be able to access Kibana and Elasticsearch using the credentials that you created.</p>

<h2>Resources:</h2>

<ul>
<li><a href="https://www.nginx.com/blog/tcp-load-balancing-udp-load-balancing-nginx-tips-tricks/">https://www.nginx.com/blog/tcp-load-balancing-udp-load-balancing-nginx-tips-tricks/</a></li>
<li><a href="https://www.elastic.co/blog/playing-http-tricks-nginx">https://www.elastic.co/blog/playing-http-tricks-nginx</a></li>
<li><a href="https://sysadmins.co.za/aws-access-kibana-5-behind-elb-via-nginx-reverse-proxy-on-custom-dns/">https://sysadmins.co.za/aws-access-kibana-5-behind-elb-via-nginx-reverse-proxy-on-custom-dns/</a></li>
</ul>


<center>
<script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Buy Me a Coffee', '#46b798', 'A6423ZIQ');kofiwidget2.draw();</script> 
</center>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AWS: IAM S3 Policy for Cyberduck to Allow Listing Buckets and Access to One Bucket]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/09/15/aws-iam-s3-policy-for-cyberduck-to-allow-listing-buckets-and-access-to-one-bucket/"/>
    <updated>2017-09-15T11:18:17-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/09/15/aws-iam-s3-policy-for-cyberduck-to-allow-listing-buckets-and-access-to-one-bucket</id>
    <content type="html"><![CDATA[<p>When using Cyberduck to access S3, and a account has restrictive policies, you may find error <code>Listing Directory: /</code> failed.</p>

<p>If you have restrictive IAM Policies in your account, this may be due to the fact that <code>S3:ListMyBuckets</code> is not allowed.</p>

<p>In this post we want to allow a user to list all buckets, so that Cyberduck can do the initial list after configuration / launch, and we would like to give the user access to their designated bucket.</p>

<h2>Creating the IAM Policy:</h2>

<p>We will create this IAM Policy and associate the policy to the user&rsquo;s account:</p>

<pre><code class="json">{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "Stmt1480515305000",
            "Effect": "Allow",
            "Action": [
                "s3:ListAllMyBuckets",
                "s3:GetBucketLocation"
            ],
            "Resource": [
                "arn:aws:s3:::*"
            ]
        },
        {
            "Sid": "Stmt1480515305002",
            "Effect": "Allow",
            "Action": [
                "s3:List*",
                "s3:GetObject",
                "s3:PutObject",
                "s3:DeleteObject"
            ],
            "Resource": [
                "arn:aws:s3:::allowed-bucket",
                "arn:aws:s3:::allowed-bucket/*"
            ]
        }
    ]
}
</code></pre>

<p>So here we should be able to list the buckets:</p>

<pre><code class="bash">$ aws --profile cyberduck s3 ls /
2017-06-08 08:27:01 allowed-bucket
2017-05-21 13:39:21 private-bucket
2016-12-21 08:23:45 confidential-bucket
2017-08-10 14:18:19 test-bucket
2016-08-03 12:38:29 datalake-bucket
</code></pre>

<p>Able to list inside the bucket, as well as Get, Put etc.</p>

<pre><code class="bash">$ aws --profile cyberduck s3 ls allowed-bucket/
                           PRE data/
</code></pre>

<p>Unable to list the buckets content which is expected, as we did not mention in the policy:</p>

<pre><code class="bash">$ aws --profile cyberduck s3 ls confidential-bucket/

An error occurred (AccessDenied) when calling the ListObjects operation: Access Denied
</code></pre>

<h2>Resources:</h2>

<ul>
<li><a href="https://aws.amazon.com/blogs/security/writing-iam-policies-how-to-grant-access-to-an-amazon-s3-bucket/">https://aws.amazon.com/blogs/security/writing-iam-policies-how-to-grant-access-to-an-amazon-s3-bucket/</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup HAProxy Load Balancer for MySQL Galera With IP Whitelisting and Backup Servers]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/08/31/setup-haproxy-load-balancer-for-mysql-galera-with-ip-whitelisting-and-backup-servers/"/>
    <updated>2017-08-31T19:15:50-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/08/31/setup-haproxy-load-balancer-for-mysql-galera-with-ip-whitelisting-and-backup-servers</id>
    <content type="html"><![CDATA[<p>Today we will setup a HAProxy Service for our 3 Node MySQL Galera Cluster</p>

<h2>Our Setup:</h2>

<ul>
<li>3 Node Galera MySQL Cluster</li>
<li>3 HAProxy Services (Each HAProxy Service Running on the MySQL Nodes)</li>
<li>MySQL Listens on Port 3307</li>
<li>HAProxy Listens on Port 3306 and Proxies through to 3307</li>
</ul>


<p>I have setup HAProxy on the same node as the MySQL Servers for my use case, but you can also setup HAProxy on a node outside the MySQL Host.</p>

<p>So essentially our MySQL Galera Cluster is a Multi Master Setup, but for now we will only accept connections from Node-A, and have Node-B and Node-C as Backup servers. Should Node-A go down, HAProxy will route connections to Node-B, and if Node-B also goes down, connections will be routed to Node-C.</p>

<p>If the Primary Node, which is Node-A recovers, connections will be restored to Node-A.</p>

<h2>Security:</h2>

<p>We use iptables to allow traffic between the nodes for port TCP/3307 and allow all traffic for Port TCP/3306, as HAProxy will allow the IP Based Access:</p>

<pre><code class="bash Iptables for Each Node">$ iptables -I INPUT -s {Node-A} -p tcp --dport 3307 -j ACCEPT
$ iptables -I INPUT -s {Node-B} -p tcp --dport 3307 -j ACCEPT
$ iptables -I INPUT -s {Node-C} -p tcp --dport 3307 -j ACCEPT
$ iptables -A INPUT -p tcp --dport 3306 -j ACCEPT
$ iptables -A INPUT -p tcp --dport 3307 -j DROP
</code></pre>

<h2>HAProxy:</h2>

<p>Installing HAProxy on Ubuntu:</p>

<pre><code class="bash Install HAProxy">$ sudo apt update 
$ sudo apt install haproxy -y
</code></pre>

<p>Configure HAProxy with a Port 3306 listener, specify your source addresses that you would like to be authorized to communicate with MySQL and then specify the servers to proxy the connections to our MySQL Galera Cluster, specifying 2 backup servers:</p>

<pre><code class="bash /etc/haproxy/haproxy.cfg">global
  log         127.0.0.1 local2
  chroot      /var/lib/haproxy
  pidfile     /var/run/haproxy.pid
  maxconn     1020
  user        haproxy
  group       haproxy
  daemon

  stats socket /var/lib/haproxy/stats.sock mode 600 level admin
  stats timeout 2m

defaults
  mode    tcp
  log     global
  option  dontlognull
  option  redispatch
  retries                   3
  timeout queue             45s
  timeout connect           5s
  timeout client            1m
  timeout server            1m
  timeout check             10s
  maxconn                   1020

listen stats
  bind    *:80
  mode    http
  stats   enable
  stats   show-legends
  stats   refresh           5s
  stats   uri               /
  stats   realm             Haproxy\ Statistics
  stats   auth              admin:secret
  stats   admin             if TRUE

listen galera-lb
  bind    *:3306
  mode    tcp
  acl     network_allowed src 10.10.1.0/24 10.32.15.2/32
  tcp-request               content accept if network_allowed
  tcp-request               content reject
  default_backend           galera-cluster

backend galera-cluster
  balance roundrobin
  server  scw-mysql-1 10.0.0.2:3307  check
  server  scw-mysql-2 10.0.0.3:3307  check backup
  server  scw-mysql-3 10.0.0.4:3307  check backup
</code></pre>

<p>Start HAProxy:</p>

<pre><code class="bash Start HAProxy Service">$ sudo systemctl enable haproxy
$ sudo systemctl restart haproxy
</code></pre>

<h2>Authorize HAProxy Hostnames to Connect to MySQL:</h2>

<p>In this case we need to allow the Hostnames to be able to connect to mysql:</p>

<pre><code class="sql">mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'secrets' WITH GRANT OPTION;
mysql&gt; FLUSH PRIVILEGES;
</code></pre>

<h2>Resources:</h2>

<ul>
<li><a href="https://raymii.org/s/snippets/haproxy_restrict_specific_urls_to_specific_ip_addresses.html">https://raymii.org/s/snippets/haproxy_restrict_specific_urls_to_specific_ip_addresses.html</a></li>
</ul>


<center>
        <script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Buy Me a Coffee', '#46b798', 'A6423ZIQ');kofiwidget2.draw();</script>
</center>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Secure Your Access to Kibana 5 and Elasticsearch 5 With Nginx for AWS]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/08/31/secure-your-access-to-kibana-5-and-elasticsearch-5-with-nginx-for-aws/"/>
    <updated>2017-08-31T10:40:09-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/08/31/secure-your-access-to-kibana-5-and-elasticsearch-5-with-nginx-for-aws</id>
    <content type="html"><![CDATA[<p>As until now, AWS does not offer VPC Support for Elasticsearch, so this make things a bit difficult authorizing Private IP Ranges.</p>

<p>One workaround would be to setup a Nginx Reverse Proxy on AWS within the your Private VPC, associate a EIP on your Nginx EC2 Instance, then authorize your EIP on your Elasticsearch IP Access Policy.</p>

<h2>Our Setup:</h2>

<p>In this setup, we will have an Internal ELB (Elastic Load Balancer), which we will associate 1 or more EC2 Nginx Instances behind the ELB, then setup our Nginx to Revere Proxy our connections through to our Elasticsearch Endpoint.</p>

<p>We will also setup Basic HTTP Authentication for our <code>/</code> elasticsearch endpoint, and our <code>/kibana</code> endpoint. But we will keep the authentication seperate from each other, so that credentials for ES and Kibana is not the same, but depending on your use case, you can allow both endpoints to reference the same credential file.</p>

<h2>Install Nginx</h2>

<p>Depending on your Linux Distribution, the package manager may differ, I am using Amazon Linux:</p>

<pre><code class="bash Install Nginx">$ sudo yum update -y
$ sudo yum install nginx httpd-tools -y
</code></pre>

<h2>Configure Nginx:</h2>

<p>Remove the default configuration and replace the <code>nginx.conf</code> with the following:</p>

<pre><code class="bash Remove Default Nginx Config">$ sudo rm -r /etc/nginx/nginx.conf
</code></pre>

<p>Main Nginx Configuration:</p>

<pre><code class="bash /etc/nginx/nginx.conf">user nginx;
worker_processes auto;
pid /run/nginx.pid;
error_log /var/log/nginx/error.log;

events {
    worker_connections 1024;
}

http {

    # Basic Settings
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    server_names_hash_bucket_size 128;

    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # Logging Settings
        log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

    access_log /var/log/nginx/access.log main;

    # Gzip Settings
    gzip on;
    gzip_disable "msie6";

    # Elasticsearch Config
    include /etc/nginx/conf.d/elasticsearch.conf;
}
</code></pre>

<p>The Reverse Proxy Configuration:</p>

<pre><code class="bash /etc/nginx/conf.d/elasticsearch.conf">server {

  listen 80;
  server_name elk.mydomain.com;

  # error logging
  error_log /var/log/nginx/elasticsearch_error.log;

  # authentication: server wide
  #auth_basic "Auth";
  #auth_basic_user_file /etc/nginx/.secrets;

  location / {

    # authentication: elasticsearch
    auth_basic "Elasticsearch Auth";
    auth_basic_user_file /etc/nginx/.secrets_elasticsearch;

    proxy_http_version 1.1;
    proxy_set_header Host https://search.eu-west-1.es.amazonaws.com;
    proxy_set_header X-Real-IP {NGINX-EIP};
    proxy_set_header Connection "Keep-Alive";
    proxy_set_header Proxy-Connection "Keep-Alive";
    proxy_set_header Authorization "";

    proxy_pass https://search.eu-west-1.es.amazonaws.com/;
    proxy_redirect https://search.eu-west-1.es.amazonaws.com/ http://{NGINX-EIP}/;

  }

  location /kibana {

    # authentication: kibana
    auth_basic "Kibana Auth";
    auth_basic_user_file /etc/nginx/.secrets_kibana;

    proxy_http_version 1.1;
    proxy_set_header Host https://search.eu-west-1.es.amazonaws.com;
    proxy_set_header X-Real-IP {NGINX-EIP};
    proxy_set_header Connection "Keep-Alive";
    proxy_set_header Proxy-Connection "Keep-Alive";
    proxy_set_header Authorization "";

    proxy_pass https://search.eu-west-1.es.amazonaws.com/_plugin/kibana/;
    proxy_redirect https://search.eu-west-1.es.amazonaws.com/_plugin/kibana/ http://{NGINX_EIP}/kibana/;

  }

  # elb checks
  location /status {
    root /usr/share/nginx/html/;
  }

}
</code></pre>

<h2>Setup Authentication:</h2>

<p>Setup the authentication for elasticsearch and kibana:</p>

<pre><code class="bash Create Auth for Kibana and Elasticsearch">$ sudo htpasswd -c /etc/nginx/.secrets_elasticsearch admin
$ sudo htpasswd -c /etc/nginx/.secrets_kibana admin
</code></pre>

<h2>Restart Nginx and Enable on Startup</h2>

<p>Restart the nginx process and enable the process on boot:</p>

<pre><code class="bash Restart Nginx">$ sudo /etc/init.d/nginx restart
$ sudo chkconfig nginx on
</code></pre>

<h2>Configure ELB:</h2>

<p>Create a New Internal ELB, set the Backend Instances on Port 80, and the healthcheck should point to <code>/status/index.html</code> as this location block does not require authentication and our ELB will be able to get a 200 reponse if all is good.
Next you can configure your Route 53 Hosted Zone, <code>elk.mydomain.com</code> to map to your ELB.</p>

<h2>End Result</h2>

<p>Now you should be able to access Elasticsearch on <code>http://elk.mydomain.com/</code> and Kibana on <code>http://elk.mydomain.com/kibana</code> after authenticating.</p>
]]></content>
  </entry>
  
</feed>
