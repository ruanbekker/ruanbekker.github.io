<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Ssh | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/ssh/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2020-10-29T09:58:32+00:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Upload Public SSH Keys Using Ansible]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/10/26/upload-public-ssh-keys-using-ansible/"/>
    <updated>2020-10-26T07:44:25+00:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/10/26/upload-public-ssh-keys-using-ansible</id>
    <content type="html"><![CDATA[<p>In this post I will demonstrate how you can use ansible to automate the task of adding one or more ssh public keys to multiple servers authorized_keys file.</p>

<p>This will be focused in a scenario where you have 5 new ssh keys that we would want to copy to our bastion hosts authorized_keys file</p>

<h2>The User Accounts</h2>

<p>We have our bastion server named <code>bastion.mydomain.com</code> where would like to create the following accounts: <code>john, bob, sarah, sam, adam</code> and also upload their personal ssh public keys to those accounts so that they can logon with their ssh private keys.</p>

<p>On my local directory, I have their ssh public keys as:</p>

<pre><code>~/workspace/sshkeys/john.pub
~/workspace/sshkeys/bob.pub
~/workspace/sshkeys/sarah.pub
~/workspace/sshkeys/sam.pub
~/workspace/sshkeys/adam.pub
</code></pre>

<p>They will be referenced in our playbook as <code>key: ".pub') }}"</code> but if they were on github we can reference them as <code>key: https://github.com/.keys</code>, more info on that can be found on the <a href="https://docs.ansible.com/ansible/2.4/authorized_key_module.html">authorized_key_module</a> documentation.</p>

<h2>The Target Server</h2>

<p>Our inventory for the target server only includes one host, but we can add as many as we want, but our inventory will look like this:</p>

<pre><code>$ cat inventory.ini
[bastion]
bastion-host ansible_host=34.x.x.x ansible_user=ubuntu ansible_ssh_private_key_file=~/.ssh/ansible.pem ansible_python_interpreter=/usr/bin/python3
[bastion:vars]
ansible_ssh_common_args='-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null'
</code></pre>

<p>Test if the target server is reachable using the user <code>ubuntu</code> using our admin accounts ssh key <code>ansible.pem</code>:</p>

<pre><code>$ ansible -i inventory.ini -m ping bastion
bastion | SUCCESS =&gt; {
    "changed": false,
    "ping": "pong"
}
</code></pre>

<h2>Our Playbook</h2>

<p>In this playbook, we will reference the users that we want to create and it will loop through those users, creating them on the target server and also use those names to match to the files on our laptop to match the ssh public keys:</p>

<pre><code>$ cat playbook.yml
---
- hosts: bastion
  become: yes
  become_user: root
  become_method: sudo
  tasks:
    - name: create local user account on the target server
      user:
        name: ''
        comment: ''
        shell: /bin/bash
        append: yes
        groups: sudo
        generate_ssh_key: yes
        ssh_key_type: rsa
      with_items:
        - john
        - bob
        - sarah
        - sam
        - adam

    - name: upload ssh public key to users authorized keys file
      authorized_key:
        user: ''
        state: present
        manage_dir: yes
        key: ".pub') }}"
      with_items:
        - john
        - bob
        - sarah
        - sam
        - adam
</code></pre>

<h2>Deploy</h2>

<p>Run the playbook:</p>

<pre><code>$ ansible-playbook -i inventory.ini ssh-setup.yml

PLAY [bastion]

TASK [Gathering Facts]
ok: [bastion-host]

TASK [create local user account on the target server]
changed: [bastion-host] =&gt; (item=john)
changed: [bastion-host] =&gt; (item=bob)
changed: [bastion-host] =&gt; (item=sarah)
changed: [bastion-host] =&gt; (item=sam)
changed: [bastion-host] =&gt; (item=adam)

TASK [upload ssh public key to users authorized keys file]
changed: [bastion-host] =&gt; (item=john)
changed: [bastion-host] =&gt; (item=bob)
changed: [bastion-host] =&gt; (item=sarah)
changed: [bastion-host] =&gt; (item=sam)
changed: [bastion-host] =&gt; (item=adam)

PLAY RECAP
bastion-host                   : ok=6    changed=5    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
</code></pre>

<p>Now when we ask one of the users, adam for example, to authenticate with:</p>

<pre><code>$ ssh -i ~/.ssh/path_to_his_private_key.pem adamin@bastion.mydomain.com
</code></pre>

<p>They should have access to the server.</p>

<h2>Thank You</h2>

<p>Thanks for reading, for more information on this module check out their documentation:</p>

<ul>
<li><a href="https://docs.ansible.com/ansible/2.4/authorized_key_module.html">https://docs.ansible.com/ansible/2.4/authorized_key_module.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Use a SSH Jump Host With Ansible]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/10/26/use-a-ssh-jump-host-with-ansible/"/>
    <updated>2020-10-26T05:25:18+00:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/10/26/use-a-ssh-jump-host-with-ansible</id>
    <content type="html"><![CDATA[<p>In this post we will demonstrate how to use a SSH Bastion or Jump Host with Ansible to reach the target server.</p>

<p>In some scenarios, the target server might be in a private range which is only accessible via a bastion host, and that counts the same for ansible as ansible is using SSH to reach to the target servers.</p>

<h2>SSH Config</h2>

<p>Our bastion host is configured as <code>bastion</code> and the config under <code>~/.ssh/config</code> looks like this:</p>

<pre><code>Host *
    Port 22
    StrictHostKeyChecking no
    UserKnownHostsFile /dev/null
    ServerAliveInterval 60
    ServerAliveCountMax 30

Host bastion
    HostName bastion.mydomain.com
    User bastion
    IdentityFile ~/.ssh/id_rsa
</code></pre>

<p>To verify that our config is working, you should be able to use:</p>

<pre><code>$ ssh bastion
</code></pre>

<h2>Using a Bastion with Ansible</h2>

<p>In order to reach our target server we need to use the bastion, so to test the SSH connection we can use this SSH one-liner. Our target server has a IP address of <code>172.31.81.94</code> and expects us to provide a <code>ansible.pem</code> private key and we need to authenticate with the <code>ubuntu</code> user:</p>

<pre><code>$ ssh -o ProxyCommand="ssh -W %h:%p -q bastion" -i ~/.ssh/ansible.pem ubuntu@172.31.81.94
</code></pre>

<p>If we can reach our server its time to include it in our playbook.</p>

<p>In our inventory:</p>

<pre><code>$ cat inventory.ini
[deployment]
server-a ansible_host=172.31.81.94 ansible_user=ubuntu ansible_ssh_private_key_file=~/.ssh/ansible.pem
[deployment:vars]
ansible_ssh_common_args='-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ProxyCommand="ssh -W %h:%p -q bastion"'
</code></pre>

<p>And our playbook which will use the ping module:</p>

<pre><code>$ cat playbook.yml
- name: Test Ping
  hosts: deployment
  tasks:
  - action: ping
</code></pre>

<p>Test it out:</p>

<pre><code>$ ansible-playbook -i inventory.ini ping.yml

PLAY [Test Ping] ***********************************************************************************************************************************************************

TASK [Gathering Facts] *****************************************************************************************************************************************************
ok: [server-a]

TASK [ping] ****************************************************************************************************************************************************************
ok: [server-a]

PLAY RECAP *****************************************************************************************************************************************************************
server-a                   : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using ProxyJump With SSH for VMs With No Public IPs]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/06/13/using-proxyjump-with-ssh-for-vms-with-no-public-ips/"/>
    <updated>2020-06-13T20:06:35+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/06/13/using-proxyjump-with-ssh-for-vms-with-no-public-ips</id>
    <content type="html"><![CDATA[<p><img src="https://img.sysadmins.co.za/wngib2.png" alt="ssh-proxy-jump" /></p>

<p>I have a dedicated server with LXD installed where I have a bunch of system containers running to host a lot of my playground services, and to access the operating system of those lxc containers, I need to SSH to the LXD host, then exec or ssh into that LXC container.</p>

<p>This became tedious and wanted a way to directly ssh to them, as they don&rsquo;t have public ip addresses, it&rsquo;s not possible but found its possible to access them using proxyjump.</p>

<pre><code>[you] -&gt; [hypervisor] -&gt; [vm on hypervisor]
</code></pre>

<p>First step is to create our ssh key:</p>

<pre><code>$ ssh-keygen -t rsa
</code></pre>

<p>Add the created public key (<code>~/.ssh/id_rsa.pub</code>) on the hypervisor and the target vm&rsquo;s <code>~/.ssh/authorized_key</code> files.</p>

<p>Then create the SSH Config on your local workstation (<code>~/.ssh/config</code>):</p>

<pre><code>Host *
  StrictHostKeyChecking no
  UserKnownHostsFile=/dev/null

Host hypervisor
  Hostname hv.domain.com
  User myuser
  IdentityFile ~/.ssh/id_rsa

Host ctr1
  Hostname 10.37.117.132
  User root
  IdentityFile ~/.ssh/id_rsa
  ProxyJump hypervisor
</code></pre>

<p>Now accessing our lxc container ctr1, is possible by doing:</p>

<pre><code>$ ssh ctr1
Warning: Permanently added 'x,x' (ECDSA) to the list of known hosts.
Warning: Permanently added '10.37.117.132' (ECDSA) to the list of known hosts.
root@ctr1~ $
</code></pre>

<p>Thank you for reading</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using a SSH Reverse Tunnel to Access Nodes on Private Ranges]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/06/13/using-a-ssh-reverse-tunnel-to-access-nodes-on-private-ranges/"/>
    <updated>2020-06-13T19:59:27+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/06/13/using-a-ssh-reverse-tunnel-to-access-nodes-on-private-ranges</id>
    <content type="html"><![CDATA[<p><img src="https://img.sysadmins.co.za/wngib2.png" alt="ssh-tunneling" /></p>

<p>Personal utility (actually just a command) that I use to reach my Raspberry Pi Nodes that has no direct route via the Internet</p>

<h2>Other Projects</h2>

<p>There&rsquo;s a lot of other tools out there that&rsquo;s already solving this issue, such as <a href="https://inlets.dev">inlets</a>, but I wanted my own, so that I can extend features to it as it pleases me.</p>

<h2>Overview</h2>

<p>This is more ore less how it looks like:</p>

<pre><code>[VPS] &lt;-- Has a Public IP
 |
 |
 [HOME NETWORK] &lt;-- Dynamic IP
   |
   |
 [rpi-01:22], [rpi-02:22] &lt;-- Private IPs
</code></pre>

<ul>
<li>SSH Tunnel is setup from the Raspberry Pi Nodes</li>
<li>Each Raspberry Pi sets up a unique port on the VPS for the tunnel to traverse to the Rpi on port 22</li>
<li>To reach Rpi-01, you hop onto the VPS and ssh to localhost port 2201</li>
<li>To reach Rpi-02, you hop onto the VPS and ssh to localhost port 2202, etc</li>
</ul>


<h2>Progress</h2>

<p>The tool will still be built, but using ssh it&rsquo;s quite easy</p>

<h2>Usage</h2>

<p>Setup the SSH Reverse Tunnel from rpi-01:</p>

<pre><code>$ ssh -i ~/.ssh/bastion.pem \
  -o StrictHostKeyChecking=no \
  -o UserKnownHostsFile=/dev/null \
  -o ServerAliveInterval=60 \
  -N -R 2201:localhost:22 \
  -p 22 ruan@bastion-9239.domain.cloud
</code></pre>

<p>Setup the SSH Reverse Tunnel from rpi-02:</p>

<pre><code>$ ssh -i ~/.ssh/bastion.pem \
  -o StrictHostKeyChecking=no \
  -o UserKnownHostsFile=/dev/null \
  -o ServerAliveInterval=60 \
  -N -R 2202:localhost:22 \
  -p 22 ruan@bastion-9239.domain.cloud
</code></pre>

<p>On the VPS, we can see that we have port 2021 and 2022 listening:</p>

<pre><code>$ netstat -tulpn
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 127.0.0.1:2201          0.0.0.0:*               LISTEN      -
tcp        0      0 127.0.0.1:2202          0.0.0.0:*               LISTEN      -
</code></pre>

<p>To connect to rpi-01, we ssh to localhost on port 2201, from the VPS:</p>

<pre><code>$ ssh -p 2201 pi@localhost
pi@rpi-01:~ $
</code></pre>

<p>To connect to rpi-02, we ssh to localhost on port 2202 from the VPS:</p>

<pre><code>$ ssh -p 2202 pi@localhost
pi@rpi-02:~ $
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Capturing 54 Million Passwords With a Docker SSH Honeypot]]></title>
    <link href="https://blog.ruanbekker.com/blog/2018/10/11/capturing-54-million-passwords-with-a-docker-ssh-honeypot/"/>
    <updated>2018-10-11T16:38:52-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2018/10/11/capturing-54-million-passwords-with-a-docker-ssh-honeypot</id>
    <content type="html"><![CDATA[<p><img src="https://res.cloudinary.com/rbekker/image/upload/v1539291851/ssh-docker-honeypot_eyhzc7.png" alt="" /></p>

<p>The last couple of days I picked up on my ELK Stack a couple thousands of SSH Brute Force Attacks, so I decided I will just revisit my SSH Server configuration, and change my SSH Port to something else for the interim. The dashboard that showed me the results at that point in time:</p>

<p><img src="https://res.cloudinary.com/rbekker/image/upload/v1539292443/kibana-failed-ssh-auth_udkxkl.png" alt="" /></p>

<p>Then I decided I actually would like to setup a SSH Honeypot to listen on Port 22 and change my SSH Server to listen on 222 and capture their IP Addresses, Usernames and Passwords that they are trying to use and dump it all in a file so that I can build up my own password dictionary :D</p>

<h2>SSH Configuration:</h2>

<p>Changing the SSH Port:</p>

<pre><code class="bash">$ sudo vim /etc/ssh/sshd_config
</code></pre>

<p>Change the port to 222:</p>

<pre><code class="bash">Port 222
</code></pre>

<p>Restart the SSH Server:</p>

<pre><code class="bash">$ sudo /etc/init.d/ssh restart
</code></pre>

<p>Verify that the SSH Server is running on the new port:</p>

<pre><code class="bash">$ sudo netstat -tulpn | grep sshd
tcp        0      0 0.0.0.0:222            0.0.0.0:*               LISTEN      28838/sshd
</code></pre>

<h2>Docker SSH Honeypot:</h2>

<p>Thanks to <a href="https://github.com/random-robbie/docker-ssh-honey">random-robbie</a>, as he had everything I was looking for on Github.</p>

<p>Setup the SSH Honeypot:</p>

<pre><code class="bash">$ git clone https://github.com/random-robbie/docker-ssh-honey
$ cd docker-ssh-honey/
$ docker build . -t local:ssh-honepot
$ docker run -itd --name ssh-honeypot -p 22:22 local:ssh-honepot
</code></pre>

<p>Once people attempt to ssh, you will get the output to stdout:</p>

<pre><code class="bash">$ docker logs -f $(docker ps -f name=ssh-honeypot -q) | grep -v 'Error exchanging' | head -10
[Tue Jul 31 01:13:41 2018] ssh-honeypot 0.0.8 by Daniel Roberson started on port 22. PID 5
[Tue Jul 31 01:19:49 2018] 1xx.1xx.1xx.1x gambaa gambaa
[Tue Jul 31 01:23:26 2018] 1xx.9x.1xx.1xx root toor
[Tue Jul 31 01:25:57 2018] 1xx.2xx.1xx.1xx root Passw0rd1234
[Tue Jul 31 01:26:00 2018] 1xx.2xx.1xx.1xx root Qwer1234
[Tue Jul 31 01:26:00 2018] 1xx.2xx.1xx.1xx root Abcd1234
[Tue Jul 31 01:26:08 2018] 1xx.2xx.1xx.1xx root ubuntu
[Tue Jul 31 01:26:09 2018] 1xx.2xx.1xx.1xx root PassWord
[Tue Jul 31 01:26:10 2018] 1xx.2xx.1xx.1xx root password321
[Tue Jul 31 01:26:15 2018] 1xx.2xx.1xx.1xx root zxcvbnm
</code></pre>

<h2>Saving results to disk:</h2>

<p>Redirecting the output to a log file, running in the foreground as a screen session:</p>

<pre><code class="bash">$ screen -S honeypot
$ docker logs -f f6cb | grep -v 'Error exchanging' | awk '{print $6, $7, $8}' &gt;&gt; /var/log/ssh-honeypot.log
</code></pre>

<p>Detach from your screen session:</p>

<pre><code class="bash">Ctrl + a; d
</code></pre>

<p>Checking out the logs</p>

<pre><code class="bash">$ head -3 /var/log/ssh-honeypot.log
2.7.2x.1x root jiefan
4x.7.2x.1x root HowAreYou
4x.7.2x.1x root Sqladmin
</code></pre>

<p>Leaving this running for a couple of months, and I have a massive password database:</p>

<pre><code class="bash">$ wc -l /var/log/honeypot/ssh.log
54184260 /var/log/honeypot/ssh.log
</code></pre>

<p>That is correct, 54 million password attempts. 5372 Unique IPs, 4082 Unique Usernames, 88829 Unique Passwords.</p>
]]></content>
  </entry>
  
</feed>
