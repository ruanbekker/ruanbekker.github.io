<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Ssh | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/ssh/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2020-10-08T00:43:27+00:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Using ProxyJump With SSH for VMs With No Public IPs]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/06/13/using-proxyjump-with-ssh-for-vms-with-no-public-ips/"/>
    <updated>2020-06-13T20:06:35+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/06/13/using-proxyjump-with-ssh-for-vms-with-no-public-ips</id>
    <content type="html"><![CDATA[<p><img src="https://img.sysadmins.co.za/wngib2.png" alt="ssh-proxy-jump" /></p>

<p>I have a dedicated server with LXD installed where I have a bunch of system containers running to host a lot of my playground services, and to access the operating system of those lxc containers, I need to SSH to the LXD host, then exec or ssh into that LXC container.</p>

<p>This became tedious and wanted a way to directly ssh to them, as they don&rsquo;t have public ip addresses, it&rsquo;s not possible but found its possible to access them using proxyjump.</p>

<pre><code>[you] -&gt; [hypervisor] -&gt; [vm on hypervisor]
</code></pre>

<p>First step is to create our ssh key:</p>

<pre><code>$ ssh-keygen -t rsa
</code></pre>

<p>Add the created public key (<code>~/.ssh/id_rsa.pub</code>) on the hypervisor and the target vm&rsquo;s <code>~/.ssh/authorized_key</code> files.</p>

<p>Then create the SSH Config on your local workstation (<code>~/.ssh/config</code>):</p>

<pre><code>Host *
  StrictHostKeyChecking no
  UserKnownHostsFile=/dev/null

Host hypervisor
  Hostname hv.domain.com
  User myuser
  IdentityFile ~/.ssh/id_rsa

Host ctr1
  Hostname 10.37.117.132
  User root
  IdentityFile ~/.ssh/id_rsa
  ProxyJump hypervisor
</code></pre>

<p>Now accessing our lxc container ctr1, is possible by doing:</p>

<pre><code>$ ssh ctr1
Warning: Permanently added 'x,x' (ECDSA) to the list of known hosts.
Warning: Permanently added '10.37.117.132' (ECDSA) to the list of known hosts.
root@ctr1~ $
</code></pre>

<p>Thank you for reading</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using a SSH Reverse Tunnel to Access Nodes on Private Ranges]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/06/13/using-a-ssh-reverse-tunnel-to-access-nodes-on-private-ranges/"/>
    <updated>2020-06-13T19:59:27+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/06/13/using-a-ssh-reverse-tunnel-to-access-nodes-on-private-ranges</id>
    <content type="html"><![CDATA[<p><img src="https://img.sysadmins.co.za/wngib2.png" alt="ssh-tunneling" /></p>

<p>Personal utility (actually just a command) that I use to reach my Raspberry Pi Nodes that has no direct route via the Internet</p>

<h2>Other Projects</h2>

<p>There&rsquo;s a lot of other tools out there that&rsquo;s already solving this issue, such as <a href="https://inlets.dev">inlets</a>, but I wanted my own, so that I can extend features to it as it pleases me.</p>

<h2>Overview</h2>

<p>This is more ore less how it looks like:</p>

<pre><code>[VPS] &lt;-- Has a Public IP
 |
 |
 [HOME NETWORK] &lt;-- Dynamic IP
   |
   |
 [rpi-01:22], [rpi-02:22] &lt;-- Private IPs
</code></pre>

<ul>
<li>SSH Tunnel is setup from the Raspberry Pi Nodes</li>
<li>Each Raspberry Pi sets up a unique port on the VPS for the tunnel to traverse to the Rpi on port 22</li>
<li>To reach Rpi-01, you hop onto the VPS and ssh to localhost port 2201</li>
<li>To reach Rpi-02, you hop onto the VPS and ssh to localhost port 2202, etc</li>
</ul>


<h2>Progress</h2>

<p>The tool will still be built, but using ssh it&rsquo;s quite easy</p>

<h2>Usage</h2>

<p>Setup the SSH Reverse Tunnel from rpi-01:</p>

<pre><code>$ ssh -i ~/.ssh/bastion.pem \
  -o StrictHostKeyChecking=no \
  -o UserKnownHostsFile=/dev/null \
  -o ServerAliveInterval=60 \
  -N -R 2201:localhost:22 \
  -p 22 ruan@bastion-9239.domain.cloud
</code></pre>

<p>Setup the SSH Reverse Tunnel from rpi-02:</p>

<pre><code>$ ssh -i ~/.ssh/bastion.pem \
  -o StrictHostKeyChecking=no \
  -o UserKnownHostsFile=/dev/null \
  -o ServerAliveInterval=60 \
  -N -R 2202:localhost:22 \
  -p 22 ruan@bastion-9239.domain.cloud
</code></pre>

<p>On the VPS, we can see that we have port 2021 and 2022 listening:</p>

<pre><code>$ netstat -tulpn
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 127.0.0.1:2201          0.0.0.0:*               LISTEN      -
tcp        0      0 127.0.0.1:2202          0.0.0.0:*               LISTEN      -
</code></pre>

<p>To connect to rpi-01, we ssh to localhost on port 2201, from the VPS:</p>

<pre><code>$ ssh -p 2201 pi@localhost
pi@rpi-01:~ $
</code></pre>

<p>To connect to rpi-02, we ssh to localhost on port 2202 from the VPS:</p>

<pre><code>$ ssh -p 2202 pi@localhost
pi@rpi-02:~ $
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Capturing 54 Million Passwords With a Docker SSH Honeypot]]></title>
    <link href="https://blog.ruanbekker.com/blog/2018/10/11/capturing-54-million-passwords-with-a-docker-ssh-honeypot/"/>
    <updated>2018-10-11T16:38:52-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2018/10/11/capturing-54-million-passwords-with-a-docker-ssh-honeypot</id>
    <content type="html"><![CDATA[<p><img src="https://res.cloudinary.com/rbekker/image/upload/v1539291851/ssh-docker-honeypot_eyhzc7.png" alt="" /></p>

<p>The last couple of days I picked up on my ELK Stack a couple thousands of SSH Brute Force Attacks, so I decided I will just revisit my SSH Server configuration, and change my SSH Port to something else for the interim. The dashboard that showed me the results at that point in time:</p>

<p><img src="https://res.cloudinary.com/rbekker/image/upload/v1539292443/kibana-failed-ssh-auth_udkxkl.png" alt="" /></p>

<p>Then I decided I actually would like to setup a SSH Honeypot to listen on Port 22 and change my SSH Server to listen on 222 and capture their IP Addresses, Usernames and Passwords that they are trying to use and dump it all in a file so that I can build up my own password dictionary :D</p>

<h2>SSH Configuration:</h2>

<p>Changing the SSH Port:</p>

<pre><code class="bash">$ sudo vim /etc/ssh/sshd_config
</code></pre>

<p>Change the port to 222:</p>

<pre><code class="bash">Port 222
</code></pre>

<p>Restart the SSH Server:</p>

<pre><code class="bash">$ sudo /etc/init.d/ssh restart
</code></pre>

<p>Verify that the SSH Server is running on the new port:</p>

<pre><code class="bash">$ sudo netstat -tulpn | grep sshd
tcp        0      0 0.0.0.0:222            0.0.0.0:*               LISTEN      28838/sshd
</code></pre>

<h2>Docker SSH Honeypot:</h2>

<p>Thanks to <a href="https://github.com/random-robbie/docker-ssh-honey">random-robbie</a>, as he had everything I was looking for on Github.</p>

<p>Setup the SSH Honeypot:</p>

<pre><code class="bash">$ git clone https://github.com/random-robbie/docker-ssh-honey
$ cd docker-ssh-honey/
$ docker build . -t local:ssh-honepot
$ docker run -itd --name ssh-honeypot -p 22:22 local:ssh-honepot
</code></pre>

<p>Once people attempt to ssh, you will get the output to stdout:</p>

<pre><code class="bash">$ docker logs -f $(docker ps -f name=ssh-honeypot -q) | grep -v 'Error exchanging' | head -10
[Tue Jul 31 01:13:41 2018] ssh-honeypot 0.0.8 by Daniel Roberson started on port 22. PID 5
[Tue Jul 31 01:19:49 2018] 1xx.1xx.1xx.1x gambaa gambaa
[Tue Jul 31 01:23:26 2018] 1xx.9x.1xx.1xx root toor
[Tue Jul 31 01:25:57 2018] 1xx.2xx.1xx.1xx root Passw0rd1234
[Tue Jul 31 01:26:00 2018] 1xx.2xx.1xx.1xx root Qwer1234
[Tue Jul 31 01:26:00 2018] 1xx.2xx.1xx.1xx root Abcd1234
[Tue Jul 31 01:26:08 2018] 1xx.2xx.1xx.1xx root ubuntu
[Tue Jul 31 01:26:09 2018] 1xx.2xx.1xx.1xx root PassWord
[Tue Jul 31 01:26:10 2018] 1xx.2xx.1xx.1xx root password321
[Tue Jul 31 01:26:15 2018] 1xx.2xx.1xx.1xx root zxcvbnm
</code></pre>

<h2>Saving results to disk:</h2>

<p>Redirecting the output to a log file, running in the foreground as a screen session:</p>

<pre><code class="bash">$ screen -S honeypot
$ docker logs -f f6cb | grep -v 'Error exchanging' | awk '{print $6, $7, $8}' &gt;&gt; /var/log/ssh-honeypot.log
</code></pre>

<p>Detach from your screen session:</p>

<pre><code class="bash">Ctrl + a; d
</code></pre>

<p>Checking out the logs</p>

<pre><code class="bash">$ head -3 /var/log/ssh-honeypot.log
2.7.2x.1x root jiefan
4x.7.2x.1x root HowAreYou
4x.7.2x.1x root Sqladmin
</code></pre>

<p>Leaving this running for a couple of months, and I have a massive password database:</p>

<pre><code class="bash">$ wc -l /var/log/honeypot/ssh.log
54184260 /var/log/honeypot/ssh.log
</code></pre>

<p>That is correct, 54 million password attempts. 5372 Unique IPs, 4082 Unique Usernames, 88829 Unique Passwords.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SSH Tools That Comes in Handy When Dealing With Multiple Servers]]></title>
    <link href="https://blog.ruanbekker.com/blog/2018/05/31/ssh-tools-that-comes-in-handy-when-dealing-with-multiple-servers/"/>
    <updated>2018-05-31T05:18:11-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2018/05/31/ssh-tools-that-comes-in-handy-when-dealing-with-multiple-servers</id>
    <content type="html"><![CDATA[<p><img src="https://objects.ruanbekker.com/assets/images/header-ssh-tips.png" alt="" /></p>

<p>When dealing with a lot of servers where you need to ssh to different servers and especially if they require different authentication from different private ssh keys, it kinda gets annoying specifying the private key you need, when you want to SSH to them.</p>

<h2>SSH Config</h2>

<p>SSH Config: <code>~/.ssh/config</code> is powerful!</p>

<p>In this config file, you can specify the remote host, the key, user and the alias, so that when you want to SSH to it, you dont have to use the fully qualified domain name or IP address.</p>

<p>Let&rsquo;s take for example our server-a with the following details:</p>

<ul>
<li>FQDN: host1.eu.compute.domain.coom</li>
<li>User: james</li>
<li>PrivateKeyFile: /path/to/key.pem</li>
<li>Disable Strict Host Checking</li>
</ul>


<p>So to access that host, you would use the following command (without ssh config):</p>

<pre><code class="bash">$ ssh -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -i /path/to/key.pem james@host1.eu.compute.domain.com
</code></pre>

<p>Now with SSH Config, open up the config file:</p>

<pre><code class="">$ vim ~/.ssh/config
</code></pre>

<p>and declare the host details:</p>

<pre><code>Host host1
  Hostname host1.eu.compute.domain.com
  User james
  IdentityFile /path/to/key.pem
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null
</code></pre>

<p>Now, if we need to SSH to it, we can do it as simply as:</p>

<pre><code class="bash">$ ssh host1
</code></pre>

<p>as it will pull in the configs from the config that is described from the host alias that you calling from the argument of the ssh binary.</p>

<h2>SSH Timeout</h2>

<p>Appending to our SSH Config, we can configure either our client or server to prevent SSH Timeouts due to inactivity.</p>

<ul>
<li>SSH Timeout on our Client:</li>
</ul>


<pre><code class="bash">$ vim ~/.ssh/config
</code></pre>

<p>Here we can set how often a NULL Packet is sent to the SSH Connections to keep the connection alive, in this case every 120 seconds:</p>

<pre><code>ServerAliveInterval 120
</code></pre>

<ul>
<li>SSH Timeout on the Servers:</li>
</ul>


<pre><code class="bash">$ vim /etc/ssh/sshd_config
</code></pre>

<p>Below we have 2 properties, the interval of how often to instruct the client connected to send a NULL packet to keep the connection alive and the max number of intervals, so for a idle connection to timeout in 24 hours, we will take 86400 seconds which is 24 hours, divide into 120 second intervals, which gives as 720 intervals.</p>

<p>So the config will look like this:</p>

<pre><code>ClientAliveInterval 120
ClientAliveCountMax 720
</code></pre>

<p>The restart the sshd service:</p>

<pre><code class="bash">$ /etc/init.d/sshd restart
</code></pre>

<h2>SSH Agent</h2>

<p>Another handy tool is <code>ssh-agent</code>, if you have password encryption on your key, everytime you need to ssh, a password will be prompted. A way to get around this is to use the ssh-agent.</p>

<p>We also want to set a TTL to the ssh-agent, as we don&rsquo;t want it to run forever (unless you want it to). In this case I will let the ssh-agent exit after 2 hours. It will also only run in the shell session from where you execute it. Lets start up our ssh-agent:</p>

<pre><code class="bash">$ eval $(ssh-agent -t 7200)
Agent pid 88760 
</code></pre>

<p>Now add the private key to the ssh-agent. If your private key is password protected, it will prompt you for the password and after successful verification the key will be added:</p>

<pre><code>$ ssh-add /path/to/key.pem
Identity added: /path/to/key.pem (/path/to/key.pem)
</code></pre>

<h2>Multiple Github Accounts:</h2>

<p>Here is a great post on how to work with different GitHub Accounts:
- <a href="https://gist.github.com/jexchan/2351996">https://gist.github.com/jexchan/2351996</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Forwarding the Docker Socket via a SSH Tunnel to Execute Docker Commands Locally]]></title>
    <link href="https://blog.ruanbekker.com/blog/2018/04/30/forwarding-the-docker-socket-via-a-ssh-tunnel-to-execute-docker-commands-locally/"/>
    <updated>2018-04-30T08:30:23-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2018/04/30/forwarding-the-docker-socket-via-a-ssh-tunnel-to-execute-docker-commands-locally</id>
    <content type="html"><![CDATA[<p>With automation in mind, when you want to execute docker commands remotely, you want to do it in a secure manner, as you don&rsquo;t want to expose your Docker port to the whole world.</p>

<p>One way in doing that, is forwarding the remote docker socket via a local port over a SSH Tunnel. With this way, you can execute docker commands locally on your workstation, as if the swarm is running on your workstation/laptop/node/bastion host etc.</p>

<p>Without the tunnel, I have a swarm on my laptop with no running services:</p>

<pre><code class="bash">$ docker service ls
ID                  NAME                   MODE                REPLICAS            IMAGE                                                               PORTS
</code></pre>

<p>As you can see, we have no services running, but the remote swarm has a couple, so after forwarding the connection, we should see our remote services.</p>

<script id="mNCC" language="javascript">
    medianet_width = "728";
    medianet_height = "90";
    medianet_crid = "218284798";
    medianet_versionId = "3111299"; 
  </script>


<script src="//contextual.media.net/nmedianet.js?cid=8CUD78FSV"></script>


<h2>Setting up the SSH Tunnel:</h2>

<p>Here we will forward the remote docker socket: <code>/var/run/docker.sock</code> to a local port bound to localhost: <code>localhost:2377</code>:</p>

<pre><code class="bash">$ screen -S docker
$ ssh -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -i ~/path/to/key.pem -NL localhost:2377:/var/run/docker.sock root@docker-managers.mydomain.com
</code></pre>

<p>Now the SSH Tunnel will be established, and you can detach your screen session, or open a new shell session. To detach your screen session: <code>'ctrl + a' then d</code></p>

<h2>Verifying that the tunnel is established:</h2>

<p>You can use netstat to verify that the port is listening:</p>

<pre><code class="bash">$ netstat -ant | grep 2377
tcp4       0      0  127.0.0.1.2377         *.*                    LISTEN
</code></pre>

<h2>Inform the Docker Client to use the Port:</h2>

<p>Now we need to inform the docker client, to use the new port to talk to the docker daemon. We do that by setting the <code>DOCKER_HOST</code> environment variable to point to <code>localhost:2377</code>:</p>

<pre><code class="bash">$ export DOCKER_HOST="localhost:2377"
</code></pre>

<p>This will remain for the lifetime of the shell session.</p>

<h2>Testing it Out:</h2>

<p>Now we can run our commands locally, and we should see the output of our remote swarm:</p>

<pre><code class="bash">$ docker service ls
ID                  NAME                   MODE                REPLICAS            IMAGE                                                               PORTS
xjta8e3ek2u2        apps_flask_reminders   replicated          3/3                 rbekker87/flask-reminders:debian
0l7ruktbqj99        apps_kibana            replicated          1/1                 kibana:latest
...
</code></pre>

<h2>Terminating our SSH Tunnel:</h2>

<p>To terminate our SSH Tunnel, reconnect to your shell session, and hit <code>ctrl + c</code>:</p>

<pre><code class="bash">$ screen -ls 
There is a screen on:
    50413.docker    (Detached)
$ screen -r 50413
</code></pre>

<p>Hit <code>ctrl + c</code> :</p>

<pre><code class="bash">CKilled by signal 2.
</code></pre>

<p>And exit the screen session:</p>

<pre><code class="bash">$ exit
</code></pre>

<p>With this way, you can do lots of automation with docker swarm, not limited to swarm, but one of them.</p>
]]></content>
  </entry>
  
</feed>
