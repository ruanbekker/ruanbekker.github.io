<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Storage | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/storage/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2018-02-16T09:14:52-05:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Guide to Setup Docker Convoy Volume Driver for Docker Swarm With NFS]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/02/16/guide-to-setup-docker-convoy-volume-driver-for-docker-swarm-with-nfs/"/>
    <updated>2018-02-16T08:51:59-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/02/16/guide-to-setup-docker-convoy-volume-driver-for-docker-swarm-with-nfs</id>
    <content type="html"><![CDATA[<p><img src="http://obj-cache.cloud.ruanbekker.com/docker-logo.png" alt="" /></p>

<p>In this post we will setup Rancher&rsquo;s Convoy Storage Plugin with NFS, to provide data persistence in Docker Swarm.</p>

<h2>The Overview:</h2>

<p>This essentially means that we will have a NFS Volume, when the service gets created on Docker Swarm, the cluster creates these volumes with path mapping, so when a container gets spawned, restarted, scaled etc, the container that gets started on the new node will be aware of the volume, and will get the data that its expecting.</p>

<p>Its also good to note that our NFS Server will be a single point of failure, therefore its also good to look at a Distributed Volume like <a href="https://sysadmins.co.za/tag/glusterfs">GlusterFS</a>, <a href="https://sysadmins.co.za/tag/xtreemfs/">XtreemFS</a>, <a href="">Ceph</a>, etc.</p>

<ul>
<li>NFS Server (10.8.133.83)</li>
<li>Rancher Convoy Plugin on Each Docker Node in the Swarm (10.8.133.83, 10.8.166.19, 10.8.142.195)</li>
</ul>


<h2>Setup NFS:</h2>

<p>Setup the NFS Server</p>

<pre><code class="bash">$ sudo apt-get install nfs-kernel-server nfs-common -y
$ mkdir /vol
$ chown -R nobody:nogroup /vol
$ echo '/vol 10.8.133.83(rw,sync,no_subtree_check) 10.8.166.19(rw,sync,no_subtree_check) 10.8.142.195(rw,sync,no_subtree_check)' &gt;&gt; /etc/exports
$ sudo systemctl restart nfs-kernel-server
$ sudo systemctl enable nfs-kernel-server
</code></pre>

<p>Setup the NFS Clients on each Docker Node:</p>

<pre><code class="bash">$ sudo apt-get install nfs-common -y
$ mount 10.8.133.83:/vol /mnt
$ umount /mnt
$ df -h
</code></pre>

<p>If you can see tht the volume is mounted, unmount it and add it to the <code>fstab</code> so the volume can be mounted on boot:</p>

<pre><code class="bash">$ sudo bash -c "echo '10.8.133.83:/vol /mnt nfs auto,nofail,noatime,nolock,intr,tcp,actimeo=1800 0 0' &gt;&gt; /etc/fstab"
$ sudo mount -a
</code></pre>

<h2>Install Rancher Convoy Plugin:</h2>

<pre><code class="bash">$ cd /tmp
$ wget https://github.com/rancher/convoy/releases/download/v0.5.0/convoy.tar.gz
$ tar xzf convoy.tar.gz
$ sudo cp convoy/convoy convoy/convoy-pdata_tools /usr/local/bin/
$ sudo mkdir -p /etc/docker/plugins/
$ sudo bash -c 'echo "unix:///var/run/convoy/convoy.sock" &gt; /etc/docker/plugins/convoy.spec'
</code></pre>

<h2>Create the init script:</h2>

<p>Thanks to <a href="https://gist.github.com/deviantony/557984d62e867e6f505577b207db6ffc">deviantony</a></p>

<pre><code class="bash">#!/bin/sh
### BEGIN INIT INFO
# Provides:
# Required-Start:    $remote_fs $syslog
# Required-Stop:     $remote_fs $syslog
# Default-Start:     2 3 4 5
# Default-Stop:      0 1 6
# Short-Description: Start daemon at boot time
# Description:       Enable service provided by daemon.
### END INIT INFO

dir="/usr/local/bin"
cmd="convoy daemon --drivers vfs --driver-opts vfs.path=/mnt/docker/volumes"
user="root"
name="convoy"

pid_file="/var/run/$name.pid"
stdout_log="/var/log/$name.log"
stderr_log="/var/log/$name.err"

get_pid() {
    cat "$pid_file"
}

is_running() {
    [ -f "$pid_file" ] &amp;&amp; ps `get_pid` &gt; /dev/null 2&gt;&amp;1
}

case "$1" in
    start)
    if is_running; then
        echo "Already started"
    else
        echo "Starting $name"
        cd "$dir"
        if [ -z "$user" ]; then
            sudo $cmd &gt;&gt; "$stdout_log" 2&gt;&gt; "$stderr_log" &amp;
        else
            sudo -u "$user" $cmd &gt;&gt; "$stdout_log" 2&gt;&gt; "$stderr_log" &amp;
        fi
        echo $! &gt; "$pid_file"
        if ! is_running; then
            echo "Unable to start, see $stdout_log and $stderr_log"
            exit 1
        fi
    fi
    ;;
    stop)
    if is_running; then
        echo -n "Stopping $name.."
        kill `get_pid`
        for i in {1..10}
        do
            if ! is_running; then
                break
            fi

            echo -n "."
            sleep 1
        done
        echo

        if is_running; then
            echo "Not stopped; may still be shutting down or shutdown may have failed"
            exit 1
        else
            echo "Stopped"
            if [ -f "$pid_file" ]; then
                rm "$pid_file"
            fi
        fi
    else
        echo "Not running"
    fi
    ;;
    restart)
    $0 stop
    if is_running; then
        echo "Unable to stop, will not attempt to start"
        exit 1
    fi
    $0 start
    ;;
    status)
    if is_running; then
        echo "Running"
    else
        echo "Stopped"
        exit 1
    fi
    ;;
    *)
    echo "Usage: $0 {start|stop|restart|status}"
    exit 1
    ;;
esac

exit 0
</code></pre>

<h2>Externally Managed Convoy Volumes</h2>

<p>One thing to note is that, after your delete a volume, you will still need to delete the directory from the path where its hosted, as the application does not do that by itself.</p>

<p>Creating the Volume Before hand:</p>

<pre><code class="bash">$ convoy create test1
test1

$ docker volume ls
DRIVER              VOLUME NAME
convoy              test1

$ cat /mnt/docker/volumes/config/vfs_volume_test1.json
{"Name":"test1","Size":0,"Path":"/mnt/docker/volumes/test1","MountPoint":"","PrepareForVM":false,"CreatedTime":"Mon Feb 05 13:07:05 +0000 2018","Snapshots":{}}
</code></pre>

<p>Viewing the volume from another node:</p>

<pre><code class="bash">$ docker volume ls
DRIVER              VOLUME NAME
convoy              test1
</code></pre>

<h2>Creating a Test Service:</h2>

<p>Create a test service to test the data persistence, our docker-compose.yml:</p>

<pre><code class="bash">version: '3.4'

volumes:
  test1:
    external: true

networks:
  appnet:
    external: true

services:
  test:
    image: alpine:edge
    command: sh -c "ping 127.0.0.1"
    volumes:
      - test1:/data
    networks:
      - appnet
</code></pre>

<p>Creating the Overlay Network and Deploying the Stack:</p>

<pre><code class="bash">$ docker network create -d overlay appnet
$ docker stack deploy -c docker-compose.yml apps
Creating service apps_test
</code></pre>

<p>Write data to the volume in the container:</p>

<pre><code>$ docker exec -it apps_test.1.iojo7fpw8jirqjs3iu8qr7qpe sh
/ # echo "ok" &gt; /data/file.txt
/ # cat /data/file.txt
ok
</code></pre>

<p>Scale the service:</p>

<pre><code class="bash">$ docker service scale apps_test=2
apps_test scaled to 2
</code></pre>

<p>Inspect to see if the new replica is on another node:</p>

<pre><code class="bash">$ docker service ps apps_test
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE               ERROR                         PORTS
myrq2pc3z26z        apps_test.1         alpine:edge         scw-docker-1        Running             Running 45 seconds ago
ny8t97l2q00c         \_ apps_test.1     alpine:edge         scw-docker-1        Shutdown            Failed 51 seconds ago       "task: non-zero exit (137)"
iojo7fpw8jir         \_ apps_test.1     alpine:edge         scw-docker-1        Shutdown            Failed about a minute ago   "task: non-zero exit (137)"
tt0nuusvgeki        apps_test.2         alpine:edge         scw-docker-2        Running             Running 15 seconds ago
</code></pre>

<p>Logon to the new container and test if the data is persisted:</p>

<pre><code class="bash">$ docker exec -it apps_test.2.tt0nuusvgekirw1c5myu720ga sh
/ # cat /data/file.txt
ok
</code></pre>

<p>Delete the Stack and Redeploy and have a look at the data we created earlier, and you will notice the data is persisted:</p>

<pre><code class="bash">$ docker stack rm apps
$ docker stack deploy -c docker-compose.yml apps
$ docker exec -it apps_test.1.la4w2sbuu8cmv6xamwxl7n0ip cat /data/file.txt
ok
$ docker stack rm apps
</code></pre>

<h2>Create Volume via Compose:</h2>

<p>You can also create the volume on service/stack creation level, so you dont need to create the volume before hand, the compose file:</p>

<pre><code class="yml">version: '3.4'

volumes:
  test2:
    driver: convoy
    driver_opts:
      size: 10

networks:
  appnet:
    external: true

services:
  test:
    image: alpine:edge
    command: sh -c "ping 127.0.0.1"
    volumes:
      - test2:/data
    networks:
      - appnet
</code></pre>

<p>Deploy the Stack:</p>

<pre><code class="bash">$ docker stack deploy -c docker-compose-new.yml apps
Creating service apps_test
</code></pre>

<p>List the volumes and you will notice that the volume was created:</p>

<pre><code class="bash">$ docker volume ls
DRIVER              VOLUME NAME
convoy              apps_test2
convoy              test1
</code></pre>

<p>Lets inspect the volume, to see more details about it:</p>

<pre><code class="bash">docker volume inspect apps_test2
[
    {
        "CreatedAt": "0001-01-01T00:00:00Z",
        "Driver": "convoy",
        "Labels": {
            "com.docker.stack.namespace": "apps"
        },
        "Mountpoint": "/mnt/docker/volumes/apps_test2",
        "Name": "apps_test2",
        "Options": {
            "size": "10"
        },
        "Scope": "local"
    }
]
</code></pre>

<p>As mentioned earlier, if you delete the volume, you need to delete the data directories as well</p>

<pre><code class="bash">$ docker volume rm test1
test1

$ ls /mnt/docker/volumes/
apps_test2  config  test1

$ rm -rf /mnt/docker/volumes/test1
</code></pre>

<p>More info about the project:
- <a href="https://github.com/rancher/convoy">https://github.com/rancher/convoy</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a NFS Server on Ubuntu]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/02/11/setup-a-nfs-server-on-ubuntu/"/>
    <updated>2018-02-11T17:26:56-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/02/11/setup-a-nfs-server-on-ubuntu</id>
    <content type="html"><![CDATA[<p><img src="https://i.snag.gy/3sUALo.jpg" alt="" /></p>

<p>Quick post on how to setup a NFS Server on Ubuntu and how to setup the client to interact with the NFS Server.</p>

<h2>Setup the Dependencies:</h2>

<pre><code class="bash">$ apt update &amp;&amp; sudo apt upgrade -y
$ sudo apt-get install nfs-kernel-server nfs-common -y
</code></pre>

<p>Create the Directory for NFS and set permissions:</p>

<pre><code class="bash">mkdir /vol
chown -R nobody:nogroup /vol
</code></pre>

<h2>Allow the Clients:</h2>

<p>We need to set in the <code>exports</code> file, the clients we would like to allow:</p>

<ul>
<li><code>rw</code>: Allows Client R/W Access to the Volume.</li>
<li><code>sync</code>: This option forces NFS to write changes to disk before replying. More stable and Consistent. Note, it does reduce the speed of file operations.</li>
<li><code>no_subtree_check</code>: This prevents subtree checking, which is a process where the host must check whether the file is actually still available in the exported tree for every request. This can cause many problems when a file is renamed while the client has it opened. In almost all cases, it is better to disable subtree checking.</li>
</ul>


<pre><code class="bash">$ echo '/vol 10.8.133.83(rw,sync,no_subtree_check) 10.8.166.19(rw,sync,no_subtree_check) 10.8.142.195(rw,sync,no_subtree_check)' &gt;&gt; /etc/exports
</code></pre>

<h2>Start the NFS Server:</h2>

<p>Restart the service and enable the service on boot:</p>

<pre><code class="bash">$ sudo systemctl restart nfs-kernel-server
$ sudo systemctl enable nfs-kernel-server
</code></pre>

<h2>Client Side:</h2>

<p>We will mount the NFS Volume to our Clients <code>/mnt</code> partition.</p>

<p>Install the dependencies:</p>

<pre><code class="bash">$ sudo apt-get install nfs-common -y
</code></pre>

<p>Test if we can mount the volume, then unmount it, as we will set the config in our <code>fstab</code>:</p>

<pre><code class="bash">$ sudo mount 10.8.133.83:/vol /mnt
$ sudo umount /mnt
$ df -h
</code></pre>

<p>Set the config in your <code>fstab</code>, then mount it from there:</p>

<pre><code class="bash">$ sudo bash -c "echo '10.8.133.83:/vol /mnt nfs auto,nofail,noatime,nolock,intr,tcp,actimeo=1800 0 0' &gt;&gt; /etc/fstab"
$ sudo mount -a
$ df -h
</code></pre>

<p>Now you shoule be able to write to your NFS Volume from your client.</p>

<p>Sources:
- <a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-16-04">1</a> <a href="https://gist.github.com/deviantony/557984d62e867e6f505577b207db6ffc%">2</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Create a ZFS Raidz1 Volume Pool on Ubuntu 16]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/08/24/create-a-zfs-raidz1-volume-pool-on-ubuntu-16/"/>
    <updated>2017-08-24T09:16:34-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/08/24/create-a-zfs-raidz1-volume-pool-on-ubuntu-16</id>
    <content type="html"><![CDATA[<p>Setting up ZFS Volume Pool on Ubuntu 16.04</p>

<h2>Installation</h2>

<pre><code>$ sudo apt-get install zfsutils-linux -y
</code></pre>

<h2>Creating the ZFS Storage Pool</h2>

<p>We will create a RAIDZ(1) Volume which is like Raid5 with Single Parity, so we can lose one of the Physical Disks before Raid failure.</p>

<p>Let&rsquo;s first have a look at our disks that we have on our server:</p>

<pre><code>$ lsblk
NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
xvda    202:0    0   8G  0 disk
└─xvda1 202:1    0   8G  0 part /
xvdf    202:80   0 100G  0 disk 
xvdg    202:80   0 100G  0 disk 
</code></pre>

<p>So we will be creating the volume consisting of <code>/dev/xvdf</code> and <code>/dev/xvdg</code> and we will name our pool: <code>storage-pool</code></p>

<pre><code>$ zpool create storage-pool raidz1 xvdf xvdg -f
</code></pre>

<h2>Listing Pools</h2>

<pre><code>$ zpool list
NAME           SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
storage-pool   199G   125K   199G         -     0%     0%  1.00x  ONLINE  -
</code></pre>

<p>We can also list the volume with <code>zfs</code>:</p>

<pre><code>$ zfs list
NAME            USED  AVAIL  REFER  MOUNTPOINT
storage-pool    125K  199G   19K    /storage-pool
</code></pre>

<h2>Mounting the Volume:</h2>

<p>You will find that the volume is already mounted:</p>

<pre><code>$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/xvda1      7.7G  1.1G  6.7G  14% /
pool            199G  125K  198G   1% /pool
</code></pre>

<h2>Resources:</h2>

<p>See how Brett Kelly from 45 Drives tried to break a Storage Cluster with GlusterFS and ZFS:</p>

<center><iframe width="740" height="400" src="https://www.youtube.com/embed/A0wV4k58RIs" frameborder="1" allowfullscreen></iframe></center>


<p>Great ZFS Performance Comparison:</p>

<ul>
<li><a href="https://calomel.org/zfs_raid_speed_capacity.html">https://calomel.org/zfs_raid_speed_capacity.html</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
