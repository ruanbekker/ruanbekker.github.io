<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Storage | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/storage/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2021-03-10T00:56:14-05:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Reduce Docker Log Size on Disk]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/12/23/reduce-docker-log-size-on-disk/"/>
    <updated>2020-12-23T04:11:35-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/12/23/reduce-docker-log-size-on-disk</id>
    <content type="html"><![CDATA[<p>In cases where you are using the defaults for logging and your application logs a lot you can consume a lot of disk space and you can run out of disk space quite quickly.</p>

<p>If it&rsquo;s a case where you already ran out of disk space, we can investigate the disk space consumed by docker logs:</p>

<pre><code>$ cd /var/lib/docker/containers
$ du -sh *
6.0G    14052251a0f13f46f65bc73d10c01408130ee8ae71529600ba5bd6bee76af4ee
1.2G    e6b40b1d30c5cf05e8cb201ca9abf6bd283d7cf7ceaa3be2a0422be7cd750a33
</code></pre>

<p>Referenced from <a href="https://blog.birkhoff.me/devops-truncate-docker-container-logs-periodically-to-free-up-server-disk-space/">https://blog.birkhoff.me/devops-truncate-docker-container-logs-periodically-to-free-up-server-disk-space/</a> you can truncate those files:</p>

<pre><code>$ sh -c 'truncate -s 0 /var/lib/docker/containers/*/*-json.log'
</code></pre>

<p>Check the size again:</p>

<pre><code>$ du -sh *
40K 14052251a0f13f46f65bc73d10c01408130ee8ae71529600ba5bd6bee76af4ee
36K e6b40b1d30c5cf05e8cb201ca9abf6bd283d7cf7ceaa3be2a0422be7cd750a33
</code></pre>

<p>To overcome this issue you can use this in logging options in your compose:</p>

<pre><code>...
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
...
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a NFS Server With Docker]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/09/20/setup-a-nfs-server-with-docker/"/>
    <updated>2020-09-20T16:07:09+00:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/09/20/setup-a-nfs-server-with-docker</id>
    <content type="html"><![CDATA[<p>In this tutorial we will setup a <strong>NFS Server</strong> using <strong>Docker</strong> for our development environment.</p>

<h2>Host Storage Path</h2>

<p>In this example we will be using our host path <code>/data/nfs-storage</code> which will host our storage for our NFS server, which will will mount to the container:</p>

<pre><code>$ mkdir -p /data/nfs-storage
</code></pre>

<h2>NFS Server</h2>

<p>Create the NFS Server with docker:</p>

<pre><code>$ docker run -itd --privileged \
  --restart unless-stopped \
  -e SHARED_DIRECTORY=/data \
  -v /data/nfs-storage:/data \
  -p 2049:2049 \
  itsthenetwork/nfs-server-alpine:12
</code></pre>

<p>We can do the same using docker-compose, for our <code>docker-compose.yml</code>:</p>

<pre><code>version: "2.1"
services:
  # https://hub.docker.com/r/itsthenetwork/nfs-server-alpine
  nfs:
    image: itsthenetwork/nfs-server-alpine:12
    container_name: nfs
    restart: unless-stopped
    privileged: true
    environment:
      - SHARED_DIRECTORY=/data
    volumes:
      - /data/nfs-storage:/data
    ports:
      - 2049:2049
</code></pre>

<p>To deploy using docker-compose:</p>

<pre><code>$ docker-compose up -d
</code></pre>

<h2>NFS Client</h2>

<p>To use a NFS Client to mount this to your filesystem, you can look at <a href="https://blog.ruanbekker.com/blog/2017/12/05/setup-a-nfs-server-on-a-raspberrypi/" rel="nofollow" target="_blank">this blogpost></a></p>

<p>In summary:</p>

<pre><code>$ sudo apt install nfs-client -y
$ sudo mount -v -o vers=4,loud 192.168.0.4:/ /mnt
</code></pre>

<p>Verify that the mount is showing:</p>

<pre><code>$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda2       109G   53G   51G  52% /
192.168.0.4:/   4.5T  2.2T  2.1T  51% /mnt
</code></pre>

<p>Now, create a test file on our NFS export:</p>

<pre><code>$ touch /mnt/file.txt
</code></pre>

<p>Verify that the test file is on the local path:</p>

<pre><code>$ ls /data/nfs-storage/
file.txt
</code></pre>

<p>If you want to load this into other client&rsquo;s <code>/etc/fstab</code>:</p>

<pre><code>192.168.0.4:/   /mnt   nfs4    _netdev,auto  0  0
</code></pre>

<h2>NFS Docker Volume Plugin</h2>

<p>You can use a NFS Volume Plugin for Docker or Docker Swarm for persistent container storage.</p>

<p>To use the NFS Volume plugin, we need to download <a href="https://github.com/ContainX/docker-volume-netshare/releases" target="_blank" rel="nofollow">docker-volume-netshare</a> from their github releases page.</p>

<pre><code>$ wget https://github.com/ContainX/docker-volume-netshare/releases/download/v0.36/docker-volume-netshare_0.36_amd64.deb
$ dpkg -i docker-volume-netshare_0.36_amd64.deb
$ service docker-volume-netshare start
</code></pre>

<p>Then your <code>docker-compose.yml</code>:</p>

<pre><code>version: '3.7'

services:
  mysql:
    image: mariadb:10.1
    networks:
      - private
    environment:
      - MYSQL_ROOT_PASSWORD=${DATABASE_PASSWORD:-admin}
      - MYSQL_DATABASE=testdb
      - MYSQL_USER=${DATABASE_USER:-admin}
      - MYSQL_PASSWORD=${DATABASE_PASSWORD:-admin}
    volumes:
      - mysql_data.vol:/var/lib/mysql

volumes:
  mysql_data.vol:
    driver: nfs
    driver_opts:
      share: 192.168.69.1:/mysql_data_vol
</code></pre>

<h2>Thank You</h2>

<p>That&rsquo;s it. Thanks for reading, follow me on Twitter and say hi! <a href="https://twitter.com/ruanbekker" rel="nofollow" target="_blank"><strong>@ruanbekker</strong></a></p><p><a href="https://saythanks.io/to/ruan.ru.bekker@gmail.com" rel="nofollow" target="_blank"><img src="https://svgshare.com/i/Pfy.svg" alt="Say Thanks!"></a></p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Persistent Volumes With K3d Kubernetes]]></title>
    <link href="https://blog.ruanbekker.com/blog/2020/02/21/persistent-volumes-with-k3d-kubernetes/"/>
    <updated>2020-02-21T00:07:48+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2020/02/21/persistent-volumes-with-k3d-kubernetes</id>
    <content type="html"><![CDATA[<p>With k3d we can mount the host to container path, and with persistent volumes we can set a hostPath for our persistent volumes. With k3d, all the nodes will be using the same volume mapping which maps back to the host.</p>

<p>We will test the data persistence by writing a file inside a container, kill the pod, then exec into the pod again and test if the data persisted</p>

<h2>The k3d Cluster</h2>

<p>Create the directory on the host where we will persist the data:</p>

<pre><code>&gt; mkdir -p /tmp/k3dvol
</code></pre>

<p>Create the cluster:</p>

<pre><code>&gt; k3d create --name "k3d-cluster" --volume /tmp/k3dvol:/tmp/k3dvol --publish "80:80" --workers 2
&gt; export KUBECONFIG="$(k3d get-kubeconfig --name='k3d-cluster')"
</code></pre>

<p>Our application will be a busybox container which will keep running with a ping command, map the persistent volume to <code>/data</code> inside the pod.</p>

<p>Our <code>app.yml</code></p>

<pre><code>apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/tmp/k3dvol"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: echo
spec:
  selector:
    matchLabels:
      app: echo
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: echo
    spec:
      volumes:
        - name: task-pv-storage
          persistentVolumeClaim:
            claimName: task-pv-claim
      containers:
      - image: busybox
        name: echo
        volumeMounts:
          - mountPath: "/data"
            name: task-pv-storage
        command: ["ping", "127.0.0.1"]
</code></pre>

<p>Deploy the workload:</p>

<pre><code>&gt; kubectl apply -f app.yml
persistentvolume/task-pv-volume created
persistentvolumeclaim/task-pv-claim created
deployment.apps/echo created
</code></pre>

<p>View the persistent volumes:</p>

<pre><code>&gt; kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS   REASON   AGE
task-pv-volume                             1Gi        RWO            Retain           Bound    default/task-pv-claim    manual                  6s
</code></pre>

<p>View the Persistent Volume Claims:</p>

<pre><code>&gt; kubectl get pvc
NAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
task-pv-claim    Bound    task-pv-volume                             1Gi        RWO            manual         11s
</code></pre>

<p>View the pods:</p>

<pre><code>&gt; kubectl get pods
NAME                   READY   STATUS    RESTARTS   AGE
echo-58fd7d9b6-x4rxj   1/1     Running   0          16s
</code></pre>

<p>Exec into the pod:</p>

<pre><code>&gt; kubectl exec -it echo-58fd7d9b6-x4rxj sh
/ # df -h
Filesystem                Size      Used Available Use% Mounted on
overlay                  58.4G     36.1G     19.3G  65% /
osxfs                   233.6G    139.7G     86.3G  62% /data
/dev/sda1                58.4G     36.1G     19.3G  65% /etc/hosts
/dev/sda1                58.4G     36.1G     19.3G  65% /dev/termination-log
/dev/sda1                58.4G     36.1G     19.3G  65% /etc/hostname
/dev/sda1                58.4G     36.1G     19.3G  65% /etc/resolv.conf
</code></pre>

<p>Write the hostname of the current pod to the persistent volume path:</p>

<pre><code>/ # echo $(hostname)
echo-58fd7d9b6-x4rxj
/ # echo $(hostname) &gt; /data/hostname.txt
/ # exit
</code></pre>

<p>Exit the pod and read the content from the host (workstation/laptop):</p>

<pre><code>&gt; cat /tmp/k3dvol/hostname.txt
echo-58fd7d9b6-x4rxj
</code></pre>

<p>Look at the host where the pod is running on:</p>

<pre><code>&gt; kubectl get nodes -o wide
NAME                       STATUS   ROLES    AGE   VERSION        INTERNAL-IP    EXTERNAL-IP   OS-IMAGE   KERNEL-VERSION     CONTAINER-RUNTIME
k3d-k3d-cluster-server     Ready    master   13m   v1.17.2+k3s1   192.168.32.2   &lt;none&gt;        Unknown    4.9.184-linuxkit   containerd://1.3.3-k3s1
k3d-k3d-cluster-worker-1   Ready    &lt;none&gt;   13m   v1.17.2+k3s1   192.168.32.4   &lt;none&gt;        Unknown    4.9.184-linuxkit   containerd://1.3.3-k3s1
k3d-k3d-cluster-worker-0   Ready    &lt;none&gt;   13m   v1.17.2+k3s1   192.168.32.3   &lt;none&gt;        Unknown    4.9.184-linuxkit   containerd://1.3.3-k3s1
</code></pre>

<p>Delete the pod:</p>

<pre><code>&gt; kubectl delete pod/echo-58fd7d9b6-x4rxj
pod "echo-58fd7d9b6-x4rxj" deleted
</code></pre>

<p>Wait until the pod is rescheduled again and verify if the pod is running on a different node:</p>

<pre><code>&gt; kubectl get pods -o wide
NAME                   READY   STATUS    RESTARTS   AGE   IP          NODE                       NOMINATED NODE   READINESS GATES
echo-58fd7d9b6-fkvbs   1/1     Running   0          35s   10.42.2.9   k3d-k3d-cluster-worker-1   &lt;none&gt;           &lt;none&gt;
</code></pre>

<p>Exec into the new pod:</p>

<pre><code>&gt; kubectl exec -it echo-58fd7d9b6-fkvbs sh
</code></pre>

<p>View if the data is persisted:</p>

<pre><code>/ # hostname
echo-58fd7d9b6-fkvbs

/ # cat /data/hostname.txt
echo-58fd7d9b6-x4rxj
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Expire Objects in AWS S3 Automatically After 30 Days]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/09/12/expire-objects-in-aws-s3-automatically-after-30-days/"/>
    <updated>2019-09-12T22:37:11+02:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/09/12/expire-objects-in-aws-s3-automatically-after-30-days</id>
    <content type="html"><![CDATA[<p><img src="https://objects.ruanbekker.com/assets/images/aws-logo.png" alt="" /></p>

<p>In AWS S3 you can make use of lifecycle policies to manage the lifetime of your objects stored in S3.</p>

<p>In this tutorial, I will show you how to delete objects automatically from S3 after 30 days.</p>

<h2>Navigate to your Bucket</h2>

<p>Head over to your AWS S3 bucket where you want to delete objects after they have been stored for 30 days:</p>

<p><img width="1039" alt="0400F9CB-9223-4FDF-8FA5-D0BC1FA8EB71" src="https://user-images.githubusercontent.com/567298/64819546-c3f2b600-d5ae-11e9-93ba-13777e9b02b0.png"></p>

<h2>Lifecycle Policies</h2>

<p>Select &ldquo;Management&rdquo; and click on &ldquo;Add lifecycle rule&rdquo;:</p>

<p><img width="701" alt="9BB26C7C-F251-45C4-AE44-A34459BD0F4B" src="https://user-images.githubusercontent.com/567298/64819628-f00e3700-d5ae-11e9-9740-8aa3608163a7.png"></p>

<p>Set a rule name of choice and you have the option to provide a prefix if you want to delete objects based on a specific prefix. I will leave this blank as I want to delete objects in the root level of the bucket. Head to next on the following section:</p>

<p><img width="700" alt="AEF8B151-3FA8-454F-AC71-778A531BD1EE" src="https://user-images.githubusercontent.com/567298/64819785-58f5af00-d5af-11e9-8485-fb0dca3a02ac.png"></p>

<p>From the &ldquo;Transitions&rdquo; section, configure the transition section, by selecting to expire the current version of the object after 30 days:</p>

<p><img width="701" alt="2B395671-A4C0-4E5A-82E7-00EE6579DB5A" src="https://user-images.githubusercontent.com/567298/64819851-7c205e80-d5af-11e9-98d7-7e1dd09bcfef.png"></p>

<p>Review the configuration:</p>

<p><img width="705" alt="F7F8E800-62FF-4156-B506-5FB9BCC148E0" src="https://user-images.githubusercontent.com/567298/64819869-893d4d80-d5af-11e9-8034-8a2e3a8939f8.png"></p>

<p>When you select &ldquo;Save&rdquo;, you should be returned to the following section:</p>

<p><img width="1041" alt="8421EBCE-9503-4259-92AA-DB66C6F532AF" src="https://user-images.githubusercontent.com/567298/64819895-99edc380-d5af-11e9-84b4-7f4cc69cfd2e.png"></p>

<h2>Housecleaning on your S3 Bucket</h2>

<p>Now 30 days after you created objects on AWS S3, they will be deleted.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Persist Vault Data With Amazon S3 as a Storage Backend]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/05/07/persist-vault-data-with-amazon-s3-as-a-storage-backend/"/>
    <updated>2019-05-07T16:01:45-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/05/07/persist-vault-data-with-amazon-s3-as-a-storage-backend</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/57256060-f1a27e00-7055-11e9-9a05-77d3fdd6c76f.png" alt="" /></p>

<p>In a previous post we have set up <a href="https://blog.ruanbekker.com/blog/2019/05/06/setup-hashicorp-vault-server-on-docker-and-cli-guide/">the vault server on docker</a>, but using a file backend to persist our data.</p>

<p>In this tutorial we will configure vault to use <a href="https://www.vaultproject.io/docs/configuration/storage/s3.html">amazon s3 as a storage backend</a> to persist our data for vault.</p>

<h2>Provision S3 Bucket</h2>

<p>Create the S3 Bucket where our data will reside:</p>

<pre><code>$ aws s3 mb --region=eu-west-1 s3://somename-vault-backend
</code></pre>

<h2>Vault Config</h2>

<p>Create the vault config, where we will provide details about our storage backend and configuration for the vault server:</p>

<pre><code>$ vim volumes/config/s3vault.json
</code></pre>

<p>Populate the config file with the following details, you will just need to provide your own credentials:</p>

<pre><code class="json">{
  "backend": {
    "s3": {
      "region": "eu-west-1",
      "access_key": "ACCESS_KEY",
      "secret_key": "SECRET_KEY",
      "bucket": "somename-vault-backend"
    }
  },
  "listener": {
    "tcp":{
      "address": "0.0.0.0:8200",
      "tls_disable": 1
    }
  },
  "ui": true
}
</code></pre>

<h2>Docker Compose</h2>

<p>As we are using docker to deploy our vault server, our docker-compose.yml:</p>

<pre><code>$ cat &gt; docker-compose.yml &lt;&lt; EOF
version: '2'
services:
  vault:
    image: vault
    container_name: vault
    ports:
      - "8200:8200"
    restart: always
    volumes:
      - ./volumes/logs:/vault/logs
      - ./volumes/file:/vault/file
      - ./volumes/config:/vault/config
    cap_add:
      - IPC_LOCK
    entrypoint: vault server -config=/vault/config/s3vault.json
EOF
</code></pre>

<p>Deploy the vault server:</p>

<pre><code>$ docker-compose up
</code></pre>

<p>Go ahead and create some secrets, then deploy the docker container on another host to test out the data persistence.</p>
]]></content>
  </entry>
  
</feed>
