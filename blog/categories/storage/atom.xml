<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Storage | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/storage/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2020-02-04T22:40:24+02:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Expire Objects in AWS S3 Automatically After 30 Days]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/09/12/expire-objects-in-aws-s3-automatically-after-30-days/"/>
    <updated>2019-09-12T22:37:11+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/09/12/expire-objects-in-aws-s3-automatically-after-30-days</id>
    <content type="html"><![CDATA[<p><img src="https://objects.ruanbekker.com/assets/images/aws-logo.png" alt="" /></p>

<p>In AWS S3 you can make use of lifecycle policies to manage the lifetime of your objects stored in S3.</p>

<p>In this tutorial, I will show you how to delete objects automatically from S3 after 30 days.</p>

<h2>Navigate to your Bucket</h2>

<p>Head over to your AWS S3 bucket where you want to delete objects after they have been stored for 30 days:</p>

<p><img width="1039" alt="0400F9CB-9223-4FDF-8FA5-D0BC1FA8EB71" src="https://user-images.githubusercontent.com/567298/64819546-c3f2b600-d5ae-11e9-93ba-13777e9b02b0.png"></p>

<h2>Lifecycle Policies</h2>

<p>Select &ldquo;Management&rdquo; and click on &ldquo;Add lifecycle rule&rdquo;:</p>

<p><img width="701" alt="9BB26C7C-F251-45C4-AE44-A34459BD0F4B" src="https://user-images.githubusercontent.com/567298/64819628-f00e3700-d5ae-11e9-9740-8aa3608163a7.png"></p>

<p>Set a rule name of choice and you have the option to provide a prefix if you want to delete objects based on a specific prefix. I will leave this blank as I want to delete objects in the root level of the bucket. Head to next on the following section:</p>

<p><img width="700" alt="AEF8B151-3FA8-454F-AC71-778A531BD1EE" src="https://user-images.githubusercontent.com/567298/64819785-58f5af00-d5af-11e9-8485-fb0dca3a02ac.png"></p>

<p>From the &ldquo;Transitions&rdquo; section, configure the transition section, by selecting to expire the current version of the object after 30 days:</p>

<p><img width="701" alt="2B395671-A4C0-4E5A-82E7-00EE6579DB5A" src="https://user-images.githubusercontent.com/567298/64819851-7c205e80-d5af-11e9-98d7-7e1dd09bcfef.png"></p>

<p>Review the configuration:</p>

<p><img width="705" alt="F7F8E800-62FF-4156-B506-5FB9BCC148E0" src="https://user-images.githubusercontent.com/567298/64819869-893d4d80-d5af-11e9-8034-8a2e3a8939f8.png"></p>

<p>When you select &ldquo;Save&rdquo;, you should be returned to the following section:</p>

<p><img width="1041" alt="8421EBCE-9503-4259-92AA-DB66C6F532AF" src="https://user-images.githubusercontent.com/567298/64819895-99edc380-d5af-11e9-84b4-7f4cc69cfd2e.png"></p>

<h2>Housecleaning on your S3 Bucket</h2>

<p>Now 30 days after you created objects on AWS S3, they will be deleted.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Persist Vault Data With Amazon S3 as a Storage Backend]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/05/07/persist-vault-data-with-amazon-s3-as-a-storage-backend/"/>
    <updated>2019-05-07T22:01:45+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/05/07/persist-vault-data-with-amazon-s3-as-a-storage-backend</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/57256060-f1a27e00-7055-11e9-9a05-77d3fdd6c76f.png" alt="" /></p>

<p>In a previous post we have set up <a href="https://blog.ruanbekker.com/blog/2019/05/06/setup-hashicorp-vault-server-on-docker-and-cli-guide/">the vault server on docker</a>, but using a file backend to persist our data.</p>

<p>In this tutorial we will configure vault to use <a href="https://www.vaultproject.io/docs/configuration/storage/s3.html">amazon s3 as a storage backend</a> to persist our data for vault.</p>

<h2>Provision S3 Bucket</h2>

<p>Create the S3 Bucket where our data will reside:</p>

<pre><code>$ aws s3 mb --region=eu-west-1 s3://somename-vault-backend
</code></pre>

<h2>Vault Config</h2>

<p>Create the vault config, where we will provide details about our storage backend and configuration for the vault server:</p>

<pre><code>$ vim volumes/config/s3vault.json
</code></pre>

<p>Populate the config file with the following details, you will just need to provide your own credentials:</p>

<pre><code class="json">{
  "backend": {
    "s3": {
      "region": "eu-west-1",
      "access_key": "ACCESS_KEY",
      "secret_key": "SECRET_KEY",
      "bucket": "somename-vault-backend"
    }
  },
  "listener": {
    "tcp":{
      "address": "0.0.0.0:8200",
      "tls_disable": 1
    }
  },
  "ui": true
}
</code></pre>

<h2>Docker Compose</h2>

<p>As we are using docker to deploy our vault server, our docker-compose.yml:</p>

<pre><code>$ cat &gt; docker-compose.yml &lt;&lt; EOF
version: '2'
services:
  vault:
    image: vault
    container_name: vault
    ports:
      - "8200:8200"
    restart: always
    volumes:
      - ./volumes/logs:/vault/logs
      - ./volumes/file:/vault/file
      - ./volumes/config:/vault/config
    cap_add:
      - IPC_LOCK
    entrypoint: vault server -config=/vault/config/s3vault.json
EOF
</code></pre>

<p>Deploy the vault server:</p>

<pre><code>$ docker-compose up
</code></pre>

<p>Go ahead and create some secrets, then deploy the docker container on another host to test out the data persistence.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a 3 Node Replicated Storage Volume With GlusterFS]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/03/05/setup-a-3-node-replicated-storage-volume-with-glusterfs/"/>
    <updated>2019-03-05T21:01:37+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/03/05/setup-a-3-node-replicated-storage-volume-with-glusterfs</id>
    <content type="html"><![CDATA[<p><img src="https://access.redhat.com/documentation/en-US/Red_Hat_Storage/2.1/html/Administration_Guide/images/Replicated_Volume.png" alt="" /></p>

<p>In one of my earlier posts on <a href="https://sysadmins.co.za/tag/glusterfs">GlusterFS</a>, we went through the steps on how to setup a <a href="https://sysadmins.co.za/setup-a-distributed-storage-volume-with-glusterfs/">Distributed Storage Volume</a>, where the end result was to have scalable storage, where size was the requirement.</p>

<p><a href="https://bekkerclothing.com/collections/developer?utm_source=blog.ruanbekker.com&utm_medium=blog&utm_campaign=leaderboard_ad" target="_blank"><img alt="bekker-clothing-developer-tshirts" src="https://user-images.githubusercontent.com/567298/70170981-7c278a80-16d6-11ea-9759-6621d02c1423.png"></a></p>

<h2>What will we be doing today with GlusterFS?</h2>

<p>Today, we will be going through the steps on how to setup a Replicated Storage Volume with GlusterFS, where we will have 3 GlusterFS Nodes, and using the replication factor of 3.</p>

<p><strong>Replication Factor of 3:</strong></p>

<p>In other words, having 3 copies of our data and in our case, since we will have 3 nodes in our cluster, a copy of our data will reside on each node.</p>

<p><strong>What about Split-Brain:</strong></p>

<p>In Clustering, we get the term Split-Brain, where a node dies or leaves the cluster, the cluster reforms itself with the available nodes and then during this reformation, instead of the remaining nodes staying with the same cluster, 2 subset of cluster are created, and they are not aware of each other, which causes data corruption, here&rsquo;s a great resource on <a href="http://techthoughts.typepad.com/managing_computers/2007/10/split-brain-quo.html">Split-Brain</a></p>

<p>To prevent Split-Brain in GlusterFS, we can setup a <a href="https://gluster.readthedocs.io/en/latest/Administrator%20Guide/arbiter-volumes-and-quorum/">Arbiter Volume</a>. In a Replica Count of 3 and Arbiter count of 1: 2 Nodes will hold the replicated data, and the 1 Node which will be the Arbiter node, will only host the file/directory names and metadata but not any data. I will write up an <a href="">article</a> on this in the future.</p>

<h2>Getting Started:</h2>

<p>Let&rsquo;s get started on setting up a 3 Node Replicated GlusterFS. Each node will have an additional drive that is 50GB in size, which will be part of our GlusterFS Replicated Volume. I will also be using Ubuntu 16.04 as my linux distro.</p>

<p><strong>Preparing DNS Resolution:</strong></p>

<p>I will install GlusterFS on each node, and in my setup I have the following DNS entries:</p>

<ul>
<li>gfs01 (10.0.0.2)</li>
<li>gfs02 (10.0.0.3)</li>
<li>gfs03 (10.0.0.4)</li>
</ul>


<p><strong>Preparing our Secondary Drives:</strong></p>

<p>I will be formatting my drives with <code>XFS</code>. Listing our block volumes:</p>

<pre><code class="bash">$ lsblk
NAME MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vdb  253:16   0 46.6G  0 disk
vda  253:0    0 18.6G  0 disk /
</code></pre>

<p>Creating the FileSystem with XFS, which we will be running on each node:</p>

<pre><code class="bash">$ mkfs.xfs /dev/vdb
</code></pre>

<p>Then creating the directories where our bricks will reside, and also add an entry to our <code>/etc/fstab</code> so that our disk gets mounted when the operating system boots:</p>

<pre><code class="bash"># node: gfs01
$ mkdir /gluster/bricks/1 -p
$ echo '/dev/vdb /gluster/bricks/1 xfs defaults 0 0' &gt;&gt; /etc/fstab
$ mount -a
$ mkdir /gluster/bricks/1/brick

# node: gfs02
$ mkdir /gluster/bricks/2 -p
$ echo '/dev/vdb /gluster/bricks/2 xfs defaults 0 0' &gt;&gt; /etc/fstab
$ mount -a
$ mkdir /gluster/bricks/2/brick

# node: gfs03
$ mkdir /gluster/bricks/3 -p
$ echo '/dev/vdb /gluster/bricks/3 xfs defaults 0 0' &gt;&gt; /etc/fstab
$ mount -a
$ mkdir /gluster/bricks/3/brick
</code></pre>

<p>After this has been done, we should see that the disks are mounted, for example on node: <code>gfs01</code>:</p>

<pre><code>$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/vda         18G  909M   17G   3% /
/dev/vdb         47G   80M   47G   1% /gluster/bricks/1
</code></pre>

<h2>Installing GlusterFS on Each Node:</h2>

<p>Installing GlusterFS, repeat this on all 3 Nodes:</p>

<pre><code class="bash">$ apt update &amp;&amp; sudo apt upgrade -y
$ apt install xfsprogs attr glusterfs-server glusterfs-common glusterfs-client -y
$ systemctl enable glusterfs-server
</code></pre>

<p>In order to add the nodes to the trusted storage pool, we will have to add them by using <code>gluster peer probe</code>. Make sure that you can resolve the hostnames to the designated IP Addresses, and that traffic is allowed.</p>

<pre><code class="bash">$ gluster peer probe gfs01
$ gluster peer probe gfs02
$ gluster peer probe gfs03
</code></pre>

<p>Now that we have added our nodes to our trusted storage pool, lets verify that by listing our pool:</p>

<pre><code class="bash">$ gluster pool list
UUID                                    Hostname                State
f63d0e77-9602-4024-8945-5a7f7332bf89    gfs02                   Connected
2d4ac6c1-0611-4e2e-b4af-9e4aa8c1556d    gfs03                   Connected
6a604cd9-9a9c-406d-b1b7-69caf166a20e    localhost               Connected
</code></pre>

<p>Great! All looks good.</p>

<h2>Create the Replicated GlusterFS Volume:</h2>

<p>Let&rsquo;s create our Replicated GlusterFS Volume, named <code>gfs</code>:</p>

<pre><code class="bash">$ gluster volume create gfs \
  replica 3 \
  gfs01:/gluster/bricks/1/brick \
  gfs02:/gluster/bricks/2/brick \
  gfs03:/gluster/bricks/2/brick 

volume create: gfs: success: please start the volume to access data
</code></pre>

<p>Now that our volume is created, lets list it to verify that it is created:</p>

<pre><code class="bash">$ gluster volume list
gfs
</code></pre>

<p>Now, start the volume:</p>

<pre><code class="bash">$ gluster volume start gfs
volume start: gfs: success
</code></pre>

<p>View the status of our volume:</p>

<pre><code class="bash">$ gluster volume status gfs
Status of volume: gfs
Gluster process                             TCP Port  RDMA Port  Online  Pid
------------------------------------------------------------------------------
Brick gfs01:/gluster/bricks/1/brick         49152     0          Y       6450
Brick gfs02:/gluster/bricks/2/brick         49152     0          Y       3460
Brick gfs03:/gluster/bricks/3/brick         49152     0          Y       3309
</code></pre>

<p>Next, view the volume inforation:</p>

<pre><code class="bash">$ gluster volume info gfs

Volume Name: gfs
Type: Replicate
Volume ID: 6f827df4-6df5-4c25-99ee-8d1a055d30f0
Status: Started
Number of Bricks: 1 x 3 = 3
Transport-type: tcp
Bricks:
Brick1: gfs01:/gluster/bricks/1/brick
Brick2: gfs02:/gluster/bricks/2/brick
Brick3: gfs03:/gluster/bricks/3/brick
</code></pre>

<h2>Security:</h2>

<p>From a GlusterFS level, it will allow clients to connect by default. To authorize these 3 nodes to connect to the GlusterFS Volume:</p>

<pre><code class="bash">$ gluster volume set gfs auth.allow 10.0.0.2,10.0.0.3,10.0.0.4
</code></pre>

<p>Then if you would like to remove this rule:</p>

<pre><code class="bash">$ gluster volume set gfs auth.allow *
</code></pre>

<h2>Mount the GlusterFS Volume to the Host:</h2>

<p>Mount the GlusterFS Volume to each node, so we will have to mount it to each node, and also append it to our <code>/etc/fstab</code> file so that it mounts on boot:</p>

<pre><code class="bash">$ echo 'localhost:/gfs /mnt glusterfs defaults,_netdev,backupvolfile-server=localhost 0 0' &gt;&gt; /etc/fstab
$ mount.glusterfs localhost:/gfs /mnt
</code></pre>

<p><strong>Verify the Mounted Volume:</strong></p>

<p>Check the mounted disks, and you will find that the Replicated GlusterFS Volume is mounted on our <code>/mnt</code> partition.</p>

<pre><code class="bash">$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/vda         18G  909M   17G   3% /
/dev/vdb         47G   80M   47G   1% /gluster/bricks/1
localhost:/gfs   47G   80M   47G   1% /mnt
</code></pre>

<p>You will note that GlusterFS Volume has a total size of 47GB usable space, which is the same size as one of our disks, but that is because we have a replicated volume with a replication factor of 3:  <code>(47 * 3 / 3)</code></p>

<p>Now we have a Storage Volume which has 3 Replicas, one copy on each node, which allows us Data Durability on our Storage.</p>

<p><p></p>

<p><center><script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init(&lsquo;Buy Me a Coffee&rsquo;, &lsquo;#46b798&rsquo;, &lsquo;A6423ZIQ&rsquo;);kofiwidget2.draw();</script></center></p>

<p><p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Container Persistent Storage for Docker Swarm Using a GlusterFS Volume Plugin]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/03/05/container-persistent-storage-for-docker-swarm-using-a-glusterfs-volume-plugin/"/>
    <updated>2019-03-05T20:18:30+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/03/05/container-persistent-storage-for-docker-swarm-using-a-glusterfs-volume-plugin</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/53351889-85572000-392a-11e9-9720-464e9318206e.jpg" alt="" /></p>

<p>From one of my previous posts I demonstrated how to provide persistent storage for your containers by using a <a href="https://blog.ruanbekker.com/blog/2018/02/16/guide-to-setup-docker-convoy-volume-driver-for-docker-swarm-with-nfs/">Convoy NFS Plugin</a>.</p>

<p>I&rsquo;ve stumbled upon one AWESOME GlusterFS Volume Plugin for Docker by <a href="https://github.com/trajano/docker-volume-plugins/tree/master/glusterfs-volume-plugin">@trajano</a>, please have a look at his repository. I&rsquo;ve been waiting for some time for one solid glusterfs volume plugin, and it works great.</p>

<p><a href="https://bekkerclothing.com/collections/developer?utm_source=blog.ruanbekker.com&utm_medium=blog&utm_campaign=leaderboard_ad" target="_blank"><img alt="bekker-clothing-developer-tshirts" src="https://user-images.githubusercontent.com/567298/70170981-7c278a80-16d6-11ea-9759-6621d02c1423.png"></a></p>

<h2>What we will be doing today</h2>

<p>We will setup a 3 node replicated glusterfs volume and show how easy it is to install the volume plugin and then demonstrate how storage from our swarms containers are persisted.</p>

<p>Our servers that we will be using will have the private ip&rsquo;s as shown below:</p>

<pre><code>10.22.125.101
10.22.125.102
10.22.125.103
</code></pre>

<h2>Setup GlusterFS</h2>

<p>Have a look at <a href="https://blog.ruanbekker.com/blog/2019/03/05/setup-a-3-node-replicated-storage-volume-with-glusterfs/?referral=github.com">this</a> post to setup the glusterfs volume.</p>

<h2>Install the GlusterFS Volume Plugin</h2>

<p>Below I&rsquo;m installing the plugin and setting the alias name as <code>glusterfs</code>, granting all permissions and keeping the plugin in a disabled state.</p>

<pre><code class="bash">$ docker plugin install --alias glusterfs trajano/glusterfs-volume-plugin --grant-all-permissions --disable
</code></pre>

<p>Set the glusterfs servers:</p>

<pre><code>$ docker plugin set glusterfs SERVERS=10.22.125.101,10.22.125.102,10.22.125.103
</code></pre>

<p>Enable the glusterfs plugin:</p>

<pre><code>$ docker plugin enable glusterfs
</code></pre>

<h2>Create a Service in Docker Swarm</h2>

<p>Deploy a sample service on docker swarm with a volume backed by glusterfs. Note that my glusterfs volume is called <code>gfs</code></p>

<pre><code class="yaml">version: "3.4"

services:
  foo:
    image: alpine
    command: ping localhost
    networks:
      - net
    volumes:
      - vol1:/tmp

networks:
  net:
    driver: overlay

volumes:
  vol1:
    driver: glusterfs
    name: "gfs/vol1"
</code></pre>

<p>Deploy the stack:</p>

<pre><code class="bash">$ docker stack deploy -c docker-compose.yml test
Creating service test_foo
</code></pre>

<p>Have a look on which node is your container running:</p>

<pre><code class="bash">$ docker service ps test_foo
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
jfwzb7yxnrxx        test_foo.1          alpine:latest       swarm-worker-1      Running             Running 37 seconds ago
</code></pre>

<p>Now jump to the <code>swarm-worker-1</code> node and verify that the container is running on that node:</p>

<pre><code class="bash">$ docker ps
CONTAINER ID        IMAGE                                          COMMAND                  CREATED             STATUS                  PORTS               NAMES
d469f341d836        alpine:latest                                  "ping localhost"           59 seconds ago      Up 57 seconds                               test_foo.1.jfwzb7yxnrxxnd0qxtcjex8lu
</code></pre>

<p>Now since the container is running on this node, we will also see that the volume defined in our task configuration will also be present:</p>

<pre><code class="bash">$ docker volume ls
DRIVER                       VOLUME NAME
glusterfs:latest             gfs/vol1
</code></pre>

<p>Exec into the container and look at the disk layout:</p>

<pre><code class="bash">$ docker exec -it d469f341d836 sh
/ # df -h
Filesystem                Size      Used Available Use% Mounted on
overlay                  45.6G      3.2G     40.0G   7% /
10.22.125.101:gfs/vol1   45.6G      3.3G     40.0G   8% /tmp
</code></pre>

<p>While you are in the container, write the hostname&rsquo;s value into a file which is mapped to the glusterfs volume:</p>

<pre><code class="bash">$ echo $HOSTNAME &gt; /tmp/data.txt
$ cat /tmp/data.txt
d469f341d836
</code></pre>

<h2>Testing Data Persistence</h2>

<p>Time to test the data persistence. Scale the service to 3 replicas, then hop onto a new node where a replica resides and check if the data was persisted.</p>

<pre><code class="bash">$ docker service scale test_foo=3
test_foo scaled to 3
overall progress: 3 out of 3 tasks
1/3: running   [==================================================&gt;]
2/3: running   [==================================================&gt;]
3/3: running   [==================================================&gt;]
verify: Service converged
</code></pre>

<p>Check where the containers are running:</p>

<pre><code class="bash">$ docker service ps test_foo
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
jfwzb7yxnrxx        test_foo.1          alpine:latest       swarm-worker-1      Running             Running 2 minutes ago
mdsg6c5b2nqb        test_foo.2          alpine:latest       swarm-worker-3      Running             Running 15 seconds ago
iybat57t4lha        test_foo.3          alpine:latest       swarm-worker-2      Running             Running 15 seconds ago
</code></pre>

<p>Hop onto the <code>swarm-worker-2</code> node and check if the data is persisted from our previous write:</p>

<pre><code class="bash">$ docker exec -it 4228529aba29 sh
$ cat /tmp/data.txt
d469f341d836
</code></pre>

<p>Now let&rsquo;s append data to that file, then delete the stack and recreate to test if the data is still persisted:</p>

<pre><code class="bash">$ echo $HOSTNAME &gt;&gt; /tmp/data.txt
$ cat /tmp/data.txt
d469f341d836
4228529aba29
</code></pre>

<p>On the manager delete the stack:</p>

<pre><code class="bash">$ docker stack rm test
Removing service test_foo
</code></pre>

<p>The deploy the stack again:</p>

<pre><code class="bash">$ docker stack deploy -c docker-compose.yml test
Creating service test_foo
</code></pre>

<p>Check where the container is running:</p>

<pre><code class="bash">$ docker service ps test_foo
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS
9d6z02m123jk        test_foo.1          alpine:latest       swarm-worker-1      Running             Running 2 seconds ago
</code></pre>

<p>Exec into the container and read the data:</p>

<pre><code class="bash">$ docker exec -it 3008b1e1bba1 cat /tmp/data.txt
d469f341d836
4228529aba29
</code></pre>

<p>And as you can see the data is persisted.</p>

<h2>Resources</h2>

<p>Please have a look and star <a href="https://github.com/trajano/docker-volume-plugins">@trajano&rsquo;s</a> repository:</p>

<ul>
<li><a href="https://github.com/trajano/docker-volume-plugins">https://github.com/trajano/docker-volume-plugins</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a Distributed Storage Volume With GlusterFS]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/03/04/setup-a-distributed-storage-volume-with-glusterfs/"/>
    <updated>2019-03-04T22:32:53+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/03/04/setup-a-distributed-storage-volume-with-glusterfs</id>
    <content type="html"><![CDATA[<p><img src="https://glusterdocs-beta.readthedocs.io/en/latest/_images/dist-volume.png" alt="" /></p>

<p>GlusterFS is a Awesome Scalable Networked Filesystem, which makes it Easy to Create Large and Scalable Storage Solutions on Commodity Hardware.</p>

<p><a href="https://bekkerclothing.com/collections/developer?utm_source=blog.ruanbekker.com&utm_medium=blog&utm_campaign=leaderboard_ad" target="_blank"><img alt="bekker-clothing-developer-tshirts" src="https://user-images.githubusercontent.com/567298/70170981-7c278a80-16d6-11ea-9759-6621d02c1423.png"></a></p>

<p><strong>Basic Concepts of GlusterFS:</strong></p>

<p>Brick:
* In GlusterFS, a brick is the basic unit of storage, represented by a directory on the server in the trusted storage pool.</p>

<p>Gluster Volume:
* A Gluster volume is a Logical Collection of Bricks.</p>

<p>Distributed Filesystem:
* The concept is to enable multiple clients to concurrently access data which is spread across multple servers in a trusted storage pool. This is also a great solution to prevent data corruption, enable highly available storage systems, etc.</p>

<p><a href="http://gluster.readthedocs.io/en/latest/Administrator%20Guide/glossary/">More concepts</a> can be retrieved from their documentation.</p>

<h2>Different GlusterFS Volume Types:</h2>

<p>With GlusterFS you can create the following types of Gluster Volumes:</p>

<ul>
<li>Distributed Volumes: (Ideal for Scalable Storage, No Data Redundancy)</li>
<li>Replicated Volumes: (Better reliability and data redundancy)</li>
<li>Distributed-Replicated Volumes: (HA of Data due to Redundancy and Scaling Storage)</li>
<li><a href="http://gluster.readthedocs.io/en/latest/Quick-Start-Guide/Architecture/">More detail</a> on GlusterFS Architecture</li>
</ul>


<h2>Setup a Distributed Gluster Volume:</h2>

<p>In this guide we will setup a 3 Node Distributed GlusterFS Volume on Ubuntu 16.04.</p>

<p>For this use case we would like to achieve a storage solution to scale the size of our storage, and not really worried about redundancy as, with a Distributed Setup we can increase the size of our volume, the more bricks we add to our GlusterFS Volume.</p>

<h2>Setup: Our Environment</h2>

<p>Each node has 2 disks, <code>/dev/xvda</code> for the Operating System wich is 20GB and <code>/dev/xvdb</code> which has 100GB. After we have created our GlusterFS Volume, we will have a Gluster Volume of 300GB.</p>

<p>Having a look at our disks:</p>

<pre><code class="bash">$ lsblk
NAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
xvda    202:0    0   20G  0 disk
└─xvda1 202:1    0   20G  0 part /
xvdb    202:16   0  100G  0 disk 
</code></pre>

<p>If you don&rsquo;t have DNS setup for your nodes, you can use your /etc/hosts file for all 3 nodes, which I will be using in this demonstration:</p>

<pre><code class="bash">$ cat /etc/hosts
172.31.13.226   gluster-node-1
172.31.9.7      gluster-node-2
172.31.15.34    gluster-node-3
127.0.0.1       localhost
</code></pre>

<h2>Install GlusterFS from the Package Manager:</h2>

<p>Note that all the steps below needs to be performed on all 3 nodes, unless specified otherwise:</p>

<pre><code class="bash">$ apt update &amp;&amp; apt upgrade -y
$ apt install xfsprogs attr glusterfs-server glusterfs-client glusterfs-common -y
</code></pre>

<h2>Format and Prepare the Gluster Disks:</h2>

<p>We will create a XFS Filesystem for our 100GB disk, create the directory path where we will mount our disk onto, and also load it into <code>/etc/fstab</code>:</p>

<pre><code class="bash">$ mkfs.xfs /dev/xvdb
$ mkdir /gluster
$ echo '/dev/xvdb /gluster xfs defaults 0 0' &gt;&gt; /etc/fstab
$ mount -a
</code></pre>

<p>After we mounted the disk, we should see that our disk is mounted to <code>/gluster</code>:</p>

<pre><code class="bash">$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/xvda1       20G  1.2G   19G   7% /
/dev/xvdb       100G   33M  100G   1% /gluster
</code></pre>

<p>After our disk is mounted, we can proceed by creating the brick directory on our disk that we mounted, from the step above:</p>

<pre><code class="bash">$ mkdir /gluster/brick
</code></pre>

<h2>Start GlusterFS Service:</h2>

<p>Enable GlusterFS on startup, start the service and make sure that the service is running:</p>

<pre><code class="bash">$ systemctl enable glusterfs-server
$ systemctl restart glusterfs-server
$ systemctl is-active glusterfs-server
active
</code></pre>

<h2>Discover All the Nodes for our Cluster:</h2>

<p>The following will only be done on one of the nodes. First we need to discover our other nodes.</p>

<p>The node that you are currently on, will be discovered by default and only needs the other 2 nodes to be discovered:</p>

<pre><code class="bash">$ gluster peer probe gluster-node-2
$ gluster peer probe gluster-node-3
</code></pre>

<p>Let&rsquo;s verify this by listing all the nodes in our cluster:</p>

<pre><code class="bash">$ gluster pool list
UUID                                    Hostname        State
6e02731c-6472-4ea4-bd48-d5dd87150e8b    gluster-node-2  Connected
9d4c2605-57ba-49e2-b5da-a970448dc886    gluster-node-3  Connected
608f027e-e953-413b-b370-ce84050a83c9    localhost       Connected
</code></pre>

<h2>Create the Distributed GlusterFS Volume:</h2>

<p>We will create a Distributed GlusterFS Volume across 3 nodes, and we will name the volume <code>gfs</code>:</p>

<pre><code class="bash">$ gluster volume create gfs \
  gluster-node-1:/gluster/brick \
  gluster-node-2:/gluster/brick \
  gluster-node-3:/gluster/brick

volume create: gfs: success: please start the volume to access data
</code></pre>

<h2>Start the GlusterFS Volume:</h2>

<p>Now start the <code>gfs</code> GlusterFS Volume:</p>

<pre><code class="bash">$ gluster volume start gfs
volume start: gfs: success
</code></pre>

<p>To get information about the volume:</p>

<pre><code class="bash">$ gluster volume info gfs

Volume Name: gfs
Type: Distribute
Volume ID: c08bc2e8-59b3-49e7-bc17-d4bc8d99a92f
Status: Started
Number of Bricks: 3
Transport-type: tcp
Bricks:
Brick1: gluster-node-1:/gluster/brick
Brick2: gluster-node-2:/gluster/brick
Brick3: gluster-node-3:/gluster/brick
Options Reconfigured:
performance.readdir-ahead: on
</code></pre>

<p>Status information about our Volume:</p>

<pre><code class="bash">$ gluster volume status

Status of volume: gfs
Gluster process                             TCP Port  RDMA Port  Online  Pid
------------------------------------------------------------------------------
Brick gluster-node-1:/gluster/brick         49152     0          Y       7139
Brick gluster-node-2:/gluster/brick         49152     0          Y       7027
Brick gluster-node-3:/gluster/brick         49152     0          Y       7099
NFS Server on localhost                     2049      0          Y       7158
NFS Server on gluster-node-2                2049      0          Y       7046
NFS Server on gluster-node-3                2049      0          Y       7118

Task Status of Volume gfs
------------------------------------------------------------------------------
There are no active volume tasks
</code></pre>

<h2>Mounting our GlusterFS Volume:</h2>

<p>On all the clients, in this case our 3 nodes, load the mount information into <code>/etc/fstab</code> and then mount the GlusterFS Volume:</p>

<pre><code class="bash">$ echo 'localhost:/gfs /mnt glusterfs defaults,_netdev,backupvolfile-server=gluster-node-1 0 0' &gt;&gt; /etc/fstab
$ mount -a
</code></pre>

<p>Now that the volume is mounted, have a look at your disk info, and you will find that you have a <code>300GB</code> GlusterFS Volume mounted:</p>

<pre><code class="bash">$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/xvda1       20G  1.3G   19G   7% /
/dev/xvdb       100G   33M  100G   1% /gluster
localhost:/gfs  300G   98M  300G   1% /mnt
</code></pre>

<p>As mentioned before, this is most probably for a scenario where you would like to achieve a high storage size and not really concerned about data availability.</p>

<p>In the next couple of weeks I will also go through the Replicated, Distributed-Replicated and <a href="https://gluster.readthedocs.io/en/latest/Administrator%20Guide/Gluster%20On%20ZFS/">GlusterFS with ZFS</a> setups.</p>

<h2>Resources:</h2>

<ul>
<li><a href="http://gluster.readthedocs.io/en/latest/Quick-Start-Guide/Terminologies/">GlusterFS Terminologies</a></li>
<li><a href="http://gluster.readthedocs.io/en/latest/Quick-Start-Guide/Architecture/">GlusterFS Architecture</a></li>
<li><a href="http://gluster.readthedocs.io/en/latest/Administrator%20Guide/Gluster%20On%20ZFS/">GlusterFS with ZFS</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
