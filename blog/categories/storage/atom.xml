<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Storage | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/storage/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2018-02-11T17:51:16-05:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Setup a NFS Server on Ubuntu]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/02/11/setup-a-nfs-server-on-ubuntu/"/>
    <updated>2018-02-11T17:26:56-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/02/11/setup-a-nfs-server-on-ubuntu</id>
    <content type="html"><![CDATA[<p><img src="https://i.snag.gy/3sUALo.jpg" alt="" /></p>

<p>Quick post on how to setup a NFS Server on Ubuntu and how to setup the client to interact with the NFS Server.</p>

<h2>Setup the Dependencies:</h2>

<pre><code class="bash">$ apt update &amp;&amp; sudo apt upgrade -y
$ sudo apt-get install nfs-kernel-server nfs-common -y
</code></pre>

<p>Create the Directory for NFS and set permissions:</p>

<pre><code class="bash">mkdir /vol
chown -R nobody:nogroup /vol
</code></pre>

<h2>Allow the Clients:</h2>

<p>We need to set in the <code>exports</code> file, the clients we would like to allow:</p>

<ul>
<li><code>rw</code>: Allows Client R/W Access to the Volume.</li>
<li><code>sync</code>: This option forces NFS to write changes to disk before replying. More stable and Consistent. Note, it does reduce the speed of file operations.</li>
<li><code>no_subtree_check</code>: This prevents subtree checking, which is a process where the host must check whether the file is actually still available in the exported tree for every request. This can cause many problems when a file is renamed while the client has it opened. In almost all cases, it is better to disable subtree checking.</li>
</ul>


<pre><code class="bash">$ echo '/vol 10.8.133.83(rw,sync,no_subtree_check) 10.8.166.19(rw,sync,no_subtree_check) 10.8.142.195(rw,sync,no_subtree_check)' &gt;&gt; /etc/exports
</code></pre>

<h2>Start the NFS Server:</h2>

<p>Restart the service and enable the service on boot:</p>

<pre><code class="bash">$ sudo systemctl restart nfs-kernel-server
$ sudo systemctl enable nfs-kernel-server
</code></pre>

<h2>Client Side:</h2>

<p>We will mount the NFS Volume to our Clients <code>/mnt</code> partition.</p>

<p>Install the dependencies:</p>

<pre><code class="bash">$ sudo apt-get install nfs-common -y
</code></pre>

<p>Test if we can mount the volume, then unmount it, as we will set the config in our <code>fstab</code>:</p>

<pre><code class="bash">$ sudo mount 10.8.133.83:/vol /mnt
$ sudo umount /mnt
$ df -h
</code></pre>

<p>Set the config in your <code>fstab</code>, then mount it from there:</p>

<pre><code class="bash">$ sudo bash -c "echo '10.8.133.83:/vol /mnt nfs auto,nofail,noatime,nolock,intr,tcp,actimeo=1800 0 0' &gt;&gt; /etc/fstab"
$ sudo mount -a
$ df -h
</code></pre>

<p>Now you shoule be able to write to your NFS Volume from your client.</p>

<p>Sources:
- <a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-16-04">1</a> <a href="https://gist.github.com/deviantony/557984d62e867e6f505577b207db6ffc%">2</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Create a ZFS Raidz1 Volume Pool on Ubuntu 16]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/08/24/create-a-zfs-raidz1-volume-pool-on-ubuntu-16/"/>
    <updated>2017-08-24T09:16:34-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/08/24/create-a-zfs-raidz1-volume-pool-on-ubuntu-16</id>
    <content type="html"><![CDATA[<p>Setting up ZFS Volume Pool on Ubuntu 16.04</p>

<h2>Installation</h2>

<pre><code>$ sudo apt-get install zfsutils-linux -y
</code></pre>

<h2>Creating the ZFS Storage Pool</h2>

<p>We will create a RAIDZ(1) Volume which is like Raid5 with Single Parity, so we can lose one of the Physical Disks before Raid failure.</p>

<p>Let&rsquo;s first have a look at our disks that we have on our server:</p>

<pre><code>$ lsblk
NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
xvda    202:0    0   8G  0 disk
└─xvda1 202:1    0   8G  0 part /
xvdf    202:80   0 100G  0 disk 
xvdg    202:80   0 100G  0 disk 
</code></pre>

<p>So we will be creating the volume consisting of <code>/dev/xvdf</code> and <code>/dev/xvdg</code> and we will name our pool: <code>storage-pool</code></p>

<pre><code>$ zpool create storage-pool raidz1 xvdf xvdg -f
</code></pre>

<h2>Listing Pools</h2>

<pre><code>$ zpool list
NAME           SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
storage-pool   199G   125K   199G         -     0%     0%  1.00x  ONLINE  -
</code></pre>

<p>We can also list the volume with <code>zfs</code>:</p>

<pre><code>$ zfs list
NAME            USED  AVAIL  REFER  MOUNTPOINT
storage-pool    125K  199G   19K    /storage-pool
</code></pre>

<h2>Mounting the Volume:</h2>

<p>You will find that the volume is already mounted:</p>

<pre><code>$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/xvda1      7.7G  1.1G  6.7G  14% /
pool            199G  125K  198G   1% /pool
</code></pre>

<h2>Resources:</h2>

<p>See how Brett Kelly from 45 Drives tried to break a Storage Cluster with GlusterFS and ZFS:</p>

<center><iframe width="740" height="400" src="https://www.youtube.com/embed/A0wV4k58RIs" frameborder="1" allowfullscreen></iframe></center>


<p>Great ZFS Performance Comparison:</p>

<ul>
<li><a href="https://calomel.org/zfs_raid_speed_capacity.html">https://calomel.org/zfs_raid_speed_capacity.html</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
