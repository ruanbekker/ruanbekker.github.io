<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Storage | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/storage/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2018-04-18T13:32:54-04:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Create a Logical Volume Using LVM on Ubuntu]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/03/30/create-a-logical-volume-using-lvm-on-ubuntu/"/>
    <updated>2018-03-30T20:38:18-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/03/30/create-a-logical-volume-using-lvm-on-ubuntu</id>
    <content type="html"><![CDATA[<p>Logical Volume Manager (LVM) - adds an extra layer between the physical disks and the file system, which allows you to resize your storage on the fly, use multiple disks, instead of one, etc.</p>

<h2>Concepts:</h2>

<p>Physical Volume:
- Physical Volume represents the actual disk / block device.</p>

<p>Volume Group:
- Volume Groups combines the collection of Logical Volumes and Physical Volumes into one administrative unit.</p>

<p>Logical Volume:
- A Logical Volume is the conceptual equivalent of a disk partition in a non-LVM system.</p>

<p>File Systems:
- File systems are built on top of logical volumes.</p>

<h2>What we are doing today:</h2>

<p>We have a disk installed on our server which is 150GB that is located on <code>/dev/vdb</code>, which we will manage via LVM and will be mounted under <code>/mnt</code></p>

<h2>Dependencies:</h2>

<p>Update and Install LVM:</p>

<pre><code>$ apt update &amp;&amp; apt upgrade -y
$ apt install lvm2 -y
$ systemctl enable lvm2-lvmetad
$ systemctl start lvm2-lvmetad
</code></pre>

<h2>Create the Logical Volume:</h2>

<p>Initialize the Physical Volume to be managed by LVM, then create the Volume Group, then go ahead to create the Logical Volume:</p>

<pre><code>$ pvcreate /dev/vdb
$ vgcreate vg1 /dev/vdb
$ lvcreate -l 100%FREE -n vol1 vg1
</code></pre>

<p>Build the Linux Filesystem with ext4 and mount the volume to the <code>/mnt</code> partition:</p>

<pre><code>$ mkfs.ext4 /dev/vg1/vol1
$ mount /dev/vg1/vol1 /mnt
$ echo '/dev/mapper/vg1-vol1 /mnt ext4 defaults,nofail 0 0' &gt;&gt; /etc/fstab
</code></pre>

<h2>Other useful commands:</h2>

<p>To list Physical Volume Info:</p>

<pre><code>$ pvs
PV         VG   Fmt  Attr PSize   PFree
/dev/vdb   vg1  lvm2 a--  139.70g    0
</code></pre>

<p>To list Volume Group Info:</p>

<pre><code>$ vgs
VG   #PV #LV #SN Attr   VSize   VFree
vg1    1   1   0 wz--n- 139.70g    0
</code></pre>

<p>And viewing the logical volume size from the volume group:</p>

<pre><code>$ vgs -o +lv_size,lv_name
VG   #PV #LV #SN Attr   VSize   VFree LSize   LV
vg1    1   1   0 wz--n- 139.70g    0  139.70g vol1
</code></pre>

<p>Information about Logical Volumes:</p>

<pre><code>$ lvs
LV   VG   Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
vol1 vg1  -wi-ao---- 139.70g
</code></pre>

<h2>Resources:</h2>

<ul>
<li><a href="https://www.thegeekdiary.com/redhat-centos-a-beginners-guide-to-lvm-logical-volume-manager/">1</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Expanding the Size of Your EBS Volume on AWS EC2 for Linux]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/03/28/expanding-the-size-of-your-ebs-volume-on-aws-ec2-for-linux/"/>
    <updated>2018-03-28T01:45:07-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/03/28/expanding-the-size-of-your-ebs-volume-on-aws-ec2-for-linux</id>
    <content type="html"><![CDATA[<p><img src="https://i.snag.gy/BJLbwQ.jpg" alt="" /></p>

<p>Resizing your EBS Volume on the fly, that is attached to your EC2 Linux instance, on Amazon Web Services.</p>

<p>We want to resize our EBS Volume from 100GB to 1000GB and at the moment my EBS Volume is 100GB, as you can see:</p>

<pre><code class="bash">$ df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        7.9G   60K  7.9G   1% /dev
tmpfs           7.9G     0  7.9G   0% /dev/shm
/dev/xvda1       99G   32G   67G  32% /
</code></pre>

<p>Now we want to resize the volume to 1000GB, without shutting down our EC2 instance.</p>

<p>Go to your EC2 Management Console, Select your EC2 Instance, scroll down to the EBS Volume, click on it and click the EBS Volume ID, from there select Actions, modify and resize the disk to the needed size. As you can see the disk is now 1000GB:</p>

<pre><code class="bash">$ lsblk
NAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
xvda    202:0    0 1000G  0 disk
xvda1 202:1    0 1000G  0 part /
</code></pre>

<p>But our partition is still 100GB:</p>

<pre><code class="bash">$ df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        7.9G   60K  7.9G   1% /dev
tmpfs           7.9G     0  7.9G   0% /dev/shm
/dev/xvda1       99G   32G   67G  32% /
</code></pre>

<p>We need to use <code>growpart</code> and <code>resize2fs</code> to resize our partition:</p>

<pre><code class="bash">$ sudo growpart /dev/xvda 1
CHANGED: disk=/dev/xvda partition=1: start=4096 old: size=209711070,end=209715166 new: size=2097147870,end=2097151966

$ sudo resize2fs /dev/xvda1
resize2fs 1.42.12 (29-Aug-2014)
Filesystem at /dev/xvda1 is mounted on /; on-line resizing required
old_desc_blocks = 7, new_desc_blocks = 63
The filesystem on /dev/xvda1 is now 262143483 (4k) blocks long.
</code></pre>

<p>Now we will have a resized partition to 100GB:</p>

<pre><code class="bash">$ df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        7.9G   60K  7.9G   1% /dev
tmpfs           7.9G     0  7.9G   0% /dev/shm
/dev/xvda1      985G   33G  952G   4% /
</code></pre>

<p>Resources:</p>

<ul>
<li><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/recognize-expanded-volume-linux.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/recognize-expanded-volume-linux.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Guide to Setup Docker Convoy Volume Driver for Docker Swarm With NFS]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/02/16/guide-to-setup-docker-convoy-volume-driver-for-docker-swarm-with-nfs/"/>
    <updated>2018-02-16T08:51:59-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/02/16/guide-to-setup-docker-convoy-volume-driver-for-docker-swarm-with-nfs</id>
    <content type="html"><![CDATA[<p><img src="http://obj-cache.cloud.ruanbekker.com/docker-logo.png" alt="" /></p>

<p>In this post we will setup <a href="https://github.com/rancher/convoy">Rancher&rsquo;s Convoy Storage Plugin</a> with NFS, to provide data persistence in Docker Swarm.</p>

<h2>The Overview:</h2>

<p>This essentially means that we will have a NFS Volume, when the service gets created on Docker Swarm, the cluster creates these volumes with path mapping, so when a container gets spawned, restarted, scaled etc, the container that gets started on the new node will be aware of the volume, and will get the data that its expecting.</p>

<p>Its also good to note that our NFS Server will be a single point of failure, therefore its also good to look at a Distributed Volume like <a href="https://sysadmins.co.za/tag/glusterfs">GlusterFS</a>, <a href="https://sysadmins.co.za/tag/xtreemfs/">XtreemFS</a>, <a href="">Ceph</a>, etc.</p>

<ul>
<li>NFS Server (10.8.133.83)</li>
<li>Rancher Convoy Plugin on Each Docker Node in the Swarm (10.8.133.83, 10.8.166.19, 10.8.142.195)</li>
</ul>


<h2>Setup NFS:</h2>

<p>Setup the NFS Server</p>

<pre><code class="bash">$ sudo apt-get install nfs-kernel-server nfs-common -y
$ mkdir /vol
$ chown -R nobody:nogroup /vol
$ echo '/vol 10.8.133.83(rw,sync,no_subtree_check) 10.8.166.19(rw,sync,no_subtree_check) 10.8.142.195(rw,sync,no_subtree_check)' &gt;&gt; /etc/exports
$ sudo systemctl restart nfs-kernel-server
$ sudo systemctl enable nfs-kernel-server
</code></pre>

<p>Setup the NFS Clients on each Docker Node:</p>

<pre><code class="bash">$ sudo apt-get install nfs-common -y
$ mount 10.8.133.83:/vol /mnt
$ umount /mnt
$ df -h
</code></pre>

<p>If you can see tht the volume is mounted, unmount it and add it to the <code>fstab</code> so the volume can be mounted on boot:</p>

<pre><code class="bash">$ sudo bash -c "echo '10.8.133.83:/vol /mnt nfs auto,nofail,noatime,nolock,intr,tcp,actimeo=1800 0 0' &gt;&gt; /etc/fstab"
$ sudo mount -a
</code></pre>

<h2>Install Rancher Convoy Plugin:</h2>

<p>The Plugin needs to be installed on each docker node that will be part of the swarm:</p>

<pre><code class="bash">$ cd /tmp
$ wget https://github.com/rancher/convoy/releases/download/v0.5.0/convoy.tar.gz
$ tar xzf convoy.tar.gz
$ sudo cp convoy/convoy convoy/convoy-pdata_tools /usr/local/bin/
$ sudo mkdir -p /etc/docker/plugins/
$ sudo bash -c 'echo "unix:///var/run/convoy/convoy.sock" &gt; /etc/docker/plugins/convoy.spec'
</code></pre>

<h2>Create the init script:</h2>

<p>Thanks to <a href="https://gist.github.com/deviantony/557984d62e867e6f505577b207db6ffc">deviantony</a></p>

<pre><code class="bash">#!/bin/sh
### BEGIN INIT INFO
# Provides:
# Required-Start:    $remote_fs $syslog
# Required-Stop:     $remote_fs $syslog
# Default-Start:     2 3 4 5
# Default-Stop:      0 1 6
# Short-Description: Start daemon at boot time
# Description:       Enable service provided by daemon.
### END INIT INFO

dir="/usr/local/bin"
cmd="convoy daemon --drivers vfs --driver-opts vfs.path=/mnt/docker/volumes"
user="root"
name="convoy"

pid_file="/var/run/$name.pid"
stdout_log="/var/log/$name.log"
stderr_log="/var/log/$name.err"

get_pid() {
    cat "$pid_file"
}

is_running() {
    [ -f "$pid_file" ] &amp;&amp; ps `get_pid` &gt; /dev/null 2&gt;&amp;1
}

case "$1" in
    start)
    if is_running; then
        echo "Already started"
    else
        echo "Starting $name"
        cd "$dir"
        if [ -z "$user" ]; then
            sudo $cmd &gt;&gt; "$stdout_log" 2&gt;&gt; "$stderr_log" &amp;
        else
            sudo -u "$user" $cmd &gt;&gt; "$stdout_log" 2&gt;&gt; "$stderr_log" &amp;
        fi
        echo $! &gt; "$pid_file"
        if ! is_running; then
            echo "Unable to start, see $stdout_log and $stderr_log"
            exit 1
        fi
    fi
    ;;
    stop)
    if is_running; then
        echo -n "Stopping $name.."
        kill `get_pid`
        for i in {1..10}
        do
            if ! is_running; then
                break
            fi

            echo -n "."
            sleep 1
        done
        echo

        if is_running; then
            echo "Not stopped; may still be shutting down or shutdown may have failed"
            exit 1
        else
            echo "Stopped"
            if [ -f "$pid_file" ]; then
                rm "$pid_file"
            fi
        fi
    else
        echo "Not running"
    fi
    ;;
    restart)
    $0 stop
    if is_running; then
        echo "Unable to stop, will not attempt to start"
        exit 1
    fi
    $0 start
    ;;
    status)
    if is_running; then
        echo "Running"
    else
        echo "Stopped"
        exit 1
    fi
    ;;
    *)
    echo "Usage: $0 {start|stop|restart|status}"
    exit 1
    ;;
esac

exit 0
</code></pre>

<p>Make the script executable:</p>

<pre><code class="bash">$ chmod +x /etc/init.d/convoy
</code></pre>

<p>Enable the service on boot:</p>

<pre><code class="bash">$ sudo systemctl enable convoy
</code></pre>

<p>Start the service:</p>

<pre><code class="bash">$ sudo /etc/init.d/convoy start
</code></pre>

<p>This should be done on all the nodes.</p>

<h2>Externally Managed Convoy Volumes</h2>

<p>One thing to note is that, after your delete a volume, you will still need to delete the directory from the path where its hosted, as the application does not do that by itself.</p>

<p>Creating the Volume Before hand:</p>

<pre><code class="bash">$ convoy create test1
test1

$ docker volume ls
DRIVER              VOLUME NAME
convoy              test1

$ cat /mnt/docker/volumes/config/vfs_volume_test1.json
{"Name":"test1","Size":0,"Path":"/mnt/docker/volumes/test1","MountPoint":"","PrepareForVM":false,"CreatedTime":"Mon Feb 05 13:07:05 +0000 2018","Snapshots":{}}
</code></pre>

<p>Viewing the volume from another node:</p>

<pre><code class="bash">$ docker volume ls
DRIVER              VOLUME NAME
convoy              test1
</code></pre>

<h2>Creating a Test Service:</h2>

<p>Create a test service to test the data persistence, our docker-compose.yml:</p>

<pre><code class="bash">version: '3.4'

volumes:
  test1:
    external: true

networks:
  appnet:
    external: true

services:
  test:
    image: alpine:edge
    command: sh -c "ping 127.0.0.1"
    volumes:
      - test1:/data
    networks:
      - appnet
</code></pre>

<p>Creating the Overlay Network and Deploying the Stack:</p>

<pre><code class="bash">$ docker network create -d overlay appnet
$ docker stack deploy -c docker-compose.yml apps
Creating service apps_test
</code></pre>

<p>Write data to the volume in the container:</p>

<pre><code>$ docker exec -it apps_test.1.iojo7fpw8jirqjs3iu8qr7qpe sh
/ # echo "ok" &gt; /data/file.txt
/ # cat /data/file.txt
ok
</code></pre>

<p>Scale the service:</p>

<pre><code class="bash">$ docker service scale apps_test=2
apps_test scaled to 2
</code></pre>

<p>Inspect to see if the new replica is on another node:</p>

<pre><code class="bash">$ docker service ps apps_test
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE               ERROR                         PORTS
myrq2pc3z26z        apps_test.1         alpine:edge         scw-docker-1        Running             Running 45 seconds ago
ny8t97l2q00c         \_ apps_test.1     alpine:edge         scw-docker-1        Shutdown            Failed 51 seconds ago       "task: non-zero exit (137)"
iojo7fpw8jir         \_ apps_test.1     alpine:edge         scw-docker-1        Shutdown            Failed about a minute ago   "task: non-zero exit (137)"
tt0nuusvgeki        apps_test.2         alpine:edge         scw-docker-2        Running             Running 15 seconds ago
</code></pre>

<p>Logon to the new container and test if the data is persisted:</p>

<pre><code class="bash">$ docker exec -it apps_test.2.tt0nuusvgekirw1c5myu720ga sh
/ # cat /data/file.txt
ok
</code></pre>

<p>Delete the Stack and Redeploy and have a look at the data we created earlier, and you will notice the data is persisted:</p>

<pre><code class="bash">$ docker stack rm apps
$ docker stack deploy -c docker-compose.yml apps
$ docker exec -it apps_test.1.la4w2sbuu8cmv6xamwxl7n0ip cat /data/file.txt
ok
$ docker stack rm apps
</code></pre>

<h2>Create Volume via Compose:</h2>

<p>You can also create the volume on service/stack creation level, so you dont need to create the volume before hand, the compose file:</p>

<pre><code class="yml">version: '3.4'

volumes:
  test2:
    driver: convoy
    driver_opts:
      size: 10

networks:
  appnet:
    external: true

services:
  test:
    image: alpine:edge
    command: sh -c "ping 127.0.0.1"
    volumes:
      - test2:/data
    networks:
      - appnet
</code></pre>

<p>Deploy the Stack:</p>

<pre><code class="bash">$ docker stack deploy -c docker-compose-new.yml apps
Creating service apps_test
</code></pre>

<p>List the volumes and you will notice that the volume was created:</p>

<pre><code class="bash">$ docker volume ls
DRIVER              VOLUME NAME
convoy              apps_test2
convoy              test1
</code></pre>

<p>Lets inspect the volume, to see more details about it:</p>

<pre><code class="bash">docker volume inspect apps_test2
[
    {
        "CreatedAt": "0001-01-01T00:00:00Z",
        "Driver": "convoy",
        "Labels": {
            "com.docker.stack.namespace": "apps"
        },
        "Mountpoint": "/mnt/docker/volumes/apps_test2",
        "Name": "apps_test2",
        "Options": {
            "size": "10"
        },
        "Scope": "local"
    }
]
</code></pre>

<p>As mentioned earlier, if you delete the volume, you need to delete the data directories as well</p>

<pre><code class="bash">$ docker volume rm test1
test1

$ ls /mnt/docker/volumes/
apps_test2  config  test1

$ rm -rf /mnt/docker/volumes/test1
</code></pre>

<p>More info about the project:
- <a href="https://github.com/rancher/convoy">https://github.com/rancher/convoy</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a NFS Server on Ubuntu]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/02/11/setup-a-nfs-server-on-ubuntu/"/>
    <updated>2018-02-11T17:26:56-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/02/11/setup-a-nfs-server-on-ubuntu</id>
    <content type="html"><![CDATA[<p><img src="https://i.snag.gy/3sUALo.jpg" alt="" /></p>

<p>Quick post on how to setup a NFS Server on Ubuntu and how to setup the client to interact with the NFS Server.</p>

<h2>Setup the Dependencies:</h2>

<pre><code class="bash">$ apt update &amp;&amp; sudo apt upgrade -y
$ sudo apt-get install nfs-kernel-server nfs-common -y
</code></pre>

<p>Create the Directory for NFS and set permissions:</p>

<pre><code class="bash">mkdir /vol
chown -R nobody:nogroup /vol
</code></pre>

<h2>Allow the Clients:</h2>

<p>We need to set in the <code>exports</code> file, the clients we would like to allow:</p>

<ul>
<li><code>rw</code>: Allows Client R/W Access to the Volume.</li>
<li><code>sync</code>: This option forces NFS to write changes to disk before replying. More stable and Consistent. Note, it does reduce the speed of file operations.</li>
<li><code>no_subtree_check</code>: This prevents subtree checking, which is a process where the host must check whether the file is actually still available in the exported tree for every request. This can cause many problems when a file is renamed while the client has it opened. In almost all cases, it is better to disable subtree checking.</li>
</ul>


<pre><code class="bash">$ echo '/vol 10.8.133.83(rw,sync,no_subtree_check) 10.8.166.19(rw,sync,no_subtree_check) 10.8.142.195(rw,sync,no_subtree_check)' &gt;&gt; /etc/exports
</code></pre>

<h2>Start the NFS Server:</h2>

<p>Restart the service and enable the service on boot:</p>

<pre><code class="bash">$ sudo systemctl restart nfs-kernel-server
$ sudo systemctl enable nfs-kernel-server
</code></pre>

<h2>Client Side:</h2>

<p>We will mount the NFS Volume to our Clients <code>/mnt</code> partition.</p>

<p>Install the dependencies:</p>

<pre><code class="bash">$ sudo apt-get install nfs-common -y
</code></pre>

<p>Test if we can mount the volume, then unmount it, as we will set the config in our <code>fstab</code>:</p>

<pre><code class="bash">$ sudo mount 10.8.133.83:/vol /mnt
$ sudo umount /mnt
$ df -h
</code></pre>

<p>Set the config in your <code>fstab</code>, then mount it from there:</p>

<pre><code class="bash">$ sudo bash -c "echo '10.8.133.83:/vol /mnt nfs auto,nofail,noatime,nolock,intr,tcp,actimeo=1800 0 0' &gt;&gt; /etc/fstab"
$ sudo mount -a
$ df -h
</code></pre>

<p>Now you shoule be able to write to your NFS Volume from your client.</p>

<p>Sources:
- <a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-16-04">1</a> <a href="https://gist.github.com/deviantony/557984d62e867e6f505577b207db6ffc%">2</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Create a ZFS Raidz1 Volume Pool on Ubuntu 16]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/08/24/create-a-zfs-raidz1-volume-pool-on-ubuntu-16/"/>
    <updated>2017-08-24T09:16:34-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/08/24/create-a-zfs-raidz1-volume-pool-on-ubuntu-16</id>
    <content type="html"><![CDATA[<p>Setting up ZFS Volume Pool on Ubuntu 16.04</p>

<h2>Installation</h2>

<pre><code>$ sudo apt-get install zfsutils-linux -y
</code></pre>

<h2>Creating the ZFS Storage Pool</h2>

<p>We will create a RAIDZ(1) Volume which is like Raid5 with Single Parity, so we can lose one of the Physical Disks before Raid failure.</p>

<p>Let&rsquo;s first have a look at our disks that we have on our server:</p>

<pre><code>$ lsblk
NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
xvda    202:0    0   8G  0 disk
└─xvda1 202:1    0   8G  0 part /
xvdf    202:80   0 100G  0 disk 
xvdg    202:80   0 100G  0 disk 
</code></pre>

<p>So we will be creating the volume consisting of <code>/dev/xvdf</code> and <code>/dev/xvdg</code> and we will name our pool: <code>storage-pool</code></p>

<pre><code>$ zpool create storage-pool raidz1 xvdf xvdg -f
</code></pre>

<h2>Listing Pools</h2>

<pre><code>$ zpool list
NAME           SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
storage-pool   199G   125K   199G         -     0%     0%  1.00x  ONLINE  -
</code></pre>

<p>We can also list the volume with <code>zfs</code>:</p>

<pre><code>$ zfs list
NAME            USED  AVAIL  REFER  MOUNTPOINT
storage-pool    125K  199G   19K    /storage-pool
</code></pre>

<h2>Mounting the Volume:</h2>

<p>You will find that the volume is already mounted:</p>

<pre><code>$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/xvda1      7.7G  1.1G  6.7G  14% /
pool            199G  125K  198G   1% /pool
</code></pre>

<h2>Resources:</h2>

<p>See how Brett Kelly from 45 Drives tried to break a Storage Cluster with GlusterFS and ZFS:</p>

<center><iframe width="740" height="400" src="https://www.youtube.com/embed/A0wV4k58RIs" frameborder="1" allowfullscreen></iframe></center>


<p>Great ZFS Performance Comparison:</p>

<ul>
<li><a href="https://calomel.org/zfs_raid_speed_capacity.html">https://calomel.org/zfs_raid_speed_capacity.html</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
