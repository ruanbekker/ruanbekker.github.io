<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Storage | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/storage/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2018-10-23T16:23:00-04:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Setup a 3 Node Ceph Storage Cluster on Ubuntu 16]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/06/13/setup-a-3-node-ceph-storage-cluster-on-ubuntu-16/"/>
    <updated>2018-06-13T18:22:06-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/06/13/setup-a-3-node-ceph-storage-cluster-on-ubuntu-16</id>
    <content type="html"><![CDATA[<p><img src="https://ceph.com/wp-content/uploads/2016/07/Ceph_Logo_Standard_RGB_120411_fa.png" alt="" /></p>

<p>For some time now, I wanted to do a setup of Ceph, and I finally got the time to do it. This setup was done on Ubuntu 16.04</p>

<h2>What is Ceph</h2>

<p>Ceph is a storage platform that implements object storage on a single distributed computer cluster and provides interfaces for object, block and file-level storage.</p>

<ul>
<li>Object Storage:</li>
</ul>


<p>Ceph provides seemless access to objects via native language bindings or via the REST interface, RadosGW and also compatible for applications written for S3 and Swift.</p>

<ul>
<li>Block Storage:</li>
</ul>


<p>Ceph&rsquo;s Rados Block Device (RBD) provides access to block device images that are replicated and striped across the storage cluster.</p>

<ul>
<li>File System:</li>
</ul>


<p>Ceph provides a network file system (CephFS) that aims for high performance.</p>

<h2>Our Setup</h2>

<p>We will have 4 nodes. 1 Admin node where we will deploy our cluster with, and 3 nodes that will hold the data:</p>

<ul>
<li>ceph-admin (10.0.8.2)</li>
<li>ceph-node1 (10.0.8.3)</li>
<li>ceph-node2 (10.0.8.4)</li>
<li>ceph-node3 (10.0.8.5)</li>
</ul>


<h2>Host Entries</h2>

<p>If you don&rsquo;t have dns for your servers, setup the <code>/etc/hosts</code> file so that the names can resolves to the ip addresses:</p>

<pre><code class="bash ">10.0.8.2 ceph-admin
10.0.8.3 ceph-node1
10.0.8.4 ceph-node2
10.0.8.5 ceph-node3
</code></pre>

<h2>User Accounts and Passwordless SSH</h2>

<p>Setup the <code>ceph-system</code> user accounts on all the servers:</p>

<pre><code class="bash">$ useradd -d /home/ceph-system -s /bin/bash -m ceph-system
$ passwd ceph-system
</code></pre>

<p>Setup the created user part of the sudoers that is able to issue sudo commands without a pssword:</p>

<pre><code class="bash">$ echo "ceph-system ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/ceph-system
$ chmod 0440 /etc/sudoers.d/ceph-system
</code></pre>

<p>Switch user to <code>ceph-system</code> and generate SSH keys and copy the keys from the <code>ceph-admin</code> server to the ceph-nodes:</p>

<pre><code class="bash">$ sudo su - ceph-system
$ ssh-keygen -t rsa -f ~/.ssh/id_rsa -P ""
$ ssh-copy-id ceph-system@ceph-node1
$ ssh-copy-id ceph-system@ceph-node2
$ ssh-copy-id ceph-system@ceph-node3
$ ssh-copy-id ceph-system@ceph-admin
</code></pre>

<h2>Pre-Requisite Software:</h2>

<p>Install Python and Ceph Deploy on each node:</p>

<pre><code>$ sudo apt-get install python -y
$ sudo apt install ceph-deploy -y
</code></pre>

<p>Note: Please skip this section if you have additional disks on your servers.</p>

<p>The instances that im using to test this setup only has one disk, so I will be creating loop block devices using allocated files. This is not recommended as when the disk fails, all the (files/block device images) will be gone with that. But since im demonstrating this, I will create the block devices from a file:</p>

<p>I will be creating a 12GB file on each node</p>

<pre><code>$ sudo mkdir /raw-disks 
$ sudo dd if=/dev/zero of=/raw-disks/rd0 bs=1M count=12288
</code></pre>

<p>The use losetup to create the loop0 block device:</p>

<pre><code>$ sudo losetup /dev/loop0 /raw-disks/rd0
</code></pre>

<p>As you can see the loop device is showing when listing the block devices:</p>

<pre><code>$ lsblk
NAME      MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
loop0       7:0    0   12G  0 loop
</code></pre>

<h2>Install Ceph</h2>

<p>Now let&rsquo;s install ceph using ceph-deploy to all our nodes:</p>

<pre><code>$ sudo apt update &amp;&amp; sudo apt upgrade -y
$ ceph-deploy install ceph-admin ceph-node1 ceph-node2 ceph-node3
</code></pre>

<p>The version I was running at the time:</p>

<pre><code>$ ceph --version
ceph version 10.2.9
</code></pre>

<h2>Initialize Ceph</h2>

<p>Initialize the Cluster with 3 Monitors:</p>

<pre><code>$ ceph-deploy new ceph-node1 ceph-node2 ceph-node3
</code></pre>

<p>Add the initial monitors and gather the keys from the previous command:</p>

<pre><code>$ ceph-deploy mon create-initial
</code></pre>

<p>At this point, we should be able to scan the block devices on our nodes:</p>

<pre><code>$ ceph-deploy disk list ceph-node3
[ceph-node3][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[ceph-node3][DEBUG ] /dev/loop0 other
</code></pre>

<h2>Prepare the Disks:</h2>

<p>First we will zap the block devices and then prepare to create the partitions:</p>

<pre><code>$ ceph-deploy disk zap ceph-node1:/dev/loop0 ceph-node2:/dev/loop0 ceph-node3:/dev/loop0
$ ceph-deploy osd prepare ceph-node1:/dev/loop0 ceph-node2:/dev/loop0 ceph-node3:/dev/loop0
[ceph_deploy.osd][DEBUG ] Host ceph-node1 is now ready for osd use.
[ceph_deploy.osd][DEBUG ] Host ceph-node2 is now ready for osd use.
[ceph_deploy.osd][DEBUG ] Host ceph-node3 is now ready for osd use.
</code></pre>

<p>When you scan the nodes for their disks, you will notice that the partitions has been created:</p>

<pre><code>$ ceph-deploy disk list ceph-node1 
[ceph-node1][DEBUG ] /dev/loop0p2 ceph journal, for /dev/loop0p1 
[ceph-node1][DEBUG ] /dev/loop0p1 ceph data, active, cluster ceph, osd.0, journal /dev/loop0p2
</code></pre>

<p>Now let&rsquo;s activate the OSD&rsquo;s by using the data partitions:</p>

<pre><code>$ ceph-deploy osd activate ceph-node1:/dev/loop0p1 ceph-node2:/dev/loop0p1 ceph-node3:/dev/loop0p1
</code></pre>

<h2>Redistribute Keys:</h2>

<p>Copy the configuration files and admin key to your admin node and ceph data nodes:</p>

<pre><code>$ ceph-deploy admin ceph-admin ceph-node1 ceph-node2 ceph-node3
</code></pre>

<p>If you would like to add more OSD&rsquo;s (not tested):</p>

<pre><code>$ ceph-deploy disk zap ceph-node1:/dev/loop1 ceph-node2:/dev/loop1 ceph-node3:/dev/loop1
$ ceph-deploy osd prepare ceph-node1:/dev/loop1 ceph-node2:/dev/loop1 ceph-node3:/dev/loop1 
$ ceph-deploy osd activate ceph-node2:/dev/loop1p1:/dev/loop1p2 ceph-node2:/dev/loop1p1:/dev/loop1p2 ceph-node3:/dev/loop1p1:/dev/loop1p2
$ ceph-deploy admin ceph-node1 ceph-node2 ceph-node3
</code></pre>

<h2>Ceph Status:</h2>

<p>Have a look at your cluster status:</p>

<pre><code>$ sudo ceph -s
    cluster 8d704c8a-ac19-4454-a89f-89a5d5b7d94d
     health HEALTH_OK
     monmap e1: 3 mons at {ceph-node1=10.0.8.3:6789/0,ceph-node2=10.0.8.4:6789/0,ceph-node3=10.0.8.5:6789/0}
            election epoch 10, quorum 0,1,2 ceph-node2,ceph-node3,ceph-node1
     osdmap e14: 3 osds: 3 up, 3 in
            flags sortbitwise,require_jewel_osds
      pgmap v29: 64 pgs, 1 pools, 0 bytes data, 0 objects
            100 MB used, 18298 MB / 18398 MB avail
                  64 active+clean
</code></pre>

<p>Everything looks good. Also change the permissions on this file, on all the nodes in order to execute the ceph, rados commands:</p>

<pre><code>$ sudo chmod +r /etc/ceph/ceph.client.admin.keyring
</code></pre>

<h2>Storage Pools:</h2>

<p>List your pool in your Ceph Cluster:</p>

<pre><code>$ rados lspools
rbd
</code></pre>

<p>Let&rsquo;s create a new storage pool called <code>mypool</code>:</p>

<pre><code>$ ceph osd pool create mypool 32 32 
pool 'mypool' created
</code></pre>

<p>Let&rsquo;s the list the storage pools again:</p>

<pre><code>$ rados lspools 
rbd 
mypool
</code></pre>

<p>You can also use the ceph command to list the pools:</p>

<pre><code>$ ceph osd pool ls 
rbd 
mypool
</code></pre>

<p>Create a Block Device Image:</p>

<pre><code>$ rbd create --size 1024 mypool/disk1 --image-feature layering
</code></pre>

<p>List the Block Device Images under your Pool:</p>

<pre><code>$ rbd list mypool
disk1
</code></pre>

<p>Retrieve information from your image:</p>

<pre><code>$ rbd info mypool/disk1
rbd image 'disk1':
        size 1024 MB in 256 objects
        order 22 (4096 kB objects)
        block_name_prefix: rbd_data.1021643c9869
        format: 2
        features: layering
        flags:
        create_timestamp: Thu Jun  7 23:48:23 2018
</code></pre>

<p>Create a local mapping of the image to a block device:</p>

<pre><code>$ sudo rbd map mypool/disk1
/dev/rbd0
</code></pre>

<p>Now we have a block device available at <code>/dev/rbd0</code>. Go ahead and mount it to <code>/mnt</code>:</p>

<pre><code>$ sudo mount /dev/rbd0 /mnt
</code></pre>

<p>We can then see it when we list our mounted disk partitions:</p>

<pre><code>$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda1        19G   13G  5.2G  72% /
/dev/rbd0       976M  1.3M  908M   1% /mnt
</code></pre>

<p>We can also resize the disk on the fly, let&rsquo;s resize it from 1GB to 2GB:</p>

<pre><code>$ rbd resize mypool/disk1 --size 2048
Resizing image: 100% complete...done.
</code></pre>

<p>To grow the space we can use resize2fs for ext4 partitions and xfs_growfs for xfs partitions:</p>

<pre><code>$ sudo resize2fs /dev/rbd0 
resize2fs 1.42.13 (17-May-2015)
Filesystem at /dev/rbd0 is mounted on /mnt; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 1
The filesystem on /dev/rbd0 is now 524288 (4k) blocks long.
</code></pre>

<p>When we look at our mounted partitions, you will notice that the size of our mounted partition has been increased in size:</p>

<pre><code>$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda1        19G   13G  5.2G   72% /
/dev/rbd0       2.0G  1.5M  1.9G   1% /mnt
</code></pre>

<h2>Object Storage RadosGW</h2>

<p>Let&rsquo;s create a new pool where we will store our objects:</p>

<pre><code>$ ceph osd pool create object-pool 32 32
pool 'object-pool' created
</code></pre>

<p>We will now create a local file, push the file to our object storage service, then delete our local file, download the file as a file with a different name, and read the contents:</p>

<p>Create the local file:</p>

<pre><code>$ echo "ok" &gt; test.txt
</code></pre>

<p>Push the local file to our pool in our object storage:</p>

<pre><code>$ rados put objects/data/test.txt ./test.txt --pool object-pool
</code></pre>

<p>List the pool (note that this can be executed from any node):</p>

<pre><code>$ $ rados ls --pool object-pool
objects/data/test.txt
</code></pre>

<p>Delete the local file, download the file from our object storage and read the contents:</p>

<pre><code>$ rm -rf test.txt 

$ rados get objects/data/test.txt ./newfile.txt --pool object-pool

$ cat ./newfile.txt 
ok
</code></pre>

<p>View the disk space from our storage-pool:</p>

<pre><code>$ rados df --pool object-pool
pool name                 KB      objects       clones     degraded      unfound           rd        rd KB           wr        wr KB
object-pool                1            1            0            0            0            0            0            1            1
  total used          261144           37
  total avail       18579372
  total space       18840516
</code></pre>

<h2>Resources:</h2>

<ul>
<li><a href="https://stackoverflow.com/questions/39589696/ceph-too-many-pgs-per-osd-all-you-need-to-know">https://stackoverflow.com/questions/39589696/ceph-too-many-pgs-per-osd-all-you-need-to-know</a></li>
<li><a href="https://github.com/lucj/swarm-rexray-ceph">https://github.com/lucj/swarm-rexray-ceph</a></li>
<li><a href="http://docs.ceph.com/docs/mimic/rbd/">http://docs.ceph.com/docs/mimic/rbd/</a></li>
<li><a href="http://docs.ceph.com/docs/mimic/radosgw/">http://docs.ceph.com/docs/mimic/radosgw/</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Create a Logical Volume Using LVM on Ubuntu]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/03/30/create-a-logical-volume-using-lvm-on-ubuntu/"/>
    <updated>2018-03-30T20:38:18-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/03/30/create-a-logical-volume-using-lvm-on-ubuntu</id>
    <content type="html"><![CDATA[<p>Logical Volume Manager (LVM) - adds an extra layer between the physical disks and the file system, which allows you to resize your storage on the fly, use multiple disks, instead of one, etc.</p>

<h2>Concepts:</h2>

<p>Physical Volume:
- Physical Volume represents the actual disk / block device.</p>

<p>Volume Group:
- Volume Groups combines the collection of Logical Volumes and Physical Volumes into one administrative unit.</p>

<p>Logical Volume:
- A Logical Volume is the conceptual equivalent of a disk partition in a non-LVM system.</p>

<p>File Systems:
- File systems are built on top of logical volumes.</p>

<h2>What we are doing today:</h2>

<p>We have a disk installed on our server which is 150GB that is located on <code>/dev/vdb</code>, which we will manage via LVM and will be mounted under <code>/mnt</code></p>

<h2>Dependencies:</h2>

<p>Update and Install LVM:</p>

<pre><code>$ apt update &amp;&amp; apt upgrade -y
$ apt install lvm2 -y
$ systemctl enable lvm2-lvmetad
$ systemctl start lvm2-lvmetad
</code></pre>

<h2>Create the Logical Volume:</h2>

<p>Initialize the Physical Volume to be managed by LVM, then create the Volume Group, then go ahead to create the Logical Volume:</p>

<pre><code>$ pvcreate /dev/vdb
$ vgcreate vg1 /dev/vdb
$ lvcreate -l 100%FREE -n vol1 vg1
</code></pre>

<p>Build the Linux Filesystem with ext4 and mount the volume to the <code>/mnt</code> partition:</p>

<pre><code>$ mkfs.ext4 /dev/vg1/vol1
$ mount /dev/vg1/vol1 /mnt
$ echo '/dev/mapper/vg1-vol1 /mnt ext4 defaults,nofail 0 0' &gt;&gt; /etc/fstab
</code></pre>

<h2>Other useful commands:</h2>

<p>To list Physical Volume Info:</p>

<pre><code>$ pvs
PV         VG   Fmt  Attr PSize   PFree
/dev/vdb   vg1  lvm2 a--  139.70g    0
</code></pre>

<p>To list Volume Group Info:</p>

<pre><code>$ vgs
VG   #PV #LV #SN Attr   VSize   VFree
vg1    1   1   0 wz--n- 139.70g    0
</code></pre>

<p>And viewing the logical volume size from the volume group:</p>

<pre><code>$ vgs -o +lv_size,lv_name
VG   #PV #LV #SN Attr   VSize   VFree LSize   LV
vg1    1   1   0 wz--n- 139.70g    0  139.70g vol1
</code></pre>

<p>Information about Logical Volumes:</p>

<pre><code>$ lvs
LV   VG   Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
vol1 vg1  -wi-ao---- 139.70g
</code></pre>

<h2>Resources:</h2>

<ul>
<li><a href="https://www.thegeekdiary.com/redhat-centos-a-beginners-guide-to-lvm-logical-volume-manager/">1</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Expanding the Size of Your EBS Volume on AWS EC2 for Linux]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/03/28/expanding-the-size-of-your-ebs-volume-on-aws-ec2-for-linux/"/>
    <updated>2018-03-28T01:45:07-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/03/28/expanding-the-size-of-your-ebs-volume-on-aws-ec2-for-linux</id>
    <content type="html"><![CDATA[<p><img src="https://i.snag.gy/BJLbwQ.jpg" alt="" /></p>

<p>Resizing your EBS Volume on the fly, that is attached to your EC2 Linux instance, on Amazon Web Services.</p>

<p>We want to resize our EBS Volume from 100GB to 1000GB and at the moment my EBS Volume is 100GB, as you can see:</p>

<pre><code class="bash">$ df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        7.9G   60K  7.9G   1% /dev
tmpfs           7.9G     0  7.9G   0% /dev/shm
/dev/xvda1       99G   32G   67G  32% /
</code></pre>

<p>Now we want to resize the volume to 1000GB, without shutting down our EC2 instance.</p>

<p>Go to your EC2 Management Console, Select your EC2 Instance, scroll down to the EBS Volume, click on it and click the EBS Volume ID, from there select Actions, modify and resize the disk to the needed size. As you can see the disk is now 1000GB:</p>

<pre><code class="bash">$ lsblk
NAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
xvda    202:0    0 1000G  0 disk
xvda1 202:1    0 1000G  0 part /
</code></pre>

<p>But our partition is still 100GB:</p>

<pre><code class="bash">$ df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        7.9G   60K  7.9G   1% /dev
tmpfs           7.9G     0  7.9G   0% /dev/shm
/dev/xvda1       99G   32G   67G  32% /
</code></pre>

<p>We need to use <code>growpart</code> and <code>resize2fs</code> to resize our partition:</p>

<pre><code class="bash">$ sudo growpart /dev/xvda 1
CHANGED: disk=/dev/xvda partition=1: start=4096 old: size=209711070,end=209715166 new: size=2097147870,end=2097151966

$ sudo resize2fs /dev/xvda1
resize2fs 1.42.12 (29-Aug-2014)
Filesystem at /dev/xvda1 is mounted on /; on-line resizing required
old_desc_blocks = 7, new_desc_blocks = 63
The filesystem on /dev/xvda1 is now 262143483 (4k) blocks long.
</code></pre>

<p>Now we will have a resized partition to 100GB:</p>

<pre><code class="bash">$ df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        7.9G   60K  7.9G   1% /dev
tmpfs           7.9G     0  7.9G   0% /dev/shm
/dev/xvda1      985G   33G  952G   4% /
</code></pre>

<p>Resources:</p>

<ul>
<li><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/recognize-expanded-volume-linux.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/recognize-expanded-volume-linux.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Guide to Setup Docker Convoy Volume Driver for Docker Swarm With NFS]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/02/16/guide-to-setup-docker-convoy-volume-driver-for-docker-swarm-with-nfs/"/>
    <updated>2018-02-16T08:51:59-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/02/16/guide-to-setup-docker-convoy-volume-driver-for-docker-swarm-with-nfs</id>
    <content type="html"><![CDATA[<p><img src="http://obj-cache.cloud.ruanbekker.com/docker-logo.png" alt="" /></p>

<p>In this post we will setup <a href="https://github.com/rancher/convoy">Rancher&rsquo;s Convoy Storage Plugin</a> with NFS, to provide data persistence in Docker Swarm.</p>

<h2>The Overview:</h2>

<p>This essentially means that we will have a NFS Volume, when the service gets created on Docker Swarm, the cluster creates these volumes with path mapping, so when a container gets spawned, restarted, scaled etc, the container that gets started on the new node will be aware of the volume, and will get the data that its expecting.</p>

<p>Its also good to note that our NFS Server will be a single point of failure, therefore its also good to look at a Distributed Volume like <a href="https://sysadmins.co.za/tag/glusterfs">GlusterFS</a>, <a href="https://sysadmins.co.za/tag/xtreemfs/">XtreemFS</a>, <a href="">Ceph</a>, etc.</p>

<ul>
<li>NFS Server (10.8.133.83)</li>
<li>Rancher Convoy Plugin on Each Docker Node in the Swarm (10.8.133.83, 10.8.166.19, 10.8.142.195)</li>
</ul>


<h2>Setup NFS:</h2>

<p>Setup the NFS Server</p>

<p><em>Update:</em></p>

<p>In order for the containers to be able to change permissions, you need to set <code>(rw,async,no_subtree_check,no_wdelay,crossmnt,insecure,all_squash,insecure_locks,sec=sys,anonuid=0,anongid=0)</code></p>

<ul>
<li><a href="https://github.com/rancher/rancher/issues/6452">https://github.com/rancher/rancher/issues/6452</a></li>
</ul>


<pre><code class="bash">$ sudo apt-get install nfs-kernel-server nfs-common -y
$ mkdir /vol
$ chown -R nobody:nogroup /vol
$ echo '/vol 10.8.133.83(rw,sync,no_subtree_check) 10.8.166.19(rw,sync,no_subtree_check) 10.8.142.195(rw,sync,no_subtree_check)' &gt;&gt; /etc/exports
$ sudo systemctl restart nfs-kernel-server
$ sudo systemctl enable nfs-kernel-server
</code></pre>

<p>Setup the NFS Clients on each Docker Node:</p>

<pre><code class="bash">$ sudo apt-get install nfs-common -y
$ mount 10.8.133.83:/vol /mnt
$ umount /mnt
$ df -h
</code></pre>

<p>If you can see tht the volume is mounted, unmount it and add it to the <code>fstab</code> so the volume can be mounted on boot:</p>

<pre><code class="bash">$ sudo bash -c "echo '10.8.133.83:/vol /mnt nfs auto,nofail,noatime,nolock,intr,tcp,actimeo=1800 0 0' &gt;&gt; /etc/fstab"
$ sudo mount -a
</code></pre>

<h2>Install Rancher Convoy Plugin:</h2>

<p>The Plugin needs to be installed on each docker node that will be part of the swarm:</p>

<pre><code class="bash">$ cd /tmp
$ wget https://github.com/rancher/convoy/releases/download/v0.5.0/convoy.tar.gz
$ tar xzf convoy.tar.gz
$ sudo cp convoy/convoy convoy/convoy-pdata_tools /usr/local/bin/
$ sudo mkdir -p /etc/docker/plugins/
$ sudo bash -c 'echo "unix:///var/run/convoy/convoy.sock" &gt; /etc/docker/plugins/convoy.spec'
</code></pre>

<h2>Create the init script:</h2>

<p>Thanks to <a href="https://gist.github.com/deviantony/557984d62e867e6f505577b207db6ffc">deviantony</a></p>

<pre><code class="bash">#!/bin/sh
### BEGIN INIT INFO
# Provides:
# Required-Start:    $remote_fs $syslog
# Required-Stop:     $remote_fs $syslog
# Default-Start:     2 3 4 5
# Default-Stop:      0 1 6
# Short-Description: Start daemon at boot time
# Description:       Enable service provided by daemon.
### END INIT INFO

dir="/usr/local/bin"
cmd="convoy daemon --drivers vfs --driver-opts vfs.path=/mnt/docker/volumes"
user="root"
name="convoy"

pid_file="/var/run/$name.pid"
stdout_log="/var/log/$name.log"
stderr_log="/var/log/$name.err"

get_pid() {
    cat "$pid_file"
}

is_running() {
    [ -f "$pid_file" ] &amp;&amp; ps `get_pid` &gt; /dev/null 2&gt;&amp;1
}

case "$1" in
    start)
    if is_running; then
        echo "Already started"
    else
        echo "Starting $name"
        cd "$dir"
        if [ -z "$user" ]; then
            sudo $cmd &gt;&gt; "$stdout_log" 2&gt;&gt; "$stderr_log" &amp;
        else
            sudo -u "$user" $cmd &gt;&gt; "$stdout_log" 2&gt;&gt; "$stderr_log" &amp;
        fi
        echo $! &gt; "$pid_file"
        if ! is_running; then
            echo "Unable to start, see $stdout_log and $stderr_log"
            exit 1
        fi
    fi
    ;;
    stop)
    if is_running; then
        echo -n "Stopping $name.."
        kill `get_pid`
        for i in {1..10}
        do
            if ! is_running; then
                break
            fi

            echo -n "."
            sleep 1
        done
        echo

        if is_running; then
            echo "Not stopped; may still be shutting down or shutdown may have failed"
            exit 1
        else
            echo "Stopped"
            if [ -f "$pid_file" ]; then
                rm "$pid_file"
            fi
        fi
    else
        echo "Not running"
    fi
    ;;
    restart)
    $0 stop
    if is_running; then
        echo "Unable to stop, will not attempt to start"
        exit 1
    fi
    $0 start
    ;;
    status)
    if is_running; then
        echo "Running"
    else
        echo "Stopped"
        exit 1
    fi
    ;;
    *)
    echo "Usage: $0 {start|stop|restart|status}"
    exit 1
    ;;
esac

exit 0
</code></pre>

<p>Make the script executable:</p>

<pre><code class="bash">$ chmod +x /etc/init.d/convoy
</code></pre>

<p>Enable the service on boot:</p>

<pre><code class="bash">$ sudo systemctl enable convoy
</code></pre>

<p>Start the service:</p>

<pre><code class="bash">$ sudo /etc/init.d/convoy start
</code></pre>

<p>This should be done on all the nodes.</p>

<h2>Externally Managed Convoy Volumes</h2>

<p>One thing to note is that, after your delete a volume, you will still need to delete the directory from the path where its hosted, as the application does not do that by itself.</p>

<p>Creating the Volume Before hand:</p>

<pre><code class="bash">$ convoy create test1
test1

$ docker volume ls
DRIVER              VOLUME NAME
convoy              test1

$ cat /mnt/docker/volumes/config/vfs_volume_test1.json
{"Name":"test1","Size":0,"Path":"/mnt/docker/volumes/test1","MountPoint":"","PrepareForVM":false,"CreatedTime":"Mon Feb 05 13:07:05 +0000 2018","Snapshots":{}}
</code></pre>

<p>Viewing the volume from another node:</p>

<pre><code class="bash">$ docker volume ls
DRIVER              VOLUME NAME
convoy              test1
</code></pre>

<h2>Creating a Test Service:</h2>

<p>Create a test service to test the data persistence, our docker-compose.yml:</p>

<pre><code class="bash">version: '3.4'

volumes:
  test1:
    external: true

networks:
  appnet:
    external: true

services:
  test:
    image: alpine:edge
    command: sh -c "ping 127.0.0.1"
    volumes:
      - test1:/data
    networks:
      - appnet
</code></pre>

<p>Creating the Overlay Network and Deploying the Stack:</p>

<pre><code class="bash">$ docker network create -d overlay appnet
$ docker stack deploy -c docker-compose.yml apps
Creating service apps_test
</code></pre>

<p>Write data to the volume in the container:</p>

<pre><code>$ docker exec -it apps_test.1.iojo7fpw8jirqjs3iu8qr7qpe sh
/ # echo "ok" &gt; /data/file.txt
/ # cat /data/file.txt
ok
</code></pre>

<p>Scale the service:</p>

<pre><code class="bash">$ docker service scale apps_test=2
apps_test scaled to 2
</code></pre>

<p>Inspect to see if the new replica is on another node:</p>

<pre><code class="bash">$ docker service ps apps_test
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE               ERROR                         PORTS
myrq2pc3z26z        apps_test.1         alpine:edge         scw-docker-1        Running             Running 45 seconds ago
ny8t97l2q00c         \_ apps_test.1     alpine:edge         scw-docker-1        Shutdown            Failed 51 seconds ago       "task: non-zero exit (137)"
iojo7fpw8jir         \_ apps_test.1     alpine:edge         scw-docker-1        Shutdown            Failed about a minute ago   "task: non-zero exit (137)"
tt0nuusvgeki        apps_test.2         alpine:edge         scw-docker-2        Running             Running 15 seconds ago
</code></pre>

<p>Logon to the new container and test if the data is persisted:</p>

<pre><code class="bash">$ docker exec -it apps_test.2.tt0nuusvgekirw1c5myu720ga sh
/ # cat /data/file.txt
ok
</code></pre>

<p>Delete the Stack and Redeploy and have a look at the data we created earlier, and you will notice the data is persisted:</p>

<pre><code class="bash">$ docker stack rm apps
$ docker stack deploy -c docker-compose.yml apps
$ docker exec -it apps_test.1.la4w2sbuu8cmv6xamwxl7n0ip cat /data/file.txt
ok
$ docker stack rm apps
</code></pre>

<h2>Create Volume via Compose:</h2>

<p>You can also create the volume on service/stack creation level, so you dont need to create the volume before hand, the compose file:</p>

<pre><code class="yml">version: '3.4'

volumes:
  test2:
    driver: convoy
    driver_opts:
      size: 10

networks:
  appnet:
    external: true

services:
  test:
    image: alpine:edge
    command: sh -c "ping 127.0.0.1"
    volumes:
      - test2:/data
    networks:
      - appnet
</code></pre>

<p>Deploy the Stack:</p>

<pre><code class="bash">$ docker stack deploy -c docker-compose-new.yml apps
Creating service apps_test
</code></pre>

<p>List the volumes and you will notice that the volume was created:</p>

<pre><code class="bash">$ docker volume ls
DRIVER              VOLUME NAME
convoy              apps_test2
convoy              test1
</code></pre>

<p>Lets inspect the volume, to see more details about it:</p>

<pre><code class="bash">docker volume inspect apps_test2
[
    {
        "CreatedAt": "0001-01-01T00:00:00Z",
        "Driver": "convoy",
        "Labels": {
            "com.docker.stack.namespace": "apps"
        },
        "Mountpoint": "/mnt/docker/volumes/apps_test2",
        "Name": "apps_test2",
        "Options": {
            "size": "10"
        },
        "Scope": "local"
    }
]
</code></pre>

<p>As mentioned earlier, if you delete the volume, you need to delete the data directories as well</p>

<pre><code class="bash">$ docker volume rm test1
test1

$ ls /mnt/docker/volumes/
apps_test2  config  test1

$ rm -rf /mnt/docker/volumes/test1
</code></pre>

<p>More info about the project:
- <a href="https://github.com/rancher/convoy">https://github.com/rancher/convoy</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a NFS Server on Ubuntu]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/02/11/setup-a-nfs-server-on-ubuntu/"/>
    <updated>2018-02-11T17:26:56-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/02/11/setup-a-nfs-server-on-ubuntu</id>
    <content type="html"><![CDATA[<p><img src="https://i.snag.gy/3sUALo.jpg" alt="" /></p>

<p>Quick post on how to setup a NFS Server on Ubuntu and how to setup the client to interact with the NFS Server.</p>

<h2>Setup the Dependencies:</h2>

<pre><code class="bash">$ apt update &amp;&amp; sudo apt upgrade -y
$ sudo apt-get install nfs-kernel-server nfs-common -y
</code></pre>

<p>Create the Directory for NFS and set permissions:</p>

<pre><code class="bash">mkdir /vol
chown -R nobody:nogroup /vol
</code></pre>

<h2>Allow the Clients:</h2>

<p>We need to set in the <code>exports</code> file, the clients we would like to allow:</p>

<ul>
<li><code>rw</code>: Allows Client R/W Access to the Volume.</li>
<li><code>sync</code>: This option forces NFS to write changes to disk before replying. More stable and Consistent. Note, it does reduce the speed of file operations.</li>
<li><code>no_subtree_check</code>: This prevents subtree checking, which is a process where the host must check whether the file is actually still available in the exported tree for every request. This can cause many problems when a file is renamed while the client has it opened. In almost all cases, it is better to disable subtree checking.</li>
</ul>


<pre><code class="bash">$ echo '/vol 10.8.133.83(rw,sync,no_subtree_check) 10.8.166.19(rw,sync,no_subtree_check) 10.8.142.195(rw,sync,no_subtree_check)' &gt;&gt; /etc/exports
</code></pre>

<h2>Start the NFS Server:</h2>

<p>Restart the service and enable the service on boot:</p>

<pre><code class="bash">$ sudo systemctl restart nfs-kernel-server
$ sudo systemctl enable nfs-kernel-server
</code></pre>

<h2>Client Side:</h2>

<p>We will mount the NFS Volume to our Clients <code>/mnt</code> partition.</p>

<p>Install the dependencies:</p>

<pre><code class="bash">$ sudo apt-get install nfs-common -y
</code></pre>

<p>Test if we can mount the volume, then unmount it, as we will set the config in our <code>fstab</code>:</p>

<pre><code class="bash">$ sudo mount 10.8.133.83:/vol /mnt
$ sudo umount /mnt
$ df -h
</code></pre>

<p>Set the config in your <code>fstab</code>, then mount it from there:</p>

<pre><code class="bash">$ sudo bash -c "echo '10.8.133.83:/vol /mnt nfs auto,nofail,noatime,nolock,intr,tcp,actimeo=1800 0 0' &gt;&gt; /etc/fstab"
$ sudo mount -a
$ df -h
</code></pre>

<p>Now you shoule be able to write to your NFS Volume from your client.</p>

<p>Sources:
- <a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-16-04">1</a> <a href="https://gist.github.com/deviantony/557984d62e867e6f505577b207db6ffc%">2</a></p>
]]></content>
  </entry>
  
</feed>
