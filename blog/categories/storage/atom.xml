<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Storage | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/storage/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2020-04-08T19:30:10+02:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Persistent Volumes With K3d Kubernetes]]></title>
    <link href="http://blog.ruanbekker.com/blog/2020/02/21/persistent-volumes-with-k3d-kubernetes/"/>
    <updated>2020-02-21T00:07:48+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2020/02/21/persistent-volumes-with-k3d-kubernetes</id>
    <content type="html"><![CDATA[<p>With k3d we can mount the host to container path, and with persistent volumes we can set a hostPath for our persistent volumes. With k3d, all the nodes will be using the same volume mapping which maps back to the host.</p>

<p>We will test the data persistence by writing a file inside a container, kill the pod, then exec into the pod again and test if the data persisted</p>

<h2>The k3d Cluster</h2>

<p>Create the directory on the host where we will persist the data:</p>

<pre><code>&gt; mkdir -p /tmp/k3dvol
</code></pre>

<p>Create the cluster:</p>

<pre><code>&gt; k3d create --name "k3d-cluster" --volume /tmp/k3dvol:/tmp/k3dvol --publish "80:80" --workers 2
&gt; export KUBECONFIG="$(k3d get-kubeconfig --name='k3d-cluster')"
</code></pre>

<p>Our application will be a busybox container which will keep running with a ping command, map the persistent volume to <code>/data</code> inside the pod.</p>

<p>Our <code>app.yml</code></p>

<pre><code>apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/tmp/k3dvol"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: echo
spec:
  selector:
    matchLabels:
      app: echo
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: echo
    spec:
      volumes:
        - name: task-pv-storage
          persistentVolumeClaim:
            claimName: task-pv-claim
      containers:
      - image: busybox
        name: echo
        volumeMounts:
          - mountPath: "/data"
            name: task-pv-storage
        command: ["ping", "127.0.0.1"]
</code></pre>

<p>Deploy the workload:</p>

<pre><code>&gt; kubectl apply -f app.yml
persistentvolume/task-pv-volume created
persistentvolumeclaim/task-pv-claim created
deployment.apps/echo created
</code></pre>

<p>View the persistent volumes:</p>

<pre><code>&gt; kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS   REASON   AGE
task-pv-volume                             1Gi        RWO            Retain           Bound    default/task-pv-claim    manual                  6s
</code></pre>

<p>View the Persistent Volume Claims:</p>

<pre><code>&gt; kubectl get pvc
NAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
task-pv-claim    Bound    task-pv-volume                             1Gi        RWO            manual         11s
</code></pre>

<p>View the pods:</p>

<pre><code>&gt; kubectl get pods
NAME                   READY   STATUS    RESTARTS   AGE
echo-58fd7d9b6-x4rxj   1/1     Running   0          16s
</code></pre>

<p>Exec into the pod:</p>

<pre><code>&gt; kubectl exec -it echo-58fd7d9b6-x4rxj sh
/ # df -h
Filesystem                Size      Used Available Use% Mounted on
overlay                  58.4G     36.1G     19.3G  65% /
osxfs                   233.6G    139.7G     86.3G  62% /data
/dev/sda1                58.4G     36.1G     19.3G  65% /etc/hosts
/dev/sda1                58.4G     36.1G     19.3G  65% /dev/termination-log
/dev/sda1                58.4G     36.1G     19.3G  65% /etc/hostname
/dev/sda1                58.4G     36.1G     19.3G  65% /etc/resolv.conf
</code></pre>

<p>Write the hostname of the current pod to the persistent volume path:</p>

<pre><code>/ # echo $(hostname)
echo-58fd7d9b6-x4rxj
/ # echo $(hostname) &gt; /data/hostname.txt
/ # exit
</code></pre>

<p>Exit the pod and read the content from the host (workstation/laptop):</p>

<pre><code>&gt; cat /tmp/k3dvol/hostname.txt
echo-58fd7d9b6-x4rxj
</code></pre>

<p>Look at the host where the pod is running on:</p>

<pre><code>&gt; kubectl get nodes -o wide
NAME                       STATUS   ROLES    AGE   VERSION        INTERNAL-IP    EXTERNAL-IP   OS-IMAGE   KERNEL-VERSION     CONTAINER-RUNTIME
k3d-k3d-cluster-server     Ready    master   13m   v1.17.2+k3s1   192.168.32.2   &lt;none&gt;        Unknown    4.9.184-linuxkit   containerd://1.3.3-k3s1
k3d-k3d-cluster-worker-1   Ready    &lt;none&gt;   13m   v1.17.2+k3s1   192.168.32.4   &lt;none&gt;        Unknown    4.9.184-linuxkit   containerd://1.3.3-k3s1
k3d-k3d-cluster-worker-0   Ready    &lt;none&gt;   13m   v1.17.2+k3s1   192.168.32.3   &lt;none&gt;        Unknown    4.9.184-linuxkit   containerd://1.3.3-k3s1
</code></pre>

<p>Delete the pod:</p>

<pre><code>&gt; kubectl delete pod/echo-58fd7d9b6-x4rxj
pod "echo-58fd7d9b6-x4rxj" deleted
</code></pre>

<p>Wait until the pod is rescheduled again and verify if the pod is running on a different node:</p>

<pre><code>&gt; kubectl get pods -o wide
NAME                   READY   STATUS    RESTARTS   AGE   IP          NODE                       NOMINATED NODE   READINESS GATES
echo-58fd7d9b6-fkvbs   1/1     Running   0          35s   10.42.2.9   k3d-k3d-cluster-worker-1   &lt;none&gt;           &lt;none&gt;
</code></pre>

<p>Exec into the new pod:</p>

<pre><code>&gt; kubectl exec -it echo-58fd7d9b6-fkvbs sh
</code></pre>

<p>View if the data is persisted:</p>

<pre><code>/ # hostname
echo-58fd7d9b6-fkvbs

/ # cat /data/hostname.txt
echo-58fd7d9b6-x4rxj
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Expire Objects in AWS S3 Automatically After 30 Days]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/09/12/expire-objects-in-aws-s3-automatically-after-30-days/"/>
    <updated>2019-09-12T22:37:11+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/09/12/expire-objects-in-aws-s3-automatically-after-30-days</id>
    <content type="html"><![CDATA[<p><img src="https://objects.ruanbekker.com/assets/images/aws-logo.png" alt="" /></p>

<p>In AWS S3 you can make use of lifecycle policies to manage the lifetime of your objects stored in S3.</p>

<p>In this tutorial, I will show you how to delete objects automatically from S3 after 30 days.</p>

<h2>Navigate to your Bucket</h2>

<p>Head over to your AWS S3 bucket where you want to delete objects after they have been stored for 30 days:</p>

<p><img width="1039" alt="0400F9CB-9223-4FDF-8FA5-D0BC1FA8EB71" src="https://user-images.githubusercontent.com/567298/64819546-c3f2b600-d5ae-11e9-93ba-13777e9b02b0.png"></p>

<h2>Lifecycle Policies</h2>

<p>Select &ldquo;Management&rdquo; and click on &ldquo;Add lifecycle rule&rdquo;:</p>

<p><img width="701" alt="9BB26C7C-F251-45C4-AE44-A34459BD0F4B" src="https://user-images.githubusercontent.com/567298/64819628-f00e3700-d5ae-11e9-9740-8aa3608163a7.png"></p>

<p>Set a rule name of choice and you have the option to provide a prefix if you want to delete objects based on a specific prefix. I will leave this blank as I want to delete objects in the root level of the bucket. Head to next on the following section:</p>

<p><img width="700" alt="AEF8B151-3FA8-454F-AC71-778A531BD1EE" src="https://user-images.githubusercontent.com/567298/64819785-58f5af00-d5af-11e9-8485-fb0dca3a02ac.png"></p>

<p>From the &ldquo;Transitions&rdquo; section, configure the transition section, by selecting to expire the current version of the object after 30 days:</p>

<p><img width="701" alt="2B395671-A4C0-4E5A-82E7-00EE6579DB5A" src="https://user-images.githubusercontent.com/567298/64819851-7c205e80-d5af-11e9-98d7-7e1dd09bcfef.png"></p>

<p>Review the configuration:</p>

<p><img width="705" alt="F7F8E800-62FF-4156-B506-5FB9BCC148E0" src="https://user-images.githubusercontent.com/567298/64819869-893d4d80-d5af-11e9-8034-8a2e3a8939f8.png"></p>

<p>When you select &ldquo;Save&rdquo;, you should be returned to the following section:</p>

<p><img width="1041" alt="8421EBCE-9503-4259-92AA-DB66C6F532AF" src="https://user-images.githubusercontent.com/567298/64819895-99edc380-d5af-11e9-84b4-7f4cc69cfd2e.png"></p>

<h2>Housecleaning on your S3 Bucket</h2>

<p>Now 30 days after you created objects on AWS S3, they will be deleted.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Persist Vault Data With Amazon S3 as a Storage Backend]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/05/07/persist-vault-data-with-amazon-s3-as-a-storage-backend/"/>
    <updated>2019-05-07T22:01:45+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/05/07/persist-vault-data-with-amazon-s3-as-a-storage-backend</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/57256060-f1a27e00-7055-11e9-9a05-77d3fdd6c76f.png" alt="" /></p>

<p>In a previous post we have set up <a href="https://blog.ruanbekker.com/blog/2019/05/06/setup-hashicorp-vault-server-on-docker-and-cli-guide/">the vault server on docker</a>, but using a file backend to persist our data.</p>

<p>In this tutorial we will configure vault to use <a href="https://www.vaultproject.io/docs/configuration/storage/s3.html">amazon s3 as a storage backend</a> to persist our data for vault.</p>

<h2>Provision S3 Bucket</h2>

<p>Create the S3 Bucket where our data will reside:</p>

<pre><code>$ aws s3 mb --region=eu-west-1 s3://somename-vault-backend
</code></pre>

<h2>Vault Config</h2>

<p>Create the vault config, where we will provide details about our storage backend and configuration for the vault server:</p>

<pre><code>$ vim volumes/config/s3vault.json
</code></pre>

<p>Populate the config file with the following details, you will just need to provide your own credentials:</p>

<pre><code class="json">{
  "backend": {
    "s3": {
      "region": "eu-west-1",
      "access_key": "ACCESS_KEY",
      "secret_key": "SECRET_KEY",
      "bucket": "somename-vault-backend"
    }
  },
  "listener": {
    "tcp":{
      "address": "0.0.0.0:8200",
      "tls_disable": 1
    }
  },
  "ui": true
}
</code></pre>

<h2>Docker Compose</h2>

<p>As we are using docker to deploy our vault server, our docker-compose.yml:</p>

<pre><code>$ cat &gt; docker-compose.yml &lt;&lt; EOF
version: '2'
services:
  vault:
    image: vault
    container_name: vault
    ports:
      - "8200:8200"
    restart: always
    volumes:
      - ./volumes/logs:/vault/logs
      - ./volumes/file:/vault/file
      - ./volumes/config:/vault/config
    cap_add:
      - IPC_LOCK
    entrypoint: vault server -config=/vault/config/s3vault.json
EOF
</code></pre>

<p>Deploy the vault server:</p>

<pre><code>$ docker-compose up
</code></pre>

<p>Go ahead and create some secrets, then deploy the docker container on another host to test out the data persistence.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a 3 Node Replicated Storage Volume With GlusterFS]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/03/05/setup-a-3-node-replicated-storage-volume-with-glusterfs/"/>
    <updated>2019-03-05T21:01:37+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/03/05/setup-a-3-node-replicated-storage-volume-with-glusterfs</id>
    <content type="html"><![CDATA[<p><img src="https://access.redhat.com/documentation/en-US/Red_Hat_Storage/2.1/html/Administration_Guide/images/Replicated_Volume.png" alt="" /></p>

<p>In one of my earlier posts on <a href="https://sysadmins.co.za/tag/glusterfs">GlusterFS</a>, we went through the steps on how to setup a <a href="https://sysadmins.co.za/setup-a-distributed-storage-volume-with-glusterfs/">Distributed Storage Volume</a>, where the end result was to have scalable storage, where size was the requirement.</p>

<p><a href="https://bekkerclothing.com/collections/developer?utm_source=blog.ruanbekker.com&utm_medium=blog&utm_campaign=leaderboard_ad" target="_blank"><img alt="bekker-clothing-developer-tshirts" src="https://user-images.githubusercontent.com/567298/70170981-7c278a80-16d6-11ea-9759-6621d02c1423.png"></a></p>

<h2>What will we be doing today with GlusterFS?</h2>

<p>Today, we will be going through the steps on how to setup a Replicated Storage Volume with GlusterFS, where we will have 3 GlusterFS Nodes, and using the replication factor of 3.</p>

<p><strong>Replication Factor of 3:</strong></p>

<p>In other words, having 3 copies of our data and in our case, since we will have 3 nodes in our cluster, a copy of our data will reside on each node.</p>

<p><strong>What about Split-Brain:</strong></p>

<p>In Clustering, we get the term Split-Brain, where a node dies or leaves the cluster, the cluster reforms itself with the available nodes and then during this reformation, instead of the remaining nodes staying with the same cluster, 2 subset of cluster are created, and they are not aware of each other, which causes data corruption, here&rsquo;s a great resource on <a href="http://techthoughts.typepad.com/managing_computers/2007/10/split-brain-quo.html">Split-Brain</a></p>

<p>To prevent Split-Brain in GlusterFS, we can setup a <a href="https://gluster.readthedocs.io/en/latest/Administrator%20Guide/arbiter-volumes-and-quorum/">Arbiter Volume</a>. In a Replica Count of 3 and Arbiter count of 1: 2 Nodes will hold the replicated data, and the 1 Node which will be the Arbiter node, will only host the file/directory names and metadata but not any data. I will write up an <a href="">article</a> on this in the future.</p>

<h2>Getting Started:</h2>

<p>Let&rsquo;s get started on setting up a 3 Node Replicated GlusterFS. Each node will have an additional drive that is 50GB in size, which will be part of our GlusterFS Replicated Volume. I will also be using Ubuntu 16.04 as my linux distro.</p>

<p><strong>Preparing DNS Resolution:</strong></p>

<p>I will install GlusterFS on each node, and in my setup I have the following DNS entries:</p>

<ul>
<li>gfs01 (10.0.0.2)</li>
<li>gfs02 (10.0.0.3)</li>
<li>gfs03 (10.0.0.4)</li>
</ul>


<p><strong>Preparing our Secondary Drives:</strong></p>

<p>I will be formatting my drives with <code>XFS</code>. Listing our block volumes:</p>

<pre><code class="bash">$ lsblk
NAME MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vdb  253:16   0 46.6G  0 disk
vda  253:0    0 18.6G  0 disk /
</code></pre>

<p>Creating the FileSystem with XFS, which we will be running on each node:</p>

<pre><code class="bash">$ mkfs.xfs /dev/vdb
</code></pre>

<p>Then creating the directories where our bricks will reside, and also add an entry to our <code>/etc/fstab</code> so that our disk gets mounted when the operating system boots:</p>

<pre><code class="bash"># node: gfs01
$ mkdir /gluster/bricks/1 -p
$ echo '/dev/vdb /gluster/bricks/1 xfs defaults 0 0' &gt;&gt; /etc/fstab
$ mount -a
$ mkdir /gluster/bricks/1/brick

# node: gfs02
$ mkdir /gluster/bricks/2 -p
$ echo '/dev/vdb /gluster/bricks/2 xfs defaults 0 0' &gt;&gt; /etc/fstab
$ mount -a
$ mkdir /gluster/bricks/2/brick

# node: gfs03
$ mkdir /gluster/bricks/3 -p
$ echo '/dev/vdb /gluster/bricks/3 xfs defaults 0 0' &gt;&gt; /etc/fstab
$ mount -a
$ mkdir /gluster/bricks/3/brick
</code></pre>

<p>After this has been done, we should see that the disks are mounted, for example on node: <code>gfs01</code>:</p>

<pre><code>$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/vda         18G  909M   17G   3% /
/dev/vdb         47G   80M   47G   1% /gluster/bricks/1
</code></pre>

<h2>Installing GlusterFS on Each Node:</h2>

<p>Installing GlusterFS, repeat this on all 3 Nodes:</p>

<pre><code class="bash">$ apt update &amp;&amp; sudo apt upgrade -y
$ apt install xfsprogs attr glusterfs-server glusterfs-common glusterfs-client -y
$ systemctl enable glusterfs-server
</code></pre>

<p>In order to add the nodes to the trusted storage pool, we will have to add them by using <code>gluster peer probe</code>. Make sure that you can resolve the hostnames to the designated IP Addresses, and that traffic is allowed.</p>

<pre><code class="bash">$ gluster peer probe gfs01
$ gluster peer probe gfs02
$ gluster peer probe gfs03
</code></pre>

<p>Now that we have added our nodes to our trusted storage pool, lets verify that by listing our pool:</p>

<pre><code class="bash">$ gluster pool list
UUID                                    Hostname                State
f63d0e77-9602-4024-8945-5a7f7332bf89    gfs02                   Connected
2d4ac6c1-0611-4e2e-b4af-9e4aa8c1556d    gfs03                   Connected
6a604cd9-9a9c-406d-b1b7-69caf166a20e    localhost               Connected
</code></pre>

<p>Great! All looks good.</p>

<h2>Create the Replicated GlusterFS Volume:</h2>

<p>Let&rsquo;s create our Replicated GlusterFS Volume, named <code>gfs</code>:</p>

<pre><code class="bash">$ gluster volume create gfs \
  replica 3 \
  gfs01:/gluster/bricks/1/brick \
  gfs02:/gluster/bricks/2/brick \
  gfs03:/gluster/bricks/2/brick 

volume create: gfs: success: please start the volume to access data
</code></pre>

<p>Now that our volume is created, lets list it to verify that it is created:</p>

<pre><code class="bash">$ gluster volume list
gfs
</code></pre>

<p>Now, start the volume:</p>

<pre><code class="bash">$ gluster volume start gfs
volume start: gfs: success
</code></pre>

<p>View the status of our volume:</p>

<pre><code class="bash">$ gluster volume status gfs
Status of volume: gfs
Gluster process                             TCP Port  RDMA Port  Online  Pid
------------------------------------------------------------------------------
Brick gfs01:/gluster/bricks/1/brick         49152     0          Y       6450
Brick gfs02:/gluster/bricks/2/brick         49152     0          Y       3460
Brick gfs03:/gluster/bricks/3/brick         49152     0          Y       3309
</code></pre>

<p>Next, view the volume inforation:</p>

<pre><code class="bash">$ gluster volume info gfs

Volume Name: gfs
Type: Replicate
Volume ID: 6f827df4-6df5-4c25-99ee-8d1a055d30f0
Status: Started
Number of Bricks: 1 x 3 = 3
Transport-type: tcp
Bricks:
Brick1: gfs01:/gluster/bricks/1/brick
Brick2: gfs02:/gluster/bricks/2/brick
Brick3: gfs03:/gluster/bricks/3/brick
</code></pre>

<h2>Security:</h2>

<p>From a GlusterFS level, it will allow clients to connect by default. To authorize these 3 nodes to connect to the GlusterFS Volume:</p>

<pre><code class="bash">$ gluster volume set gfs auth.allow 10.0.0.2,10.0.0.3,10.0.0.4
</code></pre>

<p>Then if you would like to remove this rule:</p>

<pre><code class="bash">$ gluster volume set gfs auth.allow *
</code></pre>

<h2>Mount the GlusterFS Volume to the Host:</h2>

<p>Mount the GlusterFS Volume to each node, so we will have to mount it to each node, and also append it to our <code>/etc/fstab</code> file so that it mounts on boot:</p>

<pre><code class="bash">$ echo 'localhost:/gfs /mnt glusterfs defaults,_netdev,backupvolfile-server=localhost 0 0' &gt;&gt; /etc/fstab
$ mount.glusterfs localhost:/gfs /mnt
</code></pre>

<p><strong>Verify the Mounted Volume:</strong></p>

<p>Check the mounted disks, and you will find that the Replicated GlusterFS Volume is mounted on our <code>/mnt</code> partition.</p>

<pre><code class="bash">$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/vda         18G  909M   17G   3% /
/dev/vdb         47G   80M   47G   1% /gluster/bricks/1
localhost:/gfs   47G   80M   47G   1% /mnt
</code></pre>

<p>You will note that GlusterFS Volume has a total size of 47GB usable space, which is the same size as one of our disks, but that is because we have a replicated volume with a replication factor of 3:  <code>(47 * 3 / 3)</code></p>

<p>Now we have a Storage Volume which has 3 Replicas, one copy on each node, which allows us Data Durability on our Storage.</p>

<p><p></p>

<p><center><script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init(&lsquo;Buy Me a Coffee&rsquo;, &lsquo;#46b798&rsquo;, &lsquo;A6423ZIQ&rsquo;);kofiwidget2.draw();</script></center></p>

<p><p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Container Persistent Storage for Docker Swarm Using a GlusterFS Volume Plugin]]></title>
    <link href="http://blog.ruanbekker.com/blog/2019/03/05/container-persistent-storage-for-docker-swarm-using-a-glusterfs-volume-plugin/"/>
    <updated>2019-03-05T20:18:30+02:00</updated>
    <id>http://blog.ruanbekker.com/blog/2019/03/05/container-persistent-storage-for-docker-swarm-using-a-glusterfs-volume-plugin</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/53351889-85572000-392a-11e9-9720-464e9318206e.jpg" alt="" /></p>

<p>From one of my previous posts I demonstrated how to provide persistent storage for your containers by using a <a href="https://blog.ruanbekker.com/blog/2018/02/16/guide-to-setup-docker-convoy-volume-driver-for-docker-swarm-with-nfs/">Convoy NFS Plugin</a>.</p>

<p>I&rsquo;ve stumbled upon one AWESOME GlusterFS Volume Plugin for Docker by <a href="https://github.com/trajano/docker-volume-plugins/tree/master/glusterfs-volume-plugin">@trajano</a>, please have a look at his repository. I&rsquo;ve been waiting for some time for one solid glusterfs volume plugin, and it works great.</p>

<p><a href="https://bekkerclothing.com/collections/developer?utm_source=blog.ruanbekker.com&utm_medium=blog&utm_campaign=leaderboard_ad" target="_blank"><img alt="bekker-clothing-developer-tshirts" src="https://user-images.githubusercontent.com/567298/70170981-7c278a80-16d6-11ea-9759-6621d02c1423.png"></a></p>

<h2>What we will be doing today</h2>

<p>We will setup a 3 node replicated glusterfs volume and show how easy it is to install the volume plugin and then demonstrate how storage from our swarms containers are persisted.</p>

<p>Our servers that we will be using will have the private ip&rsquo;s as shown below:</p>

<pre><code>10.22.125.101
10.22.125.102
10.22.125.103
</code></pre>

<h2>Setup GlusterFS</h2>

<p>Have a look at <a href="https://blog.ruanbekker.com/blog/2019/03/05/setup-a-3-node-replicated-storage-volume-with-glusterfs/?referral=github.com">this</a> post to setup the glusterfs volume.</p>

<h2>Install the GlusterFS Volume Plugin</h2>

<p>Below I&rsquo;m installing the plugin and setting the alias name as <code>glusterfs</code>, granting all permissions and keeping the plugin in a disabled state.</p>

<pre><code class="bash">$ docker plugin install --alias glusterfs trajano/glusterfs-volume-plugin --grant-all-permissions --disable
</code></pre>

<p>Set the glusterfs servers:</p>

<pre><code>$ docker plugin set glusterfs SERVERS=10.22.125.101,10.22.125.102,10.22.125.103
</code></pre>

<p>Enable the glusterfs plugin:</p>

<pre><code>$ docker plugin enable glusterfs
</code></pre>

<h2>Create a Service in Docker Swarm</h2>

<p>Deploy a sample service on docker swarm with a volume backed by glusterfs. Note that my glusterfs volume is called <code>gfs</code></p>

<pre><code class="yaml">version: "3.4"

services:
  foo:
    image: alpine
    command: ping localhost
    networks:
      - net
    volumes:
      - vol1:/tmp

networks:
  net:
    driver: overlay

volumes:
  vol1:
    driver: glusterfs
    name: "gfs/vol1"
</code></pre>

<p>Deploy the stack:</p>

<pre><code class="bash">$ docker stack deploy -c docker-compose.yml test
Creating service test_foo
</code></pre>

<p>Have a look on which node is your container running:</p>

<pre><code class="bash">$ docker service ps test_foo
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
jfwzb7yxnrxx        test_foo.1          alpine:latest       swarm-worker-1      Running             Running 37 seconds ago
</code></pre>

<p>Now jump to the <code>swarm-worker-1</code> node and verify that the container is running on that node:</p>

<pre><code class="bash">$ docker ps
CONTAINER ID        IMAGE                                          COMMAND                  CREATED             STATUS                  PORTS               NAMES
d469f341d836        alpine:latest                                  "ping localhost"           59 seconds ago      Up 57 seconds                               test_foo.1.jfwzb7yxnrxxnd0qxtcjex8lu
</code></pre>

<p>Now since the container is running on this node, we will also see that the volume defined in our task configuration will also be present:</p>

<pre><code class="bash">$ docker volume ls
DRIVER                       VOLUME NAME
glusterfs:latest             gfs/vol1
</code></pre>

<p>Exec into the container and look at the disk layout:</p>

<pre><code class="bash">$ docker exec -it d469f341d836 sh
/ # df -h
Filesystem                Size      Used Available Use% Mounted on
overlay                  45.6G      3.2G     40.0G   7% /
10.22.125.101:gfs/vol1   45.6G      3.3G     40.0G   8% /tmp
</code></pre>

<p>While you are in the container, write the hostname&rsquo;s value into a file which is mapped to the glusterfs volume:</p>

<pre><code class="bash">$ echo $HOSTNAME &gt; /tmp/data.txt
$ cat /tmp/data.txt
d469f341d836
</code></pre>

<h2>Testing Data Persistence</h2>

<p>Time to test the data persistence. Scale the service to 3 replicas, then hop onto a new node where a replica resides and check if the data was persisted.</p>

<pre><code class="bash">$ docker service scale test_foo=3
test_foo scaled to 3
overall progress: 3 out of 3 tasks
1/3: running   [==================================================&gt;]
2/3: running   [==================================================&gt;]
3/3: running   [==================================================&gt;]
verify: Service converged
</code></pre>

<p>Check where the containers are running:</p>

<pre><code class="bash">$ docker service ps test_foo
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
jfwzb7yxnrxx        test_foo.1          alpine:latest       swarm-worker-1      Running             Running 2 minutes ago
mdsg6c5b2nqb        test_foo.2          alpine:latest       swarm-worker-3      Running             Running 15 seconds ago
iybat57t4lha        test_foo.3          alpine:latest       swarm-worker-2      Running             Running 15 seconds ago
</code></pre>

<p>Hop onto the <code>swarm-worker-2</code> node and check if the data is persisted from our previous write:</p>

<pre><code class="bash">$ docker exec -it 4228529aba29 sh
$ cat /tmp/data.txt
d469f341d836
</code></pre>

<p>Now let&rsquo;s append data to that file, then delete the stack and recreate to test if the data is still persisted:</p>

<pre><code class="bash">$ echo $HOSTNAME &gt;&gt; /tmp/data.txt
$ cat /tmp/data.txt
d469f341d836
4228529aba29
</code></pre>

<p>On the manager delete the stack:</p>

<pre><code class="bash">$ docker stack rm test
Removing service test_foo
</code></pre>

<p>The deploy the stack again:</p>

<pre><code class="bash">$ docker stack deploy -c docker-compose.yml test
Creating service test_foo
</code></pre>

<p>Check where the container is running:</p>

<pre><code class="bash">$ docker service ps test_foo
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS
9d6z02m123jk        test_foo.1          alpine:latest       swarm-worker-1      Running             Running 2 seconds ago
</code></pre>

<p>Exec into the container and read the data:</p>

<pre><code class="bash">$ docker exec -it 3008b1e1bba1 cat /tmp/data.txt
d469f341d836
4228529aba29
</code></pre>

<p>And as you can see the data is persisted.</p>

<h2>Resources</h2>

<p>Please have a look and star <a href="https://github.com/trajano/docker-volume-plugins">@trajano&rsquo;s</a> repository:</p>

<ul>
<li><a href="https://github.com/trajano/docker-volume-plugins">https://github.com/trajano/docker-volume-plugins</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
