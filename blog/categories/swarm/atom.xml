<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Swarm | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/swarm/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2018-10-23T16:49:15-04:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Setting Up a Docker Swarm Cluster on 3 RaspberryPi Nodes]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/10/23/setting-up-a-docker-swarm-cluster-on-3-raspberrypi-nodes/"/>
    <updated>2018-10-23T16:24:00-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/10/23/setting-up-a-docker-swarm-cluster-on-3-raspberrypi-nodes</id>
    <content type="html"><![CDATA[<p><img src="https://objects.ruanbekker.com/assets/images/rpi-docker-swarm.png" alt="" /></p>

<p>As the curious person that I am, I like to play around with new stuff that I stumble upon, and one of them was having a docker swarm cluster running on 3 Raspberry Pi&rsquo;s on my LAN.</p>

<p>The idea is to have 3 Raspberry Pi&rsquo;s (Model 3 B), a Manager Node, and 2 Worker Nodes, each with a 32 GB SanDisk SD Card, which I will also be part of a 3x Replicated GlusterFS Volume that will come in handy later for some data that needs persistent data.</p>

<p>More Inforamtion on: <a href="https://docs.docker.com/engine/swarm/">Docker Swarm</a></p>

<h2>Provision Raspbian on each RaspberryPi</h2>

<p>Grab the <a href="https://downloads.raspberrypi.org/raspbian_lite_latest">Latest Raspbian Lite ISO</a> and the following <a href="https://www.raspberrypi.org/documentation/installation/installing-images/">source</a> will help provisioning your RaspberryPi with Raspbian.</p>

<h2>Installing Docker on Raspberry PI</h2>

<p>On each node, run the following to install docker, and also add your user to the docker group, so that you can run docker commands with a normal user:</p>

<pre><code class="bash">$ apt-get update &amp;&amp; sudo apt-get upgrade -y
$ sudo apt-get remove docker.io
$ curl https://get.docker.com | sudo bash
$ sudo usermod -aG docker pi
</code></pre>

<p>If you have an internal DNS Server, set an A Record for each node, or for simplicity, set your hosts file on each node so that your hostname for each node responds to it&rsquo;s provisioned IP Address:</p>

<pre><code class="bash">$ cat /etc/hosts
192.168.0.2   rpi-01
192.168.0.3   rpi-02
192.168.0.4   rpi-03
</code></pre>

<p>Also, to have passwordless SSH, from each node:</p>

<pre><code class="bash">$ ssh-keygen -t rsa
$ ssh-copy-id rpi-01
$ ssh-copy-id rpi-02
$ ssh-copy-id rpi-03
</code></pre>

<h2>Initialize the Swarm</h2>

<p>Time to set up our swarm. As we have more than one network interface, we will need to setup our swarm by specifying the IP Address of our network interface that is accessible from our LAN:</p>

<pre><code class="bash">$ ifconfig eth0
eth0      Link encap:Ethernet  HWaddr a1:12:bc:d3:cd:4d
          inet addr:192.168.0.2  Bcast:192.168.0.255  Mask:255.255.255.0
</code></pre>

<p>Now that we have our IP Address, initialize the swarm on the manager node:</p>

<pre><code class="bash">pi@rpi-01:~ $ docker swarm init --advertise-addr 192.168.0.2
Swarm initialized: current node (siqyf3yricsvjkzvej00a9b8h) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join \
    --token SWMTKN-1-0eith07xkcg93lzftuhjmxaxwfa6mbkjsmjzb3d3sx9cobc2zp-97s6xzdt27y2gk3kpm0cgo6y2 \
    192.168.0.2:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.  
</code></pre>

<p>Then from <code>rpi-02</code> join the manager node of the swarm:</p>

<pre><code class="bash">pi@rpi-02:~ $ docker swarm join --token SWMTKN-1-0eith07xkcg93lzftuhjmxaxwfa6mbkjsmjzb3d3sx9cobc2zp-97s6xzdt27y2gk3kpm0cgo6y2 192.168.0.2:2377
This node joined a swarm as a worker.  
</code></pre>

<p>Then from <code>rpi-03</code> join the manager node of the swarm:</p>

<pre><code class="bash">pi@rpi-03:~ $ docker swarm join --token SWMTKN-1-0eith07xkcg93lzftuhjmxaxwfa6mbkjsmjzb3d3sx9cobc2zp-97s6xzdt27y2gk3kpm0cgo6y2 192.168.0.2:2377
This node joined a swarm as a worker.  
</code></pre>

<p>Then from the manager node: <code>rpi-01</code>, ensure that the nodes are checked in:</p>

<pre><code class="bash">pi@rpi-01:~ $ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS
62s7gx1xdm2e3gp5qoca2ru0d     rpi-03              Ready               Active
6fhyfy9yt761ar9pl84dkxck3 *   rpi-01              Ready               Active              Leader
pg0nyy9l27mtfc13qnv9kywe7     rpi-02              Ready               Active
</code></pre>

<h2>Setting Up a Replicated GlusterFS Volume</h2>

<p>I have decided to setup a replicated glusterfs volume to have data replicated throughout the cluster if I would like to have some persistent data. From each node, install the GlusterFS Client and Server:</p>

<pre><code class="bash">$ sudo apt install glusterfs-server glusterfs-client -y &amp;&amp; sudo systemctl enable glusterfs-server
</code></pre>

<p>Probe the other nodes from the manager node:</p>

<pre><code class="bash">pi@rpi-01:~ $ sudo gluster peer probe rpi-02
peer probe: success.

pi@rpi-01:~ $ sudo gluster peer probe rpi-03
peer probe: success.
</code></pre>

<p>Ensure that we can see all 3 nodes in our GlusterFS Pool:</p>

<pre><code class="bash">pi@rpi-01:~ $ sudo gluster pool list
UUID                                    Hostname        State
778c7463-ba48-43de-9f97-83a960bba99e    rpi-02          Connected
00a20a3c-5902-477e-a8fe-da35aa955b5e    rpi-03          Connected
d82fb688-c50b-405d-a26f-9cb2922cce75    localhost       Connected
</code></pre>

<p>From each node, create the directory where GlusterFS will store the data for the bricks that we will specify when creating the volume:</p>

<pre><code class="bash">pi@rpi-01:~ $ sudo mkdir -p /gluster/brick 
pi@rpi-02:~ $ sudo mkdir -p /gluster/brick
pi@rpi-03:~ $ sudo mkdir -p /gluster/brick
</code></pre>

<p>Next, create a 3 Way Replicated GlusterFS Volume:</p>

<pre><code class="bash">pi@rpi-01:~ $ sudo gluster volume create rpi-gfs replica 3 \
rpi-01:/gluster/brick \
rpi-02:/gluster/brick \
rpi-03:/gluster/brick \
force

volume create: rpi-gfs: success: please start the volume to access data
</code></pre>

<p>Start the GlusterFS Volume:</p>

<pre><code class="bash">pi@rpi-01:~ $ sudo gluster volume start rpi-gfs
volume start: rpi-gfs: success
</code></pre>

<p>Verify the GlusterFS Volume Info, and from the below output you will see that the volume is replicated 3 ways from the 3 bricks that we specified</p>

<pre><code class="bash">pi@rpi-01:~ $ sudo gluster volume info

Volume Name: rpi-gfs
Type: Replicate
Volume ID: b879db15-63e9-44ca-ad76-eeaa3e247623
Status: Started
Number of Bricks: 1 x 3 = 3
Transport-type: tcp
Bricks:
Brick1: rpi-01:/gluster/brick
Brick2: rpi-02:/gluster/brick
Brick3: rpi-03:/gluster/brick
</code></pre>

<p>Mount the GlusterFS Volume on each Node, first on <code>rpi-01</code>:</p>

<pre><code class="bash">pi@rpi-01:~ $ sudo umount /mnt
pi@rpi-01:~ $ sudo echo 'localhost:/rpi-gfs /mnt glusterfs defaults,_netdev,backupvolfile-server=localhost 0 0' &gt;&gt; /etc/fstab
pi@rpi-01:~ $ sudo mount.glusterfs localhost:/rpi-gfs /mnt
pi@rpi-01:~ $ sudo chown -R pi:docker /mnt
</code></pre>

<p>Then on <code>rpi-02</code>:</p>

<pre><code class="bash">pi@rpi-02:~ $ sudo umount /mnt
pi@rpi-02:~ $ sudo echo 'localhost:/rpi-gfs /mnt glusterfs defaults,_netdev,backupvolfile-server=localhost 0 0' &gt;&gt; /etc/fstab
pi@rpi-02:~ $ sudo mount.glusterfs localhost:/rpi-gfs /mnt
pi@rpi-02:~ $ sudo chown -R pi:docker /mnt
</code></pre>

<p>And lastly on <code>rpi-03</code>:</p>

<pre><code class="bash">pi@rpi-03:~ $ sudo umount /mnt
pi@rpi-03:~ $ sudo echo 'localhost:/rpi-gfs /mnt glusterfs defaults,_netdev,backupvolfile-server=localhost 0 0' &gt;&gt; /etc/fstab
pi@rpi-03:~ $ sudo mount.glusterfs localhost:/rpi-gfs /mnt
pi@rpi-03:~ $ sudo chown -R pi:docker /mnt
</code></pre>

<p>Then your GlusterFS Volume will be mounted on all the nodes, and when a file is written to the <code>/mnt/</code> partition, data will be replicated to all the nodes in the Cluster:</p>

<pre><code class="bash">pi@rpi-01:~ $ df -h
Filesystem          Size  Used Avail Use% Mounted on
/dev/root            30G  4.5G   24G  16% /
localhost:/rpi-gfs   30G  4.5G   24G  16% /mnt
</code></pre>

<h2>Create a Web Service on Docker Swarm:</h2>

<p>Let&rsquo;s create a Web Service in our Swarm, called <code>web</code> and by specifying <code>1</code> replica and publishing the exposed port <code>80</code> to our containers port <code>80</code>:</p>

<pre><code class="bash">pi@rpi-01:~ $ docker service create --name web --replicas 1 --publish 80:80 hypriot/rpi-busybox-httpd
vsvyanuw6q6yf4jr52m5z7vr1
</code></pre>

<p>Verifying that our Service is Started and equals to the desired replica count:</p>

<pre><code class="bash">pi@rpi-01:~ $ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE                                                    PORTS
vsvyanuw6q6y        web                 replicated          1/1                 hypriot/rpi-busybox-httpd:latest                         *:891-&gt;80/tcp
</code></pre>

<p>Inspecting the Service:</p>

<pre><code class="bash">pi@rpi-01:~ $ docker service inspect web
[
    {
        "ID": "vsvyanuw6q6yf4jr52m5z7vr1",
        "Version": {
            "Index": 2493
        },
        "CreatedAt": "2017-07-16T21:20:00.017836646Z",
        "UpdatedAt": "2017-07-16T21:20:00.026359794Z",
        "Spec": {
            "Name": "web",
            "Labels": {},
            "TaskTemplate": {
                "ContainerSpec": {
                    "Image": "hypriot/rpi-busybox-httpd:latest@sha256:c00342f952d97628bf5dda457d3b409c37df687c859df82b9424f61264f54cd1",
                    "StopGracePeriod": 10000000000,
                    "DNSConfig": {}
                },
                "Resources": {
                    "Limits": {},
                    "Reservations": {}
                },
                "RestartPolicy": {
                    "Condition": "any",
                    "Delay": 5000000000,
                    "MaxAttempts": 0
                },
                "Placement": {},
                "ForceUpdate": 0
            },
            "Mode": {
                "Replicated": {
                    "Replicas": 1
                }
            },
            "UpdateConfig": {
                "Parallelism": 1,
                "FailureAction": "pause",
                "Monitor": 5000000000,
                "MaxFailureRatio": 0,
                "Order": "stop-first"
            },
            "RollbackConfig": {
                "Parallelism": 1,
                "FailureAction": "pause",
                "Monitor": 5000000000,
                "MaxFailureRatio": 0,
                "Order": "stop-first"
            },
            "EndpointSpec": {
                "Mode": "vip",
                "Ports": [
                    {
                        "Protocol": "tcp",
                        "TargetPort": 80,
                        "PublishedPort": 80,
                        "PublishMode": "ingress"
                    }
                ]
            }
        },
        "Endpoint": {
            "Spec": {
                "Mode": "vip",
                "Ports": [
                    {
                        "Protocol": "tcp",
                        "TargetPort": 80,
                        "PublishedPort": 80,
                        "PublishMode": "ingress"
                    }
                ]
            },
            "Ports": [
                {
                    "Protocol": "tcp",
                    "TargetPort": 80,
                    "PublishedPort": 80,
                    "PublishMode": "ingress"
                }
            ],
            "VirtualIPs": [
                {
                    "NetworkID": "zjerz0xsw39icnh24enja4cgk",
                    "Addr": "10.255.0.13/16"
                }
            ]
        }
    }
]
</code></pre>

<p>Docker Swarm&rsquo;s Routing mesh takes care of the internal routing, so requests will respond even if the container is not running on the node that you are making the request against.</p>

<p>With that said, verifying on which node our service is running:</p>

<pre><code class="bash">pi@rpi-01:~ $ docker service ps web
ID                  NAME                IMAGE                              NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS
sd67cd18s5m0        web.1               hypriot/rpi-busybox-httpd:latest   rpi-02              Running             Running 2 minutes ago
</code></pre>

<p>When we make a HTTP Request to one of these Nodes IP Addresses, our request will be responded with this awesome static page:</p>

<p><img src="https://objects.ruanbekker.com/assets/images/armed-with-hypriot.jpg" alt="" /></p>

<p>We can see we only have one container in our swarm, let&rsquo;s scale that up to <code>3</code> containers:</p>

<pre><code class="bash">pi@rpi-01:~ $ docker service scale web01=3
web01 scaled to 3
</code></pre>

<p>Now that the service is scaled to 3 containers, requests will be handled using the round-robin algorithm. To ensured that the service scaled, we will see that we will have 3 replicas:</p>

<pre><code class="bash">pi@rpi-01:~ $ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE                                                    PORTS
vsvyanuw6q6y        web                 replicated          3/3                 hypriot/rpi-busybox-httpd:latest                         *:891-&gt;80/tcp
</code></pre>

<p>Verifying where these containers are running on:</p>

<pre><code class="bash">pi@rpi-01:~ $ docker service ps web01
ID                  NAME                IMAGE                              NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
sd67cd18s5m0        web.1               hypriot/rpi-busybox-httpd:latest   rpi-02              Running             Running 2 minutes ago
ope3ya7hh9j4        web.2               hypriot/rpi-busybox-httpd:latest   rpi-03              Running             Running 30 seconds ago
07m1ww7ptxro        web.3               hypriot/rpi-busybox-httpd:latest   rpi-01              Running             Running 28 seconds ago
</code></pre>

<p>Lastly, removing the service from our swarm:</p>

<pre><code class="bash">pi@rpi-01:~ $ docker service rm web01
web01
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[My PiStack Blog Proudly Hosted on My RaspberryPi Swarm Cluster]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/10/23/my-pistack-blog-proudly-hosted-on-my-raspberrypi-swarm-cluster/"/>
    <updated>2018-10-23T16:11:19-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/10/23/my-pistack-blog-proudly-hosted-on-my-raspberrypi-swarm-cluster</id>
    <content type="html"><![CDATA[<p>This is a repost of my <a href="http://blog.pistack.co.za/my-blog-proudly-hosted-on-my-raspberrypi-cluster/">first blogpost which is hosted on my Raspberry Pi Cluster (04 July 2017)</a>, that runs Docker Swarm and is served from my Home in South Africa, and can be accessed on <a href="http://blog.pistack.co.za">http://blog.pistack.co.za</a></p>

<h2>Just Look at It!</h2>

<ul>
<li>3x Raspberry Pi 3 Model B</li>
<li>Quad Core 1.2GHz Broadcom BCM2837 64bit CPU</li>
<li>1GB RAM</li>
<li>BCM43438 wireless LAN and Bluetooth Low Energy (BLE) on board</li>
<li>3x 32GB Sandisk SD Cards (Replicated GlusterFS Volume for <code>/gluster</code> partition)</li>
<li>Upgraded switched Micro USB power source up to 2.5A</li>
</ul>


<p><img src="https://objects.ruanbekker.com/assets/images/rpi-cluster.jpg" alt="" /></p>

<h2>My Setup:</h2>

<p>I have 3x <a href="https://www.raspberrypi.org/products/raspberry-pi-3-model-b/">Raspberrypi 3&rsquo;s</a>, each with a <a href="https://www.sandisk.com/home/memory-cards/sd-cards/ultra-sd">32GB SanDisk SD Card</a>, formatted with <a href="https://www.raspberrypi.org/downloads/raspbian/">Raspbian Jessie Lite</a>, powered by a <a href="https://www.pishop.co.za/store/rpi-power/anid%C3%A9es-6-port-50w-high-power-usb-hub-25aport">6 Port USB Hub</a> and networked with a <a href="https://m.takealot.com/#product_1?id=35258721">Totolink 5 Port Gigabit Switch</a>, but note that: <em>the Rpi does not support Gigabit Networking</em></p>

<p>For persistent storage I have setup a Replicated GlusterFS Volume across the 3 nodes.</p>

<p>More details on how I did the setup, can be found from the <a href="http://blog.pistack.co.za/setting-up-a-docker-swarm-cluster-on-3-raspberrypi-nodes/">Setting Up a Docker Swarm Cluster on RaspberryPi Nodes</a> blog post.</p>

<h2>Thanks!</h2>

<p>Thanks for the visit, I will blog about awesome Docker and RaspberryPi related stuff as my mind stumble along awesome ideas :)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deploy Docker Swarm Using Ansible]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/06/14/deploy-docker-swarm-using-ansible/"/>
    <updated>2018-06-14T06:05:46-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/06/14/deploy-docker-swarm-using-ansible</id>
    <content type="html"><![CDATA[<p><img src="http://obj-cache.cloud.ruanbekker.com/docker-logo.png" alt="" /></p>

<p>In this setup we will use Ansible to Deploy Docker Swarm.</p>

<p>With this setup, I have a client node, which will be my jump box, as it will be used to ssh with the docker user to my swarm nodes with passwordless ssh access.</p>

<p>The repository for the source code can be found on my <a href="https://github.com/ruanbekker/ansible-docker-swarm">Github Repository</a></p>

<h2>Pre-Check</h2>

<p>Hosts file:</p>

<pre><code>$ cat /etc/hosts
10.0.8.2 client
192.168.1.10 swarm-manager
192.168.1.11 swarm-worker-1
192.168.1.12 swarm-worker-2
</code></pre>

<p>SSH Config:</p>

<pre><code>$ cat ~/.ssh/config 
Host client
  Hostname client
  User root
  IdentityFile /tmp/key.pem
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null

Host swarm-manager
  Hostname swarm-manager
  User root
  IdentityFile /tmp/key.pem
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null

Host swarm-worker-1
  Hostname swarm-worker-1
  User root
  IdentityFile /tmp/key.pem
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null

Host swarm-worker-2
  Hostname swarm-worker-2
  User root
  IdentityFile /tmp/key.pem
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null
</code></pre>

<p>Install Ansible:</p>

<pre><code>$ apt install python-setuptools -y
$ easy_install pip
$ pip install ansible
</code></pre>

<p>Ensure passwordless ssh is working:</p>

<pre><code>$ ansible -i inventory.ini -u root -m ping all
client | SUCCESS =&gt; {
    "changed": false, 
    "ping": "pong"
}
swarm-manager | SUCCESS =&gt; {
    "changed": false, 
    "ping": "pong"
}
swarm-worker-2 | SUCCESS =&gt; {
    "changed": false, 
    "ping": "pong"
}
swarm-worker-1 | SUCCESS =&gt; {
    "changed": false, 
    "ping": "pong"
}
</code></pre>

<h2>Deploy Docker Swarm</h2>

<pre><code>$ ansible-playbook -i inventory.ini -u root deploy-swarm.yml 
PLAY RECAP 

client                     : ok=11   changed=3    unreachable=0    failed=0   
swarm-manager              : ok=18   changed=4    unreachable=0    failed=0   
swarm-worker-1             : ok=15   changed=1    unreachable=0    failed=0   
swarm-worker-2             : ok=15   changed=1    unreachable=0    failed=0   
</code></pre>

<p>SSH to the Swarm Manager and List the Nodes:</p>

<pre><code>$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
0ead0jshzkpyrw7livudrzq9o *   swarm-manager       Ready               Active              Leader              18.03.1-ce
iwyp6t3wcjdww0r797kwwkvvy     swarm-worker-1      Ready               Active                                  18.03.1-ce
ytcc86ixi0kuuw5mq5xxqamt1     swarm-worker-2      Ready               Active                                  18.03.1-ce
</code></pre>

<h2>Test Application on Swarm</h2>

<p>Create a Nginx Demo Service:</p>

<pre><code>$ docker network create --driver overlay appnet
$ docker service create --name nginx --publish 80:80 --network appnet --replicas 6 nginx
$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
k3vwvhmiqbfk        nginx               replicated          6/6                 nginx:latest        *:80-&gt;80/tcp

$ docker service ps nginx
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
tspsypgis3qe        nginx.1             nginx:latest        swarm-manager       Running             Running 34 seconds ago                       
g2f0ytwb2jjg        nginx.2             nginx:latest        swarm-worker-1      Running             Running 34 seconds ago                       
clcmew8bcvom        nginx.3             nginx:latest        swarm-manager       Running             Running 34 seconds ago                       
q293r8zwu692        nginx.4             nginx:latest        swarm-worker-2      Running             Running 34 seconds ago                       
sv7bqa5e08zw        nginx.5             nginx:latest        swarm-worker-1      Running             Running 34 seconds ago                       
r7qg9nk0a9o2        nginx.6             nginx:latest        swarm-worker-2      Running             Running 34 seconds ago   
</code></pre>

<p>Test the Application:</p>

<pre><code>$ curl -i http://192.168.1.10
HTTP/1.1 200 OK
Server: nginx/1.15.0
Date: Thu, 14 Jun 2018 10:01:34 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Tue, 05 Jun 2018 12:00:18 GMT
Connection: keep-alive
ETag: "5b167b52-264"
Accept-Ranges: bytes
</code></pre>

<p>Delete the Service:</p>

<pre><code>
$ docker service rm nginx
nginx
</code></pre>

<h2>Delete the Swarm:</h2>

<pre><code>$ ansible-playbook -i inventory.ini -u root delete-swarm.yml 

PLAY RECAP 
swarm-manager              : ok=2    changed=1    unreachable=0    failed=0   
swarm-worker-1             : ok=2    changed=1    unreachable=0    failed=0   
swarm-worker-2             : ok=2    changed=1    unreachable=0    failed=0   
</code></pre>

<p>Ensure the Nodes is removed from the Swarm, SSH to your Swarm Manager:</p>

<pre><code>$ docker node ls
Error response from daemon: This node is not a swarm manager. Use "docker swarm init" or "docker swarm join" to connect this node to swarm and try again.
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Clearing Up Disk Space on Docker Swarm by Removing Unused Data With Prune]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/06/01/clearing-up-disk-space-on-docker-swarm-by-removing-unused-data-with-prune/"/>
    <updated>2018-06-01T02:19:21-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/06/01/clearing-up-disk-space-on-docker-swarm-by-removing-unused-data-with-prune</id>
    <content type="html"><![CDATA[<p><img src="http://obj-cache.cloud.ruanbekker.com/docker-logo.png" alt="" /></p>

<p>After some time, your system can run out of disk space when running a lot of containers / volumes etc. You will find that at times, you will have a lot of unused containers, stopped containers, unused images, unused networks that is just sitting there, which consumes data on your nodes.</p>

<p>One way to clean them is by using <code>docker system prune</code>.</p>

<h2>Check Docker Disk Space</h2>

<p>The command below will show the amount of disk space consumed, and how much is reclaimable:</p>

<pre><code class="bash">$ docker system df
TYPE                TOTAL               ACTIVE              SIZE                RECLAIMABLE
Images              229                 125                 23.94GB             14.65GB (61%)
Containers          322                 16                  8.229GB             8.222GB (99%)
Local Volumes       77                  41                  698MB               19.13MB (2%)
Build Cache                                                 0B                  0B
</code></pre>

<h2>Removing Unsued Data:</h2>

<p>By using Prune, we can remove the unused resources that is consuming data:</p>

<pre><code class="bash">$ docker system prune

WARNING! This will remove:
        - all stopped containers
        - all networks not used by at least one container
        - all dangling images
        - all build cache
Are you sure you want to continue? [y/N] y

Deleted Containers:
a3d7db158e065d0c86160fd5d688875f8b7435848ea91db57ed007
47890dcfea4a105f43e790dd8ad3c6d7c4ad7e738186c034d7a46b

Deleted Networks:
traefik-net
app_appnet

Deleted Images:
deleted: sha256:5b9909c10e93afec
deleted: sha256:d81eesdfihweo3rk

Total reclaimed space: 14.18GB
</code></pre>

<p>For related <a href="https://goo.gl/L2NYxU">Docker</a> posts.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Wildcard SSL Certificate With Letsencrypt on Docker Swarm Using Traefik]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/05/28/wildcard-ssl-certificate-with-letsencrypt-on-docker-swarm-using-traefik/"/>
    <updated>2018-05-28T17:36:17-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/05/28/wildcard-ssl-certificate-with-letsencrypt-on-docker-swarm-using-traefik</id>
    <content type="html"><![CDATA[<p><img src="http://obj-cache.cloud.ruanbekker.com/traefik.png" alt="" /></p>

<p>With Letsencrypt supporting Wildcard certificates is really awesome. Now, we can setup traefik to listen on 443, acting as a reverse proxy and is doing HTTPS Termination to our Applications thats running in our Swarm.</p>

<h2>Architectural Design:</h2>

<p>At the moment we have 3 Manager Nodes, and 5 Worker Nodes:</p>

<ul>
<li>Using a Dummy Domain example.com which is set to the 3 Public IP&rsquo;s of our Manager Nodes</li>
<li>DNS is set for: <code>example.com</code> A Record to: <code>52.10.1.10</code>, <code>52.10.1.11</code>, <code>52.10.1.12</code></li>
<li>DNS is set for: <code>*.example.com</code> CNAME to <code>example.com</code></li>
<li>Any application that is spawned into our Swarm, will be labeled with a <code>traefik.frontend.rule</code> which will be routed to the service and redirected from HTTP to HTTPS</li>
</ul>


<h2>Create the Overlay Network:</h2>

<p>Create the overlay network that will be used for our stack:</p>

<pre><code class="bash">$ docker network create --driver overlay appnet
</code></pre>

<h2>Create the Compose Files for our Stacks:</h2>

<p>Create the Traefik Service Compose file, we will deploy it in Global Mode, constraint to our Manager Nodes, so that every manager node has a copy of traefik running.</p>

<pre><code class="bash">$ cat &gt; traefik-compose.yml &lt;&lt; EOF

version: "3.4"
services:
  proxy:
    image: traefik:latest
    command:
      - "--api"
      - "--entrypoints=Name:http Address::80 Redirect.EntryPoint:https"
      - "--entrypoints=Name:https Address::443 TLS"
      - "--defaultentrypoints=http,https"
      - "--acme"
      - "--acme.storage=/etc/traefik/acme/acme.json"
      - "--acme.entryPoint=https"
      - "--acme.httpChallenge.entryPoint=http"
      - "--acme.onHostRule=true"
      - "--acme.onDemand=false"
      - "--acme.email=me@example.com"
      - "--docker"
      - "--docker.swarmMode"
      - "--docker.domain=example.com"
      - "--docker.watch"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /mnt/traefik/acme.json:/etc/traefik/acme/acme.json
    networks:
      - appnet
    ports:
      - target: 80
        published: 80
        mode: host
      - target: 443
        published: 443
        mode: host
      - target: 8080
        published: 8080
        mode: host
    deploy:
      mode: global
      placement:
        constraints:
          - node.role == manager
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
networks:
  appnet:
    external: true

EOF
</code></pre>

<p>Create the Application Compose file, in this example we will be deploying a Ghost Blog:</p>

<pre><code class="bash">$ cat &gt; ghost-compose.yml &lt;&lt; EOF

version: '3.4'

services:
  blog:
    image: ghost:1.22.7-alpine
    networks:
      - appnet
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints: 
          - node.role == worker
      labels:
        - "traefik.backend.loadbalancer.sticky=false"
        - "traefik.backend.loadbalancer.swarm=true"
        - "traefik.backend=blog-1"
        - "traefik.docker.network=appnet"
        - "traefik.entrypoints=https"
        - "traefik.frontend.passHostHeader=true"
        - "traefik.frontend.rule=Host:blog.example.com"
        - "traefik.port=2368"

networks:
  appnet:
    external: true

EOF
</code></pre>

<h2>Prepare the Path for Traefik:</h2>

<p>We have a <a href="https://sysadmins.co.za/tag/glusterfs/">replicated volume</a> under our <code>/mnt</code> partition, so that all our managers can read from that path, create the file and provide the sufficient permissions:</p>

<pre><code class="bash">$ mkdir -p /mnt/traefik
$ touch /mnt/traefik/acme.json
$ chmod 600 /mnt/traefik/acme.json
</code></pre>

<h2>Deploy the Stacks:</h2>

<p>Deploy the Traefik Stack:</p>

<pre><code class="bash">$ docker stack deploy -c traefik-compose.yml traefik
</code></pre>

<p>Wait until the services are deployed:</p>

<pre><code class="bash">$ docker stack services traefik
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
f8ru5gbcgd2v        traefik_proxy       global              3/3                 traefik:latest
</code></pre>

<p>Deploy the Application Stack:</p>

<pre><code class="bash">$ docker stack deploy -c ghost-compose.yml apps
</code></pre>

<p>Verify that the Application Stack has been deployed:</p>

<pre><code class="bash">$ docker stack services apps
ID                  NAME                MODE                REPLICAS            IMAGE                          PORTS
516zlfs2cfdv        apps_blog           replicated          1/1                 ghost:1.22.7-alpine
</code></pre>

<p>At the moment we will have 2 stacks in our Swarm:</p>

<pre><code class="bash">$ docker stack ls
NAME                SERVICES
apps                1
traefik             1
</code></pre>

<h2>Test the Application:</h2>

<p>Let&rsquo;s test our blog to see if we get redirected to <a href="HTTPS:">HTTPS:</a></p>

<pre><code class="bash">$ curl -iL http://blog.example.com
HTTP/1.1 302 Found
Location: https://blog.example.com:443/
Date: Mon, 28 May 2018 22:02:41 GMT
Content-Length: 5
Content-Type: text/plain; charset=utf-8

HTTP/1.1 200 OK
Cache-Control: public, max-age=0
Content-Type: text/html; charset=utf-8
Date: Mon, 28 May 2018 22:02:42 GMT
Etag: W/"4166-J2ooSIa8gtTkYjbnr7vnPUFlRJI"
Vary: Accept-Encoding
X-Powered-By: Express
Transfer-Encoding: chunked
</code></pre>

<p>Works like a charm! Traefik FTW!</p>
]]></content>
  </entry>
  
</feed>
