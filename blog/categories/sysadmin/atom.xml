<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Sysadmin | Ruan Bekker's Blog]]></title>
  <link href="https://blog.ruanbekker.com/blog/categories/sysadmin/atom.xml" rel="self"/>
  <link href="https://blog.ruanbekker.com/"/>
  <updated>2022-08-07T09:27:58-04:00</updated>
  <id>https://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Create a RAID5 Array With Mdadm on Linux]]></title>
    <link href="https://blog.ruanbekker.com/blog/2022/06/29/create-a-raid5-array-with-mdadm-on-linux/"/>
    <updated>2022-06-29T05:02:13-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2022/06/29/create-a-raid5-array-with-mdadm-on-linux</id>
    <content type="html"><![CDATA[<p><img src="https://blog.ruanbekker.com/images/ruanbekker-raid5-array-linux.png" alt="setup-raid5-array-ubuntu-linux" /></p>

<p>In this tutorial we will setup a <a href="https://en.wikipedia.org/wiki/Standard_RAID_levels#RAID_5">RAID5</a> array, which is striping across multiple drives with distributed paritiy, which is good for redundancy. We will be using Ubuntu for our Linux Distribution, but the technique applies to other Linux Distributions as well.</p>

<h2>What are we trying to achieve</h2>

<p>We will run a server with one root disk and 6 extra disks, where we will first create our raid5 array with three disks, then I will show you how to expand your raid5 array by adding three other disks.</p>

<p>Things fail all the time, and it&rsquo;s not fun when hard drives breaks, therefore we want to do our best to prevent our applications from going down due to hardware failures. To achieve data redundancy, we want to use three hard drives, which we want to add into a raid configuration that will proviide us:</p>

<ul>
<li><a href="https://en.wikipedia.org/wiki/Data_striping">striping</a>, which is the technique of segmenting logically sequential data, so that consecutive segments are stored on different physical storage devices.</li>
<li><a href="https://en.wikipedia.org/wiki/Parity_bit#RAID">distributed parity</a>, where parity data are distributed between the physical disks, where there is only one parity block per disk, this provide protection against one physical disk failure, where the minimum number of disks are three.</li>
</ul>


<p>This is how a RAID5 array looks like (image from diskpart.com):</p>

<p><img src="https://user-images.githubusercontent.com/567298/176410333-0ff98867-dfb5-4fe3-a037-cc5d20014ab5.png" alt="raid5" /></p>

<h2>Hardware Overview</h2>

<p>We will have a Linux server with one root disk and six extra disks:</p>

<pre><code class="bash">$ lsblk
NAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
xvda    202:0    0    8G  0 disk
└─xvda1 202:1    0    8G  0 part /
xvdb    202:16   0   10G  0 disk
xvdc    202:32   0   10G  0 disk
xvdd    202:48   0   10G  0 disk
xvde    202:64   0   10G  0 disk
xvdf    202:80   0   10G  0 disk
xvdg    202:96   0   10G  0 disk
</code></pre>

<h2>Dependencies</h2>

<p>We require <code>mdadm</code> to create our raid configuration:</p>

<pre><code class="bash">$ sudo apt update
$ sudo apt install mdadm -y
</code></pre>

<h2>Format Disks</h2>

<p>First we will format and partition the following disks: <code>/dev/xvdb</code>, <code>/dev/xvdc</code>, <code>/dev/xvdd</code>, I will demonstrate the process for one disk, but repeat them for the other as well:</p>

<pre><code class="bash">$ fdisk /dev/xvdc

Welcome to fdisk (util-linux 2.34).
Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.

The old ext4 signature will be removed by a write command.

Device does not contain a recognized partition table.
Created a new DOS disklabel with disk identifier 0x26a2d2f6.

Command (m for help): n
Partition type
   p   primary (0 primary, 0 extended, 4 free)
   e   extended (container for logical partitions)
Select (default p): p
Partition number (1-4, default 1): 1
First sector (2048-20971519, default 2048):
Last sector, +/-sectors or +/-size{K,M,G,T,P} (2048-20971519, default 20971519):

Created a new partition 1 of type 'Linux' and of size 10 GiB.

Command (m for help): t
Selected partition 1
Hex code (type L to list all codes): fd
Changed type of partition 'Linux' to 'Linux raid autodetect'.

Command (m for help): w
The partition table has been altered.
Calling ioctl() to re-read partition table.
Syncing disks.
</code></pre>

<h2>Create RAID5 Array</h2>

<p>Using <code>mdadm</code>, create the <code>/dev/md0</code> device, by specifying the raid level and the disks that we want to add to the array:</p>

<pre><code class="bash">$ mdadm --create /dev/md0 --level=5 --raid-devices=3 /dev/xvdb1 /dev/xvdc1 /dev/xvdd1
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
</code></pre>

<p>Now that our device has been added, we can monitor the process:</p>

<pre><code class="bash">$ cat /proc/mdstat
Personalities : [raid6] [raid5] [raid4]
md0 : active raid5 xvdd1[3] xvdc1[1] xvdb1[0]
      20951040 blocks super 1.2 level 5, 512k chunk, algorithm 2 [3/2] [UU_]
      [==&gt;..................]  recovery = 11.5% (1212732/10475520) finish=4.7min speed=32103K/sec

unused devices: &lt;none&gt;
</code></pre>

<p>As you can see, currently its at 11.5%, give it some time to let it complete, you should treat the following as a completed state:</p>

<pre><code class="bash">$ cat /proc/mdstat
Personalities : [raid6] [raid5] [raid4]
md0 : active raid5 xvdd1[3] xvdc1[1] xvdb1[0]
      20951040 blocks super 1.2 level 5, 512k chunk, algorithm 2 [3/3] [UUU]

unused devices: &lt;none&gt;
</code></pre>

<p>We can also inspect devices with <code>mdadm</code>:</p>

<pre><code class="bash">$ mdadm -E /dev/xvd[b-d]1
/dev/xvdb1:
          Magic : a92b4efc
        Version : 1.2
    Feature Map : 0x0
     Array UUID : ea997bce:a530519c:ae41022e:0f4306bf
           Name : ip-172-31-3-57:0  (local to host ip-172-31-3-57)
  Creation Time : Wed Jan 12 13:36:39 2022
     Raid Level : raid5
   Raid Devices : 3

 Avail Dev Size : 20951040 (9.99 GiB 10.73 GB)
     Array Size : 20951040 (19.98 GiB 21.45 GB)
    Data Offset : 18432 sectors
   Super Offset : 8 sectors
   Unused Space : before=18280 sectors, after=0 sectors
          State : clean
    Device UUID : 8305a179:3ef96520:6c7b41dd:bdc7401f

    Update Time : Wed Jan 12 13:42:14 2022
  Bad Block Log : 512 entries available at offset 136 sectors
       Checksum : 1f9b4887 - correct
         Events : 18

         Layout : left-symmetric
     Chunk Size : 512K

   Device Role : Active device 0
   Array State : AAA ('A' == active, '.' == missing, 'R' == replacing)
</code></pre>

<p>To get information about your raid5 device:</p>

<pre><code>$ mdadm --detail /dev/md0
/dev/md0:
           Version : 1.2
     Creation Time : Wed Jan 12 13:36:39 2022
        Raid Level : raid5
        Array Size : 20951040 (19.98 GiB 21.45 GB)
     Used Dev Size : 10475520 (9.99 GiB 10.73 GB)
      Raid Devices : 3
     Total Devices : 3
       Persistence : Superblock is persistent

       Update Time : Wed Jan 12 13:42:14 2022
             State : clean
    Active Devices : 3
   Working Devices : 3
    Failed Devices : 0
     Spare Devices : 0

            Layout : left-symmetric
        Chunk Size : 512K

Consistency Policy : resync

              Name : ip-172-31-3-57:0  (local to host ip-172-31-3-57)
              UUID : ea997bce:a530519c:ae41022e:0f4306bf
            Events : 18

    Number   Major   Minor   RaidDevice State
       0     202       17        0      active sync   /dev/xvdb1
       1     202       33        1      active sync   /dev/xvdc1
       3     202       49        2      active sync   /dev/xvdd1
</code></pre>

<h2>Create Filesystems</h2>

<p>We will use our <code>/dev/md0</code> device and create a <code>ext4</code> filesystem:</p>

<pre><code class="bash">$ mkfs.ext4 /dev/md0
mke2fs 1.45.5 (07-Jan-2020)
Creating filesystem with 5237760 4k blocks and 1310720 inodes
Filesystem UUID: 579f045e-d270-4ff2-b36b-8dc506c27c5f
Superblock backups stored on blocks:
    32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208,
    4096000

Allocating group tables: done
Writing inode tables: done
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done
</code></pre>

<p>We can then verify that by looking at our block devices using <code>lsblk</code>:</p>

<pre><code class="bash">$ lsblk
NAME    MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
xvda    202:0    0    8G  0 disk
└─xvda1 202:1    0    8G  0 part  /
xvdb    202:16   0   10G  0 disk
└─xvdb1 202:17   0   10G  0 part
  └─md0   9:0    0   20G  0 raid5
xvdc    202:32   0   10G  0 disk
└─xvdc1 202:33   0   10G  0 part
  └─md0   9:0    0   20G  0 raid5
xvdd    202:48   0   10G  0 disk
└─xvdd1 202:49   0   10G  0 part
  └─md0   9:0    0   20G  0 raid5
xvde    202:64   0   10G  0 disk
xvdf    202:80   0   10G  0 disk
xvdg    202:96   0   10G  0 disk
</code></pre>

<p>Now we can mount our device to <code>/mnt</code>:</p>

<pre><code class="bash">$ mount /dev/md0 /mnt
</code></pre>

<p>We can verify that the device is mounted by using <code>df</code>:</p>

<pre><code class="bash">$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/root       7.7G  1.5G  6.3G  19% /
/dev/md0         20G   45M   19G   1% /mnt
</code></pre>

<p>To persist the device across reboots, add it to the <code>/etc/fstab</code> file:</p>

<pre><code class="bash">$ cat /etc/fstab
/dev/md0                /mnt     ext4   defaults                0 0
</code></pre>

<p>Now our filesystem which is mounted at <code>/mnt</code> is ready to be used.</p>

<h2>RAID Configuration (across reboots)</h2>

<p>By default RAID doesn’t have a config file, therefore we need to save it manually. If this step is not followed RAID device will not be in md0, but perhaps something else.</p>

<p>So, we must have to save the configuration to persist across reboots, when it reboot it gets loaded to the kernel and RAID will also get loaded.</p>

<pre><code class="bash">$ mdadm --detail --scan --verbose &gt;&gt; /etc/mdadm.conf
</code></pre>

<p>Note: Saving the configuration will keep the RAID level stable in the md0 device.</p>

<h2>Adding Spare Devices</h2>

<p>Earlier I mentioned that we have spare disks that we can use to expand our raid device. After they have been formatted we can add them as spare devices to our raid setup:</p>

<pre><code class="bash">$ mdadm --add /dev/md0 /dev/xvde1 /dev/xvdf1 /dev/xvdg1
mdadm: added /dev/xvde1
mdadm: added /dev/xvdf1
mdadm: added /dev/xvdg1
</code></pre>

<p>Verify our change by viewing the detail of our device:</p>

<pre><code class="bash">$ mdadm --detail /dev/md0
/dev/md0:
           Version : 1.2
     Creation Time : Wed Jan 12 13:36:39 2022
        Raid Level : raid5
        Array Size : 20951040 (19.98 GiB 21.45 GB)
     Used Dev Size : 10475520 (9.99 GiB 10.73 GB)
      Raid Devices : 3
     Total Devices : 6
       Persistence : Superblock is persistent

       Update Time : Wed Jan 12 14:28:23 2022
             State : clean
    Active Devices : 3
   Working Devices : 6
    Failed Devices : 0
     Spare Devices : 3

            Layout : left-symmetric
        Chunk Size : 512K

Consistency Policy : resync

              Name : ip-172-31-3-57:0  (local to host ip-172-31-3-57)
              UUID : ea997bce:a530519c:ae41022e:0f4306bf
            Events : 27

    Number   Major   Minor   RaidDevice State
       0     202       17        0      active sync   /dev/xvdb1
       1     202       33        1      active sync   /dev/xvdc1
       3     202       49        2      active sync   /dev/xvdd1

       4     202       65        -      spare   /dev/xvde1
       5     202       81        -      spare   /dev/xvdf1
       6     202       97        -      spare   /dev/xvdg1
</code></pre>

<p>As you can see it&rsquo;s only spares at this moment, we can use the spares for data storage, by growing our device:</p>

<pre><code class="bash">$ mdadm --grow --raid-devices=6 /dev/md0
</code></pre>

<p>Verify:</p>

<pre><code class="bash">$ mdadm --detail /dev/md0
/dev/md0:
           Version : 1.2
     Creation Time : Wed Jan 12 13:36:39 2022
        Raid Level : raid5
        Array Size : 20951040 (19.98 GiB 21.45 GB)
     Used Dev Size : 10475520 (9.99 GiB 10.73 GB)
      Raid Devices : 6
     Total Devices : 6
       Persistence : Superblock is persistent

       Update Time : Wed Jan 12 15:15:31 2022
             State : clean, reshaping
    Active Devices : 6
   Working Devices : 6
    Failed Devices : 0
     Spare Devices : 0

            Layout : left-symmetric
        Chunk Size : 512K

Consistency Policy : resync

    Reshape Status : 0% complete
     Delta Devices : 3, (3-&gt;6)

              Name : ip-172-31-3-57:0  (local to host ip-172-31-3-57)
              UUID : ea997bce:a530519c:ae41022e:0f4306bf
            Events : 36

    Number   Major   Minor   RaidDevice State
       0     202       17        0      active sync   /dev/xvdb1
       1     202       33        1      active sync   /dev/xvdc1
       3     202       49        2      active sync   /dev/xvdd1
       6     202       97        3      active sync   /dev/xvdg1
       5     202       81        4      active sync   /dev/xvdf1
       4     202       65        5      active sync   /dev/xvde1
</code></pre>

<p>Wait for the raid to rebuild, by viewing the <code>mdstat</code>::</p>

<pre><code class="bash">$ cat /proc/mdstat
Personalities : [raid6] [raid5] [raid4]
md0 : active raid5 xvdg1[6] xvdf1[5] xvde1[4] xvdd1[3] xvdc1[1] xvdb1[0]
      20951040 blocks super 1.2 level 5, 512k chunk, algorithm 2 [6/6] [UUUUUU]
      [&gt;....................]  reshape =  0.7% (76772/10475520) finish=18.0min speed=9596K/sec

unused devices: &lt;none&gt;
</code></pre>

<h2>Resizing our Filesystem</h2>

<p>Once we added the spares and growed our device, we need to run integrity checks, then we can resize the volume. But first, we need to unmount our filesystem:</p>

<pre><code class="bash">$ umount /mnt
</code></pre>

<p>Run a integrity check:</p>

<pre><code class="bash">$ e2fsck -f /dev/md0
e2fsck 1.45.5 (07-Jan-2020)
Pass 1: Checking inodes, blocks, and sizes
Pass 2: Checking directory structure
Pass 3: Checking directory connectivity
Pass 4: Checking reference counts
Pass 5: Checking group summary information
/dev/md0: 12/1310720 files (0.0% non-contiguous), 126323/5237760 blocks
</code></pre>

<p>Once that has passed, resize the file system:</p>

<pre><code class="bash">$ resize2fs /dev/md0
resize2fs 1.45.5 (07-Jan-2020)
Resizing the filesystem on /dev/md0 to 13094400 (4k) blocks.
The filesystem on /dev/md0 is now 13094400 (4k) blocks long.
</code></pre>

<p>Then we remount our filesystem:</p>

<pre><code class="bash">$ mount /dev/md0 /mnt
</code></pre>

<p>After the filesystem has been mounted, we can view the disk size and confirm that the size increased:</p>

<pre><code class="bash">$ df -h /mnt
Filesystem      Size  Used Avail Use% Mounted on
/dev/md0         50G   52M   47G   1% /mnt
</code></pre>

<h2>Thank You</h2>

<p>Thanks for reading, feel free to check out my <a href="https://ruan.dev/">website</a>, feel free to subscribe to my <a href="http://digests.ruanbekker.com/?via=ruanbekker-blog">newsletter</a> or follow me at <a href="https://twitter.com/ruanbekker">@ruanbekker</a> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Elasticsearch Templates Tutorial]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/04/06/elasticsearch-templates-tutorial/"/>
    <updated>2019-04-06T15:41:53-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/04/06/elasticsearch-templates-tutorial</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/53352581-b3892f80-392b-11e9-9532-5db5cbfc8f1c.jpg" alt="elasticsearch" /></p>

<p>Elasticsearch Index templates allow you to define templates that will automatically be applied on index creation time. The templates can include both settings and mappings..</p>

<h2>What are we doing?</h2>

<p>We want to create a template on how we would a target index to look like. It should consist of 1 primary shard and 2 replica shards and we want to update the mapping that we can make use of text and keyword string fields.</p>

<p>So then whenever we create an index which matches our template, the template will be applied on index creation.</p>

<h2>String Fields</h2>

<p>We will make use of the following string fields in our mappings which will be included in our templates:</p>

<p><strong>Text</strong>:</p>

<p>A field to index full-text values, such as the body of an email or the description of a product. These fields are analyzed, that is they are passed through an analyzer to convert the string into a list of individual terms before being indexed. The analysis process allows Elasticsearch to search for individual words within each full text field</p>

<p><strong>Keyword"</strong>:</p>

<p>A field to index structured content such as email addresses, hostnames, status codes, zip codes or tags.</p>

<p>They are typically used for filtering (Find me all blog posts where status is published), for sorting, and for aggregations. Keyword fields are only searchable by their exact value</p>

<h2>Note about templates:</h2>

<p>Couple of things to keep in mind:</p>

<pre><code>1. Templates gets referenced on index creation and does not affect existing indexes
2. When you update a template, you need to specify the exact template, the payload overwrites the whole template
</code></pre>

<p>View your current templates in your cluster:</p>

<pre><code>$ curl -XGET http://localhost:9200/_cat/templates?v
name                          index_patterns             order      version
.monitoring-kibana            [.monitoring-kibana-6-*]   0          6020099
filebeat-6.3.1                [filebeat-6.3.1-*]         1
</code></pre>

<p>Create the template <code>foobar_docs</code> which will match any indexes matching <code>foo-*</code> and <code>bar-*</code> which will inherit index settings of 1 primary shards and 2 replica shards and also apply a mapping template shown below:</p>

<pre><code>$ curl -H 'Content-Type: application/json' -XPUT http://localhost:9200/_template/foobar_docs -d '
{
  "index_patterns": [
    "foo-*", "bar-*"
  ], 
  "settings": {
    "number_of_shards": 1, 
    "number_of_replicas": 2
  }, 
  "mappings": {
    "type1": {
      "_source": {"enabled": true}, 
      "properties": {"created_at": {"type": "date"}, 
      "title": {"type": "text"}, 
      "status": {"type": "keyword"}, 
      "content": {"type":"text"}, 
      "first_name": {"type": "keyword"}, 
      "last_name": {"type": "keyword"}, 
      "age": {"type":"integer"}, 
      "registered": {"type": "boolean"}
      }
    }
  }
}'
{"acknowledged":true}
</code></pre>

<p>View the template from the api:</p>

<pre><code>$ curl -XGET http://localhost:9200/_cat/templates/foobar_docs?v
name        index_patterns order version
foobar_docs [foo-*, bar-*] 0
</code></pre>

<p>Create a index that will match the templates definition:</p>

<pre><code>$ curl -H 'Content-Type: application/json' -XPUT http://localhost:9200/test-2018.07.20
{"acknowledged":true,"shards_acknowledged":true,"index":"test-2018.07.20"}
</code></pre>

<p>Verify that the index has been created:</p>

<pre><code>$ curl -XGET http://localhost:9200/_cat/indices/test-2018.07.20?v
health status index           uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   test-2018.07.20 -5XOfl0GTEGeHycTwL51vQ   5   1          0            0        2kb          1.1kb
</code></pre>

<p>We can also inspect the template like shown below:</p>

<pre><code>$ curl -XGET http://localhost:9200/_template/foobar_docs?pretty
{
  "foobar_docs" : {
    "order" : 0,
    "index_patterns" : [
      "foo-*",
      "bar-*"
    ],
    "settings" : {
      "index" : {
        "number_of_shards" : "1",
        "number_of_replicas" : "2"
      }
    },
    "mappings" : {
      "type1" : {
        "_source" : {
          "enabled" : true
        },
        "properties" : {
          "created_at" : {
            "type" : "date"
          },
          "title" : {
            "type" : "text"
          },
          "status" : {
            "type" : "keyword"
          },
          "content" : {
            "type" : "text"
          },
          "first_name" : {
            "type" : "keyword"
          },
          "last_name" : {
            "type" : "keyword"
          },
          "age" : {
            "type" : "integer"
          },
          "registered" : {
            "type" : "boolean"
          }
        }
      }
    },
    "aliases" : { }
  }
}
</code></pre>

<p>Ingest a document to your index:</p>

<pre><code>$ curl -H 'Content-Type: application/json' -XPOST http://localhost:9200/foo-2018.07.20/type1/ -d '
{
  "title": "this is a post", 
  "status": "active", 
  "content": "introduction post", 
  "first_name": "ruan", 
  "last_name": "bekker", 
  "age": "31", 
  "registered": "true"
}'
</code></pre>

<p>Run a search against your elasticsearch index to view the data:</p>

<pre><code>$ curl -XGET http://localhost:9200/foo-2018.07.20/_search?pretty
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "foo-2018.07.20",
        "_type" : "type1",
        "_id" : "ZYfotmQB9mQGWzJT7W2y",
        "_score" : 1.0,
        "_source" : {
          "title" : "this is a post",
          "status" : "active",
          "content" : "introduction post",
          "first_name" : "ruan",
          "last_name" : "bekker",
          "age" : "31",
          "registered" : "true"
        }
      }
    ]
  }
}
</code></pre>

<p>Create another document:</p>

<pre><code>$ curl -H 'Content-Type: application/json' -XPOST http://localhost:9200/foo-2018.07.20/type1/ -d '
{
  "created_at": 1532077144, 
  "title": "this is a another post", 
  "status": "ae", 
  "content": "introduction post", 
  "first_name": "stefan", 
  "last_name": "bester", 
  "age": 34, 
  "registered": "true"
}'
</code></pre>

<p>As you guessed, executing another search against elasticsearch shows us both documents:</p>

<pre><code>$ curl -XGET http://localhost:9200/foo-2018.07.20/_search?pretty
{
  "took" : 0,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "foo-2018.07.20",
        "_type" : "type1",
        "_id" : "ZYfotmQB9mQGWzJT7W2y",
        "_score" : 1.0,
        "_source" : {
          "title" : "this is a post",
          "status" : "active",
          "content" : "introduction post",
          "first_name" : "ruan",
          "last_name" : "bekker",
          "age" : "31",
          "registered" : "true"
        }
      },
      {
        "_index" : "foo-2018.07.20",
        "_type" : "type1",
        "_id" : "rofrtmQB9mQGWzJTxnvp",
        "_score" : 1.0,
        "_source" : {
          "created_at" : 1532077144,
          "title" : "this is a another post",
          "status" : "active",
          "content" : "introduction post",
          "first_name" : "stefan",
          "last_name" : "bester",
          "age" : 34,
          "registered" : "true"
        }
      }
    ]
  }
}
</code></pre>

<p>Let&rsquo;s run a search query for any documents matching people with the age between <strong>30</strong> and <strong>40</strong>:</p>

<pre><code>$ curl -H 'Content-Type: application/json' -XGET http://localhost:9200/foo-2018.07.20/_search?pretty -d '{"query": {"range": {"age": {"gte": 30, "lte": 40}}}}'
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "foo-2018.07.20",
        "_type" : "type1",
        "_id" : "ZYfotmQB9mQGWzJT7W2y",
        "_score" : 1.0,
        "_source" : {
          "title" : "this is a post",
          "status" : "active",
          "content" : "introduction post",
          "first_name" : "ruan",
          "last_name" : "bekker",
          "age" : "31",
          "registered" : "true"
        }
      },
      {
        "_index" : "foo-2018.07.20",
        "_type" : "type1",
        "_id" : "rofrtmQB9mQGWzJTxnvp",
        "_score" : 1.0,
        "_source" : {
          "created_at" : 1532077144,
          "title" : "this is a another post",
          "status" : "active",
          "content" : "introduction post",
          "first_name" : "stefan",
          "last_name" : "bester",
          "age" : 34,
          "registered" : "true"
        }
      }
    ]
  }
}
</code></pre>

<p>Search for people with the age between <strong>32</strong> and <strong>40</strong>:</p>

<pre><code>$ curl -H 'Content-Type: application/json' -XGET http://localhost:9200/foo-2018.07.20/_search?pretty -d '{"query": {"range": {"age": {"gte": 32, "lte": 40}}}}'
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "foo-2018.07.20",
        "_type" : "type1",
        "_id" : "rofrtmQB9mQGWzJTxnvp",
        "_score" : 1.0,
        "_source" : {
          "created_at" : 1532077144,
          "title" : "this is a another post",
          "status" : "active",
          "content" : "introduction post",
          "first_name" : "stefan",
          "last_name" : "bester",
          "age" : 34,
          "registered" : "true"
        }
      }
    ]
  }
}
</code></pre>

<p>Let&rsquo;s say we want to update our template with <code>refresh_interval</code>, primary shards of 2 and replicas of 1 settings:</p>

<pre><code>$ curl -H 'Content-Type: application/json' -XPUT http://localhost:9200/_template/foobar_docs -d '
{
  "index_patterns": ["foo-*", "bar-*"], 
  "settings": {"number_of_shards": 2, "number_of_replicas": 1, "refresh_interval": "15s"}
}'
</code></pre>

<p>View the template, as you can see the target template will look exactly like the data body that we are posting to the template api:</p>

<pre><code>$ curl -XGET http://localhost:9200/_template/foobar_docs?pretty
{
  "foobar_docs" : {
    "order" : 0,
    "index_patterns" : [
      "foo-*",
      "bar-*"
    ],
    "settings" : {
      "index" : {
        "number_of_shards" : "2",
        "number_of_replicas" : "1",
        "refresh_interval" : "15s"
      }
    },
    "mappings" : { },
    "aliases" : { }
  }
}
</code></pre>

<p>View our current index, as you can see the index is unaffected of the template change as only new indexes will retrieve the update of the template:</p>

<pre><code>$ curl -XGET http://localhost:9200/_cat/indices/foo-2018.07.20?v
health status index          uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   foo-2018.07.20 ol1pGugrQCKd0xES4R6oFg   1   2          2            0     20.4kb         10.2kb
</code></pre>

<p>Create a new index to verify that the template&rsquo;s config is pulled into the new index:</p>

<pre><code>$ curl -H 'Content-Type: application/json' -XPUT http://localhost:9200/foo-2018.07.20-new
</code></pre>

<p>View the elasticsearch indexes to verify the behavior:</p>

<pre><code>$ curl -XGET http://localhost:9200/_cat/indices/foo-2018.07.*?v
health status index              uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   foo-2018.07.20     ol1pGugrQCKd0xES4R6oFg   1   2          2            0     20.4kb         10.2kb
green  open   foo-2018.07.20-new g6Ii8jtKRFa1zDVB2IsDBQ   2   1          0            0       920b           460b
</code></pre>

<p>Delete the indexes:</p>

<pre><code>$ curl -XDELETE http://localhost:9200/foo-*
{"acknowledged":true}
</code></pre>

<p>Delete the templates:</p>

<pre><code>$ curl -XDELETE 'http://localhost:9200/_template/foobar_docs'
{"acknowledged":true}
</code></pre>

<p>Verify that the templates are gone:</p>

<pre><code>$ curl -XGET http://localhost:9200/_cat/templates/foobar_docs?v
name index_patterns order version
</code></pre>

<h2>Resources:</h2>

<p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-templates.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-templates.html</a>
<a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.3/mapping-types.html">https://www.elastic.co/guide/en/elasticsearch/reference/6.3/mapping-types.html</a>
<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-range-query.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-range-query.html</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Reindex Your Elasticsearch Indexes Tutorial]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/04/06/reindex-your-elasticsearch-indexes-tutorial/"/>
    <updated>2019-04-06T15:37:18-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/04/06/reindex-your-elasticsearch-indexes-tutorial</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/53352581-b3892f80-392b-11e9-9532-5db5cbfc8f1c.jpg" alt="" /></p>

<p>At times you may find that the indexes in your cluster are not queried that often but you still want them around. But you also want to reduce the resource footprint by reducing the number of shards, and perhaps increase the refresh interval.</p>

<p>For refresh interval, if new data comes in and we dont care to have it available near real time, we can set the refresh interval for example to 60 seconds, so the index will only have the data available every 60 seconds. (default: 1s)</p>

<h2>Reindexing Elasticsearch Indexes</h2>

<p>In this example we will use the scenario where we have daily indexes with 5 primary shards and 1 set of replicas and we would like to create a weekly index with 1 primary shard, 1 replica and the refresh interval of 60 seconds, and reindex the previous weeks data into our weekly index.</p>

<p>Create the target weekly index with the mentioned configuration:</p>

<pre><code>$ curl -H "Content-Type: application/json" -XPUT 'http://127.0.0.1:9200/my-index-2019.01.01-07' -d '
{
    "settings": {
        "number_of_shards": "1",
        "number_of_replicas": "1",
        "refresh_interval" : "60s"
    }
}
'
</code></pre>

<p>Ensure the index exist:</p>

<pre><code>$ curl -s -XGET 'http://127.0.0.1:9200/_cat/indices/my-index-2019.01.01*?v'
health status index                    uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   my-index-2019.01.01      wbFEJCApSpSlbOXzb1Tjxw   5   1      22007            0      6.6mb          3.2mb
green  open   my-index-2019.01.02      cbDmJR7pbpRT3O2x46fj20   5   1      28031            0      7.2mb          3.4mb
..
green  open   my-index-2019.01.01-07   mJR7pJ9O4T3O9jzyI943ca   1   1          0            0       466b           233b
</code></pre>

<p>Create the reindex job, specify the source indexes and the destination index where the data must be reindexed to:</p>

<pre><code>$ curl -s -H 'Content-Type: application/json' -XPOST 'http://127.0.0.1:9200/_reindex' -d '
{
    "source": {
        "index": [
            "my-index-2019.01.01",
            "my-index-2019.01.02",
            "my-index-2019.01.03",
            "my-index-2019.01.04",
            "my-index-2019.01.05",
            "my-index-2019.01.06",
            "my-index-2019.01.07"
        ]
    },
    "dest": {
        "index": "my-index-2019.01.01-07"
    }
}
'
</code></pre>

<p>You can use the tasks api to monitor the progress:</p>

<pre><code>$ curl -s -XGET 'http://127.0.0.1:9200/_cat/tasks?'
indices:data/write/bulk        -3MIFskURPKxd1tg8P2j0w:912621270 -                                transport 1538459598188 22:53:18 3.1ms       x.x.x.x -3MIFsk
indices:data/write/bulk[s]     -3MIFskURPKxd1tg8P2j0w:912621271 -3MIFskURPKxd1tg8P2j0w:816648230 transport 1538459598188 22:53:18 3.1ms       x.x.x.x -3MIFsk
</code></pre>

<p>You manipulate the output of the tasks api by either fetching specific actions:</p>

<pre><code>$ curl -s -XGET 'http://127.0.0.1:9200/_tasks?actions=*data/write/reindex&amp;detailed&amp;pretty'
</code></pre>

<p>Or viewing detailed output:</p>

<pre><code>$ curl -s -XGET 'http://127.0.0.1:9200/_cat/tasks?detailed' | grep 'indices:data/write/reindex'
indices:data/write/reindex     IvoqWoUqSgGCQ0ELG21nhg:740560815 -                                transport 1538462294714 23:38:14 1.7m        x.x.x.x IvoqWoU reindex from [my-index-2019.01.01] to [my-index-2019.01.01-07]
</code></pre>

<p>Or you could get the json response:</p>

<pre><code>$ curl -s -XGET 'http://127.0.0.1:9200/_tasks?actions=*data/write/reindex&amp;detailed&amp;pretty'
{
  "nodes" : {
    "xx" : {
      "name" : "xx",
      "roles" : [ "data", "ingest" ],
      "tasks" : {
        "xx:876452606" : {
          "node" : "xx",
          "id" : 776452606,
          "type" : "transport",
          "action" : "indices:data/write/reindex",
          "status" : {
            "total" : 4785475,
            "updated" : 0,
            "created" : 234000,
            "deleted" : 0,
            "batches" : 235,
            "version_conflicts" : 0,
            "noops" : 0,
            "retries" : {
              "bulk" : 0,
              "search" : 0
            },
            "throttled_millis" : 0,
            "requests_per_second" : -1.0,
            "throttled_until_millis" : 0
          },
          "description" : "reindex from [my-index-2019.01.07] to [my-index-2019.01.01-07]",
          "start_time_in_millis" : 1538462901120,
          "running_time_in_nanos" : 64654161339,
          "cancellable" : true
        }
      }
    }
  }
}
</code></pre>

<p>Anyways, moving along. Reindex jobs will always be listed as a <code>data/write/reindex</code> action, so we can count the output:</p>

<pre><code>$ curl -s -XGET 'http://127.0.0.1:9200/_cat/tasks?'  | grep 'data/write/reindex' | wc -l
</code></pre>

<p>If the response is 0 then all the tasks completed and we can have a look at our index again:</p>

<pre><code>$ curl -s -XGET 'http://127.0.0.1:9200/_cat/indices/my-index-2019.01.0*?v'
health status index                    uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   my-index-2019.01.01      wbFEJCApSpSlbOXzb1Tjxw   5   1      22007            0      6.6mb          3.2mb
green  open   my-index-2019.01.02      cbDmJR7pbpRT3O2x46fj20   5   1      28031            0      7.2mb          3.4mb
..
green  open   my-index-2019.01.01-07   mJR7pJ9O4T3O9jzyI943ca   1   1     322007            0     45.9mb         22.9mb
</code></pre>

<p>Now that we can verify that the reindex tasks finished and we can see the aggregated result in our target index, we can delete our source indexes:</p>

<pre><code>$ curl -XDELETE 'http://127.0.0.1:9200/my-index-2019.01.01,my-index-2019.01.02,my-index-2019.01.03,my-index-2019.01.04,my-index-2019.01.05,my-index-2019.01.06,my-index-2019.01.07'
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Shrink Your Elasticsearch Index by Reducing the Shard Count With the Shards API]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/04/06/shrink-your-elasticsearch-index-by-reducing-the-shard-count-with-the-shards-api/"/>
    <updated>2019-04-06T15:33:48-04:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/04/06/shrink-your-elasticsearch-index-by-reducing-the-shard-count-with-the-shards-api</id>
    <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/567298/53352581-b3892f80-392b-11e9-9532-5db5cbfc8f1c.jpg" alt="elasticsearch" /></p>

<p>Resize your Elasticsearch Index with fewer Primary Shards by using the Shrink API.</p>

<p>In Elasticsearch, every index consists of multiple shards and every shard in your elasticsearch cluster contributes to the usage of your cpu, memory, file descriptors etc. This definitely helps for performance in parallel processing. As for an example with time series data, you would write and read a lot to an index with ie the current date.</p>

<p>If that index drops in requests and only read from the index every now and then, we dont need that many shards anymore and if we have multiple indexes, they may build up and take up unessacary compute power.</p>

<p>For a scenario where we want to reduce the size of our indexes, we can use the Shrink API to reduce the number of primary shards.</p>

<h2>The Shrink API</h2>

<p>The shrink index API allows you to shrink an existing index into a new index with fewer primary shards. The requested number of primary shards in the target index must be a factor of the number of shards in the source index. For example an index with 8 primary shards can be shrunk into 4, 2 or 1 primary shards or an index with 15 primary shards can be shrunk into 5, 3 or 1. If the number of shards in the index is a prime number it can only be shrunk into a single primary shard. Before shrinking, a (primary or replica) copy of every shard in the index must be present on the same node.</p>

<p>Steps on Shrinking:</p>

<p>Create the target index with the same definition as the source index, but with a smaller number of primary shards.
Then it hard-links segments from the source index into the target index.
Finally, it recovers the target index as though it were a closed index which had just been re-opened.</p>

<h2>Reduce the Primary Shards of an Index.</h2>

<p>As you may know, you can only set the Primary Shards on Index Creation time and Replica Shards you can set on the fly.</p>

<p>In this example we have a source index: <code>my-index-2019.01.10</code> with 5 primary shards and 1 replica shard, which gives us 10 shards for that index, that we would like to shrink to an index named: <code>archive_my-index-2019.01.10</code> with 1 primary shard and 1 replica shard, which will give us 2 shards for that index.</p>

<p>Have a look at your index:</p>

<pre><code>$ curl -XGET "http://127.0.0.1:9200/_cat/indices/my-index-2019.01.*?v"
health status index                                     uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   my-index-2019.01.10                       xAijUTSevXirdyTZTN3cuA   5   1   80795533            0      5.9gb          2.9gb
green  open   my-index-2019.01.11                       yb8Cjy9eQwqde8mJhR_vlw   5   5   80590481            0      5.7gb          2.8gb
...
</code></pre>

<p>And have a look at the nodes, as we will relocate the shards to a specific node:</p>

<pre><code>$ curl http://127.0.0.1:9200/_cat/nodes?v
ip            heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
x.x.x.x             8          98   0    0.04    0.03     0.01 m         -      3E9yp60
x.x.x.x            65          99   4    0.43    0.23     0.36 di        -      znFrs18
</code></pre>

<p>In this demonstration we only have 2 nodes with a replication factor of 1, which means a index&rsquo;s shards will always reside on both nodes. In a case with more nodes, we need to ensure that we choose a node where a primary index reside on.</p>

<p>Look at the shards api, by passing the index name to get the index to shard allocation:</p>

<pre><code>$ curl http://127.0.0.1:9200/_cat/shards/my-index-2019.01.10?v'
index               shard prirep state   docs  store ip       node
my-index-2019.01.10 2     p      STARTED  193  101mb x.x.x.x  Lq9P7eP
my-index-2019.01.10 2     r      STARTED  193  101mb x.x.x.x  F5edOwK
my-index-2019.01.10 4     p      STARTED  197  101mb x.x.x.x  Lq9P7eP
my-index-2019.01.10 4     r      STARTED  197  101mb x.x.x.x  F5edOwK
my-index-2019.01.10 3     r      STARTED  184  101mb x.x.x.x  Lq9P7eP
my-index-2019.01.10 3     p      STARTED  184  101mb x.x.x.x  F5edOwK
my-index-2019.01.10 1     r      STARTED  180  101mb x.x.x.x  Lq9P7eP
my-index-2019.01.10 1     p      STARTED  180  101mb x.x.x.x  F5edOwK
my-index-2019.01.10 0     p      STARTED  187  101mb x.x.x.x  Lq9P7eP
my-index-2019.01.10 0     r      STARTED  187  101mb x.x.x.x  F5edOwK
</code></pre>

<p>Create the target index:</p>

<pre><code>$ curl -XPUT -H 'Content-Type: application/json' http://127.0.0.1:9200/archive_my-index-2019.01.10 -d '
{
    "settings": {
        "number_of_shards": "1",
        "number_of_replicas": "1"
    }
}
'
</code></pre>

<p>Set the index as read only and relocate every copy of shard to node we indentified in a previous step:</p>

<pre><code>$ curl -XPUT -H 'Content-Type: application/json' http://127.0.0.1:9200/my-index-2019.01.10/_settings -d '
{
    "settings": {
        "index.routing.allocation.require._name": "Lq9P7eP",
        "index.blocks.write": true
    }
}
'
</code></pre>

<p>Now shrink the source index (my-index-2019.01.10) to the target index (archive_my-index-2019.01.10):</p>

<pre><code>$ curl -XPOST -H 'Content-Type: application/json' http://127.0.0.1:9200/my-index-2019.01.10/_shrink/archive_my-index-2019.01.10
</code></pre>

<p>You can monitor the progress by using the Recovery API:</p>

<pre><code>$ curl -s -XGET "http://127.0.0.1:9200/_cat/recovery/my-index-2019.01.10?human&amp;detailed=true"
my-index-2019.01.10 0 23.3s peer done x.x.x.x  F5edOwK x.x.x.x Lq9P7eP n/a n/a 15 15 100.0% 15 635836677 635836677 100.0% 635836677 0 0 100.0%
my-index-2019.01.10 1 22s   peer done x.x.x.x  Lq9P7eP x.x.x.x Lq9P7eP n/a n/a 15 15 100.0% 15 636392649 636392649 100.0% 636392649 0 0 100.0%
my-index-2019.01.10 2 19.6s peer done x.x.x.x  F5edOwK x.x.x.x Lq9P7eP n/a n/a 15 15 100.0% 15 636809671 636809671 100.0% 636809671 0 0 100.0%
my-index-2019.01.10 3 21.5s peer done x.x.x.x  Lq9P7eP x.x.x.x Lq9P7eP n/a n/a 15 15 100.0% 15 636378870 636378870 100.0% 636378870 0 0 100.0%
my-index-2019.01.10 4 23.3s peer done x.x.x.x F5edOwK- x.x.x.x Lq9P7eP n/a n/a 15 15 100.0% 15 636545756 636545756 100.0% 636545756 0 0 100.0%
</code></pre>

<p>You can also pass aliases as your table columns for output:</p>

<pre><code>$ curl -s -XGET "http://127.0.0.1:9200/_cat/recovery/my-index-2019.01.10?v&amp;detailed=true&amp;h=index,shard,time,ty,st,shost,thost,f,fp,b,bp"
index                            shard time  ty   st   shost         thost        f  fp     b         bp
my-index-2019.01.10              0     23.3s peer done x.x.x.x x.x.x.x 15 100.0% 635836677 100.0%
...
</code></pre>

<p>When the job is done, have a look at your indexes:</p>

<pre><code>$ curl -XGET "http://127.0.0.1:9200/_cat/indices/*my-index-2019.01.10?v"
health status index                                     uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   archive_my-index-2019.01.10               PAijUTSeRvirdyTZTN3cuA   1   1   80795533            0      5.9gb          2.9gb
green  open   my-index-2019.01.10                       Cb8Cjy9CQwqde8mJhR_vlw   5   1   80795533            0      2.9gb          2.9gb
</code></pre>

<p>Remove the block on your old index in order to make it writable:</p>

<pre><code>$ curl -XPUT -H 'Content-Type: application/json' http://127.0.0.1:9200/my-index-2019.01.10/_settings" -d '
{
    "settings": {
        "index.routing.allocation.require._name": null,
        "index.blocks.write": null
    }
}
'
</code></pre>

<p>Delete the old index:</p>

<pre><code>$ curl -XDELETE -H 'Content-Type: application/json' http://127.0.0.1:9200/my-index-2019.01.10
</code></pre>

<p>Note:, On AWS Elasticsearch Service, if you dont remove the block and you trigger a redeployment, you will end up with something like this. Shard may still be constraint to a node.</p>

<pre><code>$ curl -s -XGET ${ES_HOST/_cat/allocation?v
shards disk.indices disk.used disk.avail disk.total disk.percent host          ip  node
     0           0b    51.2gb    956.5gb   1007.8gb            5 x.x.x.x  x.x.x.x  ap9Mx1R
     1        3.6gb    54.9gb    952.8gb   1007.8gb            5 x.x.x.x  x.x.x.x  PqmoQpN   &lt;-----------
     0           0b    51.2gb    956.5gb   1007.8gb            5 x.x.x.x  x.x.x.x  5p7x4Lc
     0           0b    51.2gb    956.5gb   1007.8gb            5 x.x.x.x  x.x.x.x  c8kniP3
     0           0b    51.2gb    956.5gb   1007.8gb            5 x.x.x.x  x.x.x.x  jPwlwsD
     0           0b    51.2gb    956.5gb   1007.8gb            5 x.x.x.x  x.x.x.x  ljos4mu
   481      904.1gb   990.3gb    521.3gb      1.4tb           65 x.x.x.x  x.x.x.x  qAF-gIU
   481      820.2gb   903.6gb    608.1gb      1.4tb           59 x.x.x.x  x.x.x.x  dR3sNwA
   481      824.6gb   909.1gb    602.6gb      1.4tb           60 x.x.x.x  x.x.x.x  fvL4A9X
   481      792.7gb   876.5gb    635.2gb      1.4tb           57 x.x.x.x  x.x.x.x  lk4svht
   481      779.2gb   864.4gb    647.3gb      1.4tb           57 x.x.x.x  x.x.x.x  uLsej9m
     0           0b    51.2gb    956.5gb   1007.8gb            5 x.x.x.x  x.x.x.x  yM4Ka9l
</code></pre>

<h2>Resources:</h2>

<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-shrink-index.html">https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-shrink-index.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sysadmin Linux Troubleshooting Cheatsheet]]></title>
    <link href="https://blog.ruanbekker.com/blog/2019/02/10/sysadmin-linux-troubleshooting-cheatsheet/"/>
    <updated>2019-02-10T14:17:54-05:00</updated>
    <id>https://blog.ruanbekker.com/blog/2019/02/10/sysadmin-linux-troubleshooting-cheatsheet</id>
    <content type="html"><![CDATA[<p>This is a one pager of all the commands I use when I have to troubleshoot problems. This post will be updated as time goes by.</p>

<h2>Curl / Web Response Times</h2>

<p>Template file:</p>

<pre><code class="bash">$ cat curl-format.txt
time_namelookup:  %{time_namelookup}\n
time_connect:  %{time_connect}\n
time_appconnect:  %{time_appconnect}\n
time_pretransfer:  %{time_pretransfer}\n
time_redirect:  %{time_redirect}\n
time_starttransfer:  %{time_starttransfer}\n
----------\n
time_total:  %{time_total}\n
</code></pre>

<p>The host header, source addres, destination address:</p>

<pre><code class="bash">$ curl -sk -w "@curl-format.txt" -o /dev/null -H "Host: remote-host.mydomain.com" 10.0.2.10 https://10.244.0.240:443 -L

time_namelookup:  0.012178
time_connect:  0.012225
time_appconnect:  0.062149
time_pretransfer:  0.062175
time_redirect:  0.000172
time_starttransfer:  0.125631
----------
time_total:  0.125849
</code></pre>

<h2>MTR / Network Latencies / Packetloss</h2>

<p>No dns, TCP, counts, port, source address, destination address:</p>

<pre><code class="bash">$ mtr -n -T -c 10 --port 443 10.2.0.2 10.244.10.5 --report
Start: Sun Feb 10 19:04:50 2019
HOST: my-internet-gatewewy         Loss%   Snt   Last   Avg  Best  Wrst StDev
  1.|-- 172.18.110.22              0.0%    10    0.3   0.3   0.3   0.3   0.0
  2.|-- 172.18.110.22              0.0%    10    0.3   0.3   0.3   0.3   0.0
  3.|-- 172.18.110.22              0.0%    10    0.3   0.3   0.3   0.3   0.0
</code></pre>

<h2>TCPTraceroute</h2>

<p>No dns, TCP, port, source address, destination address:</p>

<pre><code class="bash">$ traceroute -T -n -p 443 -s 10.80.4.7 10.2.129.4; done
traceroute to 10.2.129.4 (10.2.129.4), 30 hops max, 60 byte packets
 1  10.80.4.1   0.322 ms  0.291 ms  0.224 ms
 2  10.2.129.4  179.090 ms  179.022 ms  179.023 ms
</code></pre>

<h2>Connection Related:</h2>

<p>Connection flow: <a href="https://askubuntu.com/questions/538443/whats-the-difference-between-port-status-listening-time-wait-close-wait">Thanks to</a></p>

<pre><code>Consider two programs attempting a socket connection (call them a and b). Both set up sockets and transition to the LISTEN state. Then one program (say a) tries to connect to the other (b). a sends a request and enters the SYN_SENT state, and b receives the request and enters the SYN_RECV state. When b acknowledges the request, they enter the ESTABLISHED state, and do their business. Now a couple of things can happen:

    a wishes to close the connection, and enters FIN_WAIT1. b receives the FIN request, sends an ACK (then a enters FIN_WAIT2), enters CLOSE_WAIT, tells a it is closing down and the enters LAST_ACK. Once a acknowledges this (and enters TIME_WAIT), b enters CLOSE. a waits a bit to see if anythings is left, then enters CLOSE.
    a and b have finished their business and decide to close the connection (simultaneous closing). When a is in FIN_WAIT, and instead of receiving an ACK from b, it receives a FIN (as b wishes to close it as well), a enters CLOSING. But there are still some messages to send (the ACK that a is supposed to get for its original FIN), and once this ACK arrives, a enters TIME_WAIT as usual.
</code></pre>

<p>Active Connections:</p>

<pre><code>$ netstat -n -A  inet | grep -v "127.0.0.1"
</code></pre>

<p>Established Connections:</p>

<pre><code>$ netstat -nputw | grep ESTABLISHED
$ netstat -antp | grep :3306 | grep ESTABLISHED
</code></pre>

<p>Time Wait Connections:</p>

<pre><code>$ netstat -antp | grep TIME_WAIT
</code></pre>

<p>How many connections:</p>

<pre><code>$ wc -l /proc/net/tcp
</code></pre>

<p>Listing Open files per Port:</p>

<pre><code>$ lsof -i:3306
</code></pre>

<p>Listing Open files per User:</p>

<pre><code>$ lsof -u glassfish
</code></pre>

<h2>Network Throughput</h2>

<p>You can test the network throughput between two linux hosts with <code>iperf</code>:</p>

<p>On side-a we will start the server in TCP mode:</p>

<pre><code>$ iperf -s
------------------------------------------------------------
Server listening on TCP port 5001
TCP window size:  128 KByte (default)
------------------------------------------------------------
</code></pre>

<p>On side-b we will start the client, which connects to the server:</p>

<pre><code>$ iperf -c 192.168.1.213
------------------------------------------------------------
Client connecting to 192.168.1.213, TCP port 5001
TCP window size: 43.8 KByte (default)
------------------------------------------------------------
[  3] local 192.168.0.114 port 43870 connected with 192.168.1.213 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.0 sec  11.4 MBytes  9.54 Mbits/sec
</code></pre>

<p>We can also run this in UDP mode where the server will run <code>iperf -s -u</code> and the client will run <code>iperf -c host-address -u</code></p>

<h2>Resources</h2>

<ul>
<li><a href="https://aws.amazon.com/premiumsupport/knowledge-center/troubleshoot-vpn-packet-loss/">AWS: Troubleshoot VPN Latencies</a></li>
<li><a href="https://www.linode.com/docs/networking/diagnostics/diagnosing-network-issues-with-mtr/">Linode: Diagnose Network Issues with MTR</a></li>
</ul>


<h2>Thank You</h2>

<p>Please feel free to show support by, <strong>sharing</strong> this post, making a <strong>donation</strong>, <strong>subscribing</strong> or <strong>reach out to me</strong> if you want me to demo and write up on any specific tech topic.</p>

<center>
<form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_top">
<input type="hidden" name="cmd" value="_s-xclick" />
<input type="hidden" name="hosted_button_id" value="W7CBGYTCWGANQ" />
<input type="image" src="https://user-images.githubusercontent.com/567298/49853901-461c3700-fdf1-11e8-9d80-8a424a3173af.png" border="0" name="submit" title="PayPal - The safer, easier way to pay online!" alt="Donate with PayPal button" />
<img alt="" border="0" src="https://www.paypal.com/en_ZA/i/scr/pixel.gif" width="1" height="1" />
</form>
</center>


<p><br></p>

<script type="text/javascript">
  ( function() {
    if (window.CHITIKA === undefined) { window.CHITIKA = { 'units' : [] }; };
    var unit = {"calltype":"async[2]","publisher":"rbekker87","width":728,"height":90,"sid":"Chitika Default"};
    var placement_id = window.CHITIKA.units.length;
    window.CHITIKA.units.push(unit);
    document.write('<div id="chitikaAdBlock-' + placement_id + '"></div>');
}());
</script>


<script type="text/javascript" src="//cdn.chitika.net/getads.js" async></script>

]]></content>
  </entry>
  
</feed>
