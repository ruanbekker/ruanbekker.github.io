<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Ubuntu | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/ubuntu/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2018-02-16T09:22:17-05:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Setup a NFS Server on Ubuntu]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/02/11/setup-a-nfs-server-on-ubuntu/"/>
    <updated>2018-02-11T17:26:56-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/02/11/setup-a-nfs-server-on-ubuntu</id>
    <content type="html"><![CDATA[<p><img src="https://i.snag.gy/3sUALo.jpg" alt="" /></p>

<p>Quick post on how to setup a NFS Server on Ubuntu and how to setup the client to interact with the NFS Server.</p>

<h2>Setup the Dependencies:</h2>

<pre><code class="bash">$ apt update &amp;&amp; sudo apt upgrade -y
$ sudo apt-get install nfs-kernel-server nfs-common -y
</code></pre>

<p>Create the Directory for NFS and set permissions:</p>

<pre><code class="bash">mkdir /vol
chown -R nobody:nogroup /vol
</code></pre>

<h2>Allow the Clients:</h2>

<p>We need to set in the <code>exports</code> file, the clients we would like to allow:</p>

<ul>
<li><code>rw</code>: Allows Client R/W Access to the Volume.</li>
<li><code>sync</code>: This option forces NFS to write changes to disk before replying. More stable and Consistent. Note, it does reduce the speed of file operations.</li>
<li><code>no_subtree_check</code>: This prevents subtree checking, which is a process where the host must check whether the file is actually still available in the exported tree for every request. This can cause many problems when a file is renamed while the client has it opened. In almost all cases, it is better to disable subtree checking.</li>
</ul>


<pre><code class="bash">$ echo '/vol 10.8.133.83(rw,sync,no_subtree_check) 10.8.166.19(rw,sync,no_subtree_check) 10.8.142.195(rw,sync,no_subtree_check)' &gt;&gt; /etc/exports
</code></pre>

<h2>Start the NFS Server:</h2>

<p>Restart the service and enable the service on boot:</p>

<pre><code class="bash">$ sudo systemctl restart nfs-kernel-server
$ sudo systemctl enable nfs-kernel-server
</code></pre>

<h2>Client Side:</h2>

<p>We will mount the NFS Volume to our Clients <code>/mnt</code> partition.</p>

<p>Install the dependencies:</p>

<pre><code class="bash">$ sudo apt-get install nfs-common -y
</code></pre>

<p>Test if we can mount the volume, then unmount it, as we will set the config in our <code>fstab</code>:</p>

<pre><code class="bash">$ sudo mount 10.8.133.83:/vol /mnt
$ sudo umount /mnt
$ df -h
</code></pre>

<p>Set the config in your <code>fstab</code>, then mount it from there:</p>

<pre><code class="bash">$ sudo bash -c "echo '10.8.133.83:/vol /mnt nfs auto,nofail,noatime,nolock,intr,tcp,actimeo=1800 0 0' &gt;&gt; /etc/fstab"
$ sudo mount -a
$ df -h
</code></pre>

<p>Now you shoule be able to write to your NFS Volume from your client.</p>

<p>Sources:
- <a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-16-04">1</a> <a href="https://gist.github.com/deviantony/557984d62e867e6f505577b207db6ffc%">2</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a Site to Site IPsec VPN With Strongswan and PreShared Key Authentication]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/02/11/setup-a-site-to-site-ipsec-vpn-with-strongswan-and-preshared-key-authentication/"/>
    <updated>2018-02-11T16:09:37-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/02/11/setup-a-site-to-site-ipsec-vpn-with-strongswan-and-preshared-key-authentication</id>
    <content type="html"><![CDATA[<p><img src="https://i.snag.gy/sWn8zc.jpg" alt="" /></p>

<p>Today we will setup a Site to Site ipsec VPN with Strongswan, which will be configured with PreShared Key Authentication.</p>

<p>After our tunnels are established, we will be able to reach the private ips over the vpn tunnels.</p>

<h2>Get the Dependencies:</h2>

<p>Update your repository indexes and install strongswan:</p>

<pre><code class="bash">$ apt update &amp;&amp; sudo apt upgrade -y
$ apt install strongswan -y
</code></pre>

<p>Set the following kernel parameters:</p>

<pre><code class="bash">$ cat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOF
echo net.ipv4.ip_forward = 1 
net.ipv4.conf.all.accept_redirects = 0 
net.ipv4.conf.all.send_redirects = 0
EOF

$ sysctl -p /etc/sysctl.conf
</code></pre>

<h2>Generate Preshared Key:</h2>

<p>We will need a preshared key that both servers will use:</p>

<pre><code>$ openssl rand -base64 64
87zRQqylaoeF5I8o4lRhwvmUzf+pYdDpsCOlesIeFA/2xrtxKXJTbCPZgqplnXgPX5uprL+aRgxD8ua7MmdWaQ
</code></pre>

<h2>Details of our 2 Sites:</h2>

<p>Site A:</p>

<pre><code class="bash">Location: Paris, France
External IP: 51.15.139.201
Internal IP: 10.10.27.1/24
</code></pre>

<p>Site B:</p>

<pre><code class="bash">Location: Amsterdam, Netherlands
External IP: 51.15.44.48
Internal IP: 10.9.141.1/24
</code></pre>

<h2>Configure Site A:</h2>

<p>We will setup our VPN Gateway in Site A (Paris), first to setup the <code>/etc/ipsec.secrets</code> file:</p>

<pre><code class="bash">$ cat /etc/ipsec.secrets
# source      destination
51.15.139.201 51.15.44.48 : PSK "87zRQqylaoeF5I8o4lRhwvmUzf+pYdDpsCOlesIeFA/2xrtxKXJTbCPZgqplnXgPX5uprL+aRgxD8ua7MmdWaQ"
</code></pre>

<p>Now to setup our VPN configuration in <code>/etc/ipsec.conf</code>:</p>

<pre><code>cat /etc/ipsec.conf
# basic configuration
config setup
        charondebug="all"
        uniqueids=yes
        strictcrlpolicy=no

# connection to amsterdam datacenter
conn paris-to-amsterdam
    authby=secret
    left=%defaultroute
    leftid=51.15.139.201
    leftsubnet=10.10.27.1/24
    right=51.15.44.48
    rightsubnet=10.9.141.1/24
    ike=aes256-sha2_256-modp1024!
    esp=aes256-sha2_256!
    keyingtries=0
    ikelifetime=1h
    lifetime=8h
    dpddelay=30
    dpdtimeout=120
    dpdaction=restart
    auto=start
</code></pre>

<p>Firewall Rules:</p>

<pre><code class="bash">$ sudo iptables -t nat -A POSTROUTING -s 10.9.141.0/24 -d 10.10.27.0/24 -j MASQUERADE
</code></pre>

<h2>Configure Site B:</h2>

<p>We will setup our VPN Gateway in Site B (Amsterdam), setup the <code>/etc/ipsec.secrets</code> file:</p>

<pre><code class="bash">$ cat /etc/ipsec.secrets
51.15.44.48 51.15.139.201 : PSK "87zRQqylaoeF5I8o4lRhwvmUzf+pYdDpsCOlesIeFA/2xrtxKXJTbCPZgqplnXgPX5uprL+aRgxD8ua7MmdWaQ"
</code></pre>

<p>Next to setup our VPN Configuration:</p>

<pre><code>cat /etc/ipsec.conf
# basic configuration
config setup
        charondebug="all"
        uniqueids=yes
        strictcrlpolicy=no

# connection to paris datacenter
conn amsterdam-to-paris
    authby=secret
    left=%defaultroute
    leftid=51.15.44.48
    leftsubnet=10.9.141.1/24
    right=51.15.139.201
    rightsubnet=10.10.27.1/24
    ike=aes256-sha2_256-modp1024!
    esp=aes256-sha2_256!
    keyingtries=0
    ikelifetime=1h
    lifetime=8h
    dpddelay=30
    dpdtimeout=120
    dpdaction=restart
    auto=start
</code></pre>

<p>Firewall Rules:</p>

<pre><code class="bash">$ sudo iptables -t nat -A POSTROUTING -s 10.10.27.0/24 -d 10.9.41.0/24 -J MASQUERADE
</code></pre>

<h2>Start the VPN:</h2>

<p>Start the VPN on both ends:</p>

<pre><code class="bash">$ sudo ipsec restart
</code></pre>

<p>Get the status of the tunnel, in this case we are logged onto our Site A (Paris) Server:</p>

<pre><code>$ sudo ipsec status
Security Associations (1 up, 0 connecting):
paris-to-amsterdam[2]: ESTABLISHED 14 minutes ago, 10.10.27.161[51.15.139.201]...51.15.44.48[51.15.44.48]
paris-to-amsterdam{1}:  INSTALLED, TUNNEL, reqid 1, ESP in UDP SPIs: c8c868ee_i c9d58dbd_o
paris-to-amsterdam{1}:   10.10.27.1/24 === 10.9.141.1/24
</code></pre>

<p>Test if we can see the remote end on its private range:</p>

<pre><code class="bash">$ ping 10.9.141.97
PING 10.9.141.97 (10.9.141.97) 56(84) bytes of data.
64 bytes from 10.9.141.97: icmp_seq=1 ttl=64 time=14.6 ms
</code></pre>

<p>Set the service to start on boot:</p>

<pre><code class="bash">$ sudo systemctl enable strongswan
</code></pre>

<p>Then your VPN should be setup correctly.</p>

<h2>Other useful commands:</h2>

<p>Start / Stop / Status:</p>

<pre><code class="bash">$ sudo ipsec up connection-name
$ sudo ipsec down connection-name

$ sudo ipsec restart
$ sudo ipsec status
$ sudo ipsec statusall
</code></pre>

<p>Get the Policies and States of the IPsec Tunnel:</p>

<pre><code class="bash">$ sudo ip xfrm state
$ sudo ip xfrm policy
</code></pre>

<p>Reload the secrets, while the service is running:</p>

<pre><code class="bash">$ sudo ipsec rereadsecrets
</code></pre>

<p>Check if traffic flows through the tunnel:</p>

<pre><code class="bash">$ sudo tcpdump esp
</code></pre>

<h2>Adding more connections to your config:</h2>

<p>If you have to add another site to your config, the example of the <code>ipsec.secrets</code> will look like:</p>

<pre><code class="bash">$ cat /etc/ipsec.secrets
51.15.139.201 51.15.44.48 : PSK "87zRQqylaoeF5I8o4lRhwvmUzf+pYdDpsCOlesIeFA/2xrtxKXJTbCPZgqplnXgPX5uprL+aRgxD8ua7MmdWaQ"
51.15.139.201 51.15.87.41  : PSK "87zRQqylaoeF5I8o4lRhwvmUzf+pYdDpsCOlesIeFA/2xrtxKXJTbCPZgqplnXgPX5uprL+aRgxD8ua7MmdWaQ"
</code></pre>

<p>And the <code>ipsec.conf</code>:</p>

<pre><code class="bash">cat /etc/ipsec.conf
# basic configuration
config setup
        charondebug="all"
        uniqueids=yes
        strictcrlpolicy=no

# connection to amsterdam datacenter
conn paris-to-amsterdam
    authby=secret
    left=%defaultroute
    leftid=51.15.139.201
    leftsubnet=10.10.27.161/32
    right=51.15.44.48
    rightsubnet=10.9.141.97/32
    ike=aes256-sha2_256-modp1024!
    esp=aes256-sha2_256!
    keyingtries=0
    ikelifetime=1h
    lifetime=8h
    dpddelay=30
    dpdtimeout=120
    dpdaction=restart
    auto=start

# connection to frankfurt datacenter
conn paris-to-frankfurt
    authby=secret
    left=%defaultroute
    leftid=51.15.139.201
    leftsubnet=10.10.27.1/24
    right=51.15.87.41
    rightsubnet=10.9.137.1/24
    ike=aes256-sha2_256-modp1024!
    esp=aes256-sha2_256!
    keyingtries=0
    ikelifetime=1h
    lifetime=8h
    dpddelay=30
    dpdtimeout=120
    dpdaction=restart
    auto=start
</code></pre>

<p>Just remember to configure the config on the Frankfurt VPN Gateway, and the example of the status output will look like the following:</p>

<pre><code class="bash">$ sudo ipsec status
Security Associations (2 up, 0 connecting):
paris-to-frankfurt[2]: ESTABLISHED 102 seconds ago, 10.10.27.161[51.15.139.201]...51.15.87.41[51.15.87.41]
paris-to-frankfurt{1}:  INSTALLED, TUNNEL, reqid 2, ESP in UDP SPIs: cbc62a1f_i c95b8f78_o
paris-to-frankfurt{1}:   10.10.27.1/24 === 10.9.137.1/24
paris-to-amsterdam[1]: ESTABLISHED 102 seconds ago, 10.10.27.161[51.15.139.201]...51.15.44.48[51.15.44.48]
paris-to-amsterdam{2}:  INSTALLED, TUNNEL, reqid 1, ESP in UDP SPIs: c7b36756_i cc54053c_o
paris-to-amsterdam{2}:   10.10.27.1/24 === 10.9.141.1/24
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a 3 Node Kubernetes Cluster on Ubuntu]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/12/11/setup-a-3-node-kubernetes-cluster-on-ubuntu/"/>
    <updated>2017-12-11T09:31:47-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/12/11/setup-a-3-node-kubernetes-cluster-on-ubuntu</id>
    <content type="html"><![CDATA[<p><img src="https://kumorilabs.com/img/blog/kubernetes-logo.png" alt="" /></p>

<p>Setup a 3 Node Kubernetes Cluster on Ubuntu 16.04</p>

<h2>What is Kubernetes?</h2>

<p>As referenced from their <a href="https://kubernetes.io/">website</a>:</p>

<ul>
<li>&ldquo;Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.&rdquo;</li>
</ul>


<h2>Our Setup:</h2>

<p>For this setup I will be using 3 AWS EC2 Instances with Ubuntu 16.04. One node will act as the master node, and the other 2 nodes, will act as nodes, previously named minions.</p>

<p>We will deploy Kubernetes on all 3 nodes, the master will be the node where we will initialize our cluster, deploy our weave network, applications and we will execute the join command on the worker nodes to join the master to form the cluster.</p>

<h2>Deploy Kubernetes: Master</h2>

<p>The following commands will be used to install Kubernetes, it will be executed with root permissions:</p>

<pre><code class="bash">$ apt update &amp;&amp; sudo apt upgrade -y
$ sudo apt install docker.io apt-transport-https -qy
$ sudo apt update
$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
$ sudo su -c 'echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" &gt; /etc/apt/sources.list.d/app' root
$ apt update
$ sudo apt install kubelet kubeadm kubernetes-cni -y
</code></pre>

<p>Now we would like to set up the master by initializing the cluster:</p>

<pre><code class="bash">$ sudo kubeadm init --kubernetes-version stable-1.8
</code></pre>

<p>The output will provide you with instructions to setup the configurations for the master node, and provide you with a join token for your worker nodes, remember to make not of this token string, as we will need it later for our worker nodes. As your normal user, run the following to setup the config:</p>

<p>Remember to not run this as root, and as the normal user:</p>

<pre><code class="bash">$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre>

<p>Now we need to deploy a network for our pods:</p>

<pre><code class="bash">$ kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
</code></pre>

<p>Lets confirm if all our resources are in its desired state, a small snippet of the output will look like the one below:</p>

<pre><code class="bash">$ kubectl get all -n kube-system

...
NAME                                          READY     STATUS    RESTARTS   AGE
po/etcd-ip-172-31-40-211                      1/1       Running   0          6h
po/kube-apiserver-ip-172-31-40-211            1/1       Running   0          6h
</code></pre>

<p>Once all of the resources are in its desired state, we can head along to our worker nodes, to join them to the cluster</p>

<h2>Deploy Kubernetes: Worker Nodes</h2>

<p>As I have 2 worker nodes, we will need to deploy the following on both of our worker nodes, first to deploy Kubernetes on our nodes with root permission:</p>

<pre><code class="bash">$ apt update &amp;&amp; sudo apt upgrade -y
$ sudo apt install docker.io apt-transport-https -qy
$ sudo apt update
$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
$ sudo su -c 'echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" &gt; /etc/apt/sources.list.d/app' root
$ apt update
$ sudo apt install kubelet kubeadm kubernetes-cni -y
</code></pre>

<p>Once Kubernetes is installed, join the Master node by executing the join command:</p>

<pre><code class="bash">$ sudo kubeadm join --token 49abf7.247d663db97f8504 172.31.40.211:6443 --discovery-token-ca-cert-hash sha256:3a3b301cfbac0995c69a0115989ea384230470d6836ae0e13e71dbdf15ffbb48
</code></pre>

<p>Do the 2 steps on the other node, then head back to the master node.</p>

<h2>Verifying if All Nodes are Checked In</h2>

<p>To verify if all nodes are available and reachable in the cluster:</p>

<pre><code class="bash">$ kubectl get nodes
NAME               STATUS    ROLES     AGE       VERSION
ip-172-31-36-68    Ready     &lt;none&gt;    6h        v1.8.5
ip-172-31-40-211   Ready     master    6h        v1.8.5
ip-172-31-44-80    Ready     &lt;none&gt;    6h        v1.8.5
</code></pre>

<h2>Deploy Services to Kubernetes:</h2>

<p>Kubernetes has Awesome Examples on their <a href="https://github.com/kubernetes/kubernetes/tree/master/examples">Github Repository</a>.</p>

<p>Since the awesomeness of <a href="https://github.com/openfaas">OpenFaas</a>, I will deploy OpenFaas on Kubernetes:</p>

<pre><code class="bash">$ git clone https://github.com/openfaas/faas-netes
$ cd faas-netes
$ kubectl apply -f faas.yml,monitoring.yml,rbac.yml
</code></pre>

<p>Give it about a minute or so, then you should see the pods running in their desired state:</p>

<pre><code class="bash">$ kubectl get pods
NAME                           READY     STATUS    RESTARTS   AGE
alertmanager-77b4b476b-zxtcz   1/1       Running   0          4h
crypto-7d8b7f999c-7l85k        1/1       Running   0          1h
faas-netesd-64fb9b4dfb-hc8gh   1/1       Running   0          4h
gateway-69c9d949f-q57zh        1/1       Running   0          4h
prometheus-7fbfd8bfb8-d4cft    1/1       Running   0          4h
</code></pre>

<p>When we have the desired state, head over to the OpenFaas Gateway WebUI: <code>http://master-public-ip:31112/ui/</code>, select &ldquo;Deploy New Function&rdquo;, you can use your own function or select one from the store.</p>

<p>I am going to use Figlet from the store, once the pod has been deployed, select the function, enter any text into the request body and select invoke. I have used my name and surname, and turns out into:</p>

<pre><code class="bash"> ____                      ____       _    _             
|  _ \ _   _  __ _ _ __   | __ )  ___| | _| | _____ _ __ 
| |_) | | | |/ _` | '_ \  |  _ \ / _ \ |/ / |/ / _ \ '__|
|  _ &lt;| |_| | (_| | | | | | |_) |  __/   &lt;|   &lt;  __/ |   
|_| \_\\__,_|\__,_|_| |_| |____/ \___|_|\_\_|\_\___|_|   
</code></pre>

<h2>Resources:</h2>

<ul>
<li><a href="https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/">Kubernetes Overview</a></li>
<li><a href="https://kubernetes.io/docs/concepts/">Kubernetes Concepts</a></li>
<li><a href="https://blog.alexellis.io/tag/kubernetes/">Kubernetes Blogs</a></li>
<li><a href="https://blog.alexellis.io/tag/openfaas/">OpenFaas Blogs</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a 3 Node Galera MariaDB Cluster on Ubuntu 16]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/11/22/setup-a-3-node-galera-mariadb-cluster-on-ubuntu-16/"/>
    <updated>2017-11-22T18:17:14-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/11/22/setup-a-3-node-galera-mariadb-cluster-on-ubuntu-16</id>
    <content type="html"><![CDATA[<p><img src="https://i.snag.gy/lpT6Du.jpg" alt="" /></p>

<p>Today we will setup a 3-Node Galera MariaDB Cluster which is a Multi Master MySQL/MariaDB Cluster on Ubuntu 16.04</p>

<h2>Our Server Details:</h2>

<pre><code class="bash">172.31.11.174     mysql-1
172.31.13.206     mysql-2
172.31.6.93       mysql-3
</code></pre>

<h2>Update Repo Index and Upgrade:</h2>

<p>Update the repository indexes and install the needed packages:</p>

<pre><code class="bash">$ sudo apt update &amp;&amp; sudo apt upgrade -y
</code></pre>

<p>Install the needed repository and packages:</p>

<pre><code class="bash">$ apt install software-properties-common -y
$ apt-key adv --recv-keys --keyserver hkp://keyserver.ubuntu.com:80 0xF1656F24C74CD1D8
$ add-apt-repository 'deb [arch=amd64,i386,ppc64el] http://mirror.lstn.net/mariadb/repo/10.1/ubuntu xenial main'
$ apt update
$ apt install mariadb-server rsync -y
</code></pre>

<h2>Configuration:</h2>

<pre><code class="bash">cat &gt; /etc/mysql/conf.d/galera.cnf &lt;&lt; EOF
[mysqld]
binlog_format=ROW
default-storage-engine=innodb
innodb_autoinc_lock_mode=2
bind-address=0.0.0.0

# Galera Provider Configuration
wsrep_on=ON
wsrep_provider=/usr/lib/galera/libgalera_smm.so

# Galera Cluster Configuration
wsrep_cluster_name="my-galera-cluster"
wsrep_cluster_address="gcomm://172.31.11.174,172.31.13.206,172.31.6.93"
# Galera Synchronization Configuration
wsrep_sst_method=rsync

# Galera Node Configuration
wsrep_node_address="172.31.11.174"
wsrep_node_name="mysql-1"
EOF
</code></pre>

<p>Comment out bind-address, so that MariaDB process is reachable from other nodes, by default it wont be in the config, but just to make sure, if it is uncommented, comment the config:</p>

<pre><code class="bash /etc/mysql/my.cnf"># bind-address = 127.0.0.1
</code></pre>

<p>Stop the MariaDB Process:</p>

<pre><code class="bash">$ systemctl stop mariadb
</code></pre>

<p>Note: Repeat the above steps on all 3 nodes.</p>

<h2>Initialize the Cluster:</h2>

<p>On the First Node, Initialize the Galera Cluster:</p>

<pre><code class="bash">$ /usr/bin/galera_new_cluster
$ systemctl enable mariadb
</code></pre>

<p>Check how many nodes are active in the Cluster:</p>

<pre><code class="mysql">$ mysql -u root -p -e "SHOW STATUS LIKE 'wsrep_cluster_size';"
Enter password:
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| wsrep_cluster_size | 1     |
+--------------------+-------+
</code></pre>

<h2>Node-2: Start and Enable MariaDB</h2>

<pre><code class="bash">$ systemctl start mariadb
$ systemctl enable mariadb
</code></pre>

<p>Verify that the Node has checked in with the Cluster:</p>

<pre><code class="mysql">$ mysql -u root -p -e "SHOW STATUS LIKE 'wsrep_cluster_size';"
Enter password:
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| wsrep_cluster_size | 2     |
+--------------------+-------+
</code></pre>

<h2>Node-3: Start and Enable MariaDB</h2>

<pre><code class="bash">$ systemctl start mariadb
$ systemctl enable mariadb
</code></pre>

<p>Verify that the Node has checked in with the Cluster:</p>

<pre><code class="mysql">$ mysql -u root -p -e "SHOW STATUS LIKE 'wsrep_cluster_size';"
Enter password:
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| wsrep_cluster_size | 3     |
+--------------------+-------+
</code></pre>

<h2>Create a Database, Table and Record:</h2>

<p>Write some data to the table, then reboot the node, in this example on node-1, then logon to node-2 check the number of nodes that&rsquo;s active in the cluster, which should be 2, then at the same time, look if the data is replicated:</p>

<h2>Node-1: Writing the Data to Our Galera Cluster</h2>

<pre><code class="mysql">MariaDB [(none)]&gt; create database test;
MariaDB [(none)]&gt; use test;
MariaDB [test]&gt;   create database test;
MariaDB [test]&gt;   create table foo (name VARCHAR(20));
MariaDB [test]&gt;   insert into foo values('ruan');
MariaDB [test]&gt;   select * from foo;
+------+
| name |
+------+
| ruan |
+------+
</code></pre>

<p>Now that our data is in our database, reboot the node, logon to node-2 and check if the data is replicated:</p>

<pre><code class="mysql">$ mysql -u root -p
MariaDB [(none)]&gt; use test;
MariaDB [test]&gt;   select * from foo;
+------+
| name |
+------+
| ruan |
+------+
</code></pre>

<p>While the one node is rebooting, check how many nodes are checked into our cluster:</p>

<pre><code class="mysql">$ mysql -u root -p -e "SHOW STATUS LIKE 'wsrep_cluster_size';"
Enter password:
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| wsrep_cluster_size | 2     |
+--------------------+-------+
</code></pre>

<p>Our data is replicated, and after waiting for a couple of seconds, we retry our command to see if the rebooted node checked into the cluster:</p>

<pre><code class="mysql">$ mysql -u root -p -e "SHOW STATUS LIKE 'wsrep_cluster_size';"
Enter password:
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| wsrep_cluster_size | 3     |
+--------------------+-------+
</code></pre>

<p>We can confirm that the node that was rebooted, has checked in with the cluster again.</p>

<h2>Firewall Rules opened while testing:</h2>

<p>TCP: <code>3306, 4567, 4568, 4444</code>
UDP: <code>4567</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a Concourse-CI Server on Ubuntu 16]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/11/07/setup-a-concourse-ci-server-on-ubuntu-16/"/>
    <updated>2017-11-07T17:55:46-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/11/07/setup-a-concourse-ci-server-on-ubuntu-16</id>
    <content type="html"><![CDATA[<p><img src="https://i.snag.gy/gzkdu9.jpg?nocache=1511644783495" alt="" /></p>

<p>Concourse is a Pipeline Based Continious Integration system written in Go</p>

<h2>Resources:</h2>

<ul>
<li><a href="https://concourse.ci/">https://concourse.ci/</a></li>
<li><a href="https://github.com/concourse/concourse">https://github.com/concourse/concourse</a></li>
<li><a href="https://concourse.ci/hello-world.html">https://concourse.ci/hello-world.html</a></li>
<li><a href="https://github.com/starkandwayne/concourse-tutorial">https://github.com/starkandwayne/concourse-tutorial</a></li>
</ul>


<h2>What is Concourse CI:</h2>

<p>Concourse CI is a Continious Integration Platform. Concourse enables you to construct pipelines with a yaml configuration that can consist out of 3 core concepts, tasks, resources, and jobs that compose them. For more information about this have a look at their <a href="https://concourse.ci/concepts.html">docs</a></p>

<h2>What will we be doing today</h2>

<p>We will setup a Concourse Server on Ubuntu 16.04 and run the traditional <code>Hello, World</code> pipeline</p>

<h2>Setup the Server:</h2>

<p>Concourse needs <code>PostgresSQL 9.3+</code></p>

<pre><code class="bash">$ apt update &amp;&amp; apt upgrade -y
$ apt install postgresql postgresql-contrib -y
$ systemctl enable postgresql
</code></pre>

<p>Create the Database and User for Concourse on Postgres:</p>

<pre><code class="bash">$ sudo -u postgres createuser concourse
$ sudo -u postgres createdb --owner=concourse atc
</code></pre>

<p>Download the Concourse and Fly Cli Binaries:</p>

<pre><code class="bash">$ wget https://github.com/concourse/concourse/releases/download/v3.6.0/concourse_linux_amd64
$ wget https://github.com/concourse/concourse/releases/download/v3.6.0/fly_linux_amd64
$ chmod +x concourse_linux_amd64 fly_linux_amd64
$ mv concourse_linux_amd64 /usr/bin/concourse
$ mv fly_linux_amd64 /usr/bin/fly
</code></pre>

<p>Create the Encryption Keys:</p>

<pre><code class="bash">$ mkdir /etc/concourse
$ ssh-keygen -t rsa -q -N '' -f /etc/concourse/tsa_host_key
$ ssh-keygen -t rsa -q -N '' -f /etc/concourse/worker_key
$ ssh-keygen -t rsa -q -N '' -f /etc/concourse/session_signing_key
$ cp /etc/concourse/worker_key.pub /etc/concourse/authorized_worker_keys
</code></pre>

<p>Concourse Web Process Configuration:</p>

<pre><code class="bash">$ cat /etc/concourse/web_environment

CONCOURSE_SESSION_SIGNING_KEY=/etc/concourse/session_signing_key
CONCOURSE_TSA_HOST_KEY=/etc/concourse/tsa_host_key
CONCOURSE_TSA_AUTHORIZED_KEYS=/etc/concourse/authorized_worker_keys
CONCOURSE_POSTGRES_SOCKET=/var/run/postgresql

CONCOURSE_BASIC_AUTH_USERNAME=admin
CONCOURSE_BASIC_AUTH_PASSWORD=secret
CONCOURSE_EXTERNAL_URL=http://10.20.30.40:8080
</code></pre>

<p>Concourse Worker Process Configuration:</p>

<pre><code class="bash">$ cat /etc/concourse/worker_environment

CONCOURSE_WORK_DIR=/var/lib/concourse
CONCOURSE_TSA_WORKER_PRIVATE_KEY=/etc/concourse/worker_key
CONCOURSE_TSA_PUBLIC_KEY=/etc/concourse/tsa_host_key.pub
CONCOURSE_TSA_HOST=127.0.0.1
</code></pre>

<p>Create a Concourse user:</p>

<pre><code class="bash">$ sudo adduser --system --group concourse
$ sudo chown -R concourse:concourse /etc/concourse
$ sudo chmod 600 /etc/concourse/*_environment
</code></pre>

<p>Create SystemD Unit Files, first for the Web Service:</p>

<pre><code class="bash">$ cat /etc/systemd/system/concourse-web.service

[Unit]
Description=Concourse CI web process (ATC and TSA)
After=postgresql.service

[Service]
User=concourse
Restart=on-failure
EnvironmentFile=/etc/concourse/web_environment
ExecStart=/usr/bin/concourse web

[Install]
WantedBy=multi-user.target
</code></pre>

<p>Then the SystemD Unit File for the Worker Service:</p>

<pre><code class="bash">$ cat /etc/systemd/system/concourse-worker.service

[Unit]
Description=Concourse CI worker process
After=concourse-web.service

[Service]
User=root
Restart=on-failure
EnvironmentFile=/etc/concourse/worker_environment
ExecStart=/usr/bin/concourse worker

[Install]
WantedBy=multi-user.target
</code></pre>

<p>Start and Enable the Services:</p>

<pre><code class="bash">$ systemctl start concourse-web concourse-worker
$ systemctl enable concourse-web concourse-worker
$ systemctl status concourse-web concourse-worker

$ systemctl is-active concourse-worker concourse-web
active
active
</code></pre>

<p>The listening ports should more or less look like the following:</p>

<pre><code class="bash">$ netstat -tulpn

Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 127.0.0.1:7777          0.0.0.0:*               LISTEN      4530/concourse
tcp        0      0 127.0.0.1:7788          0.0.0.0:*               LISTEN      4530/concourse
tcp        0      0 127.0.0.1:8079          0.0.0.0:*               LISTEN      4525/concourse
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1283/sshd
tcp        0      0 127.0.0.1:5432          0.0.0.0:*               LISTEN      4047/postgres
tcp6       0      0 :::36159                :::*                    LISTEN      4525/concourse
tcp6       0      0 :::46829                :::*                    LISTEN      4525/concourse
tcp6       0      0 :::2222                 :::*                    LISTEN      4525/concourse
tcp6       0      0 :::8080                 :::*                    LISTEN      4525/concourse
tcp6       0      0 :::22                   :::*                    LISTEN      1283/sshd
udp        0      0 0.0.0.0:68              0.0.0.0:*                           918/dhclient
udp        0      0 0.0.0.0:42165           0.0.0.0:*                           4530/concourse
</code></pre>

<h2>Client Side:</h2>

<p>I will be using a the Fly cli from a Mac, so first we need to download the fly-cli for Mac:</p>

<pre><code class="bash">$ wget https://github.com/concourse/concourse/releases/download/v3.6.0/fly_darwin_amd64
$ chmod +x fly_darwin_amd64
$ alias fly='./fly_darwin_amd64'
</code></pre>

<p>Next, we need to setup our Concourse Target by Authenticating against our Concourse Endpoint, lets setup our target with the name <code>ci</code>:</p>

<pre><code class="bash">$ fly -t ci login -c http://10.20.30.40:8080
logging in to team 'main'

username: admin
password:

target saved
</code></pre>

<p>Lets list our targets:</p>

<pre><code class="bash">$ fly targets
name  url                        team  expiry
ci    http://10.20.30.40:8080    main  Wed, 08 Nov 2017 15:32:59 UTC
</code></pre>

<p>Listing Registered Workers:</p>

<pre><code class="bash">$ fly -t ci workers
name              containers  platform  tags  team  state    version
ip-172-31-12-134  0           linux     none  none  running  1.2
</code></pre>

<p>Listing Active Containers:</p>

<pre><code class="bash">$ fly -t ci containers
handle                                worker            pipeline     job            build #  build id  type   name                  attempt
</code></pre>

<h2>Hello World Pipeline:</h2>

<p>Let&rsquo;s create a basic pipeline, that will print out <code>Hello, World!</code>:</p>

<p>Our <code>hello-world.yml</code></p>

<pre><code class="yml">jobs:
- name: my-job
  plan:
  - task: say-hello
    config:
      platform: linux
      image_resource:
        type: docker-image
        source:
          repository: alpine
          tag: edge
      run:
        path: /bin/sh
        args:
        - -c
        - |
          echo "============="
          echo "Hello, World!"
          echo "============="
</code></pre>

<p>Applying the configuration to our pipeline:</p>

<pre><code class="bash">$ fly -t ci set-pipeline -p yeeehaa -c hello-world.yml
jobs:
  job my-job has been added:
    name: my-job
    plan:
    - task: say-hello
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: alpine
            tag: edge
        run:
          path: /bin/sh
          args:
          - -c
          - |
            echo "============="
            echo "Hello, World!"
            echo "============="

apply configuration? [yN]: y
pipeline created!
you can view your pipeline here: http://10.20.30.40:8080/teams/main/pipelines/yeeehaa

the pipeline is currently paused. to unpause, either:
  - run the unpause-pipeline command
  - click play next to the pipeline in the web ui
</code></pre>

<p>We can browse to the WebUI to unpause the pipeline, but since I like to do everything on cli as far as possible, I will unpause the pipeline via cli:</p>

<pre><code class="bash">$ fly -t ci unpause-pipeline -p yeeehaa
unpaused 'yeeehaa'
</code></pre>

<p>Now our Pipeline is unpaused, but since we did not specify any triggers, we need to manually trigger the pipeline to run, you can either via the WebUI, select your pipeline which in this case will be named <code>yeeehaa</code> and then select the job, which will be <code>my-job</code> then hit the <code>+</code> sign, which will trigger the pipeline.</p>

<p>I will be using the cli:</p>

<pre><code class="bash">$ fly -t ci trigger-job --job yeeehaa/my-job
started yeeehaa/my-job #1
</code></pre>

<p>Via the WebUI on <code>http://10.20.30.40:8080/teams/main/pipelines/yeeehaa/jobs/my-job/builds/1</code> you should see the <code>Hello, World!</code> output, or via the cli, we also have the option to see the output, so let&rsquo;s trigger it again, but this time passing the <code>--watch</code> flag:</p>

<pre><code class="bash">$ fly -t ci trigger-job --job yeeehaa/my-job --watch
started yeeehaa/my-job #2

initializing
running /bin/sh -c echo "============="
echo "Hello, World!"
echo "============="

=============
Hello, World!
=============
succeeded
</code></pre>

<p>Listing our Workers and Containers again:</p>

<pre><code class="bash">$ fly -t ci workers
name              containers  platform  tags  team  state    version
ip-172-31-12-134  2           linux     none  none  running  1.2

$ fly -t ci containers
handle                                worker            pipeline     job         build #  build id  type   name           attempt
36982955-54fd-4c1b-57b8-216486c58db8  ip-172-31-12-134  yeeehaa      my-job      2        729       task   say-hello      n/a
</code></pre>
]]></content>
  </entry>
  
</feed>
