<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Ubuntu | Ruan Bekker's Blog]]></title>
  <link href="http://blog.ruanbekker.com/blog/categories/ubuntu/atom.xml" rel="self"/>
  <link href="http://blog.ruanbekker.com/"/>
  <updated>2018-03-29T20:17:43-04:00</updated>
  <id>http://blog.ruanbekker.com/</id>
  <author>
    <name><![CDATA[Ruan]]></name>
    <email><![CDATA[ruan@ruanbekker.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Setup Payara Application Server on Ubuntu 16.04]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/03/29/setup-payara-application-server-on-ubuntu-16-dot-04/"/>
    <updated>2018-03-29T19:57:40-04:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/03/29/setup-payara-application-server-on-ubuntu-16-dot-04</id>
    <content type="html"><![CDATA[<p><img src="https://i.snag.gy/CJMlTj.jpg" alt="" /></p>

<p>Today we will setup Payara 5 on Ubuntu 16.04</p>

<h2>About:</h2>

<p>Payara is an Open Source Java Application Server.</p>

<h2>Pre-Requirements:</h2>

<p>Update and Install Java 8:</p>

<pre><code class="bash">$ apt update &amp;&amp; apt upgrade -y
$ apt-get install wget curl unzip software-properties-common python-software-properties -y
$ add-apt-repository ppa:webupd8team/java
$ apt-get update
$ apt-get install oracle-java8-installer -y
$ source /etc/profile.d/jdk.sh
</code></pre>

<h2>Install Payara:</h2>

<p>Download and Install Payara 5:</p>

<pre><code class="bash">$ cd /usr/local
$ wget --content-disposition 'https://info.payara.fish/cs/c/?cta_guid=b9609f35-f630-492f-b3c0-238fc55f489b&amp;placement_guid=7cca6202-06a3-4c29-aee0-ca58af60528a&amp;portal_id=334594&amp;redirect_url=APefjpGt1aFvHUflpzz7Lec8jDz7CbeIIHZmgORmDSpteTCT2XjiMvjEzeY8yte3kiHi7Ph9mWDB7qUDEr96P0JS8Ev2ZFqahif2huSBfQV6lt4S6YUQpzPMrpHgf_n4VPV62NjKe8vLZBLnYkUALyR2mkrU3vWe7ME9XjHJqYPsHtxkHn-W7bYPFgY2LjEzKIYrdUsCviMgGrUh_LIbLxCESBa0N90vzaWKjK5EwZT021VaPP0jgfgvt0gF2UdtBQGcsTHrAlrb&amp;hsutk=c279766888b67917a591ec4e209cb29a&amp;canon=https%3A%2F%2Fwww.payara.fish%2Fall_downloads&amp;click=5bad781c-f4f5-422d-ba2b-5e0c2bff7098&amp;utm_referrer=https%3A%2F%2Fwww.google.co.za%2F&amp;__hstc=229474563.c279766888b67917a591ec4e209cb29a.1519832301251.1521408251653.1521485598794.4&amp;__hssc=229474563.7.1521485598794&amp;__hsfp=2442083907'

$ unzip payara-5.181.zip
$ mv payara5 payara
$ rm -rf payara-5.181.zip
</code></pre>

<h2>Permissions:</h2>

<p>Create the Payara user and Grant Permissions:</p>

<pre><code class="bash">$ echo 'export PATH=/usr/local/payara/glassfish/bin:$PATH' &gt; /etc/profile.d/payara.sh
$ addgroup --system payara
$ adduser --system --shell /bin/bash --ingroup payara payara
$ echo 'payara soft nofile 32768' &gt;&gt; /etc/security/limits.conf
$ echo 'payara hard nofile 65536' &gt;&gt; /etc/security/limits.conf
$ chown -R payara:payara /usr/local/payara
</code></pre>

<h2>Setup the Payara Domain:</h2>

<p>Switch to the Payara user, delete the default domain and start the production domain. It is useful to configure the JVM Options under the domains config directory according to your servers resources.</p>

<pre><code class="bash">$ su - payara

$ asadmin delete-domain domain1
$ asadmin change-admin-password --domain_name production # default blank pass for admin
$ asadmin --port 4848 enable-secure-admin production

$ asadmin start-domain production
$ asadmin stop-domain production

$ exit
</code></pre>

<h2>SystemD Unit File:</h2>

<p>Create the SystemD Unit File to be able to manage the state of the Payara Server via SystemD:</p>

<pre><code class="bash">$ cat &gt; /etc/systemd/system/payara.service &lt;&lt; EOF
[Unit]
Description=Payara Server
After=network.target remote-fs.target

[Service]
User=payara
WorkingDirectory=/usr/local/payara/glassfish
Environment=PATH=/usr/local/payara/glassfish/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin
Type=oneshot
RemainAfterExit=yes
ExecStart=/usr/local/payara/glassfish/bin/asadmin start-domain production
ExecReload=/usr/local/payara/glassfish/bin/asadmin restart-domain production
ExecStop=/usr/local/payara/glassfish/bin/asadmin stop-domain production
TimeoutStartSec=300
TimeoutStopSec=30

[Install]
WantedBy = multi-user.target
EOF
</code></pre>

<p>Reload the systemd daemon:</p>

<pre><code class="bash">$ systemctl daemon-reload
</code></pre>

<p>Start the Payara Service:</p>

<pre><code class="bash">$ systemctl enable payara
$ systemctl start payara
</code></pre>

<p>Verify that port 4848, 8080 and 8181 is running:</p>

<pre><code class="bash">$ netstat -tulpn | grep java
tcp        0      0 :::8080                     :::*                        LISTEN      24542/java
tcp        0      0 :::4848                     :::*                        LISTEN      24542/java
tcp        0      0 :::8181                     :::*                        LISTEN      24542/java
...
</code></pre>

<h2>Access Payara Admin UI:</h2>

<p>Access the Payara DAS via <code>https://ip-of-payara-server:4848</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a KVM Hypervisor on Ubuntu to Host Virtual Machines]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/02/20/setup-a-kvm-hypervisor-on-ubuntu-to-host-virtual-machines/"/>
    <updated>2018-02-20T06:21:56-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/02/20/setup-a-kvm-hypervisor-on-ubuntu-to-host-virtual-machines</id>
    <content type="html"><![CDATA[<p>Today we will setup a KVM (Kernel Virtual Machine) Hypervisor, where we can host Virtual Machines. In order to do so, your host needs to Support Hardware Virtualization.</p>

<h2>What we will be doing today:</h2>

<ul>
<li>Check if your host supports Hardware Virtualization</li>
<li>Setup the KVM Hypervisor</li>
<li>Setup a Alpine VM</li>
</ul>


<h2>Check for Hardware Virtualization Support:</h2>

<p>We will install the package required to do the check:</p>

<pre><code class="bash">$ sudo apt update &amp;&amp; sudo apt install cpu-checker -y
</code></pre>

<p>Once that is installed, run <code>kvm-ok</code> and if its supported, your output should look something like this:</p>

<pre><code class="bash">$ kvm-ok
INFO: /dev/kvm exists
KVM acceleration can be used
</code></pre>

<h2>Installing KVM</h2>

<p>Update your System and get the Packages required to Setup KVM:</p>

<pre><code class="bash">$ sudo apt update &amp;&amp; sudo apt upgrade -y
$ apt install bridge-utils qemu-kvm libvirt-bin virtinst -y
</code></pre>

<p>Add your user to the libvirtd group:</p>

<pre><code class="bash">$ sudo usermod -G libvirtd $USER
</code></pre>

<p>Check that the libvirtd service is running:</p>

<pre><code class="bash">$ sudo systemctl is-active libvirtd
active
</code></pre>

<p>You will also find that there is a new interface configured called <code>virbr0</code> in my case.</p>

<h2>Provision the Alpine VM and Setup OpenSSH:</h2>

<p>Get the ISO:</p>

<ul>
<li><a href="https://alpinelinux.org/downloads/">https://alpinelinux.org/downloads/</a></li>
</ul>


<pre><code class="bash">$ wget http://dl-cdn.alpinelinux.org/alpine/v3.7/releases/x86_64/alpine-virt-3.7.0-x86_64.iso
</code></pre>

<p>Provision the VM:</p>

<pre><code class="bash">$ virt-install \
--name alpine1 \
--ram 256 \
--disk path=/var/lib/libvirt/images/alpine1.img,size=8 \
--vcpus 1 \
--os-type linux \
--os-variant generic \
--network bridge:virbr0,model=virtio \
--graphics none \
--console pty,target_type=serial \
--cdrom ./alpine-virt-3.7.0-x86_64.iso 
</code></pre>

<p>After this, you will be dropped into the console:</p>

<pre><code class="bash">Starting install...
Allocating 'alpine1.img'                                                                                                           |   8 GB  00:00:01
Creating domain...                                                                                                                 |    0 B  00:00:00
Connected to domain alpine1
Escape character is ^]

ISOLINUX 6.04 6.04-pre1  Copyright (C) 1994-2015 H. Peter Anvin et al
boot:

   OpenRC 0.24.1.a941ee4a0b is starting up Linux 4.9.65-1-virthardened (x86_64)

Welcome to Alpine Linux 3.7
Kernel 4.9.65-1-virthardened on an x86_64 (/dev/ttyS0)

localhost login:
</code></pre>

<p>Login with the <code>root</code> user and no password, then setup the VM by running <code>setup-alpine</code>:</p>

<pre><code class="bash">localhost login: root
Welcome to Alpine!

localhost:~# setup-alpine
</code></pre>

<p>After completing the prompts reboot the VM by running <code>reboot</code>, then you will be dropped out of the console. Check the status of the reboot:</p>

<pre><code class="bash">$ virsh list
 Id    Name                           State
----------------------------------------------------
 2     alpine1                        running
</code></pre>

<p>As we can see our guest is running, lets console to our guest, provide the root user and password that you provided during the setup phase:</p>

<pre><code class="bash">$ virsh console 2
Connected to domain alpine1
Escape character is ^]

alpine1 login: root
Password:
Welcome to Alpine!
</code></pre>

<p>Setup OpenSSH so that we can SSH to our guest over the network:</p>

<pre><code class="bash">$ apk update
$ apk add openssh
</code></pre>

<p>Configure SSH to accept Root Passwords, this is not advisable for production environments, but for testing this is okay. For Production servers, we will rather look at Key Based Authentication etc.</p>

<pre><code class="bash">$ sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/g' /etc/ssh/sshd_config
$ /etc/init.d/sshd restart
</code></pre>

<p>Get the IP Address:</p>

<pre><code>$ ifconfig
eth0      Link encap:Ethernet  HWaddr 52:54:00:D0:48:0C
          inet addr:192.168.122.176  Bcast:192.168.122.255  Mask:255.255.255.0
          inet6 addr: fe80::5054:ff:fed0:480c/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:55 errors:0 dropped:28 overruns:0 frame:0
          TX packets:34 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:4545 (4.4 KiB)  TX bytes:3345 (3.2 KiB)
</code></pre>

<p>Exit the guest by running <code>exit</code> and <code>Ctrl + ]</code> to exit the console session.</p>

<p>Now SSH to your Alpine VM:</p>

<pre><code class="bash">$ ssh root@192.168.122.176
root@192.168.122.176's password:
Welcome to Alpine!
</code></pre>

<h2>Some Useful Commands:</h2>

<p>List Running VMs:</p>

<pre><code>$ virsh list
 Id    Name                           State
----------------------------------------------------
 3     alpine1                        running
</code></pre>

<p>Shutdown a VM:</p>

<pre><code class="bash">$ virsh shutdown alpine1
Domain alpine1 is being shutdown
</code></pre>

<p>List all VMs:</p>

<pre><code class="bash">$ virsh list --all
 Id    Name                           State
----------------------------------------------------
 -     alpine1                        shut off
</code></pre>

<p>Delete a VM:</p>

<pre><code class="bash">$ virsh shutdown alpine1 #or to force shutdown:
$ virsh destroy alpine1
$ virsh undefine alpine1
</code></pre>

<p>Any future KVM posts will be tagged under <a href="http://blog.ruanbekker.com/blog/categories/kvm?source_site=blog.ruanbekker.com?source_category=kvm">KVM</a> and Alpine posts will be available under the <a href="http://blog.ruanbekker.com/blog/categories/alpine?source_site=blog.ruanbekker.com?source_category=kvm">Alpine</a> tag.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a NFS Server on Ubuntu]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/02/11/setup-a-nfs-server-on-ubuntu/"/>
    <updated>2018-02-11T17:26:56-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/02/11/setup-a-nfs-server-on-ubuntu</id>
    <content type="html"><![CDATA[<p><img src="https://i.snag.gy/3sUALo.jpg" alt="" /></p>

<p>Quick post on how to setup a NFS Server on Ubuntu and how to setup the client to interact with the NFS Server.</p>

<h2>Setup the Dependencies:</h2>

<pre><code class="bash">$ apt update &amp;&amp; sudo apt upgrade -y
$ sudo apt-get install nfs-kernel-server nfs-common -y
</code></pre>

<p>Create the Directory for NFS and set permissions:</p>

<pre><code class="bash">mkdir /vol
chown -R nobody:nogroup /vol
</code></pre>

<h2>Allow the Clients:</h2>

<p>We need to set in the <code>exports</code> file, the clients we would like to allow:</p>

<ul>
<li><code>rw</code>: Allows Client R/W Access to the Volume.</li>
<li><code>sync</code>: This option forces NFS to write changes to disk before replying. More stable and Consistent. Note, it does reduce the speed of file operations.</li>
<li><code>no_subtree_check</code>: This prevents subtree checking, which is a process where the host must check whether the file is actually still available in the exported tree for every request. This can cause many problems when a file is renamed while the client has it opened. In almost all cases, it is better to disable subtree checking.</li>
</ul>


<pre><code class="bash">$ echo '/vol 10.8.133.83(rw,sync,no_subtree_check) 10.8.166.19(rw,sync,no_subtree_check) 10.8.142.195(rw,sync,no_subtree_check)' &gt;&gt; /etc/exports
</code></pre>

<h2>Start the NFS Server:</h2>

<p>Restart the service and enable the service on boot:</p>

<pre><code class="bash">$ sudo systemctl restart nfs-kernel-server
$ sudo systemctl enable nfs-kernel-server
</code></pre>

<h2>Client Side:</h2>

<p>We will mount the NFS Volume to our Clients <code>/mnt</code> partition.</p>

<p>Install the dependencies:</p>

<pre><code class="bash">$ sudo apt-get install nfs-common -y
</code></pre>

<p>Test if we can mount the volume, then unmount it, as we will set the config in our <code>fstab</code>:</p>

<pre><code class="bash">$ sudo mount 10.8.133.83:/vol /mnt
$ sudo umount /mnt
$ df -h
</code></pre>

<p>Set the config in your <code>fstab</code>, then mount it from there:</p>

<pre><code class="bash">$ sudo bash -c "echo '10.8.133.83:/vol /mnt nfs auto,nofail,noatime,nolock,intr,tcp,actimeo=1800 0 0' &gt;&gt; /etc/fstab"
$ sudo mount -a
$ df -h
</code></pre>

<p>Now you shoule be able to write to your NFS Volume from your client.</p>

<p>Sources:
- <a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-16-04">1</a> <a href="https://gist.github.com/deviantony/557984d62e867e6f505577b207db6ffc%">2</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a Site to Site IPsec VPN With Strongswan and PreShared Key Authentication]]></title>
    <link href="http://blog.ruanbekker.com/blog/2018/02/11/setup-a-site-to-site-ipsec-vpn-with-strongswan-and-preshared-key-authentication/"/>
    <updated>2018-02-11T16:09:37-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2018/02/11/setup-a-site-to-site-ipsec-vpn-with-strongswan-and-preshared-key-authentication</id>
    <content type="html"><![CDATA[<p><img src="https://i.snag.gy/sWn8zc.jpg" alt="" /></p>

<p>Today we will setup a Site to Site ipsec VPN with Strongswan, which will be configured with PreShared Key Authentication.</p>

<p>After our tunnels are established, we will be able to reach the private ips over the vpn tunnels.</p>

<h2>Get the Dependencies:</h2>

<p>Update your repository indexes and install strongswan:</p>

<pre><code class="bash">$ apt update &amp;&amp; sudo apt upgrade -y
$ apt install strongswan -y
</code></pre>

<p>Set the following kernel parameters:</p>

<pre><code class="bash">$ cat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOF
echo net.ipv4.ip_forward = 1 
net.ipv4.conf.all.accept_redirects = 0 
net.ipv4.conf.all.send_redirects = 0
EOF

$ sysctl -p /etc/sysctl.conf
</code></pre>

<h2>Generate Preshared Key:</h2>

<p>We will need a preshared key that both servers will use:</p>

<pre><code>$ openssl rand -base64 64
87zRQqylaoeF5I8o4lRhwvmUzf+pYdDpsCOlesIeFA/2xrtxKXJTbCPZgqplnXgPX5uprL+aRgxD8ua7MmdWaQ
</code></pre>

<h2>Details of our 2 Sites:</h2>

<p>Site A:</p>

<pre><code class="bash">Location: Paris, France
External IP: 51.15.139.201
Internal IP: 10.10.27.1/24
</code></pre>

<p>Site B:</p>

<pre><code class="bash">Location: Amsterdam, Netherlands
External IP: 51.15.44.48
Internal IP: 10.9.141.1/24
</code></pre>

<h2>Configure Site A:</h2>

<p>We will setup our VPN Gateway in Site A (Paris), first to setup the <code>/etc/ipsec.secrets</code> file:</p>

<pre><code class="bash">$ cat /etc/ipsec.secrets
# source      destination
51.15.139.201 51.15.44.48 : PSK "87zRQqylaoeF5I8o4lRhwvmUzf+pYdDpsCOlesIeFA/2xrtxKXJTbCPZgqplnXgPX5uprL+aRgxD8ua7MmdWaQ"
</code></pre>

<p>Now to setup our VPN configuration in <code>/etc/ipsec.conf</code>:</p>

<pre><code>cat /etc/ipsec.conf
# basic configuration
config setup
        charondebug="all"
        uniqueids=yes
        strictcrlpolicy=no

# connection to amsterdam datacenter
conn paris-to-amsterdam
    authby=secret
    left=%defaultroute
    leftid=51.15.139.201
    leftsubnet=10.10.27.1/24
    right=51.15.44.48
    rightsubnet=10.9.141.1/24
    ike=aes256-sha2_256-modp1024!
    esp=aes256-sha2_256!
    keyingtries=0
    ikelifetime=1h
    lifetime=8h
    dpddelay=30
    dpdtimeout=120
    dpdaction=restart
    auto=start
</code></pre>

<p>Firewall Rules:</p>

<pre><code class="bash">$ sudo iptables -t nat -A POSTROUTING -s 10.9.141.0/24 -d 10.10.27.0/24 -j MASQUERADE
</code></pre>

<h2>Configure Site B:</h2>

<p>We will setup our VPN Gateway in Site B (Amsterdam), setup the <code>/etc/ipsec.secrets</code> file:</p>

<pre><code class="bash">$ cat /etc/ipsec.secrets
51.15.44.48 51.15.139.201 : PSK "87zRQqylaoeF5I8o4lRhwvmUzf+pYdDpsCOlesIeFA/2xrtxKXJTbCPZgqplnXgPX5uprL+aRgxD8ua7MmdWaQ"
</code></pre>

<p>Next to setup our VPN Configuration:</p>

<pre><code>cat /etc/ipsec.conf
# basic configuration
config setup
        charondebug="all"
        uniqueids=yes
        strictcrlpolicy=no

# connection to paris datacenter
conn amsterdam-to-paris
    authby=secret
    left=%defaultroute
    leftid=51.15.44.48
    leftsubnet=10.9.141.1/24
    right=51.15.139.201
    rightsubnet=10.10.27.1/24
    ike=aes256-sha2_256-modp1024!
    esp=aes256-sha2_256!
    keyingtries=0
    ikelifetime=1h
    lifetime=8h
    dpddelay=30
    dpdtimeout=120
    dpdaction=restart
    auto=start
</code></pre>

<p>Firewall Rules:</p>

<pre><code class="bash">$ sudo iptables -t nat -A POSTROUTING -s 10.10.27.0/24 -d 10.9.41.0/24 -J MASQUERADE
</code></pre>

<h2>Start the VPN:</h2>

<p>Start the VPN on both ends:</p>

<pre><code class="bash">$ sudo ipsec restart
</code></pre>

<p>Get the status of the tunnel, in this case we are logged onto our Site A (Paris) Server:</p>

<pre><code>$ sudo ipsec status
Security Associations (1 up, 0 connecting):
paris-to-amsterdam[2]: ESTABLISHED 14 minutes ago, 10.10.27.161[51.15.139.201]...51.15.44.48[51.15.44.48]
paris-to-amsterdam{1}:  INSTALLED, TUNNEL, reqid 1, ESP in UDP SPIs: c8c868ee_i c9d58dbd_o
paris-to-amsterdam{1}:   10.10.27.1/24 === 10.9.141.1/24
</code></pre>

<p>Test if we can see the remote end on its private range:</p>

<pre><code class="bash">$ ping 10.9.141.97
PING 10.9.141.97 (10.9.141.97) 56(84) bytes of data.
64 bytes from 10.9.141.97: icmp_seq=1 ttl=64 time=14.6 ms
</code></pre>

<p>Set the service to start on boot:</p>

<pre><code class="bash">$ sudo systemctl enable strongswan
</code></pre>

<p>Then your VPN should be setup correctly.</p>

<h2>Other useful commands:</h2>

<p>Start / Stop / Status:</p>

<pre><code class="bash">$ sudo ipsec up connection-name
$ sudo ipsec down connection-name

$ sudo ipsec restart
$ sudo ipsec status
$ sudo ipsec statusall
</code></pre>

<p>Get the Policies and States of the IPsec Tunnel:</p>

<pre><code class="bash">$ sudo ip xfrm state
$ sudo ip xfrm policy
</code></pre>

<p>Reload the secrets, while the service is running:</p>

<pre><code class="bash">$ sudo ipsec rereadsecrets
</code></pre>

<p>Check if traffic flows through the tunnel:</p>

<pre><code class="bash">$ sudo tcpdump esp
</code></pre>

<h2>Adding more connections to your config:</h2>

<p>If you have to add another site to your config, the example of the <code>ipsec.secrets</code> will look like:</p>

<pre><code class="bash">$ cat /etc/ipsec.secrets
51.15.139.201 51.15.44.48 : PSK "87zRQqylaoeF5I8o4lRhwvmUzf+pYdDpsCOlesIeFA/2xrtxKXJTbCPZgqplnXgPX5uprL+aRgxD8ua7MmdWaQ"
51.15.139.201 51.15.87.41  : PSK "87zRQqylaoeF5I8o4lRhwvmUzf+pYdDpsCOlesIeFA/2xrtxKXJTbCPZgqplnXgPX5uprL+aRgxD8ua7MmdWaQ"
</code></pre>

<p>And the <code>ipsec.conf</code>:</p>

<pre><code class="bash">cat /etc/ipsec.conf
# basic configuration
config setup
        charondebug="all"
        uniqueids=yes
        strictcrlpolicy=no

# connection to amsterdam datacenter
conn paris-to-amsterdam
    authby=secret
    left=%defaultroute
    leftid=51.15.139.201
    leftsubnet=10.10.27.161/32
    right=51.15.44.48
    rightsubnet=10.9.141.97/32
    ike=aes256-sha2_256-modp1024!
    esp=aes256-sha2_256!
    keyingtries=0
    ikelifetime=1h
    lifetime=8h
    dpddelay=30
    dpdtimeout=120
    dpdaction=restart
    auto=start

# connection to frankfurt datacenter
conn paris-to-frankfurt
    authby=secret
    left=%defaultroute
    leftid=51.15.139.201
    leftsubnet=10.10.27.1/24
    right=51.15.87.41
    rightsubnet=10.9.137.1/24
    ike=aes256-sha2_256-modp1024!
    esp=aes256-sha2_256!
    keyingtries=0
    ikelifetime=1h
    lifetime=8h
    dpddelay=30
    dpdtimeout=120
    dpdaction=restart
    auto=start
</code></pre>

<p>Just remember to configure the config on the Frankfurt VPN Gateway, and the example of the status output will look like the following:</p>

<pre><code class="bash">$ sudo ipsec status
Security Associations (2 up, 0 connecting):
paris-to-frankfurt[2]: ESTABLISHED 102 seconds ago, 10.10.27.161[51.15.139.201]...51.15.87.41[51.15.87.41]
paris-to-frankfurt{1}:  INSTALLED, TUNNEL, reqid 2, ESP in UDP SPIs: cbc62a1f_i c95b8f78_o
paris-to-frankfurt{1}:   10.10.27.1/24 === 10.9.137.1/24
paris-to-amsterdam[1]: ESTABLISHED 102 seconds ago, 10.10.27.161[51.15.139.201]...51.15.44.48[51.15.44.48]
paris-to-amsterdam{2}:  INSTALLED, TUNNEL, reqid 1, ESP in UDP SPIs: c7b36756_i cc54053c_o
paris-to-amsterdam{2}:   10.10.27.1/24 === 10.9.141.1/24
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup a 3 Node Kubernetes Cluster on Ubuntu]]></title>
    <link href="http://blog.ruanbekker.com/blog/2017/12/11/setup-a-3-node-kubernetes-cluster-on-ubuntu/"/>
    <updated>2017-12-11T09:31:47-05:00</updated>
    <id>http://blog.ruanbekker.com/blog/2017/12/11/setup-a-3-node-kubernetes-cluster-on-ubuntu</id>
    <content type="html"><![CDATA[<p><img src="https://kumorilabs.com/img/blog/kubernetes-logo.png" alt="" /></p>

<p>Setup a 3 Node Kubernetes Cluster on Ubuntu 16.04</p>

<h2>What is Kubernetes?</h2>

<p>As referenced from their <a href="https://kubernetes.io/">website</a>:</p>

<ul>
<li>&ldquo;Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.&rdquo;</li>
</ul>


<h2>Our Setup:</h2>

<p>For this setup I will be using 3 AWS EC2 Instances with Ubuntu 16.04. One node will act as the master node, and the other 2 nodes, will act as nodes, previously named minions.</p>

<p>We will deploy Kubernetes on all 3 nodes, the master will be the node where we will initialize our cluster, deploy our weave network, applications and we will execute the join command on the worker nodes to join the master to form the cluster.</p>

<h2>Deploy Kubernetes: Master</h2>

<p>The following commands will be used to install Kubernetes, it will be executed with root permissions:</p>

<pre><code class="bash">$ apt update &amp;&amp; sudo apt upgrade -y
$ sudo apt install docker.io apt-transport-https -qy
$ sudo apt update
$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
$ sudo su -c 'echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" &gt; /etc/apt/sources.list.d/app' root
$ apt update
$ sudo apt install kubelet kubeadm kubernetes-cni -y
</code></pre>

<p>Now we would like to set up the master by initializing the cluster:</p>

<pre><code class="bash">$ sudo kubeadm init --kubernetes-version stable-1.8
</code></pre>

<p>The output will provide you with instructions to setup the configurations for the master node, and provide you with a join token for your worker nodes, remember to make not of this token string, as we will need it later for our worker nodes. As your normal user, run the following to setup the config:</p>

<p>Remember to not run this as root, and as the normal user:</p>

<pre><code class="bash">$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre>

<p>Now we need to deploy a network for our pods:</p>

<pre><code class="bash">$ kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
</code></pre>

<p>Lets confirm if all our resources are in its desired state, a small snippet of the output will look like the one below:</p>

<pre><code class="bash">$ kubectl get all -n kube-system

...
NAME                                          READY     STATUS    RESTARTS   AGE
po/etcd-ip-172-31-40-211                      1/1       Running   0          6h
po/kube-apiserver-ip-172-31-40-211            1/1       Running   0          6h
</code></pre>

<p>Once all of the resources are in its desired state, we can head along to our worker nodes, to join them to the cluster</p>

<h2>Deploy Kubernetes: Worker Nodes</h2>

<p>As I have 2 worker nodes, we will need to deploy the following on both of our worker nodes, first to deploy Kubernetes on our nodes with root permission:</p>

<pre><code class="bash">$ apt update &amp;&amp; sudo apt upgrade -y
$ sudo apt install docker.io apt-transport-https -qy
$ sudo apt update
$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
$ sudo su -c 'echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" &gt; /etc/apt/sources.list.d/app' root
$ apt update
$ sudo apt install kubelet kubeadm kubernetes-cni -y
</code></pre>

<p>Once Kubernetes is installed, join the Master node by executing the join command:</p>

<pre><code class="bash">$ sudo kubeadm join --token 49abf7.247d663db97f8504 172.31.40.211:6443 --discovery-token-ca-cert-hash sha256:3a3b301cfbac0995c69a0115989ea384230470d6836ae0e13e71dbdf15ffbb48
</code></pre>

<p>Do the 2 steps on the other node, then head back to the master node.</p>

<h2>Verifying if All Nodes are Checked In</h2>

<p>To verify if all nodes are available and reachable in the cluster:</p>

<pre><code class="bash">$ kubectl get nodes
NAME               STATUS    ROLES     AGE       VERSION
ip-172-31-36-68    Ready     &lt;none&gt;    6h        v1.8.5
ip-172-31-40-211   Ready     master    6h        v1.8.5
ip-172-31-44-80    Ready     &lt;none&gt;    6h        v1.8.5
</code></pre>

<h2>Deploy Services to Kubernetes:</h2>

<p>Kubernetes has Awesome Examples on their <a href="https://github.com/kubernetes/kubernetes/tree/master/examples">Github Repository</a>.</p>

<p>Since the awesomeness of <a href="https://github.com/openfaas">OpenFaas</a>, I will deploy OpenFaas on Kubernetes:</p>

<pre><code class="bash">$ git clone https://github.com/openfaas/faas-netes
$ cd faas-netes
$ kubectl apply -f faas.yml,monitoring.yml,rbac.yml
</code></pre>

<p>Give it about a minute or so, then you should see the pods running in their desired state:</p>

<pre><code class="bash">$ kubectl get pods
NAME                           READY     STATUS    RESTARTS   AGE
alertmanager-77b4b476b-zxtcz   1/1       Running   0          4h
crypto-7d8b7f999c-7l85k        1/1       Running   0          1h
faas-netesd-64fb9b4dfb-hc8gh   1/1       Running   0          4h
gateway-69c9d949f-q57zh        1/1       Running   0          4h
prometheus-7fbfd8bfb8-d4cft    1/1       Running   0          4h
</code></pre>

<p>When we have the desired state, head over to the OpenFaas Gateway WebUI: <code>http://master-public-ip:31112/ui/</code>, select &ldquo;Deploy New Function&rdquo;, you can use your own function or select one from the store.</p>

<p>I am going to use Figlet from the store, once the pod has been deployed, select the function, enter any text into the request body and select invoke. I have used my name and surname, and turns out into:</p>

<pre><code class="bash"> ____                      ____       _    _             
|  _ \ _   _  __ _ _ __   | __ )  ___| | _| | _____ _ __ 
| |_) | | | |/ _` | '_ \  |  _ \ / _ \ |/ / |/ / _ \ '__|
|  _ &lt;| |_| | (_| | | | | | |_) |  __/   &lt;|   &lt;  __/ |   
|_| \_\\__,_|\__,_|_| |_| |____/ \___|_|\_\_|\_\___|_|   
</code></pre>

<h2>Resources:</h2>

<ul>
<li><a href="https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/">Kubernetes Overview</a></li>
<li><a href="https://kubernetes.io/docs/concepts/">Kubernetes Concepts</a></li>
<li><a href="https://blog.alexellis.io/tag/kubernetes/">Kubernetes Blogs</a></li>
<li><a href="https://blog.alexellis.io/tag/openfaas/">OpenFaas Blogs</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
